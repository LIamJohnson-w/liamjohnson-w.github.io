<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>TQ System | All wisdom begins with memory.</title><meta name="author" content="李俊泽"><meta name="copyright" content="李俊泽"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Day011、什么是知识图谱？概念：知识图谱是以图的形式来表示实体和实体之间关系的语义网络。  节点：实体、概念 边：关系（外部）、属性（内部）    类型有两种：  实体-关系-实体【通常的说法！！】 实体-属性-属性值  2、项目的技术架构图是怎样的？  数据获取 业务数据：比较规范，一般可以直"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://liamjohnson-w.github.io/2025/09/14/TrafficDefenceDetection/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'TQ System',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-09-27 14:25:26'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css"> <script src="/live2d-widget/autoload.js"></script><script src="/live2d-widget/autoload.js"> </script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/tx.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">239</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">58</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-gamepad"></i><span> Games</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/mikutap/"><i class="fa-fw fa fa-music"></i><span> MikuTap 初音未来</span></a></li><li><a class="site-page child" href="/starbattle/"><i class="fa-fw fa fa-space-shuttle"></i><span> StartBattle 星际大战</span></a></li><li><a class="site-page child" href="/2048/"><i class="fa-fw fa fa-flag"></i><span> 2048 经典游戏</span></a></li><li><a class="site-page child" href="/battlecity/"><i class="fa-fw fa fa-arrow-circle-left"></i><span> BattleCity 坦克大战</span></a></li><li><a class="site-page child" href="/pacman/"><i class="fa-fw fa fa-bolt"></i><span> PacMan  吃豆人</span></a></li><li><a class="site-page child" href="/tetris/"><i class="fa-fw fa fa-arrows-alt"></i><span> Tetris 俄罗斯方块</span></a></li><li><a class="site-page child" href="/smallcat/"><i class="fa-fw fa fa-paw"></i><span> CatchCat 困住小猫</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-leaf"></i><span> Moments</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/diary/"><i class="fa-fw fas fa-bookmark"></i><span> Diary</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fa fa-hourglass-half"></i><span> Gallery</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-podcast"></i><span> More</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags标签</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About关于</span></a></li><li><a class="site-page child" href="/messageboard/"><i class="fa-fw fas fa-bookmark"></i><span> Messageboard留言板</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/%E8%83%8C%E6%99%AF%20(2).png')"><nav id="nav"><span id="blog-info"><a href="/" title="All wisdom begins with memory."><span class="site-name">All wisdom begins with memory.</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-gamepad"></i><span> Games</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/mikutap/"><i class="fa-fw fa fa-music"></i><span> MikuTap 初音未来</span></a></li><li><a class="site-page child" href="/starbattle/"><i class="fa-fw fa fa-space-shuttle"></i><span> StartBattle 星际大战</span></a></li><li><a class="site-page child" href="/2048/"><i class="fa-fw fa fa-flag"></i><span> 2048 经典游戏</span></a></li><li><a class="site-page child" href="/battlecity/"><i class="fa-fw fa fa-arrow-circle-left"></i><span> BattleCity 坦克大战</span></a></li><li><a class="site-page child" href="/pacman/"><i class="fa-fw fa fa-bolt"></i><span> PacMan  吃豆人</span></a></li><li><a class="site-page child" href="/tetris/"><i class="fa-fw fa fa-arrows-alt"></i><span> Tetris 俄罗斯方块</span></a></li><li><a class="site-page child" href="/smallcat/"><i class="fa-fw fa fa-paw"></i><span> CatchCat 困住小猫</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-leaf"></i><span> Moments</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/diary/"><i class="fa-fw fas fa-bookmark"></i><span> Diary</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fa fa-hourglass-half"></i><span> Gallery</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-podcast"></i><span> More</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags标签</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About关于</span></a></li><li><a class="site-page child" href="/messageboard/"><i class="fa-fw fas fa-bookmark"></i><span> Messageboard留言板</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">TQ System</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-09-13T16:00:00.000Z" title="Created 2025-09-14 00:00:00">2025-09-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-09-27T06:25:26.811Z" title="Updated 2025-09-27 14:25:26">2025-09-27</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="TQ System"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Day01"><a href="#Day01" class="headerlink" title="Day01"></a>Day01</h1><h2 id="1、什么是知识图谱？"><a href="#1、什么是知识图谱？" class="headerlink" title="1、什么是知识图谱？"></a>1、什么是知识图谱？</h2><p>概念：<code>知识图谱是以图的形式来表示实体和实体之间关系的语义网络。</code></p>
<ul>
<li>节点：实体、概念</li>
<li>边：关系（外部）、属性（内部）</li>
</ul>
<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250629194951478.png" alt="image-20250629194951478" style="zoom:67%;" />

<p>类型有两种：</p>
<ul>
<li>实体-关系-实体【通常的说法！！】</li>
<li>实体-属性-属性值</li>
</ul>
<h2 id="2、项目的技术架构图是怎样的？"><a href="#2、项目的技术架构图是怎样的？" class="headerlink" title="2、项目的技术架构图是怎样的？"></a>2、项目的技术架构图是怎样的？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250629195102287.png" alt="image-20250629195102287"></p>
<ul>
<li>数据获取<ul>
<li>业务数据：比较规范，一般可以直接使用构建知识图谱</li>
<li>采集数据：形式各异，需要进行清洗和信息抽取工作</li>
</ul>
</li>
<li>信息抽取【核心】<ul>
<li>工作：实体抽取、关系抽取、属性抽取</li>
<li>方法：规则匹配、机器学习、深度学习</li>
</ul>
</li>
<li>知识融合<ul>
<li>任务：消除冗余、解决冲突、统一表达、知识扩展</li>
<li>技术：指代消解、实体消岐、知识融合（实体对齐、关系对齐）</li>
</ul>
</li>
<li>知识加工<ul>
<li>工作：质量评估</li>
</ul>
</li>
<li>图谱搭建<ul>
<li>工作：将三元组导入到数据库中</li>
</ul>
</li>
<li>图谱应用<ul>
<li>工作：搭建问答系统</li>
</ul>
</li>
</ul>
<h2 id="3、项目用到了哪些工具？"><a href="#3、项目用到了哪些工具？" class="headerlink" title="3、项目用到了哪些工具？"></a>3、项目用到了哪些工具？</h2><ul>
<li>Doccano（多卡诺）是一种用于文本标注的开源工具，支持多种常见的文本标注任务，如命名实体识别、文本分类、关系抽取等。</li>
<li>Flask 是一个轻量级的 Python Web 框架，它的核心作用是帮助开发者快速构建 Web 应用程序和 API，实现使用URL对函数进行调用 。</li>
<li>Gunicorn是一个被广泛使用的高性能的Python WSGI UNIX HTTP服务组件(WSGI: Web Server Gateway Interface)<ul>
<li>核心作用是为 Python Web 应用（如 Flask、Django）提供生产级并发、稳定性等。</li>
<li>具有使用非常简单，轻量级的资源消耗，以及高性能等特点。</li>
</ul>
</li>
<li>Neo4j是一个高性能的图数据库，作为核心的知识存储和查询数据库。</li>
</ul>
<h2 id="4、为什么不用MySQL来存储三元组数据？"><a href="#4、为什么不用MySQL来存储三元组数据？" class="headerlink" title="4、为什么不用MySQL来存储三元组数据？"></a>4、为什么不用MySQL来存储三元组数据？</h2><ul>
<li>多跳关联查询需要多表连接，效率低</li>
<li>MySQL 是面向关系表结构设计的，缺乏对三元组语义和图结构的原生支持</li>
</ul>
<h2 id="5、什么是实体和NER？"><a href="#5、什么是实体和NER？" class="headerlink" title="5、什么是实体和NER？"></a>5、什么是实体和NER？</h2><ul>
<li>实体：<code>文本之中承载信息的语义单元</code>。如人名、地名、机构名等。</li>
<li>实体抽取：又称为命名实体识别（named entity recognition，NER），<code>指的是从文本之中抽取出命名性实体，并把这些实体划分到指定的类别。</code></li>
</ul>
<h2 id="6、命名实体识别有哪些方法？"><a href="#6、命名实体识别有哪些方法？" class="headerlink" title="6、命名实体识别有哪些方法？"></a>6、命名实体识别有哪些方法？</h2><p>（1）基于规则的方法</p>
<ul>
<li>针对有特殊上下文的实体，或实体本身有很多特征的文本，使用规则的方法简单且有效。比较适合半结构化或比较规范的文本中的进行抽取任务。</li>
<li>方法：<ul>
<li><code>【设计规则的模版（词典+正则表达式）再去进行匹配】</code></li>
</ul>
</li>
<li>优缺点<ul>
<li>优点：简单，快速。</li>
<li>缺点：适用性差，维护成本高后期甚至不能维护。</li>
</ul>
</li>
</ul>
<p>（2）基于传统机器学习的方法</p>
<ul>
<li>一般使用统计模型是把实体抽取任务转化为<code>【序列标注问题】</code>，使用IO、<code>BIO</code>、BIOES等标注方法对实体进行标注。对于文本之中的每个词，或者汉语之中的每个字，都有若干候选的标签</li>
</ul>
<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610094314077.png" alt="image-20250610094314077" style="zoom:50%;" />

<ul>
<li>基于序列标注方法的统计模型，常见的包括：支持向量机（SVM）、隐马尔科夫模型（HMM）、条件随机场（CRF）等。在实际研究之中，研究人员往往把这些模型和其他方法结合在一起。</li>
<li>优缺点<ul>
<li>优点：统计学习方法较之基于规则的方法，更加灵活和健壮，可以移植到其他领域。</li>
<li>缺点：特征的选择是至关重要的。这些模型依赖人工设计的特征和现有的自然语言处理工具（如分词工具）。<ul>
<li>常见的特征可以分为形态、词汇、句法、全局特征、外部信息等。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>（3）基于深度学习的方法</p>
<ul>
<li>大量的深度学习模型被使用到实体抽取任务之中。</li>
<li>方法：基于深度学习的方法主要使用神经网络模型，结合条件随机场模型。<ul>
<li>常用的神经网络模型包括卷积神经网络（CNN）、循环神经网络（RNN）、长短期记忆网络（LSTM）等，其中<code>【BiLSTM+CRF】</code>是目前最为常用的命名实体识别模型.</li>
</ul>
</li>
<li><code>优缺点</code><ul>
<li><code>优点：不需要人工来设计特征，同时能够取得较高的准确率和召回率。</code></li>
<li><code>缺点：这些模型十分依赖人工标注数据，标注语料的缺乏为模型的训练带来了极大的困难。</code></li>
</ul>
</li>
</ul>
<h2 id="7、举个例子描述一下如何使用规则的方法抽取实体？"><a href="#7、举个例子描述一下如何使用规则的方法抽取实体？" class="headerlink" title="7、举个例子描述一下如何使用规则的方法抽取实体？"></a>7、举个例子描述一下如何使用规则的方法抽取实体？</h2><p>例子：比如要从一段新闻报道中识别出机构名。</p>
<p>规则：</p>
<ul>
<li><p>如果一个词语以“北京”、“中央”等地名词开头，那么它可能是一个机构名的开始。</p>
</li>
<li><p>如果一个词语后面紧跟着“公司”、“集团”、“局”、“部”等词，那么它可能是一个机构名的结束。</p>
</li>
</ul>
<p>思路：</p>
<ul>
<li>1）先构建词典，用于定位结构名的结束位置</li>
<li>2）使用jieba的词性标注对文本进行序列标注，获取分词结果及对应的词性</li>
<li>3）根据规则将词标注为B、E或O<ul>
<li>其中词性为ns的，即地名的，标注为B</li>
<li>词在词典中，标注成E</li>
<li>其余标注为O</li>
</ul>
</li>
<li>4）然后使用正则表达式从标注序列中取出机构名</li>
</ul>
<h1 id="Day02"><a href="#Day02" class="headerlink" title="Day02"></a>Day02</h1><h2 id="1、LSTM面试题"><a href="#1、LSTM面试题" class="headerlink" title="1、LSTM面试题"></a>1、LSTM面试题</h2><ul>
<li>传统RNN结构为什么会出现梯度消失和爆炸问题？</li>
</ul>
<p>因为在RNN的反向传播时，梯度要经过多个时间步的链式相乘，而每个时间步使用的是相同的权重矩阵，就会造成梯度消失或爆炸！——当权重矩阵的特征值小于 1 时，梯度会指数级衰减（梯度消失）；而当特征值大于 1 时，则会指数级增长（梯度爆炸）</p>
<ul>
<li>LSTM相比RNN有什么优势？</li>
</ul>
<p>LSTM的门控机制使得LSTM可以“选择性地”记忆和遗忘信息，从而有效缓解了梯度消失和梯度爆炸的问题，能够更好地捕捉序列中的长时间依赖关系。因此，LSTM相较于普通RNN在处理长序列任务（如文本生成、语音识别、时间序列预测等）中表现更为出色。</p>
<ul>
<li>BiLSTM相比LSTM有什么特点？</li>
</ul>
<p>Bi-LSTM相比LSTM能同时捕捉前后文信息，提升序列建模效果，但计算成本更高、训练时间更长。</p>
<h2 id="2、什么是线性链条件随机场（Linear-chain-CRF）"><a href="#2、什么是线性链条件随机场（Linear-chain-CRF）" class="headerlink" title="2、什么是线性链条件随机场（Linear-chain-CRF）?"></a>2、什么是线性链条件随机场（Linear-chain-CRF）?</h2><p><code>线性链条件随机场是一类给定线性输入序列 𝑋 的条件下，输出线性标签序列 𝑌 的概率分布 𝑃(𝑌∣𝑋)的概率模型。其中每个位置的标签只依赖于它前后相邻的标签以及线性序列 𝑋 ，而不依赖于更远处的标签。</code></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912071120914.png" alt="image-20250912071120914"></p>
<h2 id="3、描述一下BiLSTM-CRF架构？"><a href="#3、描述一下BiLSTM-CRF架构？" class="headerlink" title="3、描述一下BiLSTM+CRF架构？"></a>3、描述一下BiLSTM+CRF架构？</h2><p>BiLSTM+CRF架构主要由两部分构成，</p>
<p>第一部分：使用BiLSTM生成发射分数（有输入层、词嵌入层、BiLSTM层、线性层）</p>
<ul>
<li>BiLSTM层捕捉文本前后向信息</li>
<li>线性层输出标签的概率分布</li>
</ul>
<p>第二部分：基于BiLSTM生成的发射分数使用CRF获取最优的标签路径</p>
<ul>
<li>CRF层来预测标签概率最大的标签路径</li>
<li>Viterbi解码：在预测时，解码概率最大的标签路径</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912071145607.png" alt="image-20250912071145607"></p>
<h2 id="4、CRF中的发射分数和转移分数是什么？"><a href="#4、CRF中的发射分数和转移分数是什么？" class="headerlink" title="4、CRF中的发射分数和转移分数是什么？"></a>4、CRF中的发射分数和转移分数是什么？</h2><ul>
<li>发射分数：</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912071203105.png" alt="image-20250912071203105"></p>
<ul>
<li>转移分数：</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912071224807.png" alt="image-20250912071224807"></p>
<h2 id="5、说一下CRF建模的损失函数是怎样的？"><a href="#5、说一下CRF建模的损失函数是怎样的？" class="headerlink" title="5、说一下CRF建模的损失函数是怎样的？"></a>5、说一下CRF建模的损失函数是怎样的？</h2><p>首先计算出真实路径的概率，然后让该概率值越大越好！！也就是让真实路径概率值最大时，估计未知参数的值，从而将问题转变成极大似然估计问题。</p>
<p>在问题求解中通过加负数，将求最大转换成求最小，通过求对数，将连除形式转换成对数减法形式，即<code>负对数似然损失</code> ！</p>
<p>最终损失函数有两部分组成，一部分是归一化项，一部分真实路径的分数。求解归一化项时使用的方法是<code>前向算法的动态规划</code>！</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912071251574.png" alt="image-20250912071251574"></p>
<p><code>说一下什么叫极大似然估计？</code></p>
<p>找到一组参数，使得在这些参数下，观察到的数据出现的概率最大。</p>
<h2 id="6、前向算法是什么？"><a href="#6、前向算法是什么？" class="headerlink" title="6、前向算法是什么？"></a>6、前向算法是什么？</h2><p><code>首先：单条路径的分数怎么算的？</code></p>
<p>每条路径的分数就是由对应的发射分数和转移分数组合而成的。</p>
<p>背景：如果标签数量是𝑘，文本长度是𝑛，那么有k^n条路径，不能遍历每条路径获得所有路径的分数。</p>
<p>办法：使用<code>前向算法的动态规划</code></p>
<ul>
<li>目的：计算给定观测序列的概率总和。<ul>
<li><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250630163358692.png" alt="image-20250630163358692"></li>
</ul>
</li>
<li>过程：<code>通过动态规划的方法，逐步计算每个时刻每个状态的累加概率，以得到最终的观测序列概率总和。</code></li>
<li>特点：关注所有状态路径的累加值，计算的是所有路径的概率总和。</li>
</ul>
<p>递推公式：<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250701160500169.png" alt="image-20250701160500169"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250701155652771.png" alt="image-20250701155652771"></p>
<h2 id="7、Viterbi解码是什么？"><a href="#7、Viterbi解码是什么？" class="headerlink" title="7、Viterbi解码是什么？"></a>7、Viterbi解码是什么？</h2><p>目的：寻找给定观测序列下最可能的状态序列。</p>
<p>过程：同样<code>使用动态规划，但在每一步中只保留最优路径（即最大概率路径），而不是所有路径。</code></p>
<p>特点：关注最优路径，只保留最大概率路径，而非所有路径。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912071434296.png" alt="image-20250912071434296"></p>
<h1 id="Day03"><a href="#Day03" class="headerlink" title="Day03"></a>Day03</h1><h2 id="1、在项目中，应该如何设置路径？"><a href="#1、在项目中，应该如何设置路径？" class="headerlink" title="1、在项目中，应该如何设置路径？"></a>1、在项目中，应该如何设置路径？</h2><p>1）为了项目可移植性，需要配置成相对路径</p>
<p>2）为了避免文件在调用时，路径随着调用位置变化而变化，需要使用如下的方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如何设计，让调用时不随着调用的位置变化，而路径发生变化</span></span><br><span class="line">base_dir = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;base_dir--&gt;<span class="subst">&#123;base_dir&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 路径拼接</span></span><br><span class="line">path = os.path.join(base_dir, <span class="string">&#x27;../data/labels.json&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;拼接后的path--&gt;<span class="subst">&#123;path&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>



<h2 id="2、简单说一下数据处理的最终格式要求？"><a href="#2、简单说一下数据处理的最终格式要求？" class="headerlink" title="2、简单说一下数据处理的最终格式要求？"></a>2、简单说一下数据处理的最终格式要求？</h2><p>1）分样本的，每个样本是一个句子，并且是一个X,Y数据对</p>
<p>2）训练数据为id，而不是文字或者标签值</p>
<p>3）拆分出训练集和验证集，封装到DataLoder中</p>
<p>4）每个批次中样本的长度是一样的</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250702195148758.png" alt="image-20250702195148758"></p>
<h2 id="3、在将原始数据处理成最终的格式要求时，一般可以在哪些地方做处理？"><a href="#3、在将原始数据处理成最终的格式要求时，一般可以在哪些地方做处理？" class="headerlink" title="3、在将原始数据处理成最终的格式要求时，一般可以在哪些地方做处理？"></a>3、在将原始数据处理成最终的格式要求时，一般可以在哪些地方做处理？</h2><p>1）直接读取数据文件，然后将数据加工成想要的格式。一般用于比较复杂的数据清洗和转换操作。</p>
<p>2）在构造Dataset类时做处理。可以做一些x,y的封装、数据类型转换等。</p>
<p>3）在创建Dataloader时，在自定义函数collate_fn()中做处理。可以做一些id的转换、数据类型转换、长度对齐、生成掩码张量等。</p>
<h2 id="4、在构造数据迭代器（Dataloader）时，有哪些步骤？"><a href="#4、在构造数据迭代器（Dataloader）时，有哪些步骤？" class="headerlink" title="4、在构造数据迭代器（Dataloader）时，有哪些步骤？"></a>4、在构造数据迭代器（Dataloader）时，有哪些步骤？</h2><p>1）构建Dataset类<br>2）构建自定义函数collate_fn()<br>3）构建get_data函数，获得数据迭代器</p>
<h2 id="5、统一样本长度有哪些方法？"><a href="#5、统一样本长度有哪些方法？" class="headerlink" title="5、统一样本长度有哪些方法？"></a>5、统一样本长度有哪些方法？</h2><p>1）使用sequence来处理列表</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> sequence</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">x_train: 文本的张量表示</span></span><br><span class="line"><span class="string">max_len：最大的句子长度</span></span><br><span class="line"><span class="string">可以通过padding和truncating设置补齐或截断的方向，默认是pre</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">return</span> sequence.pad_sequences(x_train, max_len, padding=<span class="string">&quot;post&quot;</span>, truncating=<span class="string">&quot;pre&quot;</span>, value=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>2）使用pad_sequence来处理张量或者张量列表</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">pad_sequence:可以对一个批次的样本进行统一长度，统一长度的方式是以该批次中最长的样本为基准</span></span><br><span class="line"><span class="string">batch_first=True,则返回的数据形状为[batch_size, max_seq_len]  padding_value是指用什么补齐</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">input_ids_padded = pad_sequence(x_train, batch_first=<span class="literal">True</span>, padding_value=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>3）使用BertTokenizer的batch_encode_plus来处理列表</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">my_tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line"><span class="comment"># 编码text2id 对多句话进行编码用batch_encode_plus函数</span></span><br><span class="line">data = my_tokenizer.batch_encode_plus(batch_text_or_text_pairs=sents,</span><br><span class="line">                                      truncation=<span class="literal">True</span>,</span><br><span class="line">                                      padding=<span class="string">&#x27;max_length&#x27;</span>,</span><br><span class="line">                                      max_length=<span class="number">500</span>,</span><br><span class="line">                                      return_tensors=<span class="string">&#x27;pt&#x27;</span>,</span><br><span class="line">                                      return_length=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>4）使用自定义的方法</p>
<h2 id="6、描述一下BiLSTM-CRF模型的架构？"><a href="#6、描述一下BiLSTM-CRF模型的架构？" class="headerlink" title="6、描述一下BiLSTM_CRF模型的架构？"></a>6、描述一下BiLSTM_CRF模型的架构？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250913074306242.png" alt="image-20250913074306242"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250913083014466.png" alt="image-20250913083014466"></p>
<h1 id="Day04"><a href="#Day04" class="headerlink" title="Day04"></a>Day04</h1><h2 id="1、训练函数基本步骤是什么？"><a href="#1、训练函数基本步骤是什么？" class="headerlink" title="1、训练函数基本步骤是什么？"></a>1、训练函数基本步骤是什么？</h2><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">1.构建数据迭代器Dataloader(包括数据处理与构建数据源Dataset)</span></span><br><span class="line"><span class="attr">2.实例化模型</span></span><br><span class="line"><span class="attr">3.实例化损失函数对象</span></span><br><span class="line"><span class="attr">4.实例化优化器对象</span></span><br><span class="line"><span class="attr">5.定义打印日志参数</span></span><br><span class="line"><span class="attr">6.开始训练</span></span><br><span class="line"><span class="attr">6.1</span> <span class="string">实现外层大循环epoch</span></span><br><span class="line">    <span class="attr">6.2</span> <span class="string">将模型设置为训练模式</span></span><br><span class="line">    <span class="attr">6.3</span> <span class="string">内部遍历数据迭代器dataloader</span></span><br><span class="line">        <span class="attr">1）将数据送入模型得到输出结果</span></span><br><span class="line">        <span class="attr">2）计算损失</span></span><br><span class="line">        <span class="attr">3）梯度清零</span>: <span class="string">optimizer.zero_grad()</span></span><br><span class="line">        <span class="attr">4）反向传播(计算梯度)</span>: <span class="string">loss.backward()</span></span><br><span class="line">        <span class="attr">5）梯度更新(参数更新)</span>: <span class="string">optimizer.step()</span></span><br><span class="line">        <span class="attr">6）打印内部训练日志</span></span><br><span class="line">    <span class="attr">6.4</span> <span class="string">使用验证集进行模型评估【将模型设置为评估模式】</span></span><br><span class="line">    <span class="attr">6.5</span> <span class="string">保存模型: torch.save(model.state_dict(), &quot;model_path&quot;)</span></span><br><span class="line"><span class="attr">6.6</span> <span class="string">打印外部训练日志</span></span><br></pre></td></tr></table></figure>

<h2 id="2、验证函数基本步骤是什么？"><a href="#2、验证函数基本步骤是什么？" class="headerlink" title="2、验证函数基本步骤是什么？"></a>2、验证函数基本步骤是什么？</h2><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">1.定义打印日志参数</span></span><br><span class="line"><span class="attr">2.将模型设置为评估模式</span></span><br><span class="line"><span class="attr">3.内部遍历数据迭代器dataloader</span></span><br><span class="line">  <span class="attr">3.1</span> <span class="string">将数据送入模型得到输出结果</span></span><br><span class="line">  <span class="attr">3.2</span> <span class="string">计算损失</span></span><br><span class="line">  <span class="attr">3.3</span> <span class="string">处理结果</span></span><br><span class="line">  <span class="attr">3.4</span> <span class="string">统计批次内指标</span></span><br><span class="line"><span class="attr">4.统计整体指标</span></span><br></pre></td></tr></table></figure>

<h2 id="3、BiLSTM-CRF模型在训练完后，可以做哪些优化来改善模型性能？"><a href="#3、BiLSTM-CRF模型在训练完后，可以做哪些优化来改善模型性能？" class="headerlink" title="3、BiLSTM_CRF模型在训练完后，可以做哪些优化来改善模型性能？"></a>3、BiLSTM_CRF模型在训练完后，可以做哪些优化来改善模型性能？</h2><p>1）模型优化</p>
<p>预训练词向量：使用预训练的词向量（如Word2Vec、GloVe、FastText）替代随机初始化的词嵌入，可以更好地捕捉词汇语义信息。</p>
<p>自注意力机制：在BiLSTM后加入自注意力层，增强模型对长距离依赖的捕捉能力。</p>
<p>调整随机失活层：可以在embedding层后添加随机失活层，也可以修改随机失活比例。</p>
<p>2）训练过程优化</p>
<ul>
<li>shuffles设置：注意真正训练时，需要将DataLoader中的shuffle设置为True</li>
<li>梯度裁剪：在反向传播时对梯度进行裁剪，防止梯度爆炸。</li>
<li>早停机制：监控验证集F1值，若连续多个epoch未提升则提前终止训练。</li>
</ul>
<p>3）训练数据优化</p>
<ul>
<li>如果训练集和验证集数据分布不同，也就是说使用的是差距很大的样本，会使模型的效果较差，所以可以将数据打散后再送到dataloader中</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>():</span><br><span class="line">    <span class="comment"># 为了能够让训练集和验证集的样本分布一致，需要先将数据集打乱，然后再去进行划分</span></span><br><span class="line">    random.seed(<span class="number">66</span>)</span><br><span class="line">    random.shuffle(datas)  <span class="comment"># 数据会原地修改</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建训练数据集：基于原始数据集，进行8:2拆分</span></span><br><span class="line">    train_dataset = NerDataset(datas[:<span class="number">6300</span>])</span><br></pre></td></tr></table></figure>

<ul>
<li>除了这种方式之外，也可以使用<code>分类采样</code>的方式。这种方式可以绝对类型上，训练集和验证集的分布是一致的。</li>
</ul>
<p>另外，还有以下方法——</p>
<p>更多数据：收集或标注更多数据，送到模型中进行训练。</p>
<p>随机替换：随机替换部分词为同义词或近义词，增强模型鲁棒性。</p>
<p>实体替换：保留实体边界，随机替换实体内容（如疾病名称、药品名称），提升实体识别泛化能力。</p>
<h2 id="4、precision、recall、f1、report的使用方式是什么？"><a href="#4、precision、recall、f1、report的使用方式是什么？" class="headerlink" title="4、precision、recall、f1、report的使用方式是什么？"></a>4、precision、recall、f1、report的使用方式是什么？</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># classification_report可以导出字典格式，修改参数：output_dict=True，可以将字典在保存为csv格式输出</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score, f1_score, classification_report</span><br><span class="line"></span><br><span class="line">precision = precision_score(golds, preds, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">recall = recall_score(golds, preds, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">f1 = f1_score(golds, preds, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">report = classification_report(golds, preds)</span><br></pre></td></tr></table></figure>

<p>其中golds和preds要求的格式要求为：</p>
<p>1）1D 数组（最常见）</p>
<ul>
<li>Python 列表：<code>[0, 1, 1, 0, 2]</code></li>
<li>NumPy 数组：<code>np.array([0, 1, 1, 0, 2])</code></li>
<li>Pandas Series：<code>pd.Series([0, 1, 1, 0, 2])</code></li>
</ul>
<p>适用于 多分类、二分类、单标签 情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]</span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<p>2）Label Indicator Array &#x2F; Sparse Matrix</p>
<p>适用于 多标签分类（multi-label classification）：</p>
<p>label indicator array：二维数组，每一列表示一个类别，值为 0&#x2F;1，表示每个类别的有无。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y_true = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">          [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">          [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]]</span><br><span class="line">y_pred = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">          [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">          [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]]</span><br></pre></td></tr></table></figure>

<p>含义是：</p>
<ul>
<li><p>第一行：属于类别 0 和 2</p>
</li>
<li><p>第二行：属于类别 1</p>
</li>
<li><p>第三行：属于类别 0 和 1</p>
</li>
</ul>
<p>把数据组装成一维列表的方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将非padding位置的预测标签和真实标签保存起来，使用的方法是：通过input_ids进行非零判断，然后得到一个boolean的张量，然后直接对这个张量进行求和，就可以获取到每个句子的真实的长度，让让再通过这个长度，使用列表切片的方式，从标签中取出真实位置对应的标签</span></span><br><span class="line"><span class="comment"># print(f&#x27;每个样本的真实长度--&gt;&#123;(input_ids&gt;0).sum(-1).tolist()&#125;&#x27;)  # [11, 13, 10, 14, 55, 22, 39, 25]</span></span><br><span class="line">real_len = (input_ids&gt;<span class="number">0</span>).<span class="built_in">sum</span>(-<span class="number">1</span>).tolist()</span><br><span class="line"><span class="comment"># 根据真实的句子长度，获取预测的标签</span></span><br><span class="line"><span class="keyword">for</span> index, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(predict):</span><br><span class="line">    preds.extend(label[:real_len[index]])</span><br><span class="line"><span class="comment"># 根据真实的句子长度，获取真实的标签</span></span><br><span class="line"><span class="keyword">for</span> index, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(labels.tolist()):</span><br><span class="line">    golds.extend(label[:real_len[index]])</span><br></pre></td></tr></table></figure>



<h2 id="5、模型预测基本步骤是什么？"><a href="#5、模型预测基本步骤是什么？" class="headerlink" title="5、模型预测基本步骤是什么？"></a>5、模型预测基本步骤是什么？</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>实例化模型</span><br><span class="line"><span class="number">2.</span>加载训练好的模型参数</span><br><span class="line"><span class="number">3.</span>处理数据</span><br><span class="line"><span class="number">4.</span>模型预测</span><br><span class="line"><span class="number">5.</span>结果处理</span><br></pre></td></tr></table></figure>



<h1 id="Day05"><a href="#Day05" class="headerlink" title="Day05"></a>Day05</h1><h2 id="1、什么是关系抽取？本质是什么？"><a href="#1、什么是关系抽取？本质是什么？" class="headerlink" title="1、什么是关系抽取？本质是什么？"></a>1、什么是关系抽取？本质是什么？</h2><p>关系抽取就是从一段文本中抽取出 (主体，关系，客体) 这样的三元组</p>
<p>本质是：<code>文本分类问题</code></p>
<h2 id="关系抽取的常用方法有哪些？"><a href="#关系抽取的常用方法有哪些？" class="headerlink" title="关系抽取的常用方法有哪些？"></a>关系抽取的常用方法有哪些？</h2><ul>
<li>基于规则方式实现关系抽取<ul>
<li>人工定义规则</li>
</ul>
</li>
<li>基于机器学习<ul>
<li>决策树、随机森林、线性回归等</li>
</ul>
</li>
<li>基于深度学习<ul>
<li>基于Pipeline流水线方法实现关系抽取：在实体识别已经完成的基础上再进行实体之间关系的抽取<ul>
<li>如：BiLSTM+Attention模型</li>
</ul>
</li>
<li>基于Joint联合抽取方法实现关系抽取：修改标注方法和模型结构直接输出文本中包含的(ei ,rk, ej)三元组<ul>
<li>如：联合解码的联合模型、参数共享的联合模型</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="3、关系抽取任务常见问题有哪些？"><a href="#3、关系抽取任务常见问题有哪些？" class="headerlink" title="3、关系抽取任务常见问题有哪些？"></a>3、关系抽取任务常见问题有哪些？</h2><ul>
<li>正常关系 (Normal) 问题：数据中只有一个实体对及关系</li>
<li>单一实体关系重叠问题 (Single Entity Overlap (SEO) )：数据中一个实体参与到了多个关系中<ul>
<li>BiLSTM+Attention模型即可解决，一个句子中有几个三元组就构建几个样本即可</li>
</ul>
</li>
<li>实体对重叠(Entity Pair Overlap (EPO))：数据中一个实体对有两种不同的关系类型<ul>
<li>Casrel模型可以解决</li>
</ul>
</li>
</ul>
<h2 id="4、基于规则的方法实现关系抽取的优缺点是什么？"><a href="#4、基于规则的方法实现关系抽取的优缺点是什么？" class="headerlink" title="4、基于规则的方法实现关系抽取的优缺点是什么？"></a>4、基于规则的方法实现关系抽取的优缺点是什么？</h2><ul>
<li>优点：实现简单、无需训练，小规模数据集容易实现.</li>
<li>缺点：<ul>
<li>无法解决复杂的场景</li>
<li>对跨领域的可移植性较差、人工制作规则的成本较高以及召回率较低.</li>
</ul>
</li>
</ul>
<h2 id="5、描述一下BiLSTM-Attention模型的架构？"><a href="#5、描述一下BiLSTM-Attention模型的架构？" class="headerlink" title="5、描述一下BiLSTM+Attention模型的架构？"></a>5、描述一下BiLSTM+Attention模型的架构？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250916071419364.png" alt="image-20250916071419364"></p>
<h2 id="6、注意力机制是什么？"><a href="#6、注意力机制是什么？" class="headerlink" title="6、注意力机制是什么？"></a>6、注意力机制是什么？</h2><p>注意力机制是什么？</p>
<p>注意力机制（Attention）是一种动态加权的方法，它通过计算“查询”（query）与一组“键”（keys）之间的相似度来为对应的“值”（values）分配不同的重要性权重，从而使模型能够在处理序列或图像等输入时，重点关注与当前任务最相关的部分信息。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250802233021882.png" alt="image-20250802233021882"></p>
<p>优势：</p>
<p>捕捉长距离依赖：不再依赖 RNN 的逐步传递，能直接建模序列中任意两位置的依赖关系。</p>
<p>并行计算：尤其在 Transformer 中，注意力计算可以大范围并行，极大加速训练。</p>
<p>可解释性：通过可视化注意力权重，可以了解到模型在处理时重点关注了哪些输入位置。</p>
<h2 id="7、描述一下BiLSTM-Attention模型中注意力机制是如何实现的？"><a href="#7、描述一下BiLSTM-Attention模型中注意力机制是如何实现的？" class="headerlink" title="7、描述一下BiLSTM+Attention模型中注意力机制是如何实现的？"></a>7、描述一下BiLSTM+Attention模型中注意力机制是如何实现的？</h2><p>首先对 BiLSTM 的输出进行非线性变换，得到初步的语义特征表示；然后通过一个可训练的权重向量和 softmax 函数，计算每个单词对整体语义的重要性权重；接着使用这些注意力权重对 BiLSTM 的输出进行加权求和，提取出句子的全局语义特征；最后通过非线性变换得到最终的上下文向量，用于后续的分类任务。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250916071359413.png" alt="image-20250916071359413"></p>
<h2 id="8、BiLSTM-Attentiom模型中数据处理的整体思路是什么？"><a href="#8、BiLSTM-Attentiom模型中数据处理的整体思路是什么？" class="headerlink" title="8、BiLSTM+Attentiom模型中数据处理的整体思路是什么？"></a>8、BiLSTM+Attentiom模型中数据处理的整体思路是什么？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250916071311082.png" alt="image-20250916071311082"></p>
<h1 id="Day06"><a href="#Day06" class="headerlink" title="Day06"></a>Day06</h1><h2 id="1、BERT预训练模型所接收的最大sequence长度是多少，为什么设置最大长度？"><a href="#1、BERT预训练模型所接收的最大sequence长度是多少，为什么设置最大长度？" class="headerlink" title="1、BERT预训练模型所接收的最大sequence长度是多少，为什么设置最大长度？"></a>1、BERT预训练模型所接收的最大sequence长度是多少，为什么设置最大长度？</h2><p>512</p>
<p>BERT 的输入除了 Token Embedding 之外，还要加上位置编码（position embeddings），以告诉模型“第 i 个 token 在序列中的位置”。它在进行embedding的时候，需要设置embedding层的 vocab_size（这里指的的是有多少个位置，而不是字符的数量），这个值会在构建模型时写死！所以，一旦写死之后，句子的最大长度就确定了，后续在使用时，就不能超过这个句子的最大长度，因为一旦超过之后，超出的位置编码就没有办法进行embedding查表了。训练BERT时，大部分语料都不超过512，所以最终指定句子的最大长度为512。模型训练好之后，在进行使用时，需要将样本统一成最大长度。</p>
<h2 id="2、对于长文本-文本长度超过512的句子-在使用BERT时-如何来构造训练样本？"><a href="#2、对于长文本-文本长度超过512的句子-在使用BERT时-如何来构造训练样本？" class="headerlink" title="2、对于长文本(文本长度超过512的句子)在使用BERT时, 如何来构造训练样本？"></a>2、对于长文本(文本长度超过512的句子)在使用BERT时, 如何来构造训练样本？</h2><p>核心就是如何进行截断。</p>
<ul>
<li>head-only方式: 这是只保留长文本头部信息的截断方式, 具体为保存前510个token (要留两个位置给[CLS]和[SEP]).</li>
<li>tail-only方式: 这是只保留长文本尾部信息的截断方式, 具体为保存最后510个token (要留两个位置给[CLS]和[SEP]).</li>
<li>head+only方式: 选择前128个token和最后382个token (文本总长度在510以内), 或者前256个token和最后254个token (文本总长度大于510).</li>
</ul>
<h2 id="3、BiLSTM-Attention模型的架构是怎样的？"><a href="#3、BiLSTM-Attention模型的架构是怎样的？" class="headerlink" title="3、BiLSTM+Attention模型的架构是怎样的？"></a>3、BiLSTM+Attention模型的架构是怎样的？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250917172341057.png" alt="image-20250917172341057"></p>
<h2 id="4、在BERT中，是如何将Token-Embedding、Segment-Embedding-和-Position-Embedding组合在一起然后送到encoder中的？"><a href="#4、在BERT中，是如何将Token-Embedding、Segment-Embedding-和-Position-Embedding组合在一起然后送到encoder中的？" class="headerlink" title="4、在BERT中，是如何将Token Embedding、Segment Embedding 和 Position Embedding组合在一起然后送到encoder中的？"></a>4、在BERT中，是如何将Token Embedding、Segment Embedding 和 Position Embedding组合在一起然后送到encoder中的？</h2><p>在 BERT 中，模型的输入表示由三部分同维度的向量按位相加得到，然后送入后续的 Transformer encoder。</p>
<p>网络模型：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250708203107793.png" alt="image-20250708203107793"></p>
<p>示意图：</p>
<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250708203422586.png" alt="image-20250708203422586" style="zoom:67%;" />

<p>示例：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250708203902969.png" alt="image-20250708203902969"></p>
<h2 id="5、两个矩阵相乘时，shape不符合要求怎么办？"><a href="#5、两个矩阵相乘时，shape不符合要求怎么办？" class="headerlink" title="5、两个矩阵相乘时，shape不符合要求怎么办？"></a>5、两个矩阵相乘时，shape不符合要求怎么办？</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">A = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">B = torch.randn(<span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;A--&gt;<span class="subst">&#123;A&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;B--&gt;<span class="subst">&#123;B&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># matmul 自动广播：</span></span><br><span class="line">result = torch.matmul(A, B)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 先手动进行广播，再进行矩阵乘法</span></span><br><span class="line">result = torch.bmm(A.expand(<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>),  B)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>

<h1 id="Day07"><a href="#Day07" class="headerlink" title="Day07"></a>Day07</h1><h2 id="1、BiLSTM-Attention模型可以做哪些优化来改善模型性能？"><a href="#1、BiLSTM-Attention模型可以做哪些优化来改善模型性能？" class="headerlink" title="1、BiLSTM+Attention模型可以做哪些优化来改善模型性能？"></a>1、BiLSTM+Attention模型可以做哪些优化来改善模型性能？</h2><p>1）模型优化</p>
<ul>
<li>句子嵌入方式：可以使用jieba分词得到词语，然后再使用词语的方式进行嵌入。</li>
<li>替换BiLSTM：将BiLSTM替换成BERT&#x2F;RoBERTa等这种预训练模型或BiGRU去做嵌入，看是否可以提供模型的语义表达能力。</li>
<li>多头注意力机制：借鉴Transformer中多头注意力机制，将单一注意力拆分到多个子空间，去捕捉不同维度的语义信息。</li>
<li>修改注意力机制的方式：使用transformer中注意力机制的计算方式或者先进行从concat再经过linear层的方式等，来计算注意力机制，看模型的性能效果。</li>
<li>调整随机失活层：调整随机失活层的位置、有无或随机失活比例，来观察模型的性能变化。</li>
</ul>
<p>2）训练过程的优化</p>
<ul>
<li>shuffle设置：注意在真正训练时，需要将dataloader中的shuffle设置为True</li>
<li>梯度裁剪：在反向传播时对梯度进行裁剪，防止梯度消失或爆炸。</li>
<li>早停机制：监控验证集上F1值或其他关键指标，如果连续多个epoch未提升或者开始下降，则提前终止训练。</li>
</ul>
<p>3）训练数据优化</p>
<ul>
<li>通过过采样或欠采样来解决样本不均衡问题</li>
<li>通过同义词替换、回译、实体替换等方法来扩充数据集。或者直接使用大模型进行训练样本的生成。</li>
</ul>
<h2 id="2、Pipeline方法的优缺点"><a href="#2、Pipeline方法的优缺点" class="headerlink" title="2、Pipeline方法的优缺点"></a>2、Pipeline方法的优缺点</h2><ul>
<li>优点：<ul>
<li>易于实现，实体模型和关系模型使用独立的数据集，不需要同时标注实体和关系的数据集.</li>
<li>两者相互独立，若关系抽取模型没训练好不会影响到实体抽取.</li>
</ul>
</li>
<li>缺点：<ul>
<li>关系和实体两者是紧密相连的，互相之间的联系没有捕捉到.</li>
<li>上游 NER 的错误会直接影响下游关系抽取，容易造成误差积累.</li>
<li>BiLSTM_Attention难以处理EPO问题</li>
</ul>
</li>
</ul>
<h2 id="3、Joint方法是什么？有哪两种类型？"><a href="#3、Joint方法是什么？有哪两种类型？" class="headerlink" title="3、Joint方法是什么？有哪两种类型？"></a>3、Joint方法是什么？有哪两种类型？</h2><p>（1）概念</p>
<p>通过修改模型结构或标注方法， 直接输出文本中包含的SPO三元组</p>
<p>（2）类型</p>
<ul>
<li>参数共享的联合模型【修改模型结构】<ul>
<li>主体、客体和关系的抽取不是严格同步进行的 (通常是依次执行，但是某些情况下也可以其中两个任务一起进行) ，各个过程都可以得到一个loss值，<code>整个模型的loss是各过程loss值之和.</code></li>
</ul>
</li>
</ul>
<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/4-1-1.png" alt="img" style="zoom:67%;" />

<ul>
<li><p>联合解码的联合模型【修改标注方法】</p>
<ul>
<li>主体、客体和关系的抽取是同时进行的，通过一个模型直接得到SPO三元组.</li>
</ul>
</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250918193641102.png" alt="image-20250918193641102"></p>
<h2 id="4、Casrel模型的架构是怎样的？"><a href="#4、Casrel模型的架构是怎样的？" class="headerlink" title="4、Casrel模型的架构是怎样的？"></a>4、Casrel模型的架构是怎样的？</h2><p>第一步：识别出句子中的Subject</p>
<ul>
<li><p>（1）<code>两个线性层+sigmoid去分类任务</code>：一个判断每个token是不是头实体的开始索引；一个判断每个token是不是头实体的结束索引。</p>
</li>
<li><p>（2）利用最近匹配原则将识别到的start和end配对，获得候选头实体集合</p>
</li>
</ul>
<p><code>第二步：根据识别出的Subject，识别出所有有可能的Relation及对应的Object</code></p>
<ul>
<li><code>（1）bert隐藏层输出+所取的Subject特征向量作为输入【若Subject存在多个字，则取平均向量】</code></li>
<li>（2）对于识别出来的每一个Subject，对应的每一种关系会解码出其Object的Start和End索引位置，与Subject类似</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250918193657010.png" alt="image-20250918193657010"></p>
<h2 id="5、说一下Casrel模型的输入输出是什么？"><a href="#5、说一下Casrel模型的输入输出是什么？" class="headerlink" title="5、说一下Casrel模型的输入输出是什么？"></a>5、说一下Casrel模型的输入输出是什么？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250918193827404.png" alt="image-20250918193827404"></p>
<h2 id="6、数据处理整体思路是怎样的？"><a href="#6、数据处理整体思路是怎样的？" class="headerlink" title="6、数据处理整体思路是怎样的？"></a>6、数据处理整体思路是怎样的？</h2><p>原始数据：<br>文件格式是json，每行都是样本<br>text为原始文本，spo_list为三元组列表【一个样本中可能有多个spo三元组】</p>
<ol>
<li>构建Dataset</li>
</ol>
<p>在构建Dataset时，直接使用json.loads()方法逐行去加载数据，存储到字典列表中，然后取出text中的原始文本和spo_list中的三元组列表进行返回</p>
<ol start="2">
<li>构建自定义函数collect_fn()<ul>
<li><code>使用bert的分词器对原始文本进行处理，获取input_ids和attention_mask</code></li>
<li>基于每个样本的input_ids和spo_list去获取训练数据的其他输入和输出</li>
<li><code>对每个样本的结果数据进行拼接，再转成tensor作为最终模型训练的数据</code></li>
</ul>
</li>
<li>构建get_data_loader()函数，获取数据迭代器<ul>
<li>分别使用train&#x2F;dev&#x2F;test.json文件构造不同的Dataloader对象即可</li>
</ul>
</li>
</ol>
<p>训练数据：</p>
<p><code>输入：input_ids, attention_mask, 所取头实体从头到尾的位置信息，所取头实体的长度</code></p>
<p><code>输出：主实体的开始、结束位置信息，客实体的开始、结束位置信息及关系信息</code></p>
<p>注意：</p>
<p>在使用每个样本的input_ids和spo_list获取 sub_head2tail、sub_len、obj_heads、obj_tails这四个值的时候，做法如下：</p>
<p>在模型训练时，取的主实体的信息是真实spo_list中的值，原因是:</p>
<p>1)如果我们取模型第一步预测出来的主实体，此时有可能这个主实体预测错了，那么它就没有对应的客实体及关系信息，此时则无法构造标签，无法计算损失!</p>
<p>2)使用teacher_forcing这种方法，使用真实的spo_list中主实体信息去训练模型，可以加快模型的收敛速度，提高训练的效率</p>
<p>而在模型预测时，只知道原始文本，不知道主实体信息，此时sub_head2tail和sub_len这两个信息则是由模型第一步预测出来的主实体。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250918193956400.png" alt="image-20250918193956400"></p>
<h1 id="Day08"><a href="#Day08" class="headerlink" title="Day08"></a>Day08</h1><h2 id="1、Casrel模型数据处理的整体思路是什么？"><a href="#1、Casrel模型数据处理的整体思路是什么？" class="headerlink" title="1、Casrel模型数据处理的整体思路是什么？"></a>1、Casrel模型数据处理的整体思路是什么？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250919191750171.png" alt="image-20250919191750171"></p>
<h2 id="2、使用Casrel模型时，遇到什么问题，如何解决的？"><a href="#2、使用Casrel模型时，遇到什么问题，如何解决的？" class="headerlink" title="2、使用Casrel模型时，遇到什么问题，如何解决的？"></a>2、使用Casrel模型时，遇到什么问题，如何解决的？</h2><p>遇到的问题：<br>如果一个样本中有多个主实体，按照Casrel模型的定义，需要先取出一个主实体，然后去预测该主实体的客实体及关系；然后用相同的方法再去处理其他主实体。这种处理方式在构建数据时比较复杂。</p>
<p>解决方案：<br>考虑到数据的情况，大部分的样本都是只有1个主实体，可以在每次训练构建训练数据集时，使用随机的方式抽取一个主实体，然后基于抽取到的这个主实体完成其客实体和关系的预测。因为训练是有多个轮次的，每次随机抽取，所以也相当于将所有的主实体都送到了模型中进行了训练。</p>
<p>在随机抽取时，每次随机抽取一个主实体，然后在客实体及关系信息中，记录该主实体所有的客实体开始位置及结束位置信息和关系。即sub_head2tail、sub_len是所取的主实体的信息，obj_heads、obj_tails为所取主实体对应的所有的客实体开始位置及结束位置信息和关系信息。</p>
<p>但是sub_heads和sub_tails需要记录一个样本的所有的主实体的开始位置信息及结束位置信息，这个不单单只记录抽取到的那个主实体，即sub_heads、sub_tails为该样本所有的主实体的开始位置和结束位置信息。</p>
<h2 id="3、Casrel模型的结构是怎样的？"><a href="#3、Casrel模型的结构是怎样的？" class="headerlink" title="3、Casrel模型的结构是怎样的？"></a>3、Casrel模型的结构是怎样的？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250919191736057.png" alt="image-20250919191736057"></p>
<h1 id="Day09"><a href="#Day09" class="headerlink" title="Day09"></a>Day09</h1><h2 id="1、Casrel模型的损失函数怎么计算的？"><a href="#1、Casrel模型的损失函数怎么计算的？" class="headerlink" title="1、Casrel模型的损失函数怎么计算的？"></a>1、Casrel模型的损失函数怎么计算的？</h2><p>损失计算:<br><code>(1)模型有4个输出，需要对4个输出分别计算损失，再进行相加，相加之和是模型的损失</code>  </p>
<p><code>(2)因为标签是二分类的结果，所以在计算损失时，需要使用BCELoss</code></p>
<p>因为后续需要保留所有token对应的索引，而不是直接拿到平均损失，所以将reduction&#x3D;’none’</p>
<p><code>(3)为了消除补齐部分对损失值的影响，需要将补齐的部分的损失置成0，然后再去进行计算非0部分的平均值，作为最终的损失</code></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250922092756052.png" alt="image-20250922092756052"></p>
<h2 id="2、AdamW相关面试题"><a href="#2、AdamW相关面试题" class="headerlink" title="2、AdamW相关面试题"></a>2、AdamW相关面试题</h2><p><strong>①什么是权重衰减？</strong></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250922092807852.png" alt="image-20250922092807852"></p>
<p><strong>②为什么能防止过拟合？</strong></p>
<ul>
<li>模型参数越大，模型越容易对训练数据拟合过头，捕捉到不必要的噪声。</li>
<li>通过惩罚参数变大，可以使一些参数变成0，可以使模型学得更简单，泛化能力更强。</li>
</ul>
<p><strong>③<code>Adam和AdamW的区别和优势是什么？</code></strong></p>
<p>AdamW 相较于 Adam 的主要区别在于<strong>权重衰减的实现方式</strong>。Adam 将 L2 正则项直接添加到梯度中，这会与自适应学习率机制耦合，导致正则化效果不稳定；而 AdamW 将权重衰减与梯度更新解耦，直接在参数更新时施加衰减，从而更符合理论上的正则化含义。优势在于：<strong>AdamW 提供了更稳定、有效的正则化效果，有助于提升模型的泛化能力</strong>，因此在许多现代深度学习任务中表现更优，已成为如 Transformers 等模型的默认优化器。</p>
<p><strong>④为什么不对”bias”, “LayerNorm.bias”, “LayerNorm.weight”做权重衰减？</strong></p>
<p>因为”bias” 和 “LayerNorm” 中的参数对模型的复杂度影响不大；另外，不做权重衰减，是为了避免干扰模型的偏移能力和归一化机制，从而保证训练稳定、性能更优。</p>
<h2 id="3、Casrel模型中，Bert为什么要参与反向传播进行参数更新？"><a href="#3、Casrel模型中，Bert为什么要参与反向传播进行参数更新？" class="headerlink" title="3、Casrel模型中，Bert为什么要参与反向传播进行参数更新？"></a>3、Casrel模型中，Bert为什么要参与反向传播进行参数更新？</h2><p>**任务特定调整：**虽然BERT是预训练的，但它并不是针对特定任务（如关系抽取）进行优化的。通过在特定任务上进行微调（即反向传播更新参数），可以使BERT的表示更适合关系抽取的任务。这样，BERT模型能够更好地理解实体间的关系。</p>
<p>**领域适应：**预训练的BERT是在大规模语料上训练的，可能没有针对具体领域的知识或语言模式。通过微调BERT，可以使其更适应目标领域的数据，改善抽取效果。</p>
<p>**经验结果：**大量后续工作和实践都表明：在下游抽取、分类、生成等任务里，给BERT或其他Transformer设置较小的学习率，整体端到端的微调，一般比“冻结+只微调顶层”要好2—5个百分点的效果，尤其在中大型数据集上。</p>
<h2 id="4、Casrel模型可以做哪些优化？"><a href="#4、Casrel模型可以做哪些优化？" class="headerlink" title="4、Casrel模型可以做哪些优化？"></a>4、Casrel模型可以做哪些优化？</h2><p>升级预训练模型：从基础 bert-base 换成效果更好的中文预训练，如 RoBERTa-wwm-ext、MacBERT、Erlangshen-RoBERTa-large 等。</p>
<p>修改主实体和bert隐藏层的融合方式：可以使用拼接的方式（Bert隐藏层输入拼接上所取主实体的平均向量；另外也可以将所取的主实体的向量前拼接N个1，其他的向量拼接N个0），或者使用增强的方式（将所取的主实体对应的张量扩大N倍）。</p>
<p>增加实体边界探索：在 subject&#x2F;object 边界预测上加一个前馈全连接层 或者是BiLSTM+Linear层，提高识别的准确性。</p>
<p>增加drop层：通过增加几个不同的drop层，提高模型的过拟合能力。</p>
<p>修改0&#x2F;1的阈值：目前设置的阈值为0.5，可以修改这个阈值进行训练或预测，比如修改成0.45,0.55等。</p>
<p>增加训练数据：可以使用数据增强，或更多标注数据。</p>
<h1 id="Day10"><a href="#Day10" class="headerlink" title="Day10"></a>Day10</h1><h2 id="1、Casrel模型在预测时，需要注意什么？"><a href="#1、Casrel模型在预测时，需要注意什么？" class="headerlink" title="1、Casrel模型在预测时，需要注意什么？"></a>1、Casrel模型在预测时，需要注意什么？</h2><p>预测思路：与其他模型不一样的地方是，不能将数据处理之后，直接调用forward方法，获取模型的最终预测结果。而是需要先将数据处理好之后，送到模型中去预测主实体信息，然后再去处理这个主实体信息，处理好之后再送入模型中去预测客实体及关系，才能最终的预测结果。所以，预测时大的步就是有2个!</p>
<p>1、预测主实体<br>先将文本送到bert分词器，获取input_ids，attention_mask</p>
<p>调用模型的get_encoded_text()，获取bert_output</p>
<p>调用模型的get_subs()方法，获取主实体开始和结束位置信息</p>
<p>抽取出主实体(先将数据转换成1或0,然后调用extract_sub方法)</p>
<p>注意:这一步有可能抽取到0个或1个或者多个主实体</p>
<p>不再预测客实体及关系需要进行遍历，去预测每个主实体的客实体及关系</p>
<p>2、预测客实体及关系</p>
<p>对每个主实体进行处理，获取sub_head2tail,sub_len</p>
<p>调用模型get_objs_and_rels()方法，预测客实体及关系</p>
<p>抽取客实体及关系(先将数据转换成1或0,然后调用extract_obj方法)</p>
<p>注意:这一步有可能抽取到0个或1个或者多个客实体及关系</p>
<p>不再解析结果，需要进行遍历，然后依次解析结果</p>
<p>3、结果解析</p>
<p>将id解析成文本，并拼接后输出</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250922193525213.png" alt="image-20250922193525213"></p>
<h2 id="2、什么是知识融合？"><a href="#2、什么是知识融合？" class="headerlink" title="2、什么是知识融合？"></a>2、什么是知识融合？</h2><p>知识融合，简单理解是将来自不同来源、格式、结构的异构数据统一整合到一个一致的知识图谱中。</p>
<h2 id="3、主要有哪些问题？"><a href="#3、主要有哪些问题？" class="headerlink" title="3、主要有哪些问题？"></a>3、主要有哪些问题？</h2><ul>
<li>消除冗余：有重复的spo三元组，需要去重<ul>
<li>去重的方式：使用python进行数据清洗、借助图数据库</li>
</ul>
</li>
<li>统一表达：不同名称的实体或关系，但表示的意思相同，需要统一。如鲁迅和周树人是同一人、父亲和爸爸是同一关系<ul>
<li>实体消歧、实体统一、关系对齐</li>
</ul>
</li>
<li>解决冲突：同一个实体或关系的描述可能存在冲突，需要找到一致性或保留一个。如 张三-籍贯-河北 与 张三-籍贯-石家庄 冲突<ul>
<li>找到一致性、可信度评估</li>
</ul>
</li>
<li>知识扩展：挖掘新知识，丰富知识图谱<ul>
<li>处理更多的语料，从而更多的三元组；可以查阅相关的资料，获取的更多的信息，补充到知识图谱中。</li>
</ul>
</li>
</ul>
<h2 id="4、什么是实体消岐（实体链接）？怎么处理？"><a href="#4、什么是实体消岐（实体链接）？怎么处理？" class="headerlink" title="4、什么是实体消岐（实体链接）？怎么处理？"></a>4、什么是实体消岐（实体链接）？怎么处理？</h2><p>定义：根据上下文信息来解决同一名称可能指代多个不同对象的问题（即一词多义）。</p>
<p>目标：确定文本中提到的具体对象，以消除歧义。</p>
<p>方法：基于规则、机器学习、深度学习</p>
<p><code>比如：可以使用 tf-idf 生成向量，然后计算向量相似度</code></p>
<h2 id="5、-什么是实体统一（实体对齐）？怎么处理？"><a href="#5、-什么是实体统一（实体对齐）？怎么处理？" class="headerlink" title="5、 什么是实体统一（实体对齐）？怎么处理？"></a>5、 什么是实体统一（实体对齐）？怎么处理？</h2><p>定义：判断多个实体是不是属于一个实体。</p>
<p>目标：将来自不同数据源中的同一实体进行识别和合并。</p>
<p>方法：</p>
<ul>
<li>基于规则：根据领域专家提供的规则，如对同义词或缩写的映射。</li>
<li>基于有监督的学习方法：训练模型自动判断实体是否相同。</li>
</ul>
<h2 id="6、什么是关系对齐（关系统一）？怎么处理？"><a href="#6、什么是关系对齐（关系统一）？怎么处理？" class="headerlink" title="6、什么是关系对齐（关系统一）？怎么处理？"></a>6、什么是关系对齐（关系统一）？怎么处理？</h2><p>定义：不同数据源可能使用不同的方式描述相同的关系，需要进行判断。</p>
<p>目标：将不同数据源中表示相同的关系进行对齐和融合。</p>
<p>方法：</p>
<ul>
<li><p>关系同义词映射：根据已知的同义词表或通过上下文分析，统一表示相同的关系。</p>
</li>
<li><p>基于嵌入的语义相似度：将关系文本编码成向量表示，在向量空间中计算它们之间的相似度，以判断不同关系是否语义一致。</p>
</li>
</ul>
<h2 id="7、如何使用TF-IDF来进行实体消歧？"><a href="#7、如何使用TF-IDF来进行实体消歧？" class="headerlink" title="7、如何使用TF-IDF来进行实体消歧？"></a>7、如何使用TF-IDF来进行实体消歧？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250923092355604.png" alt="image-20250923092355604"></p>
<h2 id="8、你们项目选用的图数据库是什么？为什么？"><a href="#8、你们项目选用的图数据库是什么？为什么？" class="headerlink" title="8、你们项目选用的图数据库是什么？为什么？"></a>8、你们项目选用的图数据库是什么？为什么？</h2><p>neo4j数据库企业版</p>
<p>因为neo4j图数据库是一个专业级的图数据库，性能强大，且企业版提供高可靠和高可用，非常稳定。在公司中使用广泛，有大量的学习资料可以参考。</p>
<h2 id="9、NEO4J数据库中有哪些概念？和spo三元组有什么关系？"><a href="#9、NEO4J数据库中有哪些概念？和spo三元组有什么关系？" class="headerlink" title="9、NEO4J数据库中有哪些概念？和spo三元组有什么关系？"></a>9、NEO4J数据库中有哪些概念？和spo三元组有什么关系？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250922193913100.png" alt="image-20250922193913100"></p>
<h2 id="10、图数据库相比传统的MySQL数据库，有哪些优势？"><a href="#10、图数据库相比传统的MySQL数据库，有哪些优势？" class="headerlink" title="10、图数据库相比传统的MySQL数据库，有哪些优势？"></a>10、图数据库相比传统的MySQL数据库，有哪些优势？</h2><ul>
<li>图数据库以“节点”和“边”来表示实体及其关系，更容易理解和建模</li>
<li>MySQL通过外键关联表，查询复杂关系时需要大量 JOIN 操作，性能随关系深度和数据量急剧下降。而图数据库在查询时是沿着连接线“遍历”，在多跳查询时速度依然很快。</li>
</ul>
<h1 id="Day11"><a href="#Day11" class="headerlink" title="Day11"></a>Day11</h1><h2 id="1、最终导入数据库的数据组织形式是怎样的？为什么？"><a href="#1、最终导入数据库的数据组织形式是怎样的？为什么？" class="headerlink" title="1、最终导入数据库的数据组织形式是怎样的？为什么？"></a>1、最终导入数据库的数据组织形式是怎样的？为什么？</h2><p>将所有数据，包括spo三元组数据和从其他地方整理的数据整理到json文件中。如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">      # 数据库中的唯一标识符。</span><br><span class="line">    <span class="attr">&quot;_id&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;$oid&quot;</span><span class="punctuation">:</span> <span class="string">&quot;5bb578b6831b973a137e3ee8&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span> </span><br><span class="line">      # 伴随疾病或并发症。</span><br><span class="line">    <span class="attr">&quot;acompany&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="string">&quot;贫血&quot;</span></span><br><span class="line">    <span class="punctuation">]</span><span class="punctuation">,</span> </span><br><span class="line">      # 疾病的分类，表明疾病所属的类别和科室。</span><br><span class="line">    <span class="attr">&quot;category&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="string">&quot;疾病百科&quot;</span><span class="punctuation">,</span> </span><br><span class="line">        <span class="string">&quot;急诊科&quot;</span></span><br><span class="line">    <span class="punctuation">]</span><span class="punctuation">,</span> </span><br><span class="line">      # 疾病的病因</span><br><span class="line">    <span class="attr">&quot;cause&quot;</span><span class="punctuation">:</span> <span class="string">&quot;吸入苯蒸气或皮肤接触苯而...&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>这样组织的原因是：</p>
<ul>
<li>可以将一个实体的多个spo三元组存储在一起，更加直观，也方便后续的处理</li>
<li>可以把类似疾病描述、治疗费用、疾病的病因等这些从其他地方获取到的信息也存在这个json中，方便以属性的方式存到知识图谱中，从而使问答的范围变大！！</li>
</ul>
<h2 id="2、导入数据库都有哪些数据？"><a href="#2、导入数据库都有哪些数据？" class="headerlink" title="2、导入数据库都有哪些数据？"></a>2、导入数据库都有哪些数据？</h2><p>将所有实体，包括疾病名称、症状、科室、食物等以节点的方式导入neo4j中。</p>
<p>将所有关系，包括疾病-症状、疾病－忌吃食物、疾病－易吃食物、疾病 - 推荐药品等关系以关系的方式导入neo4j中。</p>
<p>将实体的属性，包括疾病描述、病因、预防方式、治疗方法等以属性的方式导入到neo4j中。</p>
<p>导入方式都是使用<code>merge</code>的方式，如果之前没有则创建，如果存在则更新。</p>
<h2 id="3、问答系统分为哪几个部分，分别是做什么的？"><a href="#3、问答系统分为哪几个部分，分别是做什么的？" class="headerlink" title="3、问答系统分为哪几个部分，分别是做什么的？"></a>3、问答系统分为哪几个部分，分别是做什么的？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250925081453072.png" alt="image-20250925081453072"></p>
<h2 id="4、什么是意图识别？"><a href="#4、什么是意图识别？" class="headerlink" title="4、什么是意图识别？"></a>4、什么是意图识别？</h2><ul>
<li><p>定义：意图识别是判断用户意图的过程，即判断用户想干什么。</p>
</li>
<li><p>本质：<code>文本分类问题</code>，需要预先定义意图类别。</p>
</li>
</ul>
<h2 id="5、什么是槽位填充？"><a href="#5、什么是槽位填充？" class="headerlink" title="5、什么是槽位填充？"></a>5、什么是槽位填充？</h2><p>从用户的话语中提取出关键的参数信息，并填入预定义的结构化模板中对应的槽位，进而方便利用知识库回答用户问题或者完成某种操作。</p>
<h2 id="6、如何设计语义槽？"><a href="#6、如何设计语义槽？" class="headerlink" title="6、如何设计语义槽？"></a>6、如何设计语义槽？</h2><ul>
<li>有多少种意图，就预定义好多少种对应的语义槽</li>
<li>每个意图中需要多少个关键信息，就设计多少个槽位</li>
<li>每个槽位包括：待填充的槽位值、追问话术和歧义澄清话术【通过询问确定具体信息】、槽位预测API【通过接口从用户的输入中提取出该槽位的值（NER）】</li>
</ul>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;订电影票&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;电影名&quot;</span><span class="punctuation">:</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;槽位值&quot;</span><span class="punctuation">:</span>___<span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;追问话术&quot;</span><span class="punctuation">:</span><span class="string">&quot;请问您需要看那部电影？&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;歧义澄清话术&quot;</span><span class="punctuation">:</span><span class="string">&quot;你想看XX还是YYY&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;槽位预测&quot;</span><span class="punctuation">:</span><span class="string">&quot;/api/predict_movie_name/&quot;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">          <span class="attr">&quot;电影院名称&quot;</span><span class="punctuation">:</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;槽位值&quot;</span><span class="punctuation">:</span>___<span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;追问话术&quot;</span><span class="punctuation">:</span><span class="string">&quot;请问您要去的电影院是哪个？&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;歧义澄清话术&quot;</span><span class="punctuation">:</span><span class="string">&quot;你想看XX影院还是YY影院？&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;槽位预测&quot;</span><span class="punctuation">:</span><span class="string">&quot;/api/predict_cinema/&quot;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">。。。</span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>



<h2 id="7、医疗KBQA系统架构是怎样的？"><a href="#7、医疗KBQA系统架构是怎样的？" class="headerlink" title="7、医疗KBQA系统架构是怎样的？"></a>7、医疗KBQA系统架构是怎样的？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250925090738421.png" alt="image-20250925090738421"></p>
<h1 id="Day12"><a href="#Day12" class="headerlink" title="Day12"></a>Day12</h1><h2 id="1、医疗KBQA系统实现的步骤是什么？"><a href="#1、医疗KBQA系统实现的步骤是什么？" class="headerlink" title="1、医疗KBQA系统实现的步骤是什么？"></a>1、医疗KBQA系统实现的步骤是什么？</h2><ul>
<li>NLU模块<ul>
<li>实现第一个意图识别模型：判断是否是闲聊类的意图</li>
<li>实现第二个意图识别模型：包括13个医疗类的意图</li>
<li>实现第三个槽位填充（NER）模型：这里直接使用NER任务模型</li>
</ul>
</li>
<li>DM模块<ul>
<li>基于不同意图，设计对应的语义槽</li>
<li>槽位填充</li>
<li>根据意图执行度确定回复策略</li>
</ul>
</li>
<li>NLG模块<ul>
<li>根据回复策略去neo4j中查询答案</li>
</ul>
</li>
<li>主逻辑服务模块实现<ul>
<li>设计整个对话逻辑</li>
</ul>
</li>
</ul>
<h2 id="2、如何实现第一个意图识别模型（判断是否是闲聊类的意图）？"><a href="#2、如何实现第一个意图识别模型（判断是否是闲聊类的意图）？" class="headerlink" title="2、如何实现第一个意图识别模型（判断是否是闲聊类的意图）？"></a>2、如何实现第一个意图识别模型（判断是否是闲聊类的意图）？</h2><p>使用IT-IDF向量化器构造TF-IDF向量化器，将文本转成TF-IDF向量作为训练特征。</p>
<p>将特征向量送入逻辑回归模型和GBDT模型中进行训练。</p>
<p>将两个模型预测出的概率值结果取平均值，然后选择概率最大的标签。</p>
<h2 id="3、如何实现第二个意图识别模型（包括13个医疗类的意图）？"><a href="#3、如何实现第二个意图识别模型（包括13个医疗类的意图）？" class="headerlink" title="3、如何实现第二个意图识别模型（包括13个医疗类的意图）？"></a>3、如何实现第二个意图识别模型（包括13个医疗类的意图）？</h2><p>先将文本送入BERT模型中，学习语义特征，然后将BERT的池化层输出（pooler_output），送入全连接层得到预测的分类结果。</p>
<h2 id="4、如何实现第三个槽位填充（NER）模型（用来识别用户话语中的实体）？"><a href="#4、如何实现第三个槽位填充（NER）模型（用来识别用户话语中的实体）？" class="headerlink" title="4、如何实现第三个槽位填充（NER）模型（用来识别用户话语中的实体）？"></a>4、如何实现第三个槽位填充（NER）模型（用来识别用户话语中的实体）？</h2><p><code>（1）实现方式一：</code></p>
<p>使用课上讲的模型：BiLSTM+CRF</p>
<p>注意：</p>
<p>1）需要对数据进行处理，处理方式有两种：</p>
<ul>
<li>第一种方式：直接将数据转成课上使用的数据格式。这一步可以借助大模型来完成。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">process_train_file</span>(<span class="params">input_file=<span class="string">&#x27;train.txt&#x27;</span>, output_file=<span class="string">&#x27;processed_train.txt&#x27;</span></span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(input_file, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f_in, \</span><br><span class="line">         <span class="built_in">open</span>(output_file, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f_out:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f_in:</span><br><span class="line">            line = line.strip()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">                <span class="keyword">continue</span>  <span class="comment"># 跳过空行</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 分割原始文本和标签（假设用制表符分隔）</span></span><br><span class="line">            parts = line.split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(parts) &lt; <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">continue</span>  <span class="comment"># 跳过格式不对的行</span></span><br><span class="line"></span><br><span class="line">            text = parts[<span class="number">0</span>].strip()</span><br><span class="line">            labels = parts[<span class="number">1</span>].strip().split()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> text[-<span class="number">1</span>] <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;。&#x27;</span>, <span class="string">&#x27;?&#x27;</span>, <span class="string">&#x27;!&#x27;</span>, <span class="string">&#x27;！&#x27;</span>, <span class="string">&#x27;？&#x27;</span>]:</span><br><span class="line">                text += <span class="string">&#x27;。&#x27;</span></span><br><span class="line">                labels += [<span class="string">&#x27;O&#x27;</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 检查字符数和标签数是否匹配</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(text) != <span class="built_in">len</span>(labels):</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;警告：文本长度与标签长度不匹配，跳过该行。\n文本：&#x27;<span class="subst">&#123;text&#125;</span>&#x27;\n长度：<span class="subst">&#123;<span class="built_in">len</span>(text)&#125;</span>\n标签长度：<span class="subst">&#123;<span class="built_in">len</span>(labels)&#125;</span>&quot;</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 逐字符写入结果</span></span><br><span class="line">            <span class="keyword">for</span> char, label <span class="keyword">in</span> <span class="built_in">zip</span>(text, labels):</span><br><span class="line">                f_out.write(<span class="string">f&quot;<span class="subst">&#123;char&#125;</span>\t<span class="subst">&#123;label&#125;</span>\n&quot;</span>)</span><br><span class="line">            f_out.write(<span class="string">&quot;\n&quot;</span>)  <span class="comment"># 每个句子结束后加一个空行，便于区分</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;处理完成，结果已保存到 <span class="subst">&#123;output_file&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用函数</span></span><br><span class="line">process_train_file(<span class="string">&#x27;train.txt&#x27;</span>, <span class="string">&#x27;processed_train.txt&#x27;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>第二种方式：根据模型的特点，直接对数据进行处理。</li>
</ul>
<p>2）这个训练数据是分训练集和验证集的，不需要使用同一份数据进行拆分了！</p>
<p>3）标签数量不一样。所在在进行标签的填充的时候，填充的字符就是15了。</p>
<p>4）标签的分隔方式不一样。B-  —&gt;  B_</p>
<p><code>（2）实现方式二：</code></p>
<p>使用BERT替换掉embedding层和BiLSTM层。</p>
<p>注意点：</p>
<p>1）<strong>需要将标签进行转换！！！！因为bert将文本转成input_ids之后，它的长度会发生变化，也就是输入和标签之间的对应关系会发生变化，所以需要对标签进行处理！！！</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch_data</span>):</span><br><span class="line">    <span class="comment"># print(f&#x27;batch_data--&gt;&#123;batch_data&#125;&#x27;)</span></span><br><span class="line">    texts = [<span class="string">&#x27;&#x27;</span>.join(sample[<span class="number">0</span>]) <span class="keyword">for</span> sample <span class="keyword">in</span> batch_data]</span><br><span class="line">    tags = [sample[<span class="number">1</span>] <span class="keyword">for</span> sample <span class="keyword">in</span> batch_data]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1)将文字转成id</span></span><br><span class="line">    encoding = conf.tokenizer.batch_encode_plus(</span><br><span class="line">        texts,</span><br><span class="line">        truncation=<span class="literal">True</span>,</span><br><span class="line">        padding=<span class="literal">True</span>,</span><br><span class="line">        return_tensors=<span class="string">&#x27;pt&#x27;</span>,</span><br><span class="line">        add_special_tokens=<span class="literal">False</span></span><br><span class="line">    )</span><br><span class="line">    input_ids_padded = encoding[<span class="string">&#x27;input_ids&#x27;</span>]  <span class="comment"># Tensor[B, L]</span></span><br><span class="line">    <span class="comment"># print(f&#x27;input_ids_padded--&gt;&#123;input_ids_padded&#125;&#x27;)</span></span><br><span class="line">    attention_mask = encoding[<span class="string">&#x27;attention_mask&#x27;</span>]  <span class="comment"># Tensor[B, L]</span></span><br><span class="line">    <span class="comment"># print(f&#x27;attention_mask--&gt;&#123;attention_mask&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2)将标签对齐</span></span><br><span class="line">    B, L = input_ids_padded.size()</span><br><span class="line">    labels_padded = torch.zeros((B, L), dtype=torch.long)  <span class="comment"># 初始化全0张量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, (text, tag) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(texts, tags)):  <span class="comment"># 遍历每个样本</span></span><br><span class="line">        <span class="comment"># print(f&#x27;text--&gt;&#123;list(text)&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># print(f&#x27;tag--&gt;&#123;tag&#125;&#x27;)</span></span><br><span class="line">        input_ids = input_ids_padded[i]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取编码后的 token</span></span><br><span class="line">        tokens = conf.tokenizer.convert_ids_to_tokens(input_ids.tolist())</span><br><span class="line">        <span class="comment"># print(f&#x27;tokens--&gt; &#123;tokens&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># 构建字符到 token 索引的映射，用于定位实体，字典的key是原始文本中的索引，value是编码后token索引</span></span><br><span class="line">        char2token = &#123;&#125;</span><br><span class="line">        char_idx = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> t_idx, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokens):</span><br><span class="line">            tok = token.replace(<span class="string">&#x27;##&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> tok:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            start = text.find(tok, char_idx)  <span class="comment"># 从上次匹配结束处开始查找</span></span><br><span class="line">            <span class="keyword">if</span> start &lt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            end = start + <span class="built_in">len</span>(tok)</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(start, end):</span><br><span class="line">                char2token[c] = t_idx</span><br><span class="line">            char_idx = end</span><br><span class="line">        <span class="comment"># print(f&#x27;char2token--&gt;&#123;char2token&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 标记所有标签</span></span><br><span class="line">        max_index = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> char2token.items():</span><br><span class="line">            labels_padded[i, v] = conf.tag2id[tag[k]]</span><br><span class="line">            max_index = v</span><br><span class="line">        labels_padded[i, max_index + <span class="number">1</span>:] = <span class="number">15</span>  <span class="comment"># padding的标签</span></span><br><span class="line">        <span class="comment"># print(f&#x27;labels_padded--&gt;&#123;labels_padded[i]&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> input_ids_padded, labels_padded, attention_mask</span><br></pre></td></tr></table></figure>

<p><strong>2）在微调Bert时，需要将学习率设小一点，才能有效去修改参数。</strong></p>
<p>原始的学习率为2e-3，这个值比较大，会造成无法有效修改bert的模型参数，会造成预测结果全为0的情况。可以把它修改成3e-5。</p>
<p>另外一个优化点，是<strong>使用了学习率预热。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">优化点：学习率预热</span></span><br><span class="line"><span class="string">学习率预热的目的：让模型在初始阶段更快的使用数据，避免训练过程中学习率过大或过小带来训练不稳定或者收敛速度太慢的问题，从而提高模型训练效果和泛化性能</span></span><br><span class="line"><span class="string">实现方式：在初始阶段，将学习率从较小的值逐步增加到预设的初始值，然后按照我们设定的训练策略逐渐变小。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">get_linear_schedule_with_warmup: 使用这个方法来实现学习率预热，它的方式是从0以线性的方式增大到预设的学习率，然后再以线性的方式逐渐降低到0</span></span><br><span class="line"><span class="string">参数： optimizer：优化器对象</span></span><br><span class="line"><span class="string">num_warmup_steps：预热步数，指的是从0增加到预设的学习率所需的步数</span></span><br><span class="line"><span class="string">num_training_steps: 指的是整个训练过程的总的步数，确切来说在给定的数据集上，参数更新的次数。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">total_steps = <span class="built_in">len</span>(train_dataloader) * conf.epochs  <span class="comment"># 总的训练步数</span></span><br><span class="line">scheduler = get_linear_schedule_with_warmup(optimizer,</span><br><span class="line">                                            num_warmup_steps=<span class="number">50</span>,</span><br><span class="line">                                            num_training_steps=total_steps)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用方式如下：</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3）梯度清零: optimizer.zero_grad()</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 4）反向传播(计算梯度): loss.backward()</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 5）梯度更新(参数更新): optimizer.step()</span></span><br><span class="line">        <span class="comment"># 梯度裁剪</span></span><br><span class="line">        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=<span class="number">10</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="comment"># 学习率更新</span></span><br><span class="line">        scheduler.step()</span><br></pre></td></tr></table></figure>

<p><code>（3）实现方式三：</code></p>
<p>直接使用BERT+linear进行预测。</p>
<p>注意点：数据处理方式同方式二、模型训练方式同BiLSTM（使用交叉熵损失进行训练）。</p>
<p><code>（4）实现方式四：</code></p>
<p>结合了BERT+BiLSTM+Linear+CRF。</p>
<p><strong>优化点：使用chinese-roberta-wwm-ext替换了bert-base-chinese</strong> 只要使用bert-base-chinese的地方都可以使用它进行优化，其他的bert变种也可以拿来进行尝试。</p>
<p>替换的方式：直接从魔搭社区上进行下载，下载完成后，替换掉原始的bert-base-chinese路径即可。</p>
<p>下载方式如下（在cmd中运行）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modelscope download --model dienstag/chinese-roberta-wwm-ext --local_dir ./chinese-roberta-wwm-ext</span><br></pre></td></tr></table></figure>



<p>最终结论：方式四是相对最好的。</p>
<h1 id="NER任务"><a href="#NER任务" class="headerlink" title="NER任务"></a>NER任务</h1><h2 id="BiLSTM-CRF项目完整实现"><a href="#BiLSTM-CRF项目完整实现" class="headerlink" title="BiLSTM+CRF项目完整实现"></a>BiLSTM+CRF项目完整实现</h2><p>（1）整体步骤</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">整体实现思路(1-4数据数据预处理，5-8模型部分)</span>: <span class="string"></span></span><br><span class="line"><span class="attr">1、获取数据，例如通过人工数据标注或者第三方数据等。</span></span><br><span class="line"><span class="attr">2、对数据进行处理，构造训练数据</span></span><br><span class="line"><span class="attr">3、构建DataSet类</span></span><br><span class="line"><span class="attr">4、加载数据集</span> <span class="string">DataLoader</span></span><br><span class="line"><span class="attr">5、定义模型（embedding、线性层、CRF层）</span></span><br><span class="line"><span class="attr">6、初始化模型、loss、优化器、前向传播、反向传播、梯度更新</span></span><br><span class="line"><span class="attr">7、模型训练、评估</span></span><br><span class="line"><span class="attr">8、模型加载、测试</span></span><br></pre></td></tr></table></figure>

<p>（2）代码架构图</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250614195327232.png" alt="image-20250614195327232"></p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><h3 id="第一步-查看项目数据集"><a href="#第一步-查看项目数据集" class="headerlink" title="第一步: 查看项目数据集"></a>第一步: 查看项目数据集</h3><p>data_origin：原始数据</p>
<ul>
<li><p>四类内容：一般项目、出院情况、病史特点、诊疗经过</p>
</li>
<li><p>每类中有两种文件</p>
<p>（1）.txt结尾：<code>标注好的数据，包括其位置和类型</code></p>
</li>
</ul>
<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250612095407502.png" alt="image-20250612095407502" style="zoom: 67%;" />



<p>​				（2）txtoriginal.txt结尾：<code>原始文档</code></p>
<p>data：处理好的数据</p>
<ul>
<li><p>labels.json  实体类型文件</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&#123;</span></span><br><span class="line">  <span class="attr">&quot;治疗&quot;</span>: <span class="string">&quot;TREATMENT&quot;,</span></span><br><span class="line">  <span class="attr">&quot;身体部位&quot;</span>: <span class="string">&quot;BODY&quot;,</span></span><br><span class="line">  <span class="attr">&quot;症状和体征&quot;</span>: <span class="string">&quot;SIGNS&quot;,</span></span><br><span class="line">  <span class="attr">&quot;检查和检验&quot;</span>: <span class="string">&quot;CHECK&quot;,</span></span><br><span class="line">  <span class="attr">&quot;疾病和诊断&quot;</span>: <span class="string">&quot;DISEASE&quot;</span></span><br><span class="line"><span class="attr">&#125;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>tag2id.json  标注标签及ID</p>
</li>
</ul>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&#123;</span></span><br><span class="line"> <span class="attr">&quot;O&quot;</span>: <span class="string">0,</span></span><br><span class="line"> <span class="attr">&quot;B-TREATMENT&quot;</span>: <span class="string">1,</span></span><br><span class="line"> <span class="attr">&quot;I-TREATMENT&quot;</span>: <span class="string">2,</span></span><br><span class="line"> <span class="attr">&quot;B-BODY&quot;</span>: <span class="string">3,</span></span><br><span class="line"> <span class="attr">&quot;I-BODY&quot;</span>: <span class="string">4,</span></span><br><span class="line"> <span class="attr">&quot;B-SIGNS&quot;</span>: <span class="string">5,</span></span><br><span class="line"> <span class="attr">&quot;I-SIGNS&quot;</span>: <span class="string">6,</span></span><br><span class="line"> <span class="attr">&quot;B-CHECK&quot;</span>: <span class="string">7,</span></span><br><span class="line"> <span class="attr">&quot;I-CHECK&quot;</span>: <span class="string">8,</span></span><br><span class="line"> <span class="attr">&quot;B-DISEASE&quot;</span>: <span class="string">9,</span></span><br><span class="line"> <span class="attr">&quot;I-DISEASE&quot;</span>: <span class="string">10</span></span><br><span class="line"><span class="attr">&#125;</span></span><br></pre></td></tr></table></figure>



<h3 id="第二步-构造序列标注数据"><a href="#第二步-构造序列标注数据" class="headerlink" title="第二步: 构造序列标注数据"></a>第二步: 构造序列标注数据</h3><p><code>(1)根据标注数据 和 标签类型构建索引和标签的字典</code></p>
<p><code>(2)遍历样本数据，通过 索引和标签的字典，给相应位置打上标签，如果在字典里则将字典的value作为标签，否则就是0</code></p>
<p>难点：</p>
<p>(1)获取到所有的原始数据<br>通过os.walk()遍历原始数据所在的文件夹，得到所有数据文件</p>
<p>(2)获取原始数据对应的标注数据<br>通过文件名称的特点，将原始数据文件名中的.txtoriginal替换成”，就是对应的标注数据</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250910163559050.png" alt="image-20250910163559050"></p>
<p>（2）课堂知识补充</p>
<p>代码位置：P03_NER&#x2F;LSTM_CRF&#x2F;utils&#x2F;test.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取当前路径</span></span><br><span class="line">cur = os.getcwd()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;当前路径--&gt;<span class="subst">&#123;cur&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 切换路径</span></span><br><span class="line">os.chdir(<span class="string">&#x27;..&#x27;</span>)</span><br><span class="line">cur = os.getcwd()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;修改之后的路径--&gt;<span class="subst">&#123;cur&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 路径的拼接</span></span><br><span class="line">path = os.path.join(cur, <span class="string">&#x27;data/labels.json&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;拼接之后的路径--&gt;<span class="subst">&#123;path&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 读取json文件</span></span><br><span class="line"><span class="comment"># labels = json.load(open(path, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;))</span></span><br><span class="line"><span class="comment"># print(f&#x27;labels--&gt;&#123;labels&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如何设计，让这个代码在调用时，相对路径不随着调用位置变化而变化</span></span><br><span class="line">file_path = os.path.abspath(__file__)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;file_path--&gt;<span class="subst">&#123;file_path&#125;</span>&#x27;</span>)</span><br><span class="line">base_dir = os.path.dirname(file_path)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;base_dir--&gt;<span class="subst">&#123;base_dir&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 路径拼接</span></span><br><span class="line">path = os.path.join(base_dir, <span class="string">&#x27;../data/labels.json&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;拼接之后的路径2--&gt;<span class="subst">&#123;path&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 读取json文件</span></span><br><span class="line">labels = json.load(<span class="built_in">open</span>(path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;labels--&gt;<span class="subst">&#123;labels&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># os.walk的使用</span></span><br><span class="line">results = os.walk(os.path.join(base_dir, <span class="string">&#x27;../data_origin&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;results--&gt;<span class="subst">&#123;results&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> dir_path, dirs, files <span class="keyword">in</span> results:  <span class="comment"># 路径、文件夹（列表）、文件（列表）</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;*&#x27;</span>*<span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;dir_path--&gt;<span class="subst">&#123;dir_path&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;dirs--&gt;<span class="subst">&#123;dirs&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;files--&gt;<span class="subst">&#123;files&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>（3）代码</p>
<p>代码位置：P03_NER&#x2F;LSTM_CRF&#x2F;utils&#x2F;data_process.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取文件的绝对路径，然后根据这个路径去进行拼接</span></span><br><span class="line">base_dir = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;base_dir--&gt;<span class="subst">&#123;base_dir&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransferData</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 获取标签</span></span><br><span class="line">        self.lables_dict = json.load(<span class="built_in">open</span>(os.path.join(base_dir, <span class="string">&#x27;../data/labels.json&#x27;</span>), <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">        <span class="comment"># print(f&#x27;lables_dict--&gt;&#123;self.lables_dict&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># 原始数据路径</span></span><br><span class="line">        self.origin_path = os.path.join(base_dir, <span class="string">&#x27;../data_origin&#x27;</span>)</span><br><span class="line">        <span class="comment"># 处理后的数据路径</span></span><br><span class="line">        self.train_path = os.path.join(base_dir, <span class="string">&#x27;../data/train.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">transfer</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(self.train_path, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fw:</span><br><span class="line">            <span class="keyword">for</span> dirpath, dirnames, filenames <span class="keyword">in</span> os.walk(self.origin_path): <span class="comment"># 路径、文件夹、文件</span></span><br><span class="line">                <span class="comment"># print(f&#x27;dirpath--&gt;&#123;dirpath&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># print(f&#x27;dirnames--&gt;&#123;dirnames&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># print(f&#x27;filenames--&gt;&#123;filenames&#125;&#x27;)</span></span><br><span class="line">                <span class="keyword">for</span> filename <span class="keyword">in</span> filenames:</span><br><span class="line">                    <span class="keyword">if</span> <span class="string">&#x27;txtoriginal&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> filename:  <span class="comment"># 我们只处理包含txtoriginal文件</span></span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    <span class="comment"># print(f&#x27;filename--&gt;&#123;filename&#125;&#x27;)</span></span><br><span class="line">                    <span class="comment"># 获取原始文件路径</span></span><br><span class="line">                    file_path = os.path.join(dirpath, filename)</span><br><span class="line">                    <span class="comment"># print(f&#x27;file_path--&gt;&#123;file_path&#125;&#x27;)</span></span><br><span class="line">                    <span class="comment"># 获取标注文件路径</span></span><br><span class="line">                    label_file_path = file_path.replace(<span class="string">&#x27;.txtoriginal&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">                    <span class="comment"># print(f&#x27;label_file_path--&gt;&#123;label_file_path&#125;&#x27;)</span></span><br><span class="line">                    <span class="comment"># 调用封装的方法，处理标注数据，生成 索引和标签的字典</span></span><br><span class="line">                    label_dict = self.read_label_text(label_file_path)</span><br><span class="line">                    <span class="comment"># print(f&#x27;label_dict--&gt;&#123;label_dict&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 读取原始数据，然后进行遍历，给字符打上对应的标签</span></span><br><span class="line">                    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fr:</span><br><span class="line">                        content = fr.read().strip()</span><br><span class="line">                        <span class="comment"># 如果数据最后一位不是结束符号，则添加一个结束符号</span></span><br><span class="line">                        <span class="keyword">if</span> content[-<span class="number">1</span>] <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;。&#x27;</span>, <span class="string">&#x27;?&#x27;</span>, <span class="string">&#x27;!&#x27;</span>, <span class="string">&#x27;！&#x27;</span>, <span class="string">&#x27;？&#x27;</span>]:</span><br><span class="line">                            content += <span class="string">&#x27;。&#x27;</span></span><br><span class="line">                        <span class="comment"># 遍历原始数据，给字符打标签</span></span><br><span class="line">                        <span class="keyword">for</span> i, char <span class="keyword">in</span> <span class="built_in">enumerate</span>(content):</span><br><span class="line">                            label = label_dict.get(i, <span class="string">&#x27;O&#x27;</span>)</span><br><span class="line">                            final_str = char + <span class="string">&#x27;\t&#x27;</span> + label + <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">                            fw.write(final_str)</span><br><span class="line">                    <span class="comment"># print(&#x27;*&#x27;*50)</span></span><br><span class="line">                    <span class="comment"># break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">read_label_text</span>(<span class="params">self, label_file_path</span>):</span><br><span class="line">        label_dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(label_file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fr:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> fr:  <span class="comment"># 遍历每行数据</span></span><br><span class="line">                line = line.strip()</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="comment"># print(f&#x27;line--&gt;&#123;line&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># 获取索引和标签</span></span><br><span class="line">                line_list = line.split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">                <span class="comment"># print(f&#x27;line_list--&gt;&#123;line_list&#125;&#x27;)</span></span><br><span class="line">                start = <span class="built_in">int</span>(line_list[<span class="number">1</span>])</span><br><span class="line">                end = <span class="built_in">int</span>(line_list[<span class="number">2</span>])</span><br><span class="line">                label = self.lables_dict.get(line_list[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 进行for循环，生成索引和标签的字典</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(start, end+<span class="number">1</span>):</span><br><span class="line">                    <span class="keyword">if</span> i ` start:</span><br><span class="line">                        label_dict[i] = <span class="string">&#x27;B-&#x27;</span> + label</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        label_dict[i] = <span class="string">&#x27;I-&#x27;</span> + label</span><br><span class="line">                <span class="comment"># print(f&#x27;label_dict--&gt;&#123;label_dict&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># break</span></span><br><span class="line">        <span class="keyword">return</span> label_dict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    td = TransferData()</span><br><span class="line">    td.transfer()</span><br></pre></td></tr></table></figure>



<h3 id="第三步-编写Config类项目文件配置代码"><a href="#第三步-编写Config类项目文件配置代码" class="headerlink" title="第三步: 编写Config类项目文件配置代码"></a>第三步: 编写Config类项目文件配置代码</h3><p>（1）目的: 配置项目常用变量，一般这些变量属于不经常改变的，比如: 训练文件路径、模型训练次数、模型超参数等等</p>
<p>（2）代码</p>
<p>注意：可以修改成相对路径</p>
<p>代码位置：P03_NER&#x2F;LSTM_CRF&#x2F;config.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">base_dir = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line"><span class="comment"># print(f&#x27;base_dir--&gt;&#123;base_dir&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Config</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 如果是windows或者linux电脑（使用GPU）</span></span><br><span class="line">        self.device = <span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu:0&quot;</span></span><br><span class="line">        <span class="comment"># M1芯片及其以上的电脑（使用GPU）</span></span><br><span class="line">        <span class="comment"># self.device = &#x27;mps&#x27;</span></span><br><span class="line">        self.train_path = os.path.join(base_dir, <span class="string">&#x27;data/train.txt&#x27;</span>)</span><br><span class="line">        self.vocab_path = os.path.join(base_dir, <span class="string">&#x27;vocab/vocab.txt&#x27;</span>)</span><br><span class="line">        self.embedding_dim = <span class="number">300</span></span><br><span class="line">        self.epochs = <span class="number">5</span></span><br><span class="line">        self.batch_size = <span class="number">8</span></span><br><span class="line">        self.hidden_dim = <span class="number">256</span></span><br><span class="line">        self.lr = <span class="number">2e-3</span> <span class="comment"># crf的时候，lr可以小点，比如1e-3</span></span><br><span class="line">        self.dropout = <span class="number">0.2</span></span><br><span class="line">        self.model = <span class="string">&quot;BiLSTM_CRF&quot;</span> <span class="comment"># 可以只用&quot;BiLSTM&quot;</span></span><br><span class="line">        self.tag2id = json.load(<span class="built_in">open</span>(os.path.join(base_dir, <span class="string">&#x27;data/tag2id.json&#x27;</span>), <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = Config()</span><br><span class="line">    <span class="built_in">print</span>(conf.train_path)</span><br><span class="line">    <span class="built_in">print</span>(conf.tag2id)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="第四步-构建Dataset类与dataloader函数"><a href="#第四步-构建Dataset类与dataloader函数" class="headerlink" title="第四步: 构建Dataset类与dataloader函数"></a>第四步: 构建Dataset类与dataloader函数</h3><p>（1）整体思路</p>
<p>将句子进行拆分后，分别将文字和对应的标签放到一个列表中，然后将同一个句子的x,y列表组合成一个大的列表，然后放到三维列表中</p>
<p>将数据拆分成句子，同时将句子中的文字和对应的标签分别存到两个列表中，然后再放到一个三维列表中</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912093537529.png" alt="image-20250912093537529"></p>
<p>（2）构造(x,y)样本对，以及获取vocabs</p>
<p>代码：</p>
<p>代码位置：P03_NER&#x2F;LSTM_CRF&#x2F;utils&#x2F;common.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.config <span class="keyword">import</span> Config</span><br><span class="line"></span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造数据集：对train.txt进行处理，按照句子结束标点进行切分，得到x,y样本对，放到列表中</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_data</span>():</span><br><span class="line">    datas = []  <span class="comment"># 存储最终数据的三维列表</span></span><br><span class="line">    sample_x = []  <span class="comment"># 存储一个句子的文字</span></span><br><span class="line">    sample_y = []  <span class="comment"># 存储一个句子的标签</span></span><br><span class="line">    vocab_list = [<span class="string">&#x27;PAD&#x27;</span>, <span class="string">&#x27;UNK&#x27;</span>]  <span class="comment"># 存储所有的文字，默认添加PAD和UNK两个特殊字符</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历train.txt</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(conf.train_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>):</span><br><span class="line">        <span class="comment"># print(f&#x27;line--&gt;&#123;line&#125;&#x27;)</span></span><br><span class="line">        word_tag_list = line.strip(<span class="string">&#x27;\n&#x27;</span>).split(<span class="string">&#x27;\t&#x27;</span>)  <span class="comment"># 为了防止将数据中的空格处理掉，这里在使用strip的时候，需要指定删除的字符串</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(word_tag_list) != <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># print(f&#x27;word_tag_list--&gt;&#123;word_tag_list&#125;&#x27;)</span></span><br><span class="line">        word = word_tag_list[<span class="number">0</span>]</span><br><span class="line">        tag = word_tag_list[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 如果是空行，则跳过</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> word:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># 将文字和标签添加到列表中</span></span><br><span class="line">        sample_x.append(word)</span><br><span class="line">        sample_y.append(tag)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果遇到到一个句尾标点，则将sample_x和sample_y添加到datas中，并清空列表</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> [<span class="string">&#x27;。&#x27;</span>, <span class="string">&#x27;?&#x27;</span>, <span class="string">&#x27;!&#x27;</span>, <span class="string">&#x27;！&#x27;</span>, <span class="string">&#x27;？&#x27;</span>]:</span><br><span class="line">            datas.append([sample_x, sample_y])</span><br><span class="line">            <span class="comment"># print(f&#x27;datas--&gt;&#123;datas&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># 清空列表</span></span><br><span class="line">            sample_x = []</span><br><span class="line">            sample_y = []</span><br><span class="line">            <span class="comment"># break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果文字不在vocab_list中，则添加</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> vocab_list:</span><br><span class="line">            vocab_list.append(word)</span><br><span class="line">        <span class="comment"># print(f&#x27;vocab_list--&gt;&#123;vocab_list&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为了方便后续使用，需要将vocab_list保存到文件中</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(conf.vocab_path, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        fw.write(<span class="string">&#x27;\n&#x27;</span>.join(vocab_list))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将列表转成word2id的字典</span></span><br><span class="line">    word2id = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab_list)&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> datas, word2id</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    datas, word2id = build_data()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;datas--&gt;<span class="subst">&#123;datas[:<span class="number">5</span>]&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;word2id--&gt;<span class="subst">&#123;word2id&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;len(datas)--&gt;<span class="subst">&#123;<span class="built_in">len</span>(datas)&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;len(word2id)--&gt;<span class="subst">&#123;<span class="built_in">len</span>(word2id)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>（3）构造数据迭代器</p>
<p>步骤：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、构建Dataset类</span><br><span class="line">2、构建自定义函数collate<span class="built_in">_</span>fn()</span><br><span class="line">3、构建get<span class="built_in">_</span>data函数，获得数据迭代器</span><br></pre></td></tr></table></figure>

<p>代码：</p>
<p>调用过程：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912110823059.png" alt="image-20250912110823059"></p>
<p>代码位置：P03_NER&#x2F;LSTM_CRF&#x2F;utils&#x2F;data_loader.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence  <span class="comment"># 进行句子长度补齐或截断</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.utils.common <span class="keyword">import</span> build_data</span><br><span class="line"></span><br><span class="line">datas, word2id = build_data()</span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、构建Dataset类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NerDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, datas</span>):</span><br><span class="line">        <span class="built_in">super</span>(NerDataset, self).__init__()</span><br><span class="line">        self.datas = datas</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):  <span class="comment"># 获取数据集大小</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.datas)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):  <span class="comment"># 根据索引获取数据</span></span><br><span class="line">        <span class="comment"># print(f&#x27;index--&gt;&#123;index&#125;&#x27;)</span></span><br><span class="line">        sample = self.datas[index]</span><br><span class="line">        x = sample[<span class="number">0</span>]</span><br><span class="line">        y = sample[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_dataset</span>():</span><br><span class="line">    ner_dataset = NerDataset(datas)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;len(ner_dataset)--&gt;<span class="subst">&#123;ner_dataset.__len__()&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;len(ner_dataset)--&gt;<span class="subst">&#123;<span class="built_in">len</span>(ner_dataset)&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;ner_dataset[0]--&gt;<span class="subst">&#123;ner_dataset.__getitem__(<span class="number">0</span>)&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;ner_dataset[0]--&gt;<span class="subst">&#123;ner_dataset[<span class="number">0</span>]&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、构建自定义函数collate_fn()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch_data</span>):</span><br><span class="line">    <span class="comment"># print(f&#x27;batch_data--&gt;&#123;batch_data&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># 1)将文字和标签转成id</span></span><br><span class="line">    <span class="comment"># x_train = []</span></span><br><span class="line">    <span class="comment"># for data in batch_data:</span></span><br><span class="line">    <span class="comment">#     # 将文字转成id</span></span><br><span class="line">    <span class="comment">#     id_list = [word2id.get(word, 1) for word in data[0]]  # 如果这个文字不在字典中，则默认为1【UNK】</span></span><br><span class="line">    <span class="comment">#     # 转成tensor</span></span><br><span class="line">    <span class="comment">#     x_train.append(torch.tensor(id_list))</span></span><br><span class="line">    <span class="comment"># print(f&#x27;x_train--&gt;&#123;x_train&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># 简写</span></span><br><span class="line">    x_train = [torch.tensor([word2id.get(word, <span class="number">1</span>) <span class="keyword">for</span> word <span class="keyword">in</span> data[<span class="number">0</span>]]) <span class="keyword">for</span> data <span class="keyword">in</span> batch_data]</span><br><span class="line">    <span class="comment"># print(f&#x27;x_train--&gt;&#123;x_train&#125;&#x27;)</span></span><br><span class="line">    y_train = [torch.tensor([conf.tag2id.get(tag, <span class="number">0</span>) <span class="keyword">for</span> tag <span class="keyword">in</span> data[<span class="number">1</span>]]) <span class="keyword">for</span> data <span class="keyword">in</span> batch_data]</span><br><span class="line">    <span class="comment"># print(f&#x27;y_train--&gt;&#123;y_train&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2)统一样本长度</span></span><br><span class="line">    <span class="comment"># pad_sequence:可以对批次的样本进行统一长度处理， 统一长度的方式是以该批次中最长的样本为准，进行填充</span></span><br><span class="line">    <span class="comment"># batch_first=True，则返回的tensor的维度为[batch_size, max_len]</span></span><br><span class="line">    <span class="comment"># padding_value 当样本不足时，使用xx进行填充</span></span><br><span class="line">    input_ids = pad_sequence(x_train, batch_first=<span class="literal">True</span>, padding_value=<span class="number">0</span>)  <span class="comment"># 用PAD对应的0进行补齐</span></span><br><span class="line">    labels = pad_sequence(y_train, batch_first=<span class="literal">True</span>, padding_value=<span class="number">11</span>)  <span class="comment"># 用PAD对应的11进行补齐</span></span><br><span class="line">    <span class="comment"># print(f&#x27;input_ids--&gt;&#123;input_ids&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;labels--&gt;&#123;labels&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3）创建 attention_mask</span></span><br><span class="line">    attention_mask = (input_ids != <span class="number">0</span>).long()</span><br><span class="line">    <span class="comment"># print(f&#x27;attention_mask--&gt;&#123;attention_mask&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> input_ids, labels, attention_mask</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、构建get_data函数，获得数据迭代器</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>():</span><br><span class="line">    <span class="comment"># 构建训练数据集：基于原始数据集，进行8:2拆分</span></span><br><span class="line">    train_dataset = NerDataset(datas[:<span class="number">6300</span>])</span><br><span class="line">    <span class="comment"># 在写代码的时候，需要把shuffle设置为 Fasle; 在训练时，需要把shuffle设置为 True</span></span><br><span class="line">    train_dataloader = DataLoader(train_dataset,</span><br><span class="line">                                  batch_size=conf.batch_size,</span><br><span class="line">                                  shuffle=<span class="literal">False</span>,</span><br><span class="line">                                  collate_fn=collate_fn,</span><br><span class="line">                                  drop_last=<span class="literal">True</span></span><br><span class="line">                                  )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建验证数据集：基于原始数据集，进行8:2拆分</span></span><br><span class="line">    valid_dataset = NerDataset(datas[<span class="number">6300</span>:])</span><br><span class="line">    valid_dataloader = DataLoader(valid_dataset,</span><br><span class="line">                                  batch_size=conf.batch_size,</span><br><span class="line">                                  shuffle=<span class="literal">False</span>,</span><br><span class="line">                                  collate_fn=collate_fn,</span><br><span class="line">                                  drop_last=<span class="literal">True</span></span><br><span class="line">                                  )</span><br><span class="line">    <span class="keyword">return</span> train_dataloader, valid_dataloader</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># test_dataset()</span></span><br><span class="line">    train_dataloader, valid_dataloader = get_data()</span><br><span class="line">    <span class="comment"># for x in train_dataloader:</span></span><br><span class="line">    <span class="comment">#     print(f&#x27;x--&gt;&#123;x&#125;&#x27;)</span></span><br><span class="line">    <span class="comment">#     break</span></span><br><span class="line">    <span class="keyword">for</span> input_ids, labels, attention_mask <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;input_ids--&gt;<span class="subst">&#123;input_ids.shape&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;labels--&gt;<span class="subst">&#123;labels.shape&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;attention_mask--&gt;<span class="subst">&#123;attention_mask.shape&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<h2 id="BiLSTM-CRF模型搭建"><a href="#BiLSTM-CRF模型搭建" class="headerlink" title="BiLSTM+CRF模型搭建"></a>BiLSTM+CRF模型搭建</h2><h3 id="第一步-编写模型类的代码"><a href="#第一步-编写模型类的代码" class="headerlink" title="第一步: 编写模型类的代码"></a>第一步: 编写模型类的代码</h3><ul>
<li>构建BiLSTM模型</li>
</ul>
<p>（1）思路</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912150529230.png" alt="image-20250912150529230"></p>
<p>（2）代码</p>
<p>代码位置：P03_NER&#x2F;LSTM_CRF&#x2F;model&#x2F;BiLSTM.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.utils.data_loader <span class="keyword">import</span> word2id, get_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NERLSTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_dim, hidden_dim, dropout, tag2id, word2id</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        模型初始化</span></span><br><span class="line"><span class="string">        :param embedding_dim: 嵌入层维度 300</span></span><br><span class="line"><span class="string">        :param hidden_dim: 这里指的是BiLSTM模型输出时的维度，因为是双向LSTM，所以，隐藏层维度为 hidden_dim//2</span></span><br><span class="line"><span class="string">        :param dropout: 随机失活比例</span></span><br><span class="line"><span class="string">        :param tag2id: tag2id字典</span></span><br><span class="line"><span class="string">        :param word2id: word2id字典</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>(NERLSTM, self).__init__()</span><br><span class="line">        self.name = <span class="string">&#x27;BiLSTM&#x27;</span></span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.tag_size = <span class="built_in">len</span>(tag2id)</span><br><span class="line">        self.vocab_size = <span class="built_in">len</span>(word2id)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建词嵌入层</span></span><br><span class="line">        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)</span><br><span class="line">        <span class="comment"># 创建BiLSTM层</span></span><br><span class="line">        self.bilstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_dim // <span class="number">2</span>, bidirectional=<span class="literal">True</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 定义随机失活</span></span><br><span class="line">        self.dropout = nn.Dropout(self.dropout)</span><br><span class="line">        <span class="comment"># 定义线性层</span></span><br><span class="line">        self.linear = nn.Linear(self.hidden_dim, self.tag_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask</span>):</span><br><span class="line">        <span class="comment"># print(f&#x27;input_ids--&gt;&#123;input_ids&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># print(f&#x27;attention_mask--&gt;&#123;attention_mask&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># 送入词嵌入层</span></span><br><span class="line">        embedding = self.embedding(input_ids)</span><br><span class="line">        <span class="comment"># 送入BiLSTM层，这里直接输入embedding即可，模型会自动初始化(h0,c0)</span></span><br><span class="line">        bilstm_out, (h_n, c_n) = self.bilstm(embedding)</span><br><span class="line">        <span class="comment"># 送入随机失活层</span></span><br><span class="line">        dropout_out = self.dropout(bilstm_out)</span><br><span class="line">        <span class="comment"># 对位相乘，只保留有效位置的输出结果，将pad的部分置成0</span></span><br><span class="line">        attention_mask = attention_mask.unsqueeze(-<span class="number">1</span>)  <span class="comment"># 先对attention_mask进行升维</span></span><br><span class="line">        dropout_out = dropout_out * attention_mask</span><br><span class="line">        <span class="comment"># 送入线性层</span></span><br><span class="line">        linear_out = self.linear(dropout_out)</span><br><span class="line">        <span class="keyword">return</span> linear_out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = Config()</span><br><span class="line">    ner_lstm = NERLSTM(conf.embedding_dim, conf.hidden_dim, conf.dropout, conf.tag2id, word2id)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;ner_lstm--&gt;<span class="subst">&#123;ner_lstm&#125;</span>&#x27;</span>)</span><br><span class="line">    train_dataloader, valid_dataloader = get_data()</span><br><span class="line">    <span class="keyword">for</span> input_ids, labels, attention_mask <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        result = ner_lstm(input_ids, attention_mask)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;result--&gt;<span class="subst">&#123;result.shape&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>



<ul>
<li>构建BiLSTM_CRF模型</li>
</ul>
<p>（1）思路</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912161936723.png" alt="image-20250912161936723"></p>
<p>（2）代码</p>
<p>需要提前装一下包</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install TorchCRF</span><br></pre></td></tr></table></figure>



<blockquote>
<p>注意：由于CRF是自定义的损失函数，所以这里不再需要使用交叉熵损失等，直接使用crf封装好的方法即可，计算损失的函数定义为log_likelihood()。而在forward方法不再用于计算概率值，而是通过viterbi解码得到概率最大的标签路径。</p>
</blockquote>
<p>代码位置：P03_NER&#x2F;LSTM_CRF&#x2F;model&#x2F;BiLSTM_CRF.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> TorchCRF <span class="keyword">import</span> CRF</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.utils.data_loader <span class="keyword">import</span> word2id, get_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NERLSTM_CRF</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_dim, hidden_dim, dropout, tag2id, word2id</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        模型初始化</span></span><br><span class="line"><span class="string">        :param embedding_dim: 嵌入层维度 300</span></span><br><span class="line"><span class="string">        :param hidden_dim: 这里指的是BiLSTM模型输出时的维度，因为是双向LSTM，所以，隐藏层维度为 hidden_dim//2</span></span><br><span class="line"><span class="string">        :param dropout: 随机失活比例</span></span><br><span class="line"><span class="string">        :param tag2id: tag2id字典</span></span><br><span class="line"><span class="string">        :param word2id: word2id字典</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>(NERLSTM_CRF, self).__init__()</span><br><span class="line">        self.name = <span class="string">&#x27;BiLSTM_CRF&#x27;</span></span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.tag_size = <span class="built_in">len</span>(tag2id)</span><br><span class="line">        self.vocab_size = <span class="built_in">len</span>(word2id)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建词嵌入层</span></span><br><span class="line">        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)</span><br><span class="line">        <span class="comment"># 创建BiLSTM层</span></span><br><span class="line">        self.bilstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_dim // <span class="number">2</span>, bidirectional=<span class="literal">True</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 定义随机失活</span></span><br><span class="line">        self.dropout = nn.Dropout(self.dropout)</span><br><span class="line">        <span class="comment"># 定义线性层</span></span><br><span class="line">        self.linear = nn.Linear(self.hidden_dim, self.tag_size)</span><br><span class="line">        <span class="comment"># 创建CRF层</span></span><br><span class="line">        self.crf = CRF(self.tag_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取发射分数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_emission_score</span>(<span class="params">self, input_ids, attention_mask</span>):</span><br><span class="line">        <span class="comment"># print(f&#x27;input_ids--&gt;&#123;input_ids&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># print(f&#x27;attention_mask--&gt;&#123;attention_mask&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># 送入词嵌入层</span></span><br><span class="line">        embedding = self.embedding(input_ids)</span><br><span class="line">        <span class="comment"># 送入BiLSTM层，这里直接输入embedding即可，模型会自动初始化(h0,c0)</span></span><br><span class="line">        bilstm_out, (h_n, c_n) = self.bilstm(embedding)</span><br><span class="line">        <span class="comment"># 送入随机失活层</span></span><br><span class="line">        dropout_out = self.dropout(bilstm_out)</span><br><span class="line">        <span class="comment"># 对位相乘，只保留有效位置的输出结果，将pad的部分置成0</span></span><br><span class="line">        attention_mask = attention_mask.unsqueeze(-<span class="number">1</span>)  <span class="comment"># 先对attention_mask进行升维</span></span><br><span class="line">        dropout_out = dropout_out * attention_mask</span><br><span class="line">        <span class="comment"># 送入线性层</span></span><br><span class="line">        linear_out = self.linear(dropout_out)</span><br><span class="line">        <span class="keyword">return</span> linear_out</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算损失的函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">log_likelihood</span>(<span class="params">self, input_ids, labels, attention_mask</span>):</span><br><span class="line">        <span class="comment"># 获取发射分数</span></span><br><span class="line">        emission_score = self.get_emission_score(input_ids, attention_mask)</span><br><span class="line">        <span class="comment"># 计算损失【直接调用模型封装好的方法，将发射分数、labels、attention_mask输入进去，即可得到对数似然损失】</span></span><br><span class="line">        loss = -self.crf(emission_score, labels, attention_mask.<span class="built_in">bool</span>())</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测的函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask</span>):</span><br><span class="line">        <span class="comment"># 获取发射分数</span></span><br><span class="line">        emission_score = self.get_emission_score(input_ids, attention_mask)</span><br><span class="line">        <span class="comment"># 获取路径</span></span><br><span class="line">        predict_result = self.crf.viterbi_decode(emission_score, attention_mask.<span class="built_in">bool</span>())</span><br><span class="line">        <span class="keyword">return</span> predict_result</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = Config()</span><br><span class="line">    ner_lstm_crf = NERLSTM_CRF(conf.embedding_dim, conf.hidden_dim, conf.dropout, conf.tag2id, word2id)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;ner_lstm_crf--&gt;<span class="subst">&#123;ner_lstm_crf&#125;</span>&#x27;</span>)</span><br><span class="line">    train_dataloader, valid_dataloader = get_data()</span><br><span class="line">    <span class="keyword">for</span> input_ids, labels, attention_mask <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        loss = ner_lstm_crf.log_likelihood(input_ids, labels, attention_mask)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;loss--&gt;<span class="subst">&#123;loss&#125;</span>&#x27;</span>)</span><br><span class="line">        predict_result = ner_lstm_crf(input_ids, attention_mask)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;predict_result--&gt;<span class="subst">&#123;predict_result&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>



<h3 id="第二步-编写训练函数"><a href="#第二步-编写训练函数" class="headerlink" title="第二步: 编写训练函数"></a>第二步: 编写训练函数</h3><p>（1）基本步骤</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">训练函数基本步骤——</span></span><br><span class="line"><span class="attr">1.构建数据迭代器Dataloader(包括数据处理与构建数据源Dataset)</span></span><br><span class="line"><span class="attr">2.实例化模型</span></span><br><span class="line"><span class="attr">3.实例化损失函数对象</span></span><br><span class="line"><span class="attr">4.实例化优化器对象</span></span><br><span class="line"><span class="attr">5.定义打印日志参数</span></span><br><span class="line"><span class="attr">6.开始训练</span></span><br><span class="line"><span class="attr">6.1</span> <span class="string">实现外层大循环epoch</span></span><br><span class="line">    <span class="attr">6.2</span> <span class="string">将模型设置为训练模式</span></span><br><span class="line">    <span class="attr">6.3</span> <span class="string">内部遍历数据迭代器dataloader</span></span><br><span class="line">        <span class="attr">1）将数据送入模型得到输出结果</span></span><br><span class="line">        <span class="attr">2）计算损失</span></span><br><span class="line">        <span class="attr">3）梯度清零</span>: <span class="string">optimizer.zero_grad()</span></span><br><span class="line">        <span class="attr">4）反向传播(计算梯度)</span>: <span class="string">loss.backward()</span></span><br><span class="line">        <span class="attr">5）梯度更新(参数更新)</span>: <span class="string">optimizer.step()</span></span><br><span class="line">        <span class="attr">6）打印内部训练日志</span></span><br><span class="line">    <span class="attr">6.4</span> <span class="string">使用验证集进行模型评估【将模型设置为评估模式】</span></span><br><span class="line">    <span class="attr">6.5</span> <span class="string">保存模型: torch.save(model.state_dict(), &quot;model_path&quot;)</span></span><br><span class="line"><span class="attr">6.6</span> <span class="string">打印外部训练日志</span></span><br><span class="line"></span><br><span class="line"><span class="attr">验证函数基本步骤——</span></span><br><span class="line"><span class="attr">1.定义打印日志参数</span></span><br><span class="line"><span class="attr">2.将模型设置为评估模式</span></span><br><span class="line"><span class="attr">3.内部遍历数据迭代器dataloader</span></span><br><span class="line">  <span class="attr">3.1</span> <span class="string">将数据送入模型得到输出结果</span></span><br><span class="line">  <span class="attr">3.2</span> <span class="string">计算损失</span></span><br><span class="line">  <span class="attr">3.3</span> <span class="string">处理结果</span></span><br><span class="line">  <span class="attr">3.4</span> <span class="string">统计批次内指标</span></span><br><span class="line"><span class="attr">4.统计整体指标</span></span><br></pre></td></tr></table></figure>

<p>（2）代码</p>
<blockquote>
<p>注意：使用BiLSTM_CRF模型时，使用自定义的损失函数，封装在了log_likelihood()方法中。而forward()方法可以直接获取预测的标签类型。</p>
</blockquote>
<p>代码位置：P03_NER&#x2F;LSTM_CRF&#x2F;train.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score, classification_report, f1_score</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.model.BiLSTM <span class="keyword">import</span> NERLSTM</span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.model.BiLSTM_CRF <span class="keyword">import</span> NERLSTM_CRF</span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.utils.data_loader <span class="keyword">import</span> get_data, word2id</span><br><span class="line"></span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model2dev</span>(<span class="params">valid_dataloader, model, criterion=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    使用验证集，评估模型的效果【同时支持 BiLSTM和 BiLSTM_CRF】</span></span><br><span class="line"><span class="string">    :param valid_dataloader: 验证集的dataloader</span></span><br><span class="line"><span class="string">    :param model: 需要评估的模型实例</span></span><br><span class="line"><span class="string">    :param criterion: 损失函数对象，因为BiLSTM需要使用交叉熵损失，所以需要用到损失函数对象，而BiLSTM_CRF是不需要的，所以，需要设置默认值为None</span></span><br><span class="line"><span class="string">    :return: 评估指标</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 1.定义打印日志参数</span></span><br><span class="line">    avg_loss = <span class="number">0</span>  <span class="comment"># 保存平均损失</span></span><br><span class="line">    preds = []  <span class="comment"># 保存非padding位置的预测标签</span></span><br><span class="line">    golds = []  <span class="comment"># 保存非padding位置的真实标签</span></span><br><span class="line">    <span class="comment"># 2.将模型设置为评估模式</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="comment"># 3.内部遍历数据迭代器dataloader</span></span><br><span class="line">    <span class="keyword">for</span> index, (input_ids, labels, attention_mask) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(valid_dataloader)):</span><br><span class="line">        <span class="comment">#   3.1 将数据送入模型得到输出结果</span></span><br><span class="line">        <span class="comment"># 把数据放到gpu上</span></span><br><span class="line">        input_ids = input_ids.to(conf.device)</span><br><span class="line">        labels = labels.to(conf.device)</span><br><span class="line">        attention_mask = attention_mask.to(conf.device)</span><br><span class="line">        <span class="keyword">if</span>  conf.model ` <span class="string">&#x27;BiLSTM&#x27;</span>:</span><br><span class="line">            output = model(input_ids, attention_mask)</span><br><span class="line">            <span class="comment"># print(f&#x27;output--&gt;&#123;output.shape&#125;&#x27;)</span></span><br><span class="line">            <span class="comment">#   3.2 计算损失</span></span><br><span class="line">            <span class="comment"># 计算损失之前，需要将output形状转换为(batch_size * seq_len, tag_size)</span></span><br><span class="line">            output2 = output.view(-<span class="number">1</span>, <span class="built_in">len</span>(conf.tag2id))</span><br><span class="line">            <span class="comment"># print(f&#x27;output2--&gt;&#123;output2.shape&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># 同时需要将标签转换为(batch_size * seq_len)</span></span><br><span class="line">            labels2 = labels.view(-<span class="number">1</span>)</span><br><span class="line">            loss = criterion(output2, labels2)</span><br><span class="line">            <span class="comment"># print(f&#x27;loss--&gt;&#123;loss&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># 对损失进行累积操作</span></span><br><span class="line">            avg_loss += loss</span><br><span class="line">            <span class="comment">#   3.3 处理结果</span></span><br><span class="line">            predict = output.argmax(dim=-<span class="number">1</span>).tolist()</span><br><span class="line">            <span class="comment"># print(f&#x27;predict--&gt;&#123;predict&#125;&#x27;)</span></span><br><span class="line">        <span class="keyword">elif</span> conf.model ` <span class="string">&#x27;BiLSTM_CRF&#x27;</span>:</span><br><span class="line">            loss = model.log_likelihood(input_ids, labels, attention_mask).mean()</span><br><span class="line">            <span class="comment"># print(f&#x27;loss--&gt;&#123;loss&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># 对损失进行累积操作</span></span><br><span class="line">            avg_loss += loss</span><br><span class="line">            <span class="comment"># 3.3 处理结果</span></span><br><span class="line">            <span class="comment"># 这里的预测结果是通过维特比算法解码之后的结果，所以本身就是标签id</span></span><br><span class="line">            predict = model(input_ids, attention_mask)</span><br><span class="line">            <span class="comment"># print(f&#x27;predict--&gt;&#123;predict&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#   3.4 统计批次内指标</span></span><br><span class="line">        <span class="comment"># 将非padding位置的预测标签和真实标签保存起来，使用的方法是：通过input_ids进行非零判断，然后得到一个boolean的张量，然后直接对这个张量进行求和，就可以获取到每个句子的真实的长度，让让再通过这个长度，使用列表切片的方式，从标签中取出真实位置对应的标签</span></span><br><span class="line">        <span class="comment"># print(f&#x27;每个样本的真实长度--&gt;&#123;(input_ids&gt;0).sum(-1).tolist()&#125;&#x27;)  # [11, 13, 10, 14, 55, 22, 39, 25]</span></span><br><span class="line">        real_len = (input_ids&gt;<span class="number">0</span>).<span class="built_in">sum</span>(-<span class="number">1</span>).tolist()</span><br><span class="line">        <span class="comment"># 根据真实的句子长度，获取预测的标签</span></span><br><span class="line">        <span class="keyword">for</span> index, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(predict):</span><br><span class="line">            preds.extend(label[:real_len[index]])</span><br><span class="line">            <span class="comment"># break</span></span><br><span class="line">        <span class="comment"># 根据真实的句子长度，获取真实的标签</span></span><br><span class="line">        <span class="keyword">for</span> index, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(labels.tolist()):</span><br><span class="line">            golds.extend(label[:real_len[index]])</span><br><span class="line">            <span class="comment"># break</span></span><br><span class="line">        <span class="comment"># print(f&#x27;preds--&gt;&#123;preds&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># print(f&#x27;golds--&gt;&#123;golds&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># break</span></span><br><span class="line">    <span class="comment"># 4.统计整体指标</span></span><br><span class="line">    avg_loss = avg_loss / <span class="built_in">len</span>(valid_dataloader)</span><br><span class="line">    precision = precision_score(golds, preds, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">    recall = recall_score(golds, preds, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">    f1 = f1_score(golds, preds, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">    report = classification_report(golds, preds)</span><br><span class="line">    <span class="comment"># print(f&#x27;avg_loss--&gt;&#123;avg_loss&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;precision--&gt;&#123;precision&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;recall--&gt;&#123;recall&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;f1--&gt;&#123;f1&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;report--&gt;&#123;report&#125;&#x27;)</span></span><br><span class="line">    <span class="keyword">return</span> avg_loss, precision, recall, f1, report</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model2train</span>():</span><br><span class="line">    <span class="comment"># 1.构建数据迭代器Dataloader(包括数据处理与构建数据源Dataset)</span></span><br><span class="line">    train_dataloader, valid_dataloader = get_data()</span><br><span class="line">    <span class="comment"># 2.实例化模型</span></span><br><span class="line">    models = &#123;<span class="string">&#x27;BiLSTM&#x27;</span>: NERLSTM,</span><br><span class="line">              <span class="string">&#x27;BiLSTM_CRF&#x27;</span>: NERLSTM_CRF&#125;</span><br><span class="line">    model = models[conf.model](conf.embedding_dim, conf.hidden_dim, conf.dropout, conf.tag2id, word2id).to(conf.device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;model--&gt;<span class="subst">&#123;model&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 3.实例化损失函数对象</span></span><br><span class="line">    <span class="comment"># 忽略索引为11的标签，即[PAD]，效果就是这个表情不会参与损失的计算，也就是不产生梯度，不计算损失</span></span><br><span class="line">    <span class="comment"># 【原因padding部分并不是真实的标签，不应该影响训练】</span></span><br><span class="line">    criterion = nn.CrossEntropyLoss(ignore_index=<span class="number">11</span>)</span><br><span class="line">    <span class="comment"># 4.实例化优化器对象</span></span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=conf.lr)</span><br><span class="line">    <span class="comment"># 5.定义打印日志参数</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6.开始训练</span></span><br><span class="line">    best_f1 = -<span class="number">1.0</span></span><br><span class="line">    <span class="keyword">if</span> conf.model ` <span class="string">&#x27;BiLSTM&#x27;</span>:</span><br><span class="line">        <span class="comment"># 6.1 实现外层大循环epoch</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(conf.epochs):</span><br><span class="line">            <span class="comment"># 6.2 将模型设置为训练模式</span></span><br><span class="line">            model.train()</span><br><span class="line">            <span class="comment"># 6.3 内部遍历数据迭代器dataloader</span></span><br><span class="line">            <span class="keyword">for</span> index, (input_ids, labels, attention_mask) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(train_dataloader)):</span><br><span class="line">                <span class="comment"># print(f&#x27;input_ids--&gt;&#123;input_ids.shape&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># print(f&#x27;labels--&gt;&#123;labels.shape&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># print(f&#x27;attention_mask--&gt;&#123;attention_mask.shape&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># 1）将数据送入模型得到输出结果</span></span><br><span class="line">                <span class="comment"># 把数据放到gpu上</span></span><br><span class="line">                input_ids = input_ids.to(conf.device)</span><br><span class="line">                labels = labels.to(conf.device)</span><br><span class="line">                attention_mask = attention_mask.to(conf.device)</span><br><span class="line">                output = model(input_ids, attention_mask)</span><br><span class="line">                <span class="comment"># print(f&#x27;output--&gt;&#123;output.shape&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># 2）计算损失</span></span><br><span class="line">                <span class="comment"># 计算损失之前，需要将output形状转换为(batch_size * seq_len, tag_size)</span></span><br><span class="line">                output = output.view(-<span class="number">1</span>, <span class="built_in">len</span>(conf.tag2id))</span><br><span class="line">                <span class="comment"># 同时需要将标签转换为(batch_size * seq_len)</span></span><br><span class="line">                labels = labels.view(-<span class="number">1</span>)</span><br><span class="line">                loss = criterion(output, labels)</span><br><span class="line">                <span class="comment"># print(f&#x27;loss--&gt;&#123;loss&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># 3）梯度清零: optimizer.zero_grad()</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">                <span class="comment"># 4）反向传播(计算梯度): loss.backward()</span></span><br><span class="line">                loss.backward()</span><br><span class="line">                <span class="comment"># 5）梯度更新(参数更新): optimizer.step()</span></span><br><span class="line">                optimizer.step()</span><br><span class="line">                <span class="comment"># 6）打印内部训练日志</span></span><br><span class="line">                <span class="keyword">if</span> (index + <span class="number">1</span>) % <span class="number">200</span> ` <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;epoch:%04d------------loss:%f&#x27;</span> % (epoch, loss.item()))</span><br><span class="line">                    <span class="comment"># break</span></span><br><span class="line">            <span class="comment"># 6.4 使用验证集进行模型评估【将模型设置为评估模式】</span></span><br><span class="line">            avg_loss, precision, recall, f1, report = model2dev(valid_dataloader, model, criterion)</span><br><span class="line">            <span class="comment"># 6.5 保存模型: torch.save(model.state_dict(), &quot;model_path&quot;)</span></span><br><span class="line">            <span class="keyword">if</span> f1 &gt; best_f1:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;当前轮次为<span class="subst">&#123;epoch&#125;</span>轮次, 获取到新的最佳f1为<span class="subst">&#123;best_f1&#125;</span>, 保存模型&#x27;</span>)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;report--&gt;<span class="subst">&#123;report&#125;</span>&#x27;</span>)</span><br><span class="line">                torch.save(model.state_dict(), <span class="string">&#x27;save_model/bilstm_best.pth&#x27;</span>)</span><br><span class="line">                <span class="comment"># 更新best_f1</span></span><br><span class="line">                best_f1 = f1</span><br><span class="line">            <span class="comment"># break</span></span><br><span class="line">    <span class="keyword">elif</span> conf.model ` <span class="string">&#x27;BiLSTM_CRF&#x27;</span>:</span><br><span class="line">        <span class="comment"># 6.1 实现外层大循环epoch</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(conf.epochs):</span><br><span class="line">            <span class="comment"># 6.2 将模型设置为训练模式</span></span><br><span class="line">            model.train()</span><br><span class="line">            <span class="comment"># 6.3 内部遍历数据迭代器dataloader</span></span><br><span class="line">            <span class="keyword">for</span> index, (input_ids, labels, attention_mask) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(train_dataloader)):</span><br><span class="line">                <span class="comment"># print(f&#x27;input_ids--&gt;&#123;input_ids.shape&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># print(f&#x27;labels--&gt;&#123;labels.shape&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># print(f&#x27;attention_mask--&gt;&#123;attention_mask.shape&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># 1）将数据送入模型得到输出结果</span></span><br><span class="line">                <span class="comment"># 把数据放到gpu上</span></span><br><span class="line">                input_ids = input_ids.to(conf.device)</span><br><span class="line">                labels = labels.to(conf.device)</span><br><span class="line">                attention_mask = attention_mask.to(conf.device)</span><br><span class="line">                <span class="comment"># 2）计算损失</span></span><br><span class="line">                <span class="comment"># 直接调用log_likelihood方法</span></span><br><span class="line">                loss = model.log_likelihood(input_ids, labels, attention_mask).mean()</span><br><span class="line">                <span class="comment"># print(f&#x27;loss--&gt;&#123;loss&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># 3）梯度清零: optimizer.zero_grad()</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">                <span class="comment"># 4）反向传播(计算梯度): loss.backward()</span></span><br><span class="line">                loss.backward()</span><br><span class="line">                <span class="comment"># 5）梯度更新(参数更新): optimizer.step()</span></span><br><span class="line">                optimizer.step()</span><br><span class="line">                <span class="comment"># 6）打印内部训练日志</span></span><br><span class="line">                <span class="keyword">if</span> (index + <span class="number">1</span>) % <span class="number">200</span> ` <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;epoch:%04d------------loss:%f&#x27;</span> % (epoch, loss.item()))</span><br><span class="line">                    <span class="comment"># break</span></span><br><span class="line">            <span class="comment"># 6.4 使用验证集进行模型评估【将模型设置为评估模式】</span></span><br><span class="line">            avg_loss, precision, recall, f1, report = model2dev(valid_dataloader, model)</span><br><span class="line">            <span class="comment"># 6.5 保存模型: torch.save(model.state_dict(), &quot;model_path&quot;)</span></span><br><span class="line">            <span class="keyword">if</span> f1 &gt; best_f1:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;当前轮次为<span class="subst">&#123;epoch&#125;</span>轮次, 获取到新的最佳f1为<span class="subst">&#123;best_f1&#125;</span>, 保存模型&#x27;</span>)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;report--&gt;<span class="subst">&#123;report&#125;</span>&#x27;</span>)</span><br><span class="line">                torch.save(model.state_dict(), <span class="string">&#x27;save_model/bilstm_crf_best.pth&#x27;</span>)</span><br><span class="line">                <span class="comment"># 更新best_f1</span></span><br><span class="line">                best_f1 = f1</span><br><span class="line">            <span class="comment"># break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6.6 打印外部训练日志</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;训练结束, 总耗时: %.2f&#x27;</span> % (time.time() - start_time))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model2train()</span><br></pre></td></tr></table></figure>

<p><code>结论：</code></p>
<p>使用CRF之后，效果会比之前稍微好一些，但是训练成本会变高。</p>
<p><code>优化点：</code></p>
<p><code>（1）在正在训练时，将dataloader中的shuffle设置成true</code></p>
<p><code>（2）为了能够让训练集和验证集的样本分布一致，需要先将数据集打乱，然后再去进行划分</code></p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>():</span><br><span class="line">    <span class="comment"># 为了能够让训练集和验证集的样本分布一致，需要先将数据集打乱，然后再去进行划分</span></span><br><span class="line">    random.seed(<span class="number">66</span>)</span><br><span class="line">    random.shuffle(datas)  <span class="comment"># 数据会原地修改</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建训练数据集：基于原始数据集，进行8:2拆分</span></span><br><span class="line">    train_dataset = NerDataset(datas[:<span class="number">6300</span>])</span><br></pre></td></tr></table></figure>

<p>除了这种方式之外，也可以使用<code>分类采样</code>的方式。这种方式可以绝对类型上，训练集和验证集的分布是一致的。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250913114311551.png" alt="image-20250913114311551"></p>
<p><code>（3）训练优化：梯度裁剪</code>，它的作用是防止参数过大带来训练不稳定或者梯度爆炸</p>
<p>它实现的方式，当参数的范数大于了设置的最大范数时，所有参数会乘以缩放比例进行变小，缩放比例&#x3D;max_norm&#x2F;total_norm</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5）梯度更新(参数更新): optimizer.step()</span></span><br><span class="line">            <span class="comment"># 梯度裁剪</span></span><br><span class="line">            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=<span class="number">10</span>)</span><br><span class="line">            optimizer.step()</span><br></pre></td></tr></table></figure>

<p><code>（4）增加标注数据</code></p>
<p><code>（5）和规则进行结合，去做结果后处理</code></p>
<p>（6）日志保存</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250913120133235.png" alt="image-20250913120133235"></p>
<h3 id="第三步-编写模型预测函数"><a href="#第三步-编写模型预测函数" class="headerlink" title="第三步: 编写模型预测函数"></a>第三步: 编写模型预测函数</h3><p>（1）思路</p>
<ul>
<li>基本步骤：</li>
</ul>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">1.实例化模型</span></span><br><span class="line"><span class="attr">2.加载训练好的模型参数</span></span><br><span class="line"><span class="attr">3.处理数据</span></span><br><span class="line"><span class="attr">4.模型预测</span></span><br><span class="line"><span class="attr">5.结果处理</span></span><br></pre></td></tr></table></figure>

<ul>
<li>整体思路</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250913145456728.png" alt="image-20250913145456728"></p>
<p>（2）代码</p>
<p>代码位置：P03_NER&#x2F;LSTM_CRF&#x2F;ner_predict.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.model.BiLSTM <span class="keyword">import</span> NERLSTM</span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.model.BiLSTM_CRF <span class="keyword">import</span> NERLSTM_CRF</span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.utils.data_loader <span class="keyword">import</span> word2id</span><br><span class="line"></span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line">id2tag = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> conf.tag2id.items()&#125;</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;id2tag--&gt;<span class="subst">&#123;id2tag&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.实例化模型</span></span><br><span class="line">models = &#123;<span class="string">&#x27;BiLSTM&#x27;</span>: NERLSTM,</span><br><span class="line">          <span class="string">&#x27;BiLSTM_CRF&#x27;</span>: NERLSTM_CRF&#125;</span><br><span class="line">model = models[conf.model](conf.embedding_dim, conf.hidden_dim, conf.dropout, conf.tag2id, word2id).to(conf.device)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;model--&gt;<span class="subst">&#123;model&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 2.加载训练好的模型参数</span></span><br><span class="line"><span class="keyword">if</span> conf.model ` <span class="string">&#x27;BiLSTM&#x27;</span>:</span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">&#x27;save_model/bilstm_best.pth&#x27;</span>, weights_only=<span class="literal">True</span>))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">&#x27;save_model/bilstm_crf_best.pth&#x27;</span>, weights_only=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model2predict</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="comment"># 3.处理数据</span></span><br><span class="line">    <span class="comment"># 1)字符转id</span></span><br><span class="line">    text_id = [word2id.get(i, <span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> text]  <span class="comment"># 对于取不到的字符，用1代替</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;text_id--&gt;<span class="subst">&#123;text_id&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 2)转成张量</span></span><br><span class="line">    id_tensor = torch.tensor([text_id]).to(conf.device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;id_tensor--&gt;<span class="subst">&#123;id_tensor&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 3)构建 attention_mask</span></span><br><span class="line">    <span class="comment"># attention_mask = torch.tensor([[1] * len(text_id)]).to(conf.device)</span></span><br><span class="line">    attention_mask = (id_tensor != <span class="number">0</span>).long().to(conf.device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;attention_mask--&gt;<span class="subst">&#123;attention_mask&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 4.模型预测</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">if</span> conf.model ` <span class="string">&#x27;BiLSTM&#x27;</span>:</span><br><span class="line">            <span class="comment"># 送入模型</span></span><br><span class="line">            logits = model(id_tensor, attention_mask)</span><br><span class="line">            <span class="comment"># 通过argmax取到最大概率对应的索引</span></span><br><span class="line">            preds = logits.argmax(dim=-<span class="number">1</span>).squeeze(<span class="number">0</span>).tolist()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;preds--&gt;<span class="subst">&#123;preds&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 送入模型</span></span><br><span class="line">            preds = model(id_tensor, attention_mask)[<span class="number">0</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;preds--&gt;<span class="subst">&#123;preds&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 5.结果处理</span></span><br><span class="line">    <span class="comment"># 将id转成标签</span></span><br><span class="line">    predict_labels = [id2tag[i] <span class="keyword">for</span> i <span class="keyword">in</span> preds]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;predict_labels--&gt;<span class="subst">&#123;predict_labels&#125;</span>&#x27;</span>)</span><br><span class="line">    result_dict = extract_entities(text, predict_labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;result_dict--&gt;<span class="subst">&#123;result_dict&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result_dict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_entities</span>(<span class="params">text, tags</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    从带 BIO 标签的文本中提取实体。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        text (str): 原始文本</span></span><br><span class="line"><span class="string">        tags (List[str]): 对应文本的标签列表</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">        dict: 实体名称到类型的映射，如 &#123;&#x27;冠心病&#x27;: &#x27;DISEASE&#x27;, &#x27;糖尿病&#x27;: &#x27;DISEASE&#x27;&#125;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    entities = &#123;&#125;</span><br><span class="line">    current_entity = []</span><br><span class="line">    current_type = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> char, tag <span class="keyword">in</span> <span class="built_in">zip</span>(text, tags):</span><br><span class="line">        <span class="keyword">if</span> tag.startswith(<span class="string">&#x27;B-&#x27;</span>):</span><br><span class="line">            <span class="comment"># 开始一个新的实体</span></span><br><span class="line">            <span class="keyword">if</span> current_type <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 保存之前未完成的实体</span></span><br><span class="line">                entity_name = <span class="string">&#x27;&#x27;</span>.join(current_entity)</span><br><span class="line">                entities[entity_name] = current_type</span><br><span class="line">                current_entity = []</span><br><span class="line">                current_type = <span class="literal">None</span></span><br><span class="line">            current_type = tag[<span class="number">2</span>:]  <span class="comment"># 提取实体类型</span></span><br><span class="line">            current_entity.append(char)</span><br><span class="line">        <span class="keyword">elif</span> tag.startswith(<span class="string">&#x27;I-&#x27;</span>):</span><br><span class="line">            <span class="keyword">if</span> current_type <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> tag[<span class="number">2</span>:] ` current_type:</span><br><span class="line">                <span class="comment"># 继续当前实体</span></span><br><span class="line">                current_entity.append(char)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 结束当前实体</span></span><br><span class="line">            <span class="keyword">if</span> current_type <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                entity_name = <span class="string">&#x27;&#x27;</span>.join(current_entity)</span><br><span class="line">                entities[entity_name] = current_type</span><br><span class="line">                current_entity = []</span><br><span class="line">                current_type = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理最后可能未保存的实体</span></span><br><span class="line">    <span class="keyword">if</span> current_type <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        entity_name = <span class="string">&#x27;&#x27;</span>.join(current_entity)</span><br><span class="line">        entities[entity_name] = current_type</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> entities</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model2predict(<span class="string">&#x27;女性，88岁，农民，双滦区应营子村人，主因右髋部摔伤后疼痛肿胀，活动受限5小时于2016-10-29；11：12入院。&#x27;</span>)</span><br></pre></td></tr></table></figure>





<h1 id="关系抽取任务代码"><a href="#关系抽取任务代码" class="headerlink" title="关系抽取任务代码"></a>关系抽取任务代码</h1><h1 id="基于规则方式实现关系抽取"><a href="#基于规则方式实现关系抽取" class="headerlink" title="基于规则方式实现关系抽取"></a>基于规则方式实现关系抽取</h1><p>原理</p>
<p>基于规则实现关系抽取的原理 (主要分为三个步骤)</p>
<ul>
<li><p>第一步：定义需要抽取的关系集合，比如【夫妻关系，合作关系，，…】</p>
</li>
<li><p>第二步：遍历文章的每一句话，将每句话中非实体和非关系集合里面的词去掉</p>
</li>
<li><p>第三步：分别从实体集合和关系集合中，提取关系三元组</p>
</li>
</ul>
<p>代码实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> pseg</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要进行关系抽取的样本数据</span></span><br><span class="line">samples = [<span class="string">&quot;2014年1月8日，杨幂与刘恺威的婚礼在印度尼西亚巴厘岛举行&quot;</span>,</span><br><span class="line">           <span class="string">&quot;周星驰和吴孟达在《逃学威龙》中合作出演&quot;</span>,</span><br><span class="line">           <span class="string">&#x27;成龙出演了《警察故事》等多部经典电影&#x27;</span>]</span><br><span class="line"><span class="comment"># 定义需要抽取的关系集合</span></span><br><span class="line">relations2dict = &#123;<span class="string">&#x27;夫妻关系&#x27;</span>:[<span class="string">&#x27;结婚&#x27;</span>, <span class="string">&#x27;领证&#x27;</span>, <span class="string">&#x27;婚礼&#x27;</span>],</span><br><span class="line">                  <span class="string">&#x27;合作关系&#x27;</span>: [<span class="string">&#x27;搭档&#x27;</span>, <span class="string">&#x27;合作&#x27;</span>, <span class="string">&#x27;签约&#x27;</span>],</span><br><span class="line">                  <span class="string">&#x27;演员关系&#x27;</span>: [<span class="string">&#x27;出演&#x27;</span>, <span class="string">&#x27;角色&#x27;</span>, <span class="string">&#x27;主演&#x27;</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过jieba词性识别抽取出nr的实体和带有关系的词组</span></span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> samples:</span><br><span class="line">    entities = []  <span class="comment"># 存储实体</span></span><br><span class="line">    relations = []  <span class="comment"># 存储关系</span></span><br><span class="line">    move_index = []  <span class="comment"># 用来存储《》的索引</span></span><br><span class="line">    <span class="keyword">for</span> word, flag <span class="keyword">in</span> pseg.lcut(text):</span><br><span class="line">        <span class="keyword">if</span> flag ` <span class="string">&#x27;nr&#x27;</span>:  <span class="comment"># 如果是人名</span></span><br><span class="line">            entities.append(word)</span><br><span class="line">        <span class="keyword">elif</span> flag ` <span class="string">&#x27;x&#x27;</span>:  <span class="comment"># 如果是非语素词，则认为是《 或 》</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(move_index) ` <span class="number">0</span>:</span><br><span class="line">                move_index.append(text.index(word))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                move_index.append(text.index(word))</span><br><span class="line">                entities.append(text[move_index[<span class="number">0</span>] + <span class="number">1</span>: move_index[<span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> key, value <span class="keyword">in</span> relations2dict.items():</span><br><span class="line">                <span class="keyword">if</span> word <span class="keyword">in</span> value:</span><br><span class="line">                    relations.append(key)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;entities--&gt;<span class="subst">&#123;entities&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;relations--&gt;<span class="subst">&#123;relations&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分别从实体集合和关系集合中，提取关系三元组</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(entities) &gt;= <span class="number">2</span> <span class="keyword">and</span> <span class="built_in">len</span>(relations) &gt;= <span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;原始文本：&quot;</span>, text)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;提取结果：&#x27;</span>, entities[<span class="number">0</span>] + <span class="string">&#x27;-&gt;&#x27;</span> + relations[<span class="number">0</span>] + <span class="string">&#x27;-&gt;&#x27;</span> + entities[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;原始文本：&quot;</span>, text)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;不好意思，暂时没能从文本中提取出关系结果&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;*&#x27;</span>*<span class="number">80</span>)</span><br><span class="line">    <span class="comment"># break</span></span><br></pre></td></tr></table></figure>

<p>优缺点</p>
<ul>
<li>优点：实现简单、无需训练，小规模数据集容易实现.</li>
<li>缺点：<ul>
<li>无法解决复杂的场景</li>
<li>对跨领域的可移植性较差、人工标注成本较高以及召回率较低.</li>
</ul>
</li>
</ul>
<h1 id="Pipline方法实现关系抽取"><a href="#Pipline方法实现关系抽取" class="headerlink" title="Pipline方法实现关系抽取"></a>Pipline方法实现关系抽取</h1><h2 id="Pipeline方法的原理"><a href="#Pipeline方法的原理" class="headerlink" title="Pipeline方法的原理"></a>Pipeline方法的原理</h2><p>步骤：先完成实体抽取；再进行关系分类</p>
<p>方法</p>
<ul>
<li>CNN&#x2F;RNN及其变体</li>
<li>CNN多样性卷积核的特性有利于识别目标的结构特征，而RNN能充分考虑长距离词之间的依赖性，其记忆功能有利于识别序列</li>
</ul>
<h2 id="BiLSTM-Attention模型架构⭐️"><a href="#BiLSTM-Attention模型架构⭐️" class="headerlink" title="BiLSTM+Attention模型架构⭐️"></a>BiLSTM+Attention模型架构⭐️</h2><p>（1）模型架构</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250915105309661.png" alt="image-20250915105309661"></p>
<p>（2）注意力机制</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250915114909468.png" alt="image-20250915114909468"></p>
<h2 id="【实现】代码实现概览"><a href="#【实现】代码实现概览" class="headerlink" title="【实现】代码实现概览"></a>【实现】代码实现概览</h2><p>（1）整体步骤</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">整体实现思路(1-4数据数据预处理，5-8模型部分)</span>: <span class="string"></span></span><br><span class="line"><span class="attr">1、获取数据，例如通过人工数据标注或者第三方数据等。</span></span><br><span class="line"><span class="attr">2、对数据进行处理，构造训练数据</span></span><br><span class="line"><span class="attr">3、构建DataSet类</span></span><br><span class="line"><span class="attr">4、加载数据集</span> <span class="string">DataLoader</span></span><br><span class="line"><span class="attr">5、定义模型（embedding、线性层、CRF层）</span></span><br><span class="line"><span class="attr">6、初始化模型、loss、优化器、前向传播、反向传播、梯度更新</span></span><br><span class="line"><span class="attr">7、模型训练、评估</span></span><br><span class="line"><span class="attr">8、模型加载、测试</span></span><br></pre></td></tr></table></figure>

<p>（2）整体代码架构图</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250617003704466.png" alt="image-20250617003704466"></p>
<h2 id="【实现】数据预处理"><a href="#【实现】数据预处理" class="headerlink" title="【实现】数据预处理"></a>【实现】数据预处理</h2><h3 id="第一步-查看项目数据集-1"><a href="#第一步-查看项目数据集-1" class="headerlink" title="第一步: 查看项目数据集"></a>第一步: 查看项目数据集</h3><p>存放在data文件夹中</p>
<ul>
<li>关系类型文件  data&#x2F;relation2id.txt</li>
</ul>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">导演 0</span><br><span class="line">歌手 1</span><br><span class="line">作曲 2</span><br><span class="line">作词 3</span><br><span class="line">主演 4</span><br></pre></td></tr></table></figure>

<blockquote>
<p>relation2id.txt中包含5个类别标签, 文件共分为两列，第一列是类别名称，第二列为类别序号，中间空格符号隔开</p>
</blockquote>
<ul>
<li>训练数据集 data&#x2F;train.txt</li>
</ul>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">今晚会在哪里醒来 黄家强 歌手 《今晚会在哪里醒来》是黄家强的一首粤语歌曲，由何启弘作词，黄家强作曲编曲并演唱，收录于2007年08月01日发行的专辑《她他》中</span><br><span class="line"></span><br><span class="line">似水流年 许晓杰 作曲 似水流年，由著名作词家闫肃作词，著名音乐人许晓杰作曲，张烨演唱</span><br></pre></td></tr></table></figure>

<blockquote>
<p>train.txt 中包含18267行样本, 每行分为4列元素，元素中间用空格隔开，第一列元素为实体1、第二列元素为实体2、第三列元素为关系类型、第四列元素是原始文本</p>
</blockquote>
<ul>
<li>测试数据集 data&#x2F;test.txt</li>
</ul>
<blockquote>
<p>test.txt中包含5873行样本，数据样式通训练数据集</p>
</blockquote>
<h3 id="第二步-编写Config类项目文件配置代码"><a href="#第二步-编写Config类项目文件配置代码" class="headerlink" title="第二步: 编写Config类项目文件配置代码"></a>第二步: 编写Config类项目文件配置代码</h3><p>（1）目的: 配置项目常用变量，一般这些变量属于不经常改变的，比如: 训练文件路径、模型训练次数、模型超参数等等</p>
<p>（2）代码</p>
<p>代码位置：P04_RE&#x2F;Bilstm_Attention_RE&#x2F;config.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">base_dir = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;base_dir--&gt;<span class="subst">&#123;base_dir&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Config</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">        <span class="comment"># self.device = &quot;mps&quot;</span></span><br><span class="line">        self.train_data_path = os.path.join(base_dir, <span class="string">&#x27;data/train.txt&#x27;</span>)</span><br><span class="line">        self.test_data_path = os.path.join(base_dir, <span class="string">&#x27;data/test.txt&#x27;</span>)</span><br><span class="line">        self.rel_data_path = os.path.join(base_dir, <span class="string">&#x27;data/relation2id.txt&#x27;</span>)</span><br><span class="line">        self.embedding_dim = <span class="number">128</span> <span class="comment"># 词嵌入维度</span></span><br><span class="line">        self.pos_dim = <span class="number">32</span>  <span class="comment"># 位置嵌入维度</span></span><br><span class="line">        self.hidden_dim = <span class="number">200</span></span><br><span class="line">        self.epochs = <span class="number">50</span></span><br><span class="line">        self.batch_size = <span class="number">32</span></span><br><span class="line">        self.max_len = <span class="number">70</span>  <span class="comment"># 指定输入句子的最大长度</span></span><br><span class="line">        self.learning_rate = <span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = Config()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;train_data_path--&gt;<span class="subst">&#123;conf.train_data_path&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>



<h3 id="第三步-编写数据处理相关函数"><a href="#第三步-编写数据处理相关函数" class="headerlink" title="第三步: 编写数据处理相关函数"></a>第三步: 编写数据处理相关函数</h3><p>（1）整体思路</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250915154107971.png" alt="image-20250915154107971"></p>
<p>（2）代码</p>
<p>代码位置：P04_RE&#x2F;Bilstm_Attention_RE&#x2F;utils&#x2F;process.py</p>
<p>方法：</p>
<p>1）获取关系类型字典</p>
<p>2）处理数据，获取训练、测试数据集格式</p>
<p>3）文本数字化表示处理，得到word2id, id2word</p>
<p>4）把句子 words 转为 id 形式，并自动补全或截断为 max_len 长度。</p>
<p>5）负值相对编码处理</p>
<p>6）将id进行数字转换，防止为负数，而且进行句子长度的补齐或者截断</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> P04_RE.Bilstm_Attention_RE.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1）获取关系类型字典</span></span><br><span class="line">relation2id = &#123;&#125;</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(conf.rel_data_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        line = line.strip().split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        relation2id[line[<span class="number">0</span>]] = <span class="built_in">int</span>(line[<span class="number">1</span>])</span><br><span class="line"><span class="comment"># print(f&#x27;relation2id--&gt;&#123;relation2id&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2）处理数据，获取训练、测试数据集格式</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_txt_data</span>(<span class="params">file_path</span>):</span><br><span class="line">    datas = []  <span class="comment"># 存储每个文本中的字符</span></span><br><span class="line">    labels = []  <span class="comment"># 存储每个文本中的标签id</span></span><br><span class="line">    positionE1 = []  <span class="comment"># 存储每个文本中相对于实体1的位置</span></span><br><span class="line">    positionE2 = []  <span class="comment"># 存储每个文本中相对于实体2的位置</span></span><br><span class="line">    entities = []  <span class="comment"># 存储每个文本中的实体对，便于后续使用</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 优化点：为了保证每种关系类型的数量均衡，需要统计每个关系类型的样本数量，让每种类型的数量不超过2000</span></span><br><span class="line">    count_dict = &#123;k: <span class="number">0</span> <span class="keyword">for</span> k <span class="keyword">in</span> relation2id&#125;  <span class="comment"># 定义一个计数器，用于统计每种关系类型的数量，初始数量为0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            <span class="comment"># 1）对每个样本进行处理，按照空格进行切分，获取主实体、客实体、关系和原始文本。</span></span><br><span class="line">            line_list = line.strip().split(<span class="string">&#x27; &#x27;</span>, maxsplit=<span class="number">3</span>)  <span class="comment"># 需要使用maxsplit来指定最大的切割次数</span></span><br><span class="line">            <span class="comment"># print(f&#x27;line_list--&gt;&#123;line_list&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(line_list) != <span class="number">4</span>:  <span class="comment"># 如果切割的结果不等于4，则跳过</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> line_list[<span class="number">2</span>] <span class="keyword">not</span> <span class="keyword">in</span> relation2id:  <span class="comment"># 如果关系不在关系字典中，则跳过</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> count_dict[line_list[<span class="number">2</span>]] &gt;= <span class="number">2000</span>:  <span class="comment"># 如果当前关系类型的数量已经超过了2000，则跳过</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># 2）将关系通过relation2id字典转成具体的数值，然后放到labels列表中。</span></span><br><span class="line">            labels.append(relation2id[line_list[<span class="number">2</span>]])</span><br><span class="line">            <span class="comment"># 3）将主实体和客实体放到子列表中，然后放到entities列表中。</span></span><br><span class="line">            entities.append([line_list[<span class="number">0</span>], line_list[<span class="number">1</span>]])</span><br><span class="line">            <span class="comment"># 4）获取datas,positionE1和positionE2</span></span><br><span class="line">            sentence_str = line_list[<span class="number">3</span>]</span><br><span class="line">            <span class="comment"># 获取主实体的索引</span></span><br><span class="line">            e1_index = sentence_str.index(line_list[<span class="number">0</span>])</span><br><span class="line">            <span class="comment"># 获取客实体的索引</span></span><br><span class="line">            e2_index = sentence_str.index(line_list[<span class="number">1</span>])</span><br><span class="line">            <span class="comment"># 定义3个空列表，分别存储每个文本中的字符、主实体的相对位置编码和客实体的相对位置编码。</span></span><br><span class="line">            sentence, position1, position2 = [], [], []</span><br><span class="line">            <span class="keyword">for</span> index, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(sentence_str):</span><br><span class="line">                <span class="comment"># ①遍历原始文本，将每个字符存储到一个子列表中，遍历完成后再存到datas列表中。</span></span><br><span class="line">                sentence.append(word)</span><br><span class="line">                <span class="comment"># ②先获取主实体的索引，在遍历过程中使用原始索引-主实体的索引，获取相对于主实体的位置编码，存储到一个子列表中，遍历完成后再存到positionE1列表中。</span></span><br><span class="line">                position1.append(index - e1_index)</span><br><span class="line">                <span class="comment"># ③使用相同的方式，获取客实体的相对位置编码。</span></span><br><span class="line">                position2.append(index - e2_index)</span><br><span class="line">            <span class="comment"># ④将3个子列表分别放到datas,positionE1和positionE2列表中。</span></span><br><span class="line">            datas.append(sentence)</span><br><span class="line">            positionE1.append(position1)</span><br><span class="line">            positionE2.append(position2)</span><br><span class="line">            <span class="comment"># print(f&#x27;datas--&gt;&#123;datas&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># print(f&#x27;positionE1--&gt;&#123;positionE1&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># print(f&#x27;positionE2--&gt;&#123;positionE2&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># print(f&#x27;entities--&gt;&#123;entities&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># print(f&#x27;labels--&gt;&#123;labels&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 每处理完一个样本后，对对应的类型数量进行加一</span></span><br><span class="line">            count_dict[line_list[<span class="number">2</span>]] += <span class="number">1</span></span><br><span class="line">            <span class="comment"># break</span></span><br><span class="line">        <span class="keyword">return</span> datas, labels, positionE1, positionE2, entities</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3）文本数字化表示处理，得到 word2id, id2word</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_word_id</span>(<span class="params">file_path</span>):</span><br><span class="line">    datas, labels, positionE1, positionE2, entities = get_txt_data(file_path)</span><br><span class="line">    <span class="comment"># 初始化一个列表，用来存储所有的去重之后的字符</span></span><br><span class="line">    vocab_list = [<span class="string">&#x27;PAD&#x27;</span>, <span class="string">&#x27;UNK&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> datas:  <span class="comment"># 遍历所有的句子</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sentence:  <span class="comment"># 遍历句子中的每个字符</span></span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> vocab_list:  <span class="comment"># 如果字符不在vocab_list中，则添加到vocab_list中</span></span><br><span class="line">                vocab_list.append(word)</span><br><span class="line">    <span class="comment"># print(f&#x27;vocab_list--&gt;&#123;vocab_list&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成word2id、id2word的字典</span></span><br><span class="line">    word2id = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab_list)&#125;</span><br><span class="line">    id2word = &#123;i: word <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab_list)&#125;</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;word2id--&gt;<span class="subst">&#123;word2id&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;id2word--&gt;<span class="subst">&#123;id2word&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> word2id, id2word</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4）把句子 words 转为 id 形式，并自动补全或截断为 max_len 长度。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5）负值相对编码处理</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6）将id进行数字转换，防止为负数，而且进行句子长度的补齐或者截断</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># datas, labels, positionE1, positionE2, entities = get_txt_data(conf.train_data_path)</span></span><br><span class="line">    <span class="comment"># # print(f&#x27;labels--&gt;&#123;labels&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(Counter(labels))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    get_word_id(conf.train_data_path)</span><br></pre></td></tr></table></figure>





<h3 id="第四步-构建Dataset类与dataloader函数-1"><a href="#第四步-构建Dataset类与dataloader函数-1" class="headerlink" title="第四步: 构建Dataset类与dataloader函数"></a>第四步: 构建Dataset类与dataloader函数</h3><ul>
<li>步骤</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>构建Dataset类</span><br><span class="line"><span class="number">2.</span>构建自定义函数collate_fn()</span><br><span class="line"><span class="number">3.</span>构建get_loader_data函数，获得数据迭代器</span><br></pre></td></tr></table></figure>

<ul>
<li>代码</li>
</ul>
<p>代码位置：P04_RE&#x2F;Bilstm_Attention_RE&#x2F;utils&#x2F;data_loader.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P04_RE.Bilstm_Attention_RE.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P04_RE.Bilstm_Attention_RE.utils.process <span class="keyword">import</span> get_txt_data</span><br><span class="line"></span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.构建Dataset类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file_path</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyDataset, self).__init__()</span><br><span class="line">        self.datas, self.labels, self.positionE1, self.positionE2, self.entities = get_txt_data(file_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.datas)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.datas[index], self.labels[index], self.positionE1[index], self.positionE2[index], self.entities[index]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.构建自定义函数collate_fn()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch_data</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;batch_data--&gt;<span class="subst">&#123;batch_data&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.构建get_loader_data函数，获得数据迭代器</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_data_loader</span>():</span><br><span class="line">    <span class="comment"># 训练集</span></span><br><span class="line">    train_dataset = MyDataset(conf.train_data_path)</span><br><span class="line">    train_dataloader = DataLoader(train_dataset,</span><br><span class="line">                                  batch_size=conf.batch_size,</span><br><span class="line">                                  shuffle=<span class="literal">False</span>,  <span class="comment"># 在写代码的时候，需要把shuffle设置为 Fasle; 在训练时，需要把shuffle设置为 True</span></span><br><span class="line">                                  collate_fn=collate_fn,</span><br><span class="line">                                  drop_last=<span class="literal">True</span></span><br><span class="line">                                  )</span><br><span class="line">    <span class="comment"># 测试集</span></span><br><span class="line">    test_dataset = MyDataset(conf.test_data_path)</span><br><span class="line">    test_dataloader = DataLoader(test_dataset,</span><br><span class="line">                                 batch_size=conf.batch_size,</span><br><span class="line">                                 shuffle=<span class="literal">False</span>,</span><br><span class="line">                                 collate_fn=collate_fn,</span><br><span class="line">                                 drop_last=<span class="literal">True</span></span><br><span class="line">                                 )</span><br><span class="line">    <span class="keyword">return</span> train_dataloader, test_dataloader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># dt = MyDataset(conf.train_data_path)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;len(dt)--&gt;&#123;len(dt)&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;dt[0]--&gt;&#123;dt[0]&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    train_dataloader, test_dataloader = get_data_loader()</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;x--&gt;<span class="subst">&#123;x&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>



<h2 id="BiLSTM-Attention模型搭建"><a href="#BiLSTM-Attention模型搭建" class="headerlink" title="BiLSTM+Attention模型搭建"></a>BiLSTM+Attention模型搭建</h2><h3 id="第一步-编写模型类的代码-1"><a href="#第一步-编写模型类的代码-1" class="headerlink" title="第一步: 编写模型类的代码"></a>第一步: 编写模型类的代码</h3><p>（1）思路</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250916143847730.png" alt="image-20250916143847730"></p>
<p>（2）代码</p>
<p><code>注意：weight不能在模型定义时直接将batch_size写死，否则后期在使用时，每次传入的样本必须是相同的batch_size个。</code></p>
<p>代码位置：P04_RE&#x2F;Bilstm_Attention_RE&#x2F;model&#x2F;bilstm_atten.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P04_RE.Bilstm_Attention_RE.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P04_RE.Bilstm_Attention_RE.utils.data_loader <span class="keyword">import</span> word2id, get_data_loader</span><br><span class="line"><span class="keyword">from</span> P04_RE.Bilstm_Attention_RE.utils.process <span class="keyword">import</span> relation2id</span><br><span class="line"></span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BiLSTM_Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config, vocab_size, pos_size, tag_size</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param config: 配置文件对象</span></span><br><span class="line"><span class="string">        :param vocab_size: 文字词表的大小</span></span><br><span class="line"><span class="string">        :param pos_size: 位置编码的数量</span></span><br><span class="line"><span class="string">        :param tag_size: 标签的数量</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>(BiLSTM_Attention, self).__init__()</span><br><span class="line">        self.conf = config</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.pos_size = pos_size</span><br><span class="line">        self.tag_size = tag_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义嵌入层</span></span><br><span class="line">        self.wordEembed = nn.Embedding(self.vocab_size, self.conf.embedding_dim)</span><br><span class="line">        self.pos1Eembed = nn.Embedding(self.pos_size, self.conf.pos_dim)</span><br><span class="line">        self.pos2Eembed = nn.Embedding(self.pos_size, self.conf.pos_dim)</span><br><span class="line">        <span class="comment"># 定义双向LSTM层</span></span><br><span class="line">        self.bilstm = nn.LSTM(input_size=self.conf.embedding_dim + self.conf.pos_dim * <span class="number">2</span>,</span><br><span class="line">                              hidden_size=self.conf.hidden_dim//<span class="number">2</span>,</span><br><span class="line">                              batch_first=<span class="literal">True</span>,</span><br><span class="line">                              bidirectional=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 定义全连接层</span></span><br><span class="line">        self.fc = nn.Linear(self.conf.hidden_dim, self.tag_size)</span><br><span class="line">        <span class="comment"># 定义3个dropout层</span></span><br><span class="line">        self.dropout_embed = nn.Dropout(p=<span class="number">0.2</span>)</span><br><span class="line">        self.dropout_bilstm = nn.Dropout(p=<span class="number">0.2</span>)</span><br><span class="line">        self.dropout_attention = nn.Dropout(p=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义一个注意力参数，即wT,需要注意的是不能将batch_size写死，因为一旦写死之后，后续在训练和预测时，只能使用固定的 batch_size</span></span><br><span class="line">        <span class="comment"># 所以这里需要将第一维设置为1，在用到这个参数的时候，再进行动态的广播</span></span><br><span class="line">        self.att_weight = nn.Parameter(torch.FloatTensor(<span class="number">1</span>, <span class="number">1</span>, self.conf.hidden_dim)).to(self.conf.device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, sentence, pos1, pos2</span>):</span><br><span class="line">        <span class="comment"># 1)将sentence, pos1, pos2进行embedding，并将结果进行拼接</span></span><br><span class="line">        embeds = torch.concat([self.wordEembed(sentence), self.pos1Eembed(pos1), self.pos2Eembed(pos2)], dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># print(f&#x27;embeds--&gt;&#123;embeds.shape&#125;&#x27;)  # [2, 70, 192]</span></span><br><span class="line">        <span class="comment"># 2)将结果送入dropout层</span></span><br><span class="line">        embeds = self.dropout_embed(embeds)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3)将数据送入BiLSTM层</span></span><br><span class="line">        lstm_out, (h_n, c_n) = self.bilstm(embeds)</span><br><span class="line">        <span class="comment"># 4)将结果送入dropout层</span></span><br><span class="line">        lstm_out = self.dropout_bilstm(lstm_out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5)将bilstm层输出进行形状转变后，送入到注意力机制层</span></span><br><span class="line">        H = lstm_out.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        attention_out = self.attention(H)</span><br><span class="line">        <span class="comment"># print(f&#x27;attention_out--&gt;&#123;attention_out.shape&#125;&#x27;)  # [2, 200, 1]</span></span><br><span class="line">        <span class="comment"># 6)将结果送入dropout层</span></span><br><span class="line">        attention_out = self.dropout_attention(attention_out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 7)将注意力机制的结果先进行降维，然后送入全连接层</span></span><br><span class="line">        result = self.fc(attention_out.squeeze(-<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">self, H</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param H: [batch_size, hidden_dim, seq_len]  [2, 200, 70]</span></span><br><span class="line"><span class="string">        :return: [batch_size, hidden_dim, 1]  [2, 200, 1]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># 1)将H经过tanh激活函数，得到M</span></span><br><span class="line">        M = torch.tanh(H)</span><br><span class="line">        <span class="comment"># print(f&#x27;M--&gt;&#123;M.shape&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2)将wT和M进行相乘，然后送入softmax层，得到注意力权重</span></span><br><span class="line">        <span class="comment"># # 需要使用expand()对wT进行广播，将wT的形状变成 [batch_size, 1, 200]</span></span><br><span class="line">        <span class="comment"># wT = self.att_weight.expand(H.shape[0], 1, self.conf.hidden_dim)</span></span><br><span class="line">        <span class="comment"># print(f&#x27;wT--&gt;&#123;wT.shape&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># a = F.softmax(torch.bmm(wT, M), dim=-1)</span></span><br><span class="line">        <span class="comment"># print(f&#x27;a--&gt;&#123;a.shape&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 简写：使用matmul函数，实现att_weight的自动广播</span></span><br><span class="line">        a = F.softmax(torch.matmul(self.att_weight, M), dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># print(f&#x27;a--&gt;&#123;a.shape&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3)将a转置，然后再和H进行矩阵相乘</span></span><br><span class="line">        r = torch.bmm(H, a.transpose(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># print(f&#x27;r--&gt;&#123;r.shape&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4)返回经过tanh激活函数的结果</span></span><br><span class="line">        <span class="keyword">return</span> torch.tanh(r)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    vocab_size = <span class="built_in">len</span>(word2id)</span><br><span class="line">    pos_size = <span class="number">142</span></span><br><span class="line">    tag_size = <span class="built_in">len</span>(relation2id)</span><br><span class="line">    <span class="built_in">print</span>(vocab_size, pos_size, tag_size)</span><br><span class="line">    model = BiLSTM_Attention(conf, vocab_size, pos_size, tag_size).to(conf.device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;model--&gt;<span class="subst">&#123;model&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    train_dataloader, test_dataloader = get_data_loader()</span><br><span class="line">    <span class="keyword">for</span> datas, positionE1, positionE2, labels, entities <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        result = model(datas, positionE1, positionE2)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;result--&gt;<span class="subst">&#123;result&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="第二步-编写训练函数-1"><a href="#第二步-编写训练函数-1" class="headerlink" title="第二步: 编写训练函数"></a>第二步: 编写训练函数</h3><p>（1）基本步骤</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">训练函数基本步骤——</span></span><br><span class="line"><span class="attr">1.构建数据迭代器Dataloader(包括数据处理与构建数据源Dataset)</span></span><br><span class="line"><span class="attr">2.实例化模型</span></span><br><span class="line"><span class="attr">3.实例化损失函数对象</span></span><br><span class="line"><span class="attr">4.实例化优化器对象</span></span><br><span class="line"><span class="attr">5.定义打印日志参数</span></span><br><span class="line"><span class="attr">6.开始训练</span></span><br><span class="line"><span class="attr">6.1</span> <span class="string">实现外层大循环epoch</span></span><br><span class="line">    <span class="attr">6.2</span> <span class="string">将模型设置为训练模式</span></span><br><span class="line">    <span class="attr">6.3</span> <span class="string">内部遍历数据迭代器dataloader</span></span><br><span class="line">        <span class="attr">1）将数据送入模型得到输出结果</span></span><br><span class="line">        <span class="attr">2）计算损失</span></span><br><span class="line">        <span class="attr">3）梯度清零</span>: <span class="string">optimizer.zero_grad()</span></span><br><span class="line">        <span class="attr">4）反向传播(计算梯度)</span>: <span class="string">loss.backward()</span></span><br><span class="line">        <span class="attr">5）梯度更新(参数更新)</span>: <span class="string">optimizer.step()</span></span><br><span class="line">        <span class="attr">6）打印内部训练日志</span></span><br><span class="line">    <span class="attr">6.4</span> <span class="string">使用验证集进行模型评估【将模型设置为评估模式】</span></span><br><span class="line">    <span class="attr">6.5</span> <span class="string">保存模型: torch.save(model.state_dict(), &quot;model_path&quot;)</span></span><br><span class="line"><span class="attr">6.6</span> <span class="string">打印外部训练日志</span></span><br><span class="line"></span><br><span class="line"><span class="attr">验证函数基本步骤——</span></span><br><span class="line"><span class="attr">1.定义打印日志参数</span></span><br><span class="line"><span class="attr">2.将模型设置为评估模式</span></span><br><span class="line"><span class="attr">3.内部遍历数据迭代器dataloader</span></span><br><span class="line">  <span class="attr">3.1</span> <span class="string">将数据送入模型得到输出结果</span></span><br><span class="line">  <span class="attr">3.2</span> <span class="string">计算损失</span></span><br><span class="line">  <span class="attr">3.3</span> <span class="string">处理结果</span></span><br><span class="line">  <span class="attr">3.4</span> <span class="string">统计批次内指标</span></span><br><span class="line"><span class="attr">4.统计整体指标</span></span><br></pre></td></tr></table></figure>

<p>（2）代码</p>
<p><strong>1）课件中没有遵循标准的训练、验证流程，可以优化。</strong></p>
<p><strong>2）这次没有使用sklearn中的方法来计算指标，而是通过手动计算的，这种方法也可以熟悉一下</strong></p>
<p>代码位置：P04_RE&#x2F;Bilstm_Attention_RE&#x2F;train.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P04_RE.Bilstm_Attention_RE.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P04_RE.Bilstm_Attention_RE.model.bilstm_atten <span class="keyword">import</span> BiLSTM_Attention</span><br><span class="line"><span class="keyword">from</span> P04_RE.Bilstm_Attention_RE.utils.data_loader <span class="keyword">import</span> get_data_loader, word2id</span><br><span class="line"><span class="keyword">from</span> P04_RE.Bilstm_Attention_RE.utils.process <span class="keyword">import</span> relation2id</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model2dev</span>(<span class="params">test_dataloader, model, criterion</span>):</span><br><span class="line">    <span class="comment"># 1.定义打印日志参数</span></span><br><span class="line">    train_loss = <span class="number">0</span>  <span class="comment"># 每个批次的损失之和</span></span><br><span class="line">    total_iter_num = <span class="number">0</span>  <span class="comment"># 总的批次数</span></span><br><span class="line">    train_acc = <span class="number">0</span>  <span class="comment"># 预测正确的样本数</span></span><br><span class="line">    total_sample = <span class="number">0</span>  <span class="comment"># 总的样本数</span></span><br><span class="line">    <span class="comment"># 2.将模型设置为评估模式</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="comment"># 3.内部遍历数据迭代器dataloader</span></span><br><span class="line">    <span class="keyword">for</span> index, (datas, positionE1, positionE2, labels, entities) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(test_dataloader, desc=<span class="string">&#x27;模型评估&#x27;</span>)):</span><br><span class="line">        <span class="comment"># 3.1 将数据送入模型得到输出结果</span></span><br><span class="line">        output = model(datas, positionE1, positionE2)</span><br><span class="line">        <span class="comment"># 3.2 计算损失</span></span><br><span class="line">        loss = criterion(output, labels)</span><br><span class="line">        <span class="comment"># 3.3 处理结果</span></span><br><span class="line">        predict_labels = output.argmax(dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># predict_labels = torch.argmax(output, dim=-1)  # 另一种写法</span></span><br><span class="line">        <span class="comment"># 3.4 统计批次内指标</span></span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        train_acc += <span class="built_in">sum</span>(predict_labels ` labels)</span><br><span class="line">        total_iter_num += <span class="number">1</span></span><br><span class="line">        total_sample += labels.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 4.统计整体指标</span></span><br><span class="line">    dev_loss = train_loss / total_iter_num</span><br><span class="line">    dev_acc = train_acc / total_sample</span><br><span class="line">    <span class="keyword">return</span> dev_loss, dev_acc</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model2train</span>(<span class="params">conf, vocab_size, pos_size, tag_size</span>):</span><br><span class="line">    <span class="comment"># 1.构建数据迭代器Dataloader(包括数据处理与构建数据源Dataset)</span></span><br><span class="line">    train_dataloader, test_dataloader = get_data_loader()</span><br><span class="line">    <span class="comment"># 2.实例化模型</span></span><br><span class="line">    model = BiLSTM_Attention(conf, vocab_size, pos_size, tag_size).to(conf.device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;model--&gt;<span class="subst">&#123;model&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 3.实例化损失函数对象</span></span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="comment"># 4.实例化优化器对象</span></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=conf.learning_rate)</span><br><span class="line">    <span class="comment"># 5.定义打印日志参数</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    train_loss = <span class="number">0</span>  <span class="comment"># 每个批次的损失之和</span></span><br><span class="line">    total_iter_num = <span class="number">0</span>  <span class="comment"># 总的批次数</span></span><br><span class="line">    train_acc = <span class="number">0</span>  <span class="comment"># 预测正确的样本数</span></span><br><span class="line">    total_sample = <span class="number">0</span>  <span class="comment"># 总的样本数</span></span><br><span class="line">    best_acc = <span class="number">0</span>  <span class="comment"># 最佳准确率</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6.开始训练</span></span><br><span class="line">    <span class="comment"># 6.1 实现外层大循环epoch</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(conf.epochs):</span><br><span class="line">        <span class="comment"># 6.2 将模型设置为训练模式</span></span><br><span class="line">        model.train()</span><br><span class="line">        <span class="comment"># 6.3 内部遍历数据迭代器dataloader</span></span><br><span class="line">        <span class="keyword">for</span> index, (datas, positionE1, positionE2, labels, entities) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(train_dataloader, desc=<span class="string">&#x27;模型训练&#x27;</span>)):</span><br><span class="line">            <span class="comment"># 1）将数据送入模型得到输出结果</span></span><br><span class="line">            output = model(datas, positionE1, positionE2)</span><br><span class="line">            <span class="comment"># print(f&#x27;output--&gt;&#123;output.shape&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># 2）计算损失</span></span><br><span class="line">            loss = criterion(output, labels)</span><br><span class="line">            <span class="comment"># print(f&#x27;loss--&gt;&#123;loss.item()&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># 3）梯度清零: optimizer.zero_grad()</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            <span class="comment"># 4）反向传播(计算梯度): loss.backward()</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="comment"># 梯度裁剪</span></span><br><span class="line">            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=<span class="number">10</span>)</span><br><span class="line">            <span class="comment"># 5）梯度更新(参数更新): optimizer.step()</span></span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="comment"># 6）打印内部训练日志</span></span><br><span class="line">            train_loss += loss.item()</span><br><span class="line">            <span class="comment"># 计算预测正确的样本数</span></span><br><span class="line">            predict_labels = output.argmax(dim=-<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># print(f&#x27;predict_labels--&gt;&#123;predict_labels&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># print(f&#x27;labels--&gt;&#123;labels&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># print(f&#x27;predict_labels`labels--&gt;&#123;predict_labels ` labels&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># print(f&#x27;sum(predict_labels`labels)--&gt;&#123;sum(predict_labels ` labels)&#125;&#x27;)</span></span><br><span class="line">            train_acc += <span class="built_in">sum</span>(predict_labels ` labels)</span><br><span class="line">            total_iter_num += <span class="number">1</span></span><br><span class="line">            total_sample += labels.shape[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 每隔50次，打印日志</span></span><br><span class="line">            <span class="keyword">if</span> (index + <span class="number">1</span>) % <span class="number">50</span> ` <span class="number">0</span>:</span><br><span class="line">                loss_avg = train_loss / total_iter_num</span><br><span class="line">                acc_avg = train_acc / total_sample</span><br><span class="line">                end_time = time.time()</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;轮次：<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>，，训练损失：<span class="subst">&#123;loss_avg:<span class="number">.4</span>f&#125;</span>，训练准确率：<span class="subst">&#123;acc_avg:<span class="number">.4</span>f&#125;</span>，用时：<span class="subst">&#123;end_time - start_time:<span class="number">.2</span>f&#125;</span>秒&#x27;</span>)</span><br><span class="line">                <span class="comment"># break</span></span><br><span class="line">        <span class="comment"># 6.4 使用验证集进行模型评估【将模型设置为评估模式】</span></span><br><span class="line">        dev_loss, dev_acc = model2dev(test_dataloader, model, criterion)</span><br><span class="line">        <span class="comment"># print(f&#x27;dev_loss--&gt;&#123;dev_loss:.4f&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># print(f&#x27;dev_acc--&gt;&#123;dev_acc:.4f&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># 6.5 保存模型: torch.save(model.state_dict(), &quot;model_path&quot;)</span></span><br><span class="line">        <span class="keyword">if</span> dev_acc &gt; best_acc:</span><br><span class="line">            best_acc = dev_acc</span><br><span class="line">            torch.save(model.state_dict(), <span class="string">&quot;save_model/bilstm_atten_best.pth&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;保存模型，准确率：<span class="subst">&#123;best_acc:<span class="number">.4</span>f&#125;</span>, 平均损失：<span class="subst">&#123;dev_loss:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="comment"># break</span></span><br><span class="line">    <span class="comment"># 6.6 打印外部训练日志</span></span><br><span class="line">    end_time = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;训练时间:<span class="subst">&#123;end_time - start_time:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = Config()</span><br><span class="line">    vocab_size = <span class="built_in">len</span>(word2id)</span><br><span class="line">    pos_size = <span class="number">142</span></span><br><span class="line">    tag_size = <span class="built_in">len</span>(relation2id)</span><br><span class="line">    model2train(conf, vocab_size, pos_size, tag_size)</span><br></pre></td></tr></table></figure>



<h3 id="第三步-编写模型预测函数-1"><a href="#第三步-编写模型预测函数-1" class="headerlink" title="第三步: 编写模型预测函数"></a>第三步: 编写模型预测函数</h3><p>（1）步骤</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">1.实例化模型</span></span><br><span class="line"><span class="attr">2.加载模型参数</span></span><br><span class="line"><span class="attr">3.处理数据</span></span><br><span class="line"><span class="attr">4.模型预测</span></span><br><span class="line"><span class="attr">5.结果解析</span></span><br></pre></td></tr></table></figure>

<p>（2）代码</p>
<p>代码位置：P04_RE&#x2F;Bilstm_Attention_RE&#x2F;predict.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P04_RE.Bilstm_Attention_RE.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P04_RE.Bilstm_Attention_RE.model.bilstm_atten <span class="keyword">import</span> BiLSTM_Attention</span><br><span class="line"><span class="keyword">from</span> P04_RE.Bilstm_Attention_RE.utils.data_loader <span class="keyword">import</span> word2id</span><br><span class="line"><span class="keyword">from</span> P04_RE.Bilstm_Attention_RE.utils.process <span class="keyword">import</span> relation2id, sentence_padding, position_padding</span><br><span class="line"></span><br><span class="line">id2relation = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> relation2id.items()&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.实例化模型</span></span><br><span class="line">conf = Config()</span><br><span class="line">vocab_size = <span class="built_in">len</span>(word2id)</span><br><span class="line">pos_size = <span class="number">142</span></span><br><span class="line">tag_size = <span class="built_in">len</span>(relation2id)</span><br><span class="line">model = BiLSTM_Attention(conf, vocab_size, pos_size, tag_size).to(conf.device)</span><br><span class="line"><span class="comment"># print(f&#x27;model--&gt;&#123;model&#125;&#x27;)</span></span><br><span class="line"><span class="comment"># 2.加载模型参数</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;save_model/bilstm_atten_best.pth&#x27;</span>, weights_only=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model2predict</span>(<span class="params">sample, entity1, entity2</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param sample:  样本（句子）</span></span><br><span class="line"><span class="string">    :param entity1: 主实体</span></span><br><span class="line"><span class="string">    :param entity2: 客实体</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 3.处理数据</span></span><br><span class="line">    <span class="comment"># 3.1 通过遍历，获取中间数据</span></span><br><span class="line">    <span class="comment"># 获取主实体的索引</span></span><br><span class="line">    e1_index = sample.index(entity1)</span><br><span class="line">    <span class="comment"># 获取客实体的索引</span></span><br><span class="line">    e2_index = sample.index(entity2)</span><br><span class="line">    <span class="comment"># 定义3个空列表，分别存储每个文本中的字符、主实体的相对位置编码和客实体的相对位置编码。</span></span><br><span class="line">    sentence, position1, position2 = [], [], []</span><br><span class="line">    <span class="keyword">for</span> index, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(sample):</span><br><span class="line">        <span class="comment"># ①遍历原始文本，将每个字符存储到一个子列表中，遍历完成后再存到datas列表中。</span></span><br><span class="line">        sentence.append(word)</span><br><span class="line">        <span class="comment"># ②先获取主实体的索引，在遍历过程中使用原始索引-主实体的索引，获取相对于主实体的位置编码，存储到一个子列表中，遍历完成后再存到positionE1列表中。</span></span><br><span class="line">        position1.append(index - e1_index)</span><br><span class="line">        <span class="comment"># ③使用相同的方式，获取客实体的相对位置编码。</span></span><br><span class="line">        position2.append(index - e2_index)</span><br><span class="line">    <span class="comment"># print(f&#x27;sentence--&gt;&#123;sentence&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;position1--&gt;&#123;position1&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;position2--&gt;&#123;position2&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.2 将字符转成id，且将负数转成正数，同时对齐长度</span></span><br><span class="line">    sentece_ids = sentence_padding(sentence, word2id)</span><br><span class="line">    position1_ids = position_padding(position1)</span><br><span class="line">    position2_ids = position_padding(position2)</span><br><span class="line">    <span class="comment"># print(f&#x27;sentece_ids--&gt;&#123;sentece_ids&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;position1_ids--&gt;&#123;position1_ids&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;position2_ids--&gt;&#123;position2_ids&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.3 将数据转成张量 注意：不要忘了给数据添加一个batch_size这个维度！！！</span></span><br><span class="line">    datas_tensor = torch.tensor([sentece_ids], dtype=torch.long).to(conf.device)</span><br><span class="line">    positionE1_tensor = torch.tensor([position1_ids], dtype=torch.long).to(conf.device)</span><br><span class="line">    positionE2_tensor = torch.tensor([position2_ids], dtype=torch.long).to(conf.device)</span><br><span class="line">    <span class="comment"># print(f&#x27;datas_tensor--&gt;&#123;datas_tensor&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;positionE1_tensor--&gt;&#123;positionE1_tensor&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;positionE2_tensor--&gt;&#123;positionE2_tensor&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.模型预测</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        result = model(datas_tensor, positionE1_tensor, positionE2_tensor)</span><br><span class="line">        <span class="comment"># 5.结果解析</span></span><br><span class="line">        predict_label = torch.argmax(result[<span class="number">0</span>], dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># print(f&#x27;predict_label--&gt;&#123;predict_label&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># 将id解析成标签名称</span></span><br><span class="line">        final_label = id2relation[predict_label.item()]</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;输入的句子：<span class="subst">&#123;sample&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;主实体：<span class="subst">&#123;entity1&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;客实体：<span class="subst">&#123;entity2&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;预测结果：<span class="subst">&#123;final_label&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    sample = <span class="string">&#x27;2011年，担任爱情片《失恋33天》的编剧，该片改编自鲍鲸鲸的同名小说，由文章、白百何共同主演6&#x27;</span></span><br><span class="line">    entity1 = <span class="string">&#x27;失恋33天&#x27;</span></span><br><span class="line">    entity2 = <span class="string">&#x27;白百何&#x27;</span></span><br><span class="line">    model2predict(sample, entity1, entity2)</span><br></pre></td></tr></table></figure>



<h2 id="BiLSTM-Attention模型可以做哪些优化来改善模型性能？"><a href="#BiLSTM-Attention模型可以做哪些优化来改善模型性能？" class="headerlink" title="BiLSTM+Attention模型可以做哪些优化来改善模型性能？"></a>BiLSTM+Attention模型可以做哪些优化来改善模型性能？</h2><p><strong>1）模型优化</strong></p>
<ul>
<li>句子嵌入方式：可以使用jieba分词得到词语，然后再使用词语的方式进行嵌入。</li>
<li>替换BiLSTM：将BiLSTM替换成BERT&#x2F;RoBERTa等这种预训练模型或BiGRU去做嵌入，看是否可以提供模型的语义表达能力。</li>
<li>多头注意力机制：借鉴Transformer中多头注意力机制，将单一注意力拆分到多个子空间，去捕捉不同维度的语义信息。</li>
<li>修改注意力机制的方式：使用transformer中注意力机制的计算方式或者先进行从concat再经过linear层的方式等，来计算注意力机制，看模型的性能效果。</li>
<li>调整随机失活层：调整随机失活层的位置、有无或随机失活比例，来观察模型的性能变化。</li>
</ul>
<p><strong>2）训练过程的优化</strong></p>
<ul>
<li>shuffle设置：注意在真正训练时，需要将dataloader中的shuffle设置为True</li>
<li>梯度裁剪：在反向传播时对梯度进行裁剪，防止梯度消失或爆炸。</li>
<li>早停机制：监控验证集上F1值或其他关键指标，如果连续多个epoch未提升或者开始下降，则提前终止训练。</li>
</ul>
<p><strong>3）训练数据优化</strong></p>
<ul>
<li>通过过采样或欠采样来解决样本不均衡问题</li>
<li>通过同义词替换、回译、实体替换等方法来扩充数据集。或者直接使用大模型进行训练样本的生成。</li>
</ul>
<h2 id="Pipeline方法的优缺点"><a href="#Pipeline方法的优缺点" class="headerlink" title="Pipeline方法的优缺点"></a>Pipeline方法的优缺点</h2><ul>
<li>优点：<ul>
<li>易于实现，实体模型和关系模型使用独立的数据集，不需要同时标注实体和关系的数据集.</li>
<li>两者相互独立，若关系抽取模型没训练好不会影响到实体抽取.</li>
</ul>
</li>
<li>缺点：<ul>
<li>关系和实体两者是紧密相连的，互相之间的联系没有捕捉到.</li>
<li>上游 NER 的错误会直接影响下游关系抽取，容易造成误差积累.</li>
<li>BiLSTM+Attention模型难以处理EPO问题</li>
</ul>
</li>
</ul>
<h1 id="Joint方法实现关系抽取"><a href="#Joint方法实现关系抽取" class="headerlink" title="Joint方法实现关系抽取"></a>Joint方法实现关系抽取</h1><h2 id="Joint方法的原理"><a href="#Joint方法的原理" class="headerlink" title="Joint方法的原理"></a>Joint方法的原理</h2><p>（1）概念：通过修改标注方法和模型结构直接输出文本中包含的(ei,rk,ej)三元组</p>
<p>（2）类型</p>
<ul>
<li>参数共享的联合模型【修改模型结构】<ul>
<li>主体、客体和关系的抽取不是严格同步进行的 (<strong>通常是依次执行，但是某些情况下也可以其中两个任务一起进行</strong>) ，各个过程都可以得到一个loss值，<code>整个模型的loss是各过程loss值之和.</code></li>
</ul>
</li>
</ul>
<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/4-1-1.png" alt="img" style="zoom:67%;" />

<ul>
<li><p>联合解码的联合模型【修改标注方法】</p>
<ul>
<li>主体、客体和关系的抽取是同时进行的，通过一个模型直接得到SPO三元组.</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250918100622672.png" alt="image-20250918100622672"></p>
</li>
</ul>
<h2 id="Casrel模型架构"><a href="#Casrel模型架构" class="headerlink" title="Casrel模型架构"></a>Casrel模型架构</h2><p>架构图：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250918105237461.png" alt="image-20250918105237461"></p>
<p>详细实现方式：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/Casrel%E6%A8%A1%E5%9E%8B.png" alt="Casrel模型"></p>
<h3 id="【实现】代码实现概览-1"><a href="#【实现】代码实现概览-1" class="headerlink" title="【实现】代码实现概览"></a>【实现】代码实现概览</h3><p>（1）整体步骤</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">整体实现思路(1-4数据数据预处理，5-8模型部分)</span>: <span class="string"></span></span><br><span class="line"><span class="attr">1、获取数据，例如通过人工数据标注或者第三方数据等。</span></span><br><span class="line"><span class="attr">2、对数据进行处理，构造训练数据【合并在第4步】</span></span><br><span class="line"><span class="attr">3、构建DataSet类</span></span><br><span class="line"><span class="attr">4、加载数据集</span> <span class="string">DataLoader</span></span><br><span class="line"><span class="attr">5、定义模型</span></span><br><span class="line"><span class="attr">6、初始化模型、loss、优化器、前向传播、反向传播、梯度更新</span></span><br><span class="line"><span class="attr">7、模型训练、评估</span></span><br><span class="line"><span class="attr">8、模型加载、测试</span></span><br></pre></td></tr></table></figure>

<p>（2）整体代码架构图</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250617143211118.png" alt="image-20250617143211118"></p>
<h3 id="【实现】数据预处理-1"><a href="#【实现】数据预处理-1" class="headerlink" title="【实现】数据预处理"></a>【实现】数据预处理</h3><p>第一步: 查看项目数据集</p>
<p>存放在data文件夹中</p>
<ul>
<li>关系类型文件:  data&#x2F;relation.json</li>
</ul>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;0&quot;</span><span class="punctuation">:</span> <span class="string">&quot;出品公司&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;1&quot;</span><span class="punctuation">:</span> <span class="string">&quot;国籍&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;2&quot;</span><span class="punctuation">:</span> <span class="string">&quot;出生地&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;3&quot;</span><span class="punctuation">:</span> <span class="string">&quot;民族&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;4&quot;</span><span class="punctuation">:</span> <span class="string">&quot;出生日期&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;5&quot;</span><span class="punctuation">:</span> <span class="string">&quot;毕业院校&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;6&quot;</span><span class="punctuation">:</span> <span class="string">&quot;歌手&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;7&quot;</span><span class="punctuation">:</span> <span class="string">&quot;所属专辑&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;8&quot;</span><span class="punctuation">:</span> <span class="string">&quot;作词&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;9&quot;</span><span class="punctuation">:</span> <span class="string">&quot;作曲&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;10&quot;</span><span class="punctuation">:</span> <span class="string">&quot;连载网站&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;11&quot;</span><span class="punctuation">:</span> <span class="string">&quot;作者&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;12&quot;</span><span class="punctuation">:</span> <span class="string">&quot;出版社&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;13&quot;</span><span class="punctuation">:</span> <span class="string">&quot;主演&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;14&quot;</span><span class="punctuation">:</span> <span class="string">&quot;导演&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;15&quot;</span><span class="punctuation">:</span> <span class="string">&quot;编剧&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;16&quot;</span><span class="punctuation">:</span> <span class="string">&quot;上映时间&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;17&quot;</span><span class="punctuation">:</span> <span class="string">&quot;成立日期&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>rel.json中包含18个类别标签, json文件可以看作是一个字典，key对应关系的id，value对应关系类型.</p>
</blockquote>
<ul>
<li>训练数据集: data&#x2F;train.json</li>
</ul>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;《今晚会在哪里醒来》是黄家强的一首粤语歌曲，由何启弘作词，黄家强作曲编曲并演唱，收录于2007年08月01日发行的专辑《她他》中&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;spo_list&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;predicate&quot;</span><span class="punctuation">:</span> <span class="string">&quot;作曲&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;object_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;人物&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;subject_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;歌曲&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;object&quot;</span><span class="punctuation">:</span> <span class="string">&quot;黄家强&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;subject&quot;</span><span class="punctuation">:</span> <span class="string">&quot;今晚会在哪里醒来&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span> <span class="punctuation">&#123;</span><span class="attr">&quot;predicate&quot;</span><span class="punctuation">:</span> <span class="string">&quot;所属专辑&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;object_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;音乐专辑&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;subject_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;歌曲&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;object&quot;</span><span class="punctuation">:</span> <span class="string">&quot;她他&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;subject&quot;</span><span class="punctuation">:</span> <span class="string">&quot;今晚会在哪里醒来&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span> <span class="punctuation">&#123;</span><span class="attr">&quot;predicate&quot;</span><span class="punctuation">:</span> <span class="string">&quot;歌手&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;object_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;人物&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;subject_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;歌曲&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;object&quot;</span><span class="punctuation">:</span> <span class="string">&quot;黄家强&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;subject&quot;</span><span class="punctuation">:</span> <span class="string">&quot;今晚会在哪里醒来&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span> <span class="punctuation">&#123;</span><span class="attr">&quot;predicate&quot;</span><span class="punctuation">:</span> <span class="string">&quot;作词&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;object_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;人物&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;subject_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;歌曲&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;object&quot;</span><span class="punctuation">:</span> <span class="string">&quot;何启弘&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;subject&quot;</span><span class="punctuation">:</span> <span class="string">&quot;今晚会在哪里醒来&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>train.json中包含55433行样本, 每行为一个字典样式</p>
<p>第一个key为”text”, 对应的value为待抽取关系的中文文本, 第二个key为”spo_list”, 对应的value为句子中真实的spo关系三元组列表 (列表中含有多个spo三元组)</p>
<p>以spo_list的其中一个元素为例：元素格式为字典，其中”predictate”代表为关系类型; “object_type”代表尾实体的类型; “subject_type”代表主实体的类型; “object”代表尾实体; “subject” 代表主实体.</p>
</blockquote>
<ul>
<li>验证数据集: data&#x2F;dev.json</li>
</ul>
<blockquote>
<p>dev.json中包含11191行样本，格式同上</p>
</blockquote>
<ul>
<li>测试数据集: data&#x2F;test.json</li>
</ul>
<blockquote>
<p>est.json中包含13417行样本，格式同上</p>
</blockquote>
<p>第二步: 编写Config类项目文件配置代码</p>
<p>（1）目的: 配置项目常用变量，一般这些变量属于不经常改变的，比如: 训练文件路径、模型训练次数、模型超参数等等</p>
<p>（2）代码</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用fastNLP前需要先安装</span></span><br><span class="line">pip install fastNLP</span><br></pre></td></tr></table></figure>

<p>代码位置：P04_RE&#x2F;Casrel_RE&#x2F;config.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入必备的工具包</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 导入Vocabulary，目的：用于构建, 存储和使用 `str` 到 `int` 的一一映射</span></span><br><span class="line"><span class="keyword">from</span> fastNLP <span class="keyword">import</span> Vocabulary</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">base_dir = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;base_dir--&gt;<span class="subst">&#123;base_dir&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建配置文件Config类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Config</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 设置是否使用GPU来进行模型训练</span></span><br><span class="line">        self.device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">        <span class="comment"># self.device = &#x27;mps&#x27;</span></span><br><span class="line">        self.bert_path = os.path.join(base_dir, <span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">        self.num_rel = <span class="number">18</span>  <span class="comment"># 关系的种类数</span></span><br><span class="line">        self.batch_size = <span class="number">8</span></span><br><span class="line">        self.train_data_path = os.path.join(base_dir, <span class="string">&#x27;data/train.json&#x27;</span>)</span><br><span class="line">        self.dev_data_path = os.path.join(base_dir, <span class="string">&#x27;data/dev.json&#x27;</span>)</span><br><span class="line">        self.test_data_path = os.path.join(base_dir, <span class="string">&#x27;data/test.json&#x27;</span>)</span><br><span class="line">        self.rel_dict_path = os.path.join(base_dir, <span class="string">&#x27;data/relation.json&#x27;</span>)</span><br><span class="line">        id2rel = json.load(<span class="built_in">open</span>(self.rel_dict_path, encoding=<span class="string">&#x27;utf8&#x27;</span>))</span><br><span class="line">        <span class="comment"># print(f&#x27;id2rel--&gt;&#123;id2rel&#125;&#x27;)</span></span><br><span class="line">        self.rel_vocab = Vocabulary(padding=<span class="literal">None</span>, unknown=<span class="literal">None</span>)</span><br><span class="line">        <span class="comment"># vocab更新自己的字典，输入为list列表</span></span><br><span class="line">        self.rel_vocab.add_word_lst(<span class="built_in">list</span>(id2rel.values()))</span><br><span class="line"></span><br><span class="line">        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)</span><br><span class="line">        self.learning_rate = <span class="number">1e-5</span></span><br><span class="line">        self.bert_dim = <span class="number">768</span></span><br><span class="line">        self.epochs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = Config()</span><br><span class="line">    <span class="comment"># 通过rel_vocab获取 rel2id 和 id2rel 字典</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;rel2id--&gt;<span class="subst">&#123;conf.rel_vocab.word2idx&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;id2rel--&gt;<span class="subst">&#123;conf.rel_vocab.idx2word&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 通过rel_vocab获取id对应的rel 或者 通过rel获取对应的id</span></span><br><span class="line">    <span class="built_in">print</span>(conf.rel_vocab.to_word(<span class="number">2</span>))   <span class="comment"># 通过id对应的rel</span></span><br><span class="line">    <span class="built_in">print</span>(conf.rel_vocab.to_index(<span class="string">&#x27;出生地&#x27;</span>))  <span class="comment"># 通过rel获取对应的id</span></span><br></pre></td></tr></table></figure>

<p>第三步: 编写数据处理相关函数</p>
<p>（1）获取训练数据思路</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250919104836492.png" alt="image-20250919104836492"></p>
<p>（2）补充知识—defaultdict使用</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">defaultdict()</span> <span class="string">是 Python 标准库 collections 中的一个字典子类，它的作用是 当访问不存在的键时，自动为这个键生成一个默认值，从而避免 KeyError 错误。</span></span><br><span class="line"></span><br><span class="line"><span class="attr">语法：</span></span><br><span class="line"><span class="attr">from</span> <span class="string">collections import defaultdict</span></span><br><span class="line"><span class="attr">d</span> = <span class="string">defaultdict(default_factory)</span></span><br><span class="line"></span><br><span class="line"><span class="attr">其中</span> <span class="string">default_factory 是一个可调用对象，比如 int、list、set、lambda 等。</span></span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="comment"># defaultdict的作用就是创建一个字典，如果字典中没有对应的key，则创建一个空列表，然后进行append操作</span></span><br><span class="line">d = defaultdict(<span class="built_in">list</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;d--&gt;<span class="subst">&#123;d&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">d[<span class="string">&#x27;a&#x27;</span>].append(<span class="number">1</span>)</span><br><span class="line">d[<span class="string">&#x27;a&#x27;</span>].append(<span class="number">2</span>)</span><br><span class="line">d[<span class="string">&#x27;b&#x27;</span>].append(<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;d--&gt;<span class="subst">&#123;d&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;d[&#x27;ss&#x27;]--&gt;<span class="subst">&#123;d[<span class="string">&#x27;ss&#x27;</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 普通的字典，需要先进行初始化，才能进行赋值或者取值</span></span><br><span class="line">mydict = &#123;&#125;</span><br><span class="line">mydict[<span class="string">&#x27;a&#x27;</span>] = []</span><br><span class="line">mydict[<span class="string">&#x27;a&#x27;</span>].append(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;mydict--&gt;<span class="subst">&#123;mydict&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(mydict[<span class="string">&#x27;ss&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p>（3）代码</p>
<p>代码位置：P04_RE&#x2F;Casrel_RE&#x2F;utils&#x2F;process.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> choice</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P04_RE.Casrel_RE.config <span class="keyword">import</span> Config</span><br><span class="line"></span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">find_head_index</span>(<span class="params">inner_input_ids, entity_id</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    根据原始句子的input_ids和实体的id，找到实体的开始索引</span></span><br><span class="line"><span class="string">    :param inner_input_ids: 原始句子的input_ids</span></span><br><span class="line"><span class="string">    :param entity_id: 实体经过bert分词器之后的id</span></span><br><span class="line"><span class="string">    :return: 实体的开始索引</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    entity_len = <span class="built_in">len</span>(entity_id)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(inner_input_ids) - entity_len + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> inner_input_ids[i:i + entity_len] ` entity_id:</span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span>  <span class="comment"># 如果未找到，则返回-1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_label</span>(<span class="params">inner_input_ids, inner_triple, seq_len</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    基于输入的input_ids、三元组和序列长度，获取该样本在训练时的其他输入和输出数据</span></span><br><span class="line"><span class="string">    :param inner_input_ids: 该样本经过bert分词器后的input_ids</span></span><br><span class="line"><span class="string">    :param inner_triple:  该样本的所有三元组列表</span></span><br><span class="line"><span class="string">    :param seq_len: input_ids的长度</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 1）初始化张量——6个</span></span><br><span class="line">    inner_sub_heads = torch.zeros(seq_len)  <span class="comment"># 主实体的开始位置信息</span></span><br><span class="line">    inner_sub_tails = torch.zeros(seq_len)  <span class="comment"># 主实体的结束位置信息</span></span><br><span class="line">    inner_obj_heads = torch.zeros((seq_len, conf.num_rel))  <span class="comment"># 客实体的开始位置及关系信息</span></span><br><span class="line">    inner_obj_tails = torch.zeros((seq_len, conf.num_rel))  <span class="comment"># 客实体的结束位置及关系信息</span></span><br><span class="line">    inner_sub_head2tail = torch.zeros(seq_len)  <span class="comment"># 所取头实体从头到尾的位置信息</span></span><br><span class="line">    inner_sub_len = torch.ones(<span class="number">1</span>)  <span class="comment"># 所取头实体的长度</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2）找索引——主实体和客实体的开始及结束索引</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    需要定义一个字典，用来存储 主实体和客实体的开始及结束索引</span></span><br><span class="line"><span class="string">    &#123;(主实体1开始索引，主实体1结束索引):   [(客实体的开始索引，客实体的结束索引，关系id), ()...],</span></span><br><span class="line"><span class="string">      (主实体2开始索引，主实体2结束索引):   [(客实体的开始索引，客实体的结束索引，关系id), ()...],</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    作用：defaultdict的作用就是创建一个字典，如果字典中没有对应的key，则创建一个空列表，然后进行append操作</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    spo_dict = defaultdict(<span class="built_in">list</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实现方式：遍历 inner_triple，依次处理每个三元组</span></span><br><span class="line">    <span class="keyword">for</span> spo <span class="keyword">in</span> inner_triple:</span><br><span class="line">        <span class="comment"># print(f&#x27;spo--&gt;&#123;spo&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># ①将spo中的主实体和客实体分别经过bert分词器，转成id；同时把关系类型也转成id</span></span><br><span class="line">        sub_id = conf.tokenizer(spo[<span class="string">&#x27;subject&#x27;</span>], add_special_tokens=<span class="literal">False</span>)[<span class="string">&#x27;input_ids&#x27;</span>]</span><br><span class="line">        obj_id = conf.tokenizer(spo[<span class="string">&#x27;object&#x27;</span>], add_special_tokens=<span class="literal">False</span>)[<span class="string">&#x27;input_ids&#x27;</span>]</span><br><span class="line">        rel_id = conf.rel_vocab.to_index(spo[<span class="string">&#x27;predicate&#x27;</span>])</span><br><span class="line">        <span class="comment"># print(f&#x27;sub_id--&gt;&#123;sub_id&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># print(f&#x27;obj_id--&gt;&#123;obj_id&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># print(f&#x27;rel_id--&gt;&#123;rel_id&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># ②基于主实体和客实体的id及整个句子的input_ids，去找到主实体和客实体的开始索引</span></span><br><span class="line">        sub_head_index = find_head_index(inner_input_ids, sub_id)</span><br><span class="line">        <span class="comment"># print(f&#x27;sub_head_index--&gt;&#123;sub_head_index&#125;&#x27;)</span></span><br><span class="line">        obj_head_index = find_head_index(inner_input_ids, obj_id)</span><br><span class="line">        <span class="comment"># ③通过公式计算主实体和客实体的结束索引</span></span><br><span class="line">        <span class="keyword">if</span> sub_head_index != -<span class="number">1</span> <span class="keyword">and</span> obj_head_index != -<span class="number">1</span>:  <span class="comment"># 如果主实体和客实体的索引都找到了</span></span><br><span class="line">            sub_tail_index = sub_head_index + <span class="built_in">len</span>(sub_id) - <span class="number">1</span></span><br><span class="line">            obj_tail_index = obj_head_index + <span class="built_in">len</span>(obj_id) - <span class="number">1</span></span><br><span class="line">            <span class="comment"># ④将这些信息存储到字典中</span></span><br><span class="line">            spo_dict[(sub_head_index, sub_tail_index)].append((obj_head_index, obj_tail_index, rel_id))</span><br><span class="line">        <span class="comment"># print(f&#x27;spo_dict--&gt;&#123;spo_dict&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># break</span></span><br><span class="line">    <span class="comment"># print(f&#x27;spo_dict--&gt;&#123;spo_dict&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3）对张量进行赋1操作</span></span><br><span class="line">    <span class="keyword">if</span> spo_dict:</span><br><span class="line">        <span class="comment"># print(f&#x27;spo_dict--&gt;&#123;spo_dict&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># 3.1 先对主实体的开始位置信息和结束位置信息进行赋值，需要将所有的主实体进行赋值！</span></span><br><span class="line">        <span class="keyword">for</span> sub_head_index, sub_tail_index <span class="keyword">in</span> spo_dict:</span><br><span class="line">            inner_sub_heads[sub_head_index] = <span class="number">1</span>  <span class="comment"># 将主实体的开始索引位置置为1</span></span><br><span class="line">            inner_sub_tails[sub_tail_index] = <span class="number">1</span>  <span class="comment"># 将主实体的结束索引位置置为1</span></span><br><span class="line">        <span class="comment"># 3.2 随机抽取一个主实体，把它的信息标注到 inner_sub_head2tail，inner_sub_len</span></span><br><span class="line">        choice_head_index, choice_tail_index = choice(<span class="built_in">list</span>(spo_dict.keys()))</span><br><span class="line">        <span class="comment"># print(f&#x27;choice_head_index--&gt;&#123;choice_head_index&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># print(f&#x27;choice_tail_index--&gt;&#123;choice_tail_index&#125;&#x27;)</span></span><br><span class="line">        inner_sub_head2tail[choice_head_index:choice_tail_index + <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">        <span class="comment"># print(f&#x27;inner_sub_head2tail--&gt;&#123;inner_sub_head2tail&#125;&#x27;)</span></span><br><span class="line">        inner_sub_len = torch.tensor([choice_tail_index - choice_head_index + <span class="number">1</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        <span class="comment"># print(f&#x27;inner_sub_len--&gt;&#123;inner_sub_len&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># 3.3 对 inner_obj_heads, inner_obj_tails进行赋值，将所取主实体对应的所有客实体及关系进行赋值</span></span><br><span class="line">        <span class="keyword">for</span> obj_head_index, obj_tail_index, rel_id <span class="keyword">in</span> spo_dict.get((choice_head_index, choice_tail_index)):</span><br><span class="line">            inner_obj_heads[obj_head_index, rel_id] = <span class="number">1</span>   <span class="comment"># 将所取主实体对应所有客实体的(开始索引, 关系id)位置置为1</span></span><br><span class="line">            inner_obj_tails[obj_tail_index, rel_id] = <span class="number">1</span>  <span class="comment"># 将所取主实体对应所有客实体的(结束索引, 关系id)位置置为1</span></span><br><span class="line">        <span class="comment"># print(f&#x27;inner_obj_heads--&gt;&#123;inner_obj_heads[:, 3]&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># print(f&#x27;inner_obj_tails--&gt;&#123;inner_obj_tails[:, 3]&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inner_sub_heads, inner_sub_tails, inner_obj_heads, inner_obj_tails, inner_sub_head2tail, inner_sub_len</span><br></pre></td></tr></table></figure>

<p>第四步: 构建DataSet类与dataloader函数</p>
<p>（1）整体思路</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250918174640819.png" alt="image-20250918174640819"></p>
<p>（2）创建DataSet类</p>
<p>代码位置：P04_RE&#x2F;Casrel_RE&#x2F;utils&#x2F;data_loader.py</p>
<p>（3）补充知识—stack使用</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">torch.stack()</span> <span class="string">是 PyTorch 中用于沿新维度连接一组张量的函数。它会在指定维度上增加一个新维度，并将多个形状相同的张量堆叠在一起。</span></span><br><span class="line"></span><br><span class="line"><span class="attr">语法：</span></span><br><span class="line"><span class="attr">torch.stack(tensors,</span> <span class="string">dim=0)</span></span><br><span class="line"></span><br><span class="line"><span class="attr">其中tensors：一个张量列表，所有张量形状必须一样。</span></span><br><span class="line"><span class="attr">dim：要插入的新维度位置（默认为</span> <span class="string">0）。</span></span><br></pre></td></tr></table></figure>

<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250618103624139.png" alt="image-20250618103624139" style="zoom:67%;" />

<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">tensor1 = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])  <span class="comment"># [3]</span></span><br><span class="line">tensor2 = torch.tensor([<span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>])  <span class="comment"># [3]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dim 指的是在哪里添加新的维度</span></span><br><span class="line">tensor3 = torch.stack([tensor1, tensor2], dim=<span class="number">0</span>)  <span class="comment"># [2, 3]</span></span><br><span class="line"><span class="built_in">print</span>(tensor3)</span><br><span class="line"></span><br><span class="line">tensor4 = torch.stack([tensor1, tensor2], dim=<span class="number">1</span>)  <span class="comment"># [3, 2]</span></span><br><span class="line"><span class="built_in">print</span>(tensor4)</span><br></pre></td></tr></table></figure>

<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250919153030928.png" alt="image-20250919153030928"></p>
<p>（4）collate_fn与dataloader</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P04_RE.Casrel_RE.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P04_RE.Casrel_RE.utils.process <span class="keyword">import</span> create_label</span><br><span class="line"></span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file_path</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyDataset, self).__init__()</span><br><span class="line">        self.datas = [json.loads(line) <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)]</span><br><span class="line">        <span class="comment"># print(f&#x27;datas--&gt;&#123;self.datas[:2]&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.datas)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        text = self.datas[index][<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">        spo_list = self.datas[index][<span class="string">&#x27;spo_list&#x27;</span>]</span><br><span class="line">        <span class="keyword">return</span> text, spo_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>):</span><br><span class="line">    <span class="comment"># print(f&#x27;batch--&gt;&#123;batch&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># 1)获取每个批次的文本和三元组</span></span><br><span class="line">    text_list = [data[<span class="number">0</span>] <span class="keyword">for</span> data <span class="keyword">in</span> batch]</span><br><span class="line">    triple_list = [data[<span class="number">1</span>] <span class="keyword">for</span> data <span class="keyword">in</span> batch]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2)使用bert的分词器对原始文本进行处理，获取input_ids和attention_mask</span></span><br><span class="line">    <span class="comment"># 对整个批次的文本进行编码，设置了padding=True但是没有设置max_length，此时max_lenth就是这个批次中最长的文本的长度。</span></span><br><span class="line">    tokenizer_result = conf.tokenizer.batch_encode_plus(text_list, padding=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># print(f&#x27;tokenizer_result--&gt;&#123;tokenizer_result&#125;&#x27;)  # 输出结果中包含input_ids、token_type_ids、attention_mask</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3)基于每个样本的input_ids和spo_list去获取训练数据的其他输入和输出</span></span><br><span class="line">    <span class="comment"># 3.1 设置最终结果的列表，用来存储每个样本的输入和输出数据</span></span><br><span class="line">    sub_heads = []  <span class="comment"># 主实体开始位置信息</span></span><br><span class="line">    sub_tails = []  <span class="comment"># 主实体结束位置信息</span></span><br><span class="line">    obj_heads = []  <span class="comment"># 客实体开始位置及关系信息</span></span><br><span class="line">    obj_tails = []  <span class="comment"># 客实体结束位置及关系信息</span></span><br><span class="line">    sub_len = []  <span class="comment"># 所取头实体的长度</span></span><br><span class="line">    sub_head2tail = []  <span class="comment"># 所取头实体从头到尾的位置信息</span></span><br><span class="line">    <span class="comment"># 3.2 遍历每个样本</span></span><br><span class="line">    batch_size = <span class="built_in">len</span>(text_list)</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">        inner_input_ids = tokenizer_result[<span class="string">&#x27;input_ids&#x27;</span>][index]</span><br><span class="line">        <span class="comment"># print(f&#x27;inner_input_ids--&gt;&#123;inner_input_ids&#125;&#x27;)</span></span><br><span class="line">        inner_triple = triple_list[index]</span><br><span class="line">        <span class="comment"># print(f&#x27;inner_triple--&gt;&#123;inner_triple&#125;&#x27;)</span></span><br><span class="line">        seq_len = <span class="built_in">len</span>(inner_input_ids)  <span class="comment"># 获取分词之后 input_ids 的长度，用来指定位置的长度</span></span><br><span class="line">        <span class="comment"># print(f&#x27;seq_len--&gt;&#123;seq_len&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3.3 调用方法，实现基于输入的input_ids、三元组和序列长度，获取该样本在训练时的其他输入和输出数据</span></span><br><span class="line">        inner_sub_heads, inner_sub_tails, inner_obj_heads, inner_obj_tails, inner_sub_head2tail, inner_sub_len = create_label(inner_input_ids, inner_triple, seq_len)</span><br><span class="line">        sub_heads.append(inner_sub_heads)</span><br><span class="line">        sub_tails.append(inner_sub_tails)</span><br><span class="line">        obj_heads.append(inner_obj_heads)</span><br><span class="line">        obj_tails.append(inner_obj_tails)</span><br><span class="line">        sub_len.append(inner_sub_len)</span><br><span class="line">        sub_head2tail.append(inner_sub_head2tail)</span><br><span class="line">        <span class="comment"># break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4)对每个样本的结果数据进行拼接，再转成tensor作为最终模型训练的数据</span></span><br><span class="line">    <span class="comment"># print(f&#x27;sub_heads--&gt;&#123;sub_heads&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;sub_len--&gt;&#123;sub_len&#125;&#x27;)</span></span><br><span class="line">    input_ids = torch.tensor(tokenizer_result[<span class="string">&#x27;input_ids&#x27;</span>], dtype=torch.long).to(conf.device)</span><br><span class="line">    mask = torch.tensor(tokenizer_result[<span class="string">&#x27;attention_mask&#x27;</span>], dtype=torch.long).to(conf.device)</span><br><span class="line">    sub_head2tail = torch.stack(sub_head2tail, dim=<span class="number">0</span>).to(conf.device)</span><br><span class="line">    sub_len = torch.stack(sub_len, dim=<span class="number">0</span>).to(conf.device)</span><br><span class="line">    sub_heads = torch.stack(sub_heads, dim=<span class="number">0</span>).to(conf.device)</span><br><span class="line">    sub_tails = torch.stack(sub_tails, dim=<span class="number">0</span>).to(conf.device)</span><br><span class="line">    obj_heads = torch.stack(obj_heads, dim=<span class="number">0</span>).to(conf.device)</span><br><span class="line">    obj_tails = torch.stack(obj_tails, dim=<span class="number">0</span>).to(conf.device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5)将结果组装成字典再返回</span></span><br><span class="line">    inputs = &#123;</span><br><span class="line">        <span class="string">&#x27;input_ids&#x27;</span>: input_ids,</span><br><span class="line">        <span class="string">&#x27;mask&#x27;</span>: mask,</span><br><span class="line">        <span class="string">&#x27;sub_head2tail&#x27;</span>: sub_head2tail,</span><br><span class="line">        <span class="string">&#x27;sub_len&#x27;</span>: sub_len</span><br><span class="line">    &#125;</span><br><span class="line">    labels = &#123;</span><br><span class="line">        <span class="string">&#x27;sub_heads&#x27;</span>: sub_heads,</span><br><span class="line">        <span class="string">&#x27;sub_tails&#x27;</span>: sub_tails,</span><br><span class="line">        <span class="string">&#x27;obj_heads&#x27;</span>: obj_heads,</span><br><span class="line">        <span class="string">&#x27;obj_tails&#x27;</span>: obj_tails</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> inputs, labels</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_data_loader</span>():</span><br><span class="line">    <span class="comment"># 训练集</span></span><br><span class="line">    train_dataset = MyDataset(conf.train_data_path)</span><br><span class="line">    train_dataloader = DataLoader(dataset=train_dataset,</span><br><span class="line">                                  batch_size=conf.batch_size,</span><br><span class="line">                                  shuffle=<span class="literal">False</span>,  <span class="comment"># 在写代码的时候，需要把shuffle设置为 Fasle; 在训练时，需要把shuffle设置为 True</span></span><br><span class="line">                                  collate_fn=collate_fn,</span><br><span class="line">                                  drop_last=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 验证集</span></span><br><span class="line">    dev_dataset = MyDataset(conf.dev_data_path)</span><br><span class="line">    dev_dataloader = DataLoader(dev_dataset,</span><br><span class="line">                                batch_size=conf.batch_size,</span><br><span class="line">                                shuffle=<span class="literal">True</span>,</span><br><span class="line">                                collate_fn=collate_fn,</span><br><span class="line">                                drop_last=<span class="literal">True</span></span><br><span class="line">                                )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试集</span></span><br><span class="line">    test_dataset = MyDataset(conf.test_data_path)</span><br><span class="line">    test_dataloader = DataLoader(test_dataset,</span><br><span class="line">                                 batch_size=conf.batch_size,</span><br><span class="line">                                 shuffle=<span class="literal">True</span>,</span><br><span class="line">                                 collate_fn=collate_fn,</span><br><span class="line">                                 drop_last=<span class="literal">True</span></span><br><span class="line">                                 )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_dataloader, dev_dataloader, test_dataloader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># dataset = MyDataset(conf.train_data_path)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;len(dataset)--&gt;&#123;len(dataset)&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;dataset[0]--&gt;&#123;dataset[0]&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    train_dataloader, dev_dataloader, test_dataloader = get_data_loader()</span><br><span class="line">    <span class="comment"># for x in train_dataloader:</span></span><br><span class="line">    <span class="comment">#     print(f&#x27;x--&gt;&#123;x&#125;&#x27;)</span></span><br><span class="line">    <span class="comment">#     break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;inputs[&quot;input_ids&quot;]--&gt;<span class="subst">&#123;inputs[<span class="string">&quot;input_ids&quot;</span>].shape&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;inputs[&quot;mask&quot;]--&gt;<span class="subst">&#123;inputs[<span class="string">&quot;mask&quot;</span>].shape&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;inputs[&quot;sub_head2tail&quot;]--&gt;<span class="subst">&#123;inputs[<span class="string">&quot;sub_head2tail&quot;</span>].shape&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;inputs[&quot;sub_len&quot;]--&gt;<span class="subst">&#123;inputs[<span class="string">&quot;sub_len&quot;</span>].shape&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;labels[&quot;sub_heads&quot;]--&gt;<span class="subst">&#123;labels[<span class="string">&quot;sub_heads&quot;</span>].shape&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;labels[&quot;sub_tails&quot;]--&gt;<span class="subst">&#123;labels[<span class="string">&quot;sub_tails&quot;</span>].shape&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;labels[&quot;obj_heads&quot;]--&gt;<span class="subst">&#123;labels[<span class="string">&quot;obj_heads&quot;</span>].shape&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;labels[&quot;obj_tails&quot;]--&gt;<span class="subst">&#123;labels[<span class="string">&quot;obj_tails&quot;</span>].shape&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>



<h3 id="【掌握】Casrel模型搭建"><a href="#【掌握】Casrel模型搭建" class="headerlink" title="【掌握】Casrel模型搭建"></a>【掌握】Casrel模型搭建</h3><h3 id="第一步-编写模型类的代码-2"><a href="#第一步-编写模型类的代码-2" class="headerlink" title="第一步: 编写模型类的代码"></a>第一步: 编写模型类的代码</h3><p>（1）思路</p>
<ul>
<li>模型架构</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250919165102535.png" alt="image-20250919165102535"></p>
<ul>
<li>损失计算</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250920095715298.png" alt="image-20250920095715298"></p>
<p>（2）补充知识</p>
<p><strong>1)BCELoss</strong></p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">在</span> <span class="string">PyTorch 中，torch.nn.BCELoss 是 二元交叉熵损失函数（Binary Cross Entropy Loss），用于二分类任务中，衡量预测概率和真实标签之间的差距。</span></span><br></pre></td></tr></table></figure>

<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250621153731901.png" alt="image-20250621153731901" style="zoom:67%;" />

<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测结果，也就是通过linear之后的结果</span></span><br><span class="line">y_pred = torch.tensor([<span class="number">1.6901</span>, -<span class="number">0.5459</span>, -<span class="number">0.2469</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 真实的标签</span></span><br><span class="line">y_true = torch.tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 送入BCELoss之前，需要先送入sigmoid激活函数，转成概率值</span></span><br><span class="line">sigmoid_result = torch.sigmoid(y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;sigmoid_result--&gt;<span class="subst">&#123;sigmoid_result&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将reduction设置成none，这样BCELoss的输出结果就是每个样本的loss，而不是所有样本的loss求和或求平均</span></span><br><span class="line">loss = torch.nn.BCELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">loss_result = loss(sigmoid_result, y_true)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;loss_result--&gt;<span class="subst">&#123;loss_result&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250920091521076.png" alt="image-20250920091521076"></p>
<p><strong>2）repeat</strong></p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tensor.repeat()</span> <span class="string">是用来在各个维度上重复 tensor 的数据，从而生成一个更大的 tensor。</span></span><br><span class="line"><span class="attr">注意：repeat()</span> <span class="string">是对数据的复制，不是广播（broadcasting）</span></span><br><span class="line"></span><br><span class="line"><span class="attr">语法：</span></span><br><span class="line"><span class="attr">new_tensor</span> = <span class="string">tensor.repeat(repeats)</span></span><br><span class="line"></span><br><span class="line"><span class="attr">其中repeats：一个元组或多个整数，表示每个维度上重复的次数。</span></span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">ts = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line"><span class="built_in">print</span>(ts.shape)</span><br><span class="line">ts2 = ts.repeat((<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(ts2.shape)</span><br><span class="line"><span class="built_in">print</span>(ts2)</span><br></pre></td></tr></table></figure>

<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250920104012492.png" alt="image-20250920104012492"></p>
<p>（3）代码</p>
<p>代码位置：P04_RE&#x2F;Casrel_RE&#x2F;model&#x2F;CasrelModel.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P04_RE.Casrel_RE.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P04_RE.Casrel_RE.utils.data_loader <span class="keyword">import</span> get_data_loader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Casrel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, conf</span>):</span><br><span class="line">        <span class="built_in">super</span>(Casrel, self).__init__()</span><br><span class="line">        <span class="comment"># bert编码器</span></span><br><span class="line">        self.bert = BertModel.from_pretrained(conf.bert_path)</span><br><span class="line">        <span class="comment"># 第1个线性层，用来判断主实体的开始位置</span></span><br><span class="line">        self.sub_heads_linear = nn.Linear(conf.bert_dim, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 第2个线性层，用来判断主实体的结束位置</span></span><br><span class="line">        self.sub_tails_linear = nn.Linear(conf.bert_dim, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 第3个线性层，用来判断客实体的开始位置及关系类型</span></span><br><span class="line">        self.obj_heads_linear = nn.Linear(conf.bert_dim, conf.num_rel)</span><br><span class="line">        <span class="comment"># 第4个线性层，用来判断客实体的结束位置及关系类型</span></span><br><span class="line">        self.obj_tails_linear = nn.Linear(conf.bert_dim, conf.num_rel)</span><br><span class="line">        self.conf = conf</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, mask, sub_head2tail, sub_len</span>):</span><br><span class="line">        <span class="comment"># 1. 获取bert的输出结果</span></span><br><span class="line">        bert_output = self.get_encoded_text(input_ids, mask)</span><br><span class="line">        <span class="comment"># 2. 预测主实体的开始位置和结束位置</span></span><br><span class="line">        pre_sub_heads, pre_sub_tails = self.get_subs(bert_output)</span><br><span class="line">        <span class="comment"># print(f&#x27;pre_sub_heads--&gt;&#123;pre_sub_heads.shape&#125;&#x27;)  # [2, 73, 1]</span></span><br><span class="line">        <span class="comment"># print(f&#x27;pre_sub_tails--&gt;&#123;pre_sub_tails.shape&#125;&#x27;)  # [2, 73, 1]</span></span><br><span class="line">        <span class="comment"># 3. 预测客实体的开始位置和结束位置及关系类型</span></span><br><span class="line">        pre_obj_heads, pre_obj_tails = self.get_objs_and_rels(bert_output, sub_head2tail, sub_len)</span><br><span class="line">        <span class="comment"># print(f&#x27;pre_obj_heads--&gt;&#123;pre_obj_heads.shape&#125;&#x27;)  # [2, 73, 18]</span></span><br><span class="line">        <span class="comment"># print(f&#x27;pre_obj_tails--&gt;&#123;pre_obj_tails.shape&#125;&#x27;)  # [2, 73, 18]</span></span><br><span class="line">        <span class="comment"># 4)将结果封装到一个字典中，然后返回【需要注意的是，需要将mask一起返回，因为后续在计算损失时，需要用到mask来忽略填充位置的损失】</span></span><br><span class="line">        result_dict = &#123;</span><br><span class="line">            <span class="string">&#x27;pre_sub_heads&#x27;</span>: pre_sub_heads,</span><br><span class="line">            <span class="string">&#x27;pre_sub_tails&#x27;</span>: pre_sub_tails,</span><br><span class="line">            <span class="string">&#x27;pre_obj_heads&#x27;</span>: pre_obj_heads,</span><br><span class="line">            <span class="string">&#x27;pre_obj_tails&#x27;</span>: pre_obj_tails,</span><br><span class="line">            <span class="string">&#x27;mask&#x27;</span>: mask</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result_dict</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取bert编码结果</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_encoded_text</span>(<span class="params">self, input_ids, mask</span>):</span><br><span class="line">        <span class="comment"># bert的输出结果除了 last_hidden_state之外，还有 pooler_output。如果需要token级别的语义，则使用 last_hidden_state，如果需要句子级别的语义，则使用 pooler_output</span></span><br><span class="line">        bert_output = self.bert(input_ids, attention_mask=mask)[<span class="string">&#x27;last_hidden_state&#x27;</span>]</span><br><span class="line">        <span class="comment"># print(f&#x27;bert_output--&gt;&#123;bert_output.shape&#125;&#x27;)  # [2, 73, 768]</span></span><br><span class="line">        <span class="keyword">return</span> bert_output</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测主实体的开始位置和结束位置</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_subs</span>(<span class="params">self, bert_output</span>):</span><br><span class="line">        <span class="comment"># 获取主实体的开始位置</span></span><br><span class="line">        pre_sub_heads = torch.sigmoid(self.sub_heads_linear(bert_output))</span><br><span class="line">        <span class="comment"># 获取主实体的结束位置</span></span><br><span class="line">        pre_sub_tails = torch.sigmoid(self.sub_tails_linear(bert_output))</span><br><span class="line">        <span class="keyword">return</span> pre_sub_heads, pre_sub_tails</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测客实体的开始位置和结束位置及关系类型</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_objs_and_rels</span>(<span class="params">self, bert_output, sub_head2tail, sub_len</span>):</span><br><span class="line">        <span class="comment"># 1)先对sub_head2tail进行升维</span></span><br><span class="line">        sub_head2tail = sub_head2tail.unsqueeze(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 2)将 sub_head2tail 和 bert_output 进行矩阵乘法</span></span><br><span class="line">        matmul_result = torch.matmul(sub_head2tail, bert_output)</span><br><span class="line">        <span class="comment"># 3)除以sub_len，获取平均向量</span></span><br><span class="line">        avg_result = matmul_result / sub_len.unsqueeze(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 4)将平均向量和bert_output进行相加</span></span><br><span class="line">        sum_input = avg_result + bert_output</span><br><span class="line">        <span class="comment"># print(f&#x27;sum_input--&gt;&#123;sum_input.shape&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># 5)预测客实体的开始位置及关系类型</span></span><br><span class="line">        pre_obj_heads = torch.sigmoid(self.obj_heads_linear(sum_input))</span><br><span class="line">        <span class="comment"># 6)预测客实体的结束位置及关系类型</span></span><br><span class="line">        pre_obj_tails = torch.sigmoid(self.obj_tails_linear(sum_input))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> pre_obj_heads, pre_obj_tails</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_loss</span>(<span class="params">self, pre_sub_heads, pre_sub_tails, pre_obj_heads, pre_obj_tails,</span></span><br><span class="line"><span class="params">                     mask,</span></span><br><span class="line"><span class="params">                     sub_heads, sub_tails, obj_heads, obj_tails</span>):</span><br><span class="line">        <span class="comment"># 1)计算主实体开始位置的损失</span></span><br><span class="line">        loss1 = self.loss(pre_sub_heads, sub_heads, mask)</span><br><span class="line">        <span class="comment"># 2)计算主实体结束位置的损失</span></span><br><span class="line">        loss2 = self.loss(pre_sub_tails, sub_tails, mask)</span><br><span class="line">        <span class="comment"># 3)计算客实体的开始位置及关系的损失</span></span><br><span class="line">        mask = mask.unsqueeze(dim=<span class="number">2</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, self.conf.num_rel)</span><br><span class="line">        loss3 = self.loss(pre_obj_heads, obj_heads, mask)</span><br><span class="line">        <span class="comment"># 4)计算客实体的结束位置及关系的损失</span></span><br><span class="line">        loss4 = self.loss(pre_obj_tails, obj_tails, mask)</span><br><span class="line">        <span class="comment"># 5)计算总损失</span></span><br><span class="line">        <span class="keyword">return</span> loss1 + loss2 + loss3 + loss4</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算输出的损失</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, predict, label, mask</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param predict: 预测的结果（linear层经过sigmoid之后的结果）</span></span><br><span class="line"><span class="string">        :param label: 真实的标签</span></span><br><span class="line"><span class="string">        :param mask: attention_mask</span></span><br><span class="line"><span class="string">        :return: 平均损失</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># 为了能够正常计算，需要 predict, label, mask形状保持一致，所以先对 predict 进行降维处理</span></span><br><span class="line">        predict = predict.squeeze(dim=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 使用BCELoss计算二分类损失，注意需要设置 reduction=&#x27;none&#x27; 来保留所有的损失结果</span></span><br><span class="line">        criterion = nn.BCELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">        loss_tensor = criterion(predict, label)</span><br><span class="line">        <span class="comment"># print(f&#x27;loss_tensor---&gt;&#123;loss_tensor.shape&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># 将损失结果和mask进行对位相乘，得到去除padding位置的loss，然后再去求平均</span></span><br><span class="line">        avg_loss = (loss_tensor * mask).<span class="built_in">sum</span>() / mask.<span class="built_in">sum</span>()</span><br><span class="line">        <span class="comment"># print(f&#x27;avg_loss---&gt;&#123;avg_loss&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> avg_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = Config()</span><br><span class="line">    model = Casrel(conf).to(conf.device)</span><br><span class="line">    <span class="built_in">print</span>(model)</span><br><span class="line">    train_dataloader, dev_dataloader, test_dataloader = get_data_loader()</span><br><span class="line">    <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        <span class="comment"># result = model(inputs[&#x27;input_ids&#x27;], inputs[&#x27;mask&#x27;], inputs[&#x27;sub_head2tail&#x27;], inputs[&#x27;sub_len&#x27;])</span></span><br><span class="line">        result = model(**inputs)  <span class="comment"># **inputs表示将inputs中的键值对展开，然后使用关键字参数传入模型。需要保证字典的键与模型中的参数一致</span></span><br><span class="line">        loss = model.compute_loss(**result, **labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;loss---&gt;<span class="subst">&#123;loss&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<h3 id="第二步-编写工具类函数-训练函数-验证函数-测试函数"><a href="#第二步-编写工具类函数-训练函数-验证函数-测试函数" class="headerlink" title="第二步: 编写工具类函数,训练函数,验证函数,测试函数"></a>第二步: 编写工具类函数,训练函数,验证函数,测试函数</h3><p>（1）训练函数</p>
<p><strong>1）AdamW优化器</strong></p>
<p>拓展知识：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">no_decay = [<span class="string">&quot;bias&quot;</span>, <span class="string">&quot;LayerNorm.bias&quot;</span>, <span class="string">&quot;LayerNorm.weight&quot;</span>]</span><br><span class="line">param_optimzer = [(<span class="string">&#x27;obj_heads_linear.weight&#x27;</span>, <span class="string">&#x27;参数1&#x27;</span>), (<span class="string">&#x27;obj_heads_linear.bias&#x27;</span>, <span class="string">&#x27;参数2&#x27;</span>), (<span class="string">&#x27;ssssLayerNorm.bias&#x27;</span>, <span class="string">&#x27;参数3&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">list1 = [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_optimzer <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">any</span>(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)]</span><br><span class="line"><span class="built_in">print</span>(list1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历no_decay，判断param_name是否包含no_decay中的元素</span></span><br><span class="line">bl1 = [nd <span class="keyword">in</span> <span class="string">&#x27;obj_heads_linear.weight&#x27;</span> <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay]</span><br><span class="line"><span class="built_in">print</span>(bl1)</span><br><span class="line">bl2 = [nd <span class="keyword">in</span> <span class="string">&#x27;obj_heads_linear.bias&#x27;</span> <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay]</span><br><span class="line"><span class="built_in">print</span>(bl2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># any是一个真值判断函数，需要输入一个可迭代对象，如果迭代对象中有一个元素为真，则返回真。</span></span><br><span class="line"><span class="comment"># 在这里判断的则是param_name是否包含no_decay中的元素，如果包含则返回真，否则返回假。</span></span><br><span class="line">bl1 = <span class="built_in">any</span>(nd <span class="keyword">in</span> <span class="string">&#x27;obj_heads_linear.weight&#x27;</span> <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)</span><br><span class="line"><span class="built_in">print</span>(bl1)</span><br><span class="line">bl2 = <span class="built_in">any</span>(nd <span class="keyword">in</span> <span class="string">&#x27;obj_heads_linear.bias&#x27;</span> <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)</span><br><span class="line"><span class="built_in">print</span>(bl2)</span><br><span class="line"></span><br><span class="line">optimizer_grouped_parameters = [</span><br><span class="line">    <span class="comment"># 如果param_name不包含no_decay中的元素，则设置&quot;weight_decay&quot;: 0.01，即进行权重衰减</span></span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_optimzer <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">any</span>(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">&quot;weight_decay&quot;</span>: <span class="number">0.01</span>&#125;,</span><br><span class="line">    <span class="comment"># 如果param_name包含no_decay中的元素，则设置&quot;weight_decay&quot;: 0.0，即不进行权重衰减</span></span><br><span class="line">    &#123;<span class="string">&quot;params&quot;</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_optimzer <span class="keyword">if</span> <span class="built_in">any</span>(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">&quot;weight_decay&quot;</span>: <span class="number">0.0</span>&#125;]</span><br><span class="line"><span class="built_in">print</span>(optimizer_grouped_parameters)</span><br></pre></td></tr></table></figure>

<p><strong>2）代码</strong></p>
<p>代码位置：P04_RE&#x2F;Casrel_RE&#x2F;train.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> AdamW</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P04_RE.Casrel_RE.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P04_RE.Casrel_RE.model.CasrelModel <span class="keyword">import</span> Casrel</span><br><span class="line"><span class="keyword">from</span> P04_RE.Casrel_RE.utils.data_loader <span class="keyword">import</span> get_data_loader</span><br><span class="line"></span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model2dev</span>(<span class="params">model, dev_dataloader</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model2train</span>():</span><br><span class="line">    <span class="comment"># 1.构建数据迭代器Dataloader(包括数据处理与构建数据源Dataset)</span></span><br><span class="line">    train_dataloader, dev_dataloader, test_dataloader = get_data_loader()</span><br><span class="line">    <span class="comment"># 2.实例化模型</span></span><br><span class="line">    model = Casrel(conf).to(conf.device)</span><br><span class="line">    param_optimizer = <span class="built_in">list</span>(model.named_parameters())</span><br><span class="line">    <span class="comment"># print(f&#x27;parameters--&gt;&#123;param_optimizer&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print([name for name, param in model.named_parameters()])</span></span><br><span class="line">    <span class="comment"># 3.实例化损失函数对象(略)</span></span><br><span class="line">    <span class="comment"># 4.实例化优化器对象</span></span><br><span class="line">    no_decay = [<span class="string">&quot;bias&quot;</span>, <span class="string">&quot;LayerNorm.bias&quot;</span>, <span class="string">&quot;LayerNorm.weight&quot;</span>]  <span class="comment"># no_decay中存放不进行权重衰减的参数&#123;因为bert官方代码对这三项免于正则化&#125;</span></span><br><span class="line">    <span class="comment"># any()函数用于判断给定的可迭代参数iterable是否全部为False，则返回False，如果有一个为True，则返回True</span></span><br><span class="line">    <span class="comment"># 判断param_optimizer中所有的参数。如果不在no_decay中，则进行权重衰减;如果在no_decay中，则不进行权重衰减</span></span><br><span class="line">    optimizer_grouped_parameters = [</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_optimizer <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">any</span>(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">&quot;weight_decay&quot;</span>: <span class="number">0.01</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_optimizer <span class="keyword">if</span> <span class="built_in">any</span>(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">&quot;weight_decay&quot;</span>: <span class="number">0.0</span>&#125;]</span><br><span class="line">    optimizer = AdamW(optimizer_grouped_parameters, lr=conf.learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5.定义打印日志参数</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    train_loss = <span class="number">0</span>  <span class="comment"># 已经训练的损失之和</span></span><br><span class="line">    total_step = <span class="number">0</span>  <span class="comment"># 总的批次数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6.开始训练</span></span><br><span class="line">    <span class="comment"># 6.1 实现外层大循环epoch</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(conf.epochs):</span><br><span class="line">        <span class="comment"># 6.2 将模型设置为训练模式</span></span><br><span class="line">        model.train()</span><br><span class="line">        <span class="comment"># 6.3 内部遍历数据迭代器dataloader</span></span><br><span class="line">        <span class="keyword">for</span> index, (inputs, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(train_dataloader, desc=<span class="string">&#x27;Casrel模型训练&#x27;</span>)):</span><br><span class="line">            <span class="comment"># 1）将数据送入模型得到输出结果</span></span><br><span class="line">            outputs = model(**inputs)</span><br><span class="line">            <span class="comment"># 2）计算损失</span></span><br><span class="line">            loss = model.compute_loss(**outputs, **labels)</span><br><span class="line">            <span class="comment"># print(f&#x27;loss--&gt;&#123;loss&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># 3）梯度清零: optimizer.zero_grad()</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            <span class="comment"># 4）反向传播(计算梯度): loss.backward()</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="comment"># 梯度裁剪</span></span><br><span class="line">            nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=<span class="number">10</span>)</span><br><span class="line">            <span class="comment"># 5）梯度更新(参数更新): optimizer.step()</span></span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="comment"># 6）打印内部训练日志</span></span><br><span class="line">            train_loss += loss.item()</span><br><span class="line">            total_step += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> (index+<span class="number">1</span>) % <span class="number">200</span> ` <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;epoch:%d------------loss:%.4f&#x27;</span> % (epoch, train_loss/total_step))</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 6.4 使用验证集进行模型评估【将模型设置为评估模式】</span></span><br><span class="line">        <span class="comment"># 注意：我们最好等一个轮次训练完成之后，再去进行模型评估。而不是在一个轮次内部进行多次模型评估。对于后者总的评估次数会变多，有可能在验证集上获取更好的结果，但是在最终的测试集上效果可能更差，因为产生了过拟合。所以我们建议都是在一个轮次训练完成后，再进行模型评估。</span></span><br><span class="line">        result = model2dev(model, dev_dataloader)</span><br><span class="line">        <span class="comment"># 6.5 保存模型: torch.save(model.state_dict(), &quot;model_path&quot;)</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># 6.6 打印外部训练日志</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model2train()</span><br></pre></td></tr></table></figure>



<p><code>**3）面试题：Casrel模型中，Bert为什么要参与反向传播进行参数更新？**</code></p>
<p>**任务特定调整：**虽然BERT是预训练的，但它并不是针对特定任务（如关系抽取）进行优化的。通过在特定任务上进行微调（即反向传播更新参数），可以使BERT的表示更适合关系抽取的任务。这样，BERT模型能够更好地理解实体间的关系。</p>
<p>**领域适应：**预训练的BERT是在大规模语料上训练的，可能没有针对具体领域的知识或语言模式。通过微调BERT，可以使其更适应目标领域的数据，改善抽取效果。</p>
<p>**经验结果：**大量后续工作和实践都表明：在下游抽取、分类、生成等任务里，给BERT或其他Transformer设置较小的学习率，整体端到端的微调，一般比“冻结+只微调顶层”要好2—5个百分点的效果，尤其在中大型数据集上。</p>
<p>（2）验证函数【完整代码】</p>
<p><strong>1）思路</strong></p>
<p>整体思路：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250920144959150.png" alt="image-20250920144959150"></p>
<p>抽取主实体和客实体的思路：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250920162735604.png" alt="image-20250920162735604"></p>
<p><strong>2）代码</strong></p>
<p>代码位置：同样在 P04_RE&#x2F;Casrel_RE&#x2F;train.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> AdamW</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P04_RE.Casrel_RE.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P04_RE.Casrel_RE.model.CasrelModel <span class="keyword">import</span> Casrel</span><br><span class="line"><span class="keyword">from</span> P04_RE.Casrel_RE.utils.data_loader <span class="keyword">import</span> get_data_loader</span><br><span class="line"></span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert_score_to_zero_one</span>(<span class="params">ts</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    将传进来的张量转成0或1</span></span><br><span class="line"><span class="string">    :param ts: 传进来的张量，也就是未处理的预测结果</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    ts[ts&gt;=<span class="number">0.5</span>] = <span class="number">1</span></span><br><span class="line">    ts[ts&lt;<span class="number">0.5</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> ts</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_sub</span>(<span class="params">sub_head, sub_tail</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param sub_head: 主实体的开始位置</span></span><br><span class="line"><span class="string">    :param sub_tail: 主实体的结束位置</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 1)取出1位置所对应的索引</span></span><br><span class="line">    <span class="comment"># 使用torch.arange()函数，生成一个索引的序列，然后使用 boolean 张量来获取1所对应位置的索引</span></span><br><span class="line">    heads_index = torch.arange(<span class="number">0</span>, <span class="built_in">len</span>(sub_head), device=conf.device)[sub_head ` <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># print(f&#x27;heads_index--&gt;&#123;heads_index&#125;&#x27;)</span></span><br><span class="line">    tails_index = torch.arange(<span class="number">0</span>, <span class="built_in">len</span>(sub_tail), device=conf.device)[sub_tail ` <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># print(f&#x27;tails_index--&gt;&#123;tails_index&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2)根据开始索引和结束索引，提取实体</span></span><br><span class="line">    subs = []</span><br><span class="line">    <span class="keyword">for</span> head_index, tail_index <span class="keyword">in</span> <span class="built_in">zip</span>(heads_index, tails_index):</span><br><span class="line">        <span class="keyword">if</span> head_index &lt;= tail_index:</span><br><span class="line">            subs.append((head_index.item(), tail_index.item()))</span><br><span class="line">    <span class="comment"># print(f&#x27;subs--&gt;&#123;subs&#125;&#x27;)</span></span><br><span class="line">    <span class="keyword">return</span> subs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_obj</span>(<span class="params">obj_head, obj_tail</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param obj_head:  客实体的开始位置及关系信息</span></span><br><span class="line"><span class="string">    :param obj_tail:  客实体的结束位置及关系信息</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 为了方便取到一个关系下的所有位置信息，需要先对矩阵进行转置</span></span><br><span class="line">    obj_head = obj_head.T</span><br><span class="line">    obj_tail = obj_tail.T</span><br><span class="line">    <span class="comment"># print(f&#x27;obj_head--&gt;&#123;obj_head.shape&#125;&#x27;)  # [18, 155]</span></span><br><span class="line">    <span class="comment"># print(f&#x27;obj_tail--&gt;&#123;obj_tail.shape&#125;&#x27;)  # [18, 155]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历每个关系，抽取该关系下的客实体</span></span><br><span class="line">    obj_and_rel = []  <span class="comment"># 存储所有的客实体及关系</span></span><br><span class="line">    <span class="keyword">for</span> rel_id <span class="keyword">in</span> <span class="built_in">range</span>(conf.num_rel):</span><br><span class="line">        <span class="comment"># 获取该关系下的所有位置信息</span></span><br><span class="line">        head = obj_head[rel_id]  <span class="comment"># [155]</span></span><br><span class="line">        tail = obj_tail[rel_id]  <span class="comment"># [155]</span></span><br><span class="line">        <span class="comment"># 调用 extract_sub()方法,抽取该关系下的主实体</span></span><br><span class="line">        objs = extract_sub(head, tail)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(objs) &gt; <span class="number">0</span>: <span class="comment"># 说明抽取到了客实体</span></span><br><span class="line">            <span class="keyword">for</span> obj <span class="keyword">in</span> objs:</span><br><span class="line">                obj_and_rel.append((rel_id, obj[<span class="number">0</span>], obj[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> obj_and_rel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model2dev</span>(<span class="params">model, dev_dataloader</span>):</span><br><span class="line">    <span class="comment"># 1.定义打印日志参数</span></span><br><span class="line">    df = pd.DataFrame(columns=[<span class="string">&#x27;TP&#x27;</span>, <span class="string">&#x27;PRED&#x27;</span>, <span class="string">&quot;REAL&quot;</span>, <span class="string">&#x27;p&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;f1&#x27;</span>], index=[<span class="string">&#x27;sub&#x27;</span>, <span class="string">&#x27;triple&#x27;</span>])</span><br><span class="line">    df.fillna(<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 2.将模型设置为评估模式</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="comment"># 3.内部遍历数据迭代器dataloader</span></span><br><span class="line">    <span class="keyword">for</span> index, (inputs, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(dev_dataloader, desc=<span class="string">&#x27;Casrel模型验证&#x27;</span>)):</span><br><span class="line">        <span class="comment"># 3.1 将数据送入模型得到输出结果</span></span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">        <span class="comment"># print(f&#x27;outputs--&gt;&#123;outputs&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># 3.2 计算损失(略)</span></span><br><span class="line">        <span class="comment"># 3.3 处理结果</span></span><br><span class="line">        <span class="comment"># 1）将预测结果转成0或者1</span></span><br><span class="line">        pre_sub_heads = convert_score_to_zero_one(outputs[<span class="string">&#x27;pre_sub_heads&#x27;</span>])</span><br><span class="line">        <span class="comment"># print(f&#x27;pre_sub_heads--&gt;&#123;pre_sub_heads.shape&#125;&#x27;)</span></span><br><span class="line">        pre_sub_tails = convert_score_to_zero_one(outputs[<span class="string">&#x27;pre_sub_tails&#x27;</span>])</span><br><span class="line">        <span class="comment"># print(f&#x27;pre_sub_tails--&gt;&#123;pre_sub_tails.shape&#125;&#x27;)</span></span><br><span class="line">        pre_obj_heads = convert_score_to_zero_one(outputs[<span class="string">&#x27;pre_obj_heads&#x27;</span>])</span><br><span class="line">        <span class="comment"># print(f&#x27;pre_obj_heads--&gt;&#123;pre_obj_heads.shape&#125;&#x27;)</span></span><br><span class="line">        pre_obj_tails = convert_score_to_zero_one(outputs[<span class="string">&#x27;pre_obj_tails&#x27;</span>])</span><br><span class="line">        <span class="comment"># print(f&#x27;pre_obj_tails--&gt;&#123;pre_obj_tails.shape&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># 2)取到1位置所对应的索引，然后基于开始索引和结束索引，提取实体</span></span><br><span class="line">        <span class="comment"># 遍历批次内的每个样本</span></span><br><span class="line">        <span class="keyword">for</span> batch_index <span class="keyword">in</span> <span class="built_in">range</span>(conf.batch_size):</span><br><span class="line">            <span class="comment"># 抽取预测的主实体</span></span><br><span class="line">            pre_sub_head = pre_sub_heads[batch_index].squeeze(-<span class="number">1</span>)</span><br><span class="line">            pre_sub_tail = pre_sub_tails[batch_index].squeeze(-<span class="number">1</span>)</span><br><span class="line">            pre_sub = extract_sub(pre_sub_head, pre_sub_tail)</span><br><span class="line">            <span class="comment"># print(f&#x27;pre_sub--&gt;&#123;pre_sub&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># 抽取实际的主实体</span></span><br><span class="line">            real_sub = extract_sub(labels[<span class="string">&#x27;sub_heads&#x27;</span>][batch_index],</span><br><span class="line">                                   labels[<span class="string">&#x27;sub_tails&#x27;</span>][batch_index])</span><br><span class="line">            <span class="comment"># print(f&#x27;real_sub--&gt;&#123;real_sub&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 抽取预测的客实体</span></span><br><span class="line">            pre_obj = extract_obj(pre_obj_heads[batch_index], pre_obj_tails[batch_index])</span><br><span class="line">            <span class="comment"># print(f&#x27;pre_obj--&gt;&#123;pre_obj&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># 抽取实际的客实体</span></span><br><span class="line">            real_obj = extract_obj(labels[<span class="string">&#x27;obj_heads&#x27;</span>][batch_index],</span><br><span class="line">                                   labels[<span class="string">&#x27;obj_tails&#x27;</span>][batch_index])</span><br><span class="line">            <span class="comment"># print(f&#x27;real_obj--&gt;&#123;real_obj&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 3.4 统计批次内指标</span></span><br><span class="line">            <span class="comment"># 计算预测的主实体的个数</span></span><br><span class="line">            df.loc[<span class="string">&#x27;sub&#x27;</span>, <span class="string">&#x27;PRED&#x27;</span>] += <span class="built_in">len</span>(pre_sub)</span><br><span class="line">            <span class="comment"># 计算实际主实体的个数</span></span><br><span class="line">            df.loc[<span class="string">&#x27;sub&#x27;</span>, <span class="string">&#x27;REAL&#x27;</span>] += <span class="built_in">len</span>(real_sub)</span><br><span class="line">            <span class="comment"># 计算预测正确的主实体个数</span></span><br><span class="line">            <span class="keyword">for</span> sub <span class="keyword">in</span> pre_sub:</span><br><span class="line">                <span class="keyword">if</span> sub <span class="keyword">in</span> real_sub:</span><br><span class="line">                    df.loc[<span class="string">&#x27;sub&#x27;</span>, <span class="string">&#x27;TP&#x27;</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算预测的客实体及关系个数</span></span><br><span class="line">            df.loc[<span class="string">&#x27;triple&#x27;</span>, <span class="string">&#x27;PRED&#x27;</span>] += <span class="built_in">len</span>(pre_obj)</span><br><span class="line">            <span class="comment"># 计算实际的客实体及关系个数</span></span><br><span class="line">            df.loc[<span class="string">&#x27;triple&#x27;</span>, <span class="string">&#x27;REAL&#x27;</span>] += <span class="built_in">len</span>(real_obj)</span><br><span class="line">            <span class="comment"># 计算预测正确的客实体及关系个数</span></span><br><span class="line">            <span class="keyword">for</span> obj <span class="keyword">in</span> pre_obj:</span><br><span class="line">                <span class="keyword">if</span> obj <span class="keyword">in</span> real_obj:</span><br><span class="line">                    df.loc[<span class="string">&#x27;triple&#x27;</span>, <span class="string">&#x27;TP&#x27;</span>] += <span class="number">1</span></span><br><span class="line">            <span class="comment"># break</span></span><br><span class="line">        <span class="comment"># break</span></span><br><span class="line">    <span class="comment"># 4.统计整体指标</span></span><br><span class="line">    <span class="comment"># 4.1 计算主实体的指标</span></span><br><span class="line">    <span class="comment"># 计算精确率</span></span><br><span class="line">    sub_precision = df.loc[<span class="string">&#x27;sub&#x27;</span>, <span class="string">&#x27;TP&#x27;</span>] / df.loc[<span class="string">&#x27;sub&#x27;</span>, <span class="string">&#x27;PRED&#x27;</span>]</span><br><span class="line">    df.loc[<span class="string">&#x27;sub&#x27;</span>, <span class="string">&#x27;p&#x27;</span>] = sub_precision</span><br><span class="line">    <span class="comment"># 计算召回率</span></span><br><span class="line">    sub_recall = df.loc[<span class="string">&#x27;sub&#x27;</span>, <span class="string">&#x27;TP&#x27;</span>] / df.loc[<span class="string">&#x27;sub&#x27;</span>, <span class="string">&#x27;REAL&#x27;</span>]</span><br><span class="line">    df.loc[<span class="string">&#x27;sub&#x27;</span>, <span class="string">&#x27;r&#x27;</span>] = sub_recall</span><br><span class="line">    <span class="comment"># 计算f1</span></span><br><span class="line">    sub_f1 = <span class="number">2</span> * sub_precision * sub_recall / (sub_precision + sub_recall)</span><br><span class="line">    df.loc[<span class="string">&#x27;sub&#x27;</span>, <span class="string">&#x27;f1&#x27;</span>] = sub_f1</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.2 计算客实体的指标</span></span><br><span class="line">    <span class="comment"># 计算精确率</span></span><br><span class="line">    obj_precision = df.loc[<span class="string">&#x27;triple&#x27;</span>, <span class="string">&#x27;TP&#x27;</span>] / df.loc[<span class="string">&#x27;triple&#x27;</span>, <span class="string">&#x27;PRED&#x27;</span>]</span><br><span class="line">    df.loc[<span class="string">&#x27;triple&#x27;</span>, <span class="string">&#x27;p&#x27;</span>] = obj_precision</span><br><span class="line">    <span class="comment"># 计算召回率</span></span><br><span class="line">    obj_recall = df.loc[<span class="string">&#x27;triple&#x27;</span>, <span class="string">&#x27;TP&#x27;</span>] / df.loc[<span class="string">&#x27;triple&#x27;</span>, <span class="string">&#x27;REAL&#x27;</span>]</span><br><span class="line">    df.loc[<span class="string">&#x27;triple&#x27;</span>, <span class="string">&#x27;r&#x27;</span>] = obj_recall</span><br><span class="line">    <span class="comment"># 计算f1</span></span><br><span class="line">    obj_f1 = <span class="number">2</span> * obj_precision * obj_recall / (obj_precision + obj_recall)</span><br><span class="line">    df.loc[<span class="string">&#x27;triple&#x27;</span>, <span class="string">&#x27;f1&#x27;</span>] = obj_f1</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sub_precision, sub_recall, sub_f1, obj_precision, obj_recall, obj_f1, df</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model2train</span>():</span><br><span class="line">    <span class="comment"># 1.构建数据迭代器Dataloader(包括数据处理与构建数据源Dataset)</span></span><br><span class="line">    train_dataloader, dev_dataloader, test_dataloader = get_data_loader()</span><br><span class="line">    <span class="comment"># 2.实例化模型</span></span><br><span class="line">    model = Casrel(conf).to(conf.device)</span><br><span class="line">    param_optimizer = <span class="built_in">list</span>(model.named_parameters())</span><br><span class="line">    <span class="comment"># print(f&#x27;parameters--&gt;&#123;param_optimizer&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print([name for name, param in model.named_parameters()])</span></span><br><span class="line">    <span class="comment"># 3.实例化损失函数对象(略)</span></span><br><span class="line">    <span class="comment"># 4.实例化优化器对象</span></span><br><span class="line">    no_decay = [<span class="string">&quot;bias&quot;</span>, <span class="string">&quot;LayerNorm.bias&quot;</span>, <span class="string">&quot;LayerNorm.weight&quot;</span>]  <span class="comment"># no_decay中存放不进行权重衰减的参数&#123;因为bert官方代码对这三项免于正则化&#125;</span></span><br><span class="line">    <span class="comment"># any()函数用于判断给定的可迭代参数iterable是否全部为False，则返回False，如果有一个为True，则返回True</span></span><br><span class="line">    <span class="comment"># 判断param_optimizer中所有的参数。如果不在no_decay中，则进行权重衰减;如果在no_decay中，则不进行权重衰减</span></span><br><span class="line">    optimizer_grouped_parameters = [</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_optimizer <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">any</span>(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">&quot;weight_decay&quot;</span>: <span class="number">0.01</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_optimizer <span class="keyword">if</span> <span class="built_in">any</span>(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">&quot;weight_decay&quot;</span>: <span class="number">0.0</span>&#125;]</span><br><span class="line">    optimizer = AdamW(optimizer_grouped_parameters, lr=conf.learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5.定义打印日志参数</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    train_loss = <span class="number">0</span>  <span class="comment"># 已经训练的损失之和</span></span><br><span class="line">    total_step = <span class="number">0</span>  <span class="comment"># 总的批次数</span></span><br><span class="line">    best_triple_f1 = <span class="number">0</span>  <span class="comment"># 最佳三元组f1</span></span><br><span class="line">    <span class="comment"># 6.开始训练</span></span><br><span class="line">    <span class="comment"># 6.1 实现外层大循环epoch</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(conf.epochs):</span><br><span class="line">        <span class="comment"># 6.2 将模型设置为训练模式</span></span><br><span class="line">        model.train()</span><br><span class="line">        <span class="comment"># 6.3 内部遍历数据迭代器dataloader</span></span><br><span class="line">        <span class="keyword">for</span> index, (inputs, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(train_dataloader, desc=<span class="string">&#x27;Casrel模型训练&#x27;</span>)):</span><br><span class="line">            <span class="comment"># 1）将数据送入模型得到输出结果</span></span><br><span class="line">            outputs = model(**inputs)</span><br><span class="line">            <span class="comment"># 2）计算损失</span></span><br><span class="line">            loss = model.compute_loss(**outputs, **labels)</span><br><span class="line">            <span class="comment"># print(f&#x27;loss--&gt;&#123;loss&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># 3）梯度清零: optimizer.zero_grad()</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            <span class="comment"># 4）反向传播(计算梯度): loss.backward()</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="comment"># 梯度裁剪</span></span><br><span class="line">            nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=<span class="number">10</span>)</span><br><span class="line">            <span class="comment"># 5）梯度更新(参数更新): optimizer.step()</span></span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="comment"># 6）打印内部训练日志</span></span><br><span class="line">            train_loss += loss.item()</span><br><span class="line">            total_step += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> (index+<span class="number">1</span>) % <span class="number">20</span> ` <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;epoch:%d------------loss:%.4f&#x27;</span> % (epoch+<span class="number">1</span>, train_loss/total_step))</span><br><span class="line">                <span class="comment"># break</span></span><br><span class="line">        <span class="comment"># 6.4 使用验证集进行模型评估【将模型设置为评估模式】</span></span><br><span class="line">        <span class="comment"># 注意：我们最好等一个轮次训练完成之后，再去进行模型评估。而不是在一个轮次内部进行多次模型评估。对于后者总的评估次数会变多，有可能在验证集上获取更好的结果，但是在最终的测试集上效果可能更差，因为产生了过拟合。所以我们建议都是在一个轮次训练完成后，再进行模型评估。</span></span><br><span class="line">        result = model2dev(model, dev_dataloader)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;df--&gt;<span class="subst">&#123;result[-<span class="number">1</span>]&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="comment"># 6.5 保存模型: torch.save(model.state_dict(), &quot;model_path&quot;)</span></span><br><span class="line">        <span class="keyword">if</span> result[-<span class="number">2</span>] &gt; best_triple_f1:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;当前模型效果更好，保存模型中...当前轮次为<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, 当前模型的triple_f1为<span class="subst">&#123;result[-<span class="number">2</span>]&#125;</span>&#x27;</span>)</span><br><span class="line">            best_triple_f1 = result[-<span class="number">2</span>]</span><br><span class="line">            torch.save(model.state_dict(), <span class="string">&#x27;save_model/casrel_best_f1.pth&#x27;</span>)</span><br><span class="line">        <span class="comment"># break</span></span><br><span class="line">    <span class="comment"># 6.6 打印外部训练日志</span></span><br><span class="line">    end_time = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;训练时间:<span class="subst">&#123;end_time - start_time:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model2train()</span><br></pre></td></tr></table></figure>

<p>训练结果：</p>
<p>验证在epoch外部：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250622133007745-17506524376701.png" alt="img"></p>
<p>验证在epoch内部：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250623002127871.png" alt="image-20250623002127871">（3）测试函数</p>
<p>同验证函数，调用方式如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型测试</span></span><br><span class="line"><span class="comment"># 1.实例化模型</span></span><br><span class="line">model = Casrel(conf).to(conf.device)</span><br><span class="line"><span class="comment"># 2.加载模型参数</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;save_model/casrel_best_f1.pth&#x27;</span>, weights_only=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># 3.处理数据</span></span><br><span class="line">train_dataloader, dev_dataloader, test_dataloader = get_data_loader()</span><br><span class="line"><span class="comment"># 4.模型预测</span></span><br><span class="line">sub_precision, sub_recall, sub_f1, obj_precision, obj_recall, obj_f1, df = model2dev(model, test_dataloader)</span><br><span class="line"><span class="comment"># 5.结果解析</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;模型在测试集上的表现为：\n<span class="subst">&#123;df&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>这个是在epoch外部进行评估的模型在测试集上的表现。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250623003819689.png" alt="image-20250623003819689"></p>
<p>这个是在epoch内部进行评估的模型在测试集上的表现。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250623003858560.png" alt="image-20250623003858560"></p>
<p><code>（4）后续优化</code></p>
<p>升级预训练模型：从基础 bert-base 换成效果更好的中文预训练，如 RoBERTa-wwm-ext、MacBERT、Erlangshen-RoBERTa-large 等。</p>
<p>修改主实体和bert隐藏层的融合方式：可以使用拼接的方式（Bert隐藏层输入拼接上所取主实体的平均向量；另外也可以将所取的主实体的向量前拼接N个1，其他的向量拼接N个0），或者使用增强的方式（将所取的主实体对应的张量扩大N倍）。</p>
<p>增加实体边界探索：在 subject&#x2F;object 边界预测上加一个前馈全连接层 或者是BiLSTM+Linear层，提高识别的准确性。</p>
<p>增加drop层：通过增加几个不同的drop层，提高模型的过拟合能力。</p>
<p>修改0&#x2F;1的阈值：目前设置的阈值为0.5，可以修改这个阈值进行训练或预测，比如修改成0.45,0.55等。</p>
<p>增加训练数据：可以使用数据增强，或更多标注数据。</p>
<h3 id="第三步-编写模型预测函数-2"><a href="#第三步-编写模型预测函数-2" class="headerlink" title="第三步: 编写模型预测函数"></a>第三步: 编写模型预测函数</h3><p>（1）思路</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250922085604828.png" alt="image-20250922085604828"></p>
<p>（2）代码</p>
<p>代码位置：P04_RE&#x2F;Casrel_RE&#x2F;predict.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> P04_RE.Casrel_RE.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P04_RE.Casrel_RE.model.CasrelModel <span class="keyword">import</span> Casrel</span><br><span class="line"><span class="keyword">from</span> P04_RE.Casrel_RE.train <span class="keyword">import</span> convert_score_to_zero_one, extract_sub, extract_obj</span><br><span class="line"></span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model2predict</span>(<span class="params">sample</span>):</span><br><span class="line">    <span class="comment"># 1.实例化模型</span></span><br><span class="line">    model = Casrel(conf).to(conf.device)</span><br><span class="line">    <span class="comment"># 2.加载模型参数</span></span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">&#x27;save_model/casrel_best_f1.pth&#x27;</span>, weights_only=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.预测主实体</span></span><br><span class="line">    <span class="comment"># 3.1 先将文本送到bert分词器，获取input_ids，attention_mask</span></span><br><span class="line">    tokenizer_result = conf.tokenizer(sample)</span><br><span class="line">    <span class="comment"># print(f&#x27;tokenizer_result--&gt;&#123;tokenizer_result&#125;&#x27;)</span></span><br><span class="line">    input_ids = torch.tensor([tokenizer_result[<span class="string">&#x27;input_ids&#x27;</span>]]).to(conf.device)  <span class="comment"># 需要添加一个batch_size维度</span></span><br><span class="line">    mask = torch.tensor([tokenizer_result[<span class="string">&#x27;attention_mask&#x27;</span>]]).to(conf.device)</span><br><span class="line">    <span class="comment"># print(f&#x27;input_ids--&gt;&#123;input_ids.shape&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;mask--&gt;&#123;mask.shape&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># 3.2 调用模型的get_encoded_text()，获取bert_output</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        bert_output = model.get_encoded_text(input_ids, mask)</span><br><span class="line">        <span class="comment"># 3.3 调用模型的get_subs()方法，获取主实体开始和结束位置信息</span></span><br><span class="line">        pre_sub_heads, pre_sub_tails = model.get_subs(bert_output)</span><br><span class="line">        <span class="comment"># 3.4 抽取出主实体（先将数据转换成1或0,然后调用extract_sub方法）</span></span><br><span class="line">        sub_heads = convert_score_to_zero_one(pre_sub_heads)</span><br><span class="line">        <span class="comment"># print(f&#x27;sub_heads--&gt;&#123;sub_heads.shape&#125;&#x27;)</span></span><br><span class="line">        sub_tails = convert_score_to_zero_one(pre_sub_tails)</span><br><span class="line">        <span class="comment"># print(f&#x27;sub_tails--&gt;&#123;sub_tails.shape&#125;&#x27;)</span></span><br><span class="line">        subs = extract_sub(sub_heads.squeeze(), sub_tails.squeeze())  <span class="comment"># 注意：传入的数据应该是1维的</span></span><br><span class="line">        <span class="comment"># print(f&#x27;subs--&gt;&#123;subs&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4.预测客实体及关系</span></span><br><span class="line">        spo_list = []  <span class="comment"># 创建一个空列表，用于存储抽取到的所有三元组</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(subs) &gt; <span class="number">0</span>:  <span class="comment"># 因为有可能没有抽取到主实体，所以需要进行判断</span></span><br><span class="line">            <span class="keyword">for</span> sub <span class="keyword">in</span> subs:  <span class="comment"># 因为可能抽取到多个主实体，所以需要遍历</span></span><br><span class="line">                <span class="comment"># print(f&#x27;主实体--&gt;&#123;sub&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># 4.0 可以将抽取到的主实体id转成文字，如果文字中包含了特殊字符，则不需要进行客实体及关系的抽取了！！</span></span><br><span class="line">                <span class="comment"># 方法：需要先将input_ids转成字符列表，然后根据主实体的开始和结束索引，截取对应的字符</span></span><br><span class="line">                text_list = conf.tokenizer.convert_ids_to_tokens(input_ids[<span class="number">0</span>])</span><br><span class="line">                <span class="comment"># print(f&#x27;text_list--&gt;&#123;text_list&#125;&#x27;)</span></span><br><span class="line">                sub_str = <span class="string">&#x27;&#x27;</span>.join(text_list[sub[<span class="number">0</span>]:sub[<span class="number">1</span>]+<span class="number">1</span>])</span><br><span class="line">                <span class="comment"># print(f&#x27;主实体文字--&gt;&#123;sub_str&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># 如果主实体中包含了特殊字段，则不需要进行客实体及关系的抽取了</span></span><br><span class="line">                <span class="keyword">if</span> <span class="string">&#x27;[CLS]&#x27;</span> <span class="keyword">in</span> sub_str <span class="keyword">or</span> <span class="string">&#x27;[SEP]&#x27;</span> <span class="keyword">in</span> sub_str <span class="keyword">or</span> <span class="string">&#x27;[PAD]&#x27;</span> <span class="keyword">in</span> sub_str:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 4.1 对每个主实体进行处理，获取 sub_head2tail, sub_len</span></span><br><span class="line">                sub_head2tail = torch.zeros(<span class="built_in">len</span>(input_ids[<span class="number">0</span>])).to(conf.device)</span><br><span class="line">                sub_head2tail[sub[<span class="number">0</span>]:sub[<span class="number">1</span>]+<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">                <span class="comment"># print(f&#x27;sub_head2tail--&gt;&#123;sub_head2tail&#125;&#x27;)</span></span><br><span class="line">                sub_len = torch.tensor([sub[<span class="number">1</span>] - sub[<span class="number">0</span>] + <span class="number">1</span>], dtype=torch.<span class="built_in">float</span>).to(conf.device)</span><br><span class="line">                <span class="comment"># print(f&#x27;sub_len--&gt;&#123;sub_len&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># 4.2 调用模型get_objs_and_rels()方法，预测客实体及关系</span></span><br><span class="line">                pre_obj_heads, pre_obj_tails = model.get_objs_and_rels(bert_output, sub_head2tail.unsqueeze(<span class="number">0</span>), sub_len.unsqueeze(<span class="number">0</span>)) <span class="comment"># 需要添加一个batch_size维度</span></span><br><span class="line">                <span class="comment"># print(f&#x27;pre_obj_heads--&gt;&#123;pre_obj_heads.shape&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># print(f&#x27;pre_obj_tails--&gt;&#123;pre_obj_tails.shape&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># 4.3 抽取客实体及关系（先将数据转换成1或0,然后调用extract_obj方法）</span></span><br><span class="line">                obj_heads = convert_score_to_zero_one(pre_obj_heads)</span><br><span class="line">                obj_tails = convert_score_to_zero_one(pre_obj_tails)</span><br><span class="line">                obj_and_rels = extract_obj(obj_heads.squeeze(), obj_tails.squeeze())  <span class="comment"># 注意：需要将batch_size维度去掉</span></span><br><span class="line">                <span class="comment"># print(f&#x27;obj_and_rels--&gt;&#123;obj_and_rels&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 5.结果解析</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(obj_and_rels) ` <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">f&#x27;没有抽取到<span class="subst">&#123;sub_str&#125;</span>的客实体及关系&#x27;</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">for</span> rel_id, head, tail <span class="keyword">in</span> obj_and_rels:  <span class="comment"># 有可能抽取到了多个主实体及关系，所以需要遍历</span></span><br><span class="line">                        relation = conf.rel_vocab.to_word(rel_id)</span><br><span class="line">                        <span class="comment"># print(f&#x27;关系类型--&gt;&#123;relation&#125;&#x27;)</span></span><br><span class="line">                        obj_str = <span class="string">&#x27;&#x27;</span>.join(text_list[head:tail+<span class="number">1</span>])</span><br><span class="line">                        <span class="comment"># print(f&#x27;客实体文字--&gt;&#123;obj_str&#125;&#x27;)</span></span><br><span class="line">                        <span class="comment"># 如果客实体中包含了特殊字段，则不需要进行最终输出了</span></span><br><span class="line">                        <span class="keyword">if</span> <span class="string">&#x27;[CLS]&#x27;</span> <span class="keyword">in</span> obj_str <span class="keyword">or</span> <span class="string">&#x27;[SEP]&#x27;</span> <span class="keyword">in</span> obj_str <span class="keyword">or</span> <span class="string">&#x27;[PAD]&#x27;</span> <span class="keyword">in</span> obj_str:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                        <span class="comment"># 结果拼接</span></span><br><span class="line">                        sub_spo = &#123;&#125;</span><br><span class="line">                        sub_spo[<span class="string">&#x27;subject&#x27;</span>] = sub_str</span><br><span class="line">                        sub_spo[<span class="string">&#x27;predicate&#x27;</span>] = relation</span><br><span class="line">                        sub_spo[<span class="string">&#x27;object&#x27;</span>] = obj_str</span><br><span class="line">                        <span class="comment"># print(f&#x27;三元组--&gt;&#123;sub_spo&#125;&#x27;)</span></span><br><span class="line">                        spo_list.append(sub_spo)</span><br><span class="line"></span><br><span class="line">        result = &#123;</span><br><span class="line">            <span class="string">&#x27;text&#x27;</span>: sample,</span><br><span class="line">            <span class="string">&#x27;spo_list&#x27;</span>: spo_list</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    result = model2predict(<span class="string">&#x27;1997年，李柏光从北京大学法律系博士毕业&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>



<h1 id="文本分类任务"><a href="#文本分类任务" class="headerlink" title="文本分类任务"></a>文本分类任务</h1><ol>
<li>项目中用了那些模型，为什么选择这些模型？</li>
</ol>
<p>项目中使用了随机森林模型，fasttext模型，和bert模型，使用随机森林模型和fasttext模型作为基线模型，为了快速判断数据是否存在明显问题，如标签错误，特征缺失或者任务本身是否具有可学习性，如果基线模型能达到较高性能，说明数据存在显著规律，如果性能非常差，可能需要检查数据质量，重新评估任务定义是否合理，如果基线模型效果非常好，也就无需复杂模型。</p>
<h2 id="2-bert模型的原理是什么？如何构建的？"><a href="#2-bert模型的原理是什么？如何构建的？" class="headerlink" title="2. bert模型的原理是什么？如何构建的？"></a>2. bert模型的原理是什么？如何构建的？</h2><p>bert模型基于transformer模型的encoder部分，双向上下文建模，其中包括输入部分，编码器部分，输出部分，输入部分将文本转换为词向量，其中输入的数据结合了词向量（token embedding），位置编码（position embedding），句子编码（segment embedding），到编码器层，编码器层包括多头注意力机制层，残差链接和规范化层和前馈神经网络，多头注意力机制层用于捕捉句子的不同语义信息，q*k的转置&#x2F;根号d再通过softmax层再乘以V来计算出注意力机制，最后再融合多头的信息到残差链接层和规范化层，残差链接层防止信息丢失，保持训练稳定，层归一化加速收敛，防止过拟合。前馈神经网络层，对每个词向量进行特征强化，再经过残差链接和规范化层最后堆叠多个编码器层后到输出部分，通过全连接层输出。</p>
<ol start="3">
<li>详细讲一下你用到的随机森林的算法原理和使用\</li>
</ol>
<p>随机森林算法是一种集成学习方法，具体属于 Bagging 类型。它由多棵决策树组成</p>
<p>给定训练集D，构建多个训练子集），每个通过随机采样产生。对每个子集训练一棵决策树。在每个树节点选择划分时，仅考虑一部分特征（例如，总特征数的一小部分）。最终预测时：分类：投票多数类，回归：取预测平均值。</p>
<ol start="4">
<li>介绍一下fasttext在有监督学习下怎么使用的\</li>
</ol>
<p>项目中使用的是fasttext有监督学习的文本分类，将文本转化为词向量，词向量相加求平均为句向量，句向量输入到线性分类器，通过交叉熵来计算损失。</p>
<h2 id="5-模型蒸馏有哪几种方式？如何进行软标签蒸馏的？"><a href="#5-模型蒸馏有哪几种方式？如何进行软标签蒸馏的？" class="headerlink" title="5. 模型蒸馏有哪几种方式？如何进行软标签蒸馏的？"></a>5. 模型蒸馏有哪几种方式？如何进行软标签蒸馏的？</h2><p>模型蒸馏有3种方式，硬标签蒸馏，软标签蒸馏，中间层蒸馏，软标签蒸馏，是将复杂模型的知识传递给简单模型，构建教师模型和学生模型，教师模型和学生模型的softmax输出引入温度参数T，当T小于1时学生模型学到的知识少，趋向于硬标签，当T大于1时，曲线更加平滑，学生模型可以学习到更多的知识，而T趋向于无穷大，那概率分布相当于平均分布。最后引入alpha参数来衡量教师模型和学生模型的知识占比，当alpha趋向于0，倾向于教师模型，当alpha趋向于1，倾向于学生模型。</p>
<h2 id="6-介绍下模型量化，介绍下模型量化有几种方式，在项目中如何使用的"><a href="#6-介绍下模型量化，介绍下模型量化有几种方式，在项目中如何使用的" class="headerlink" title="6. 介绍下模型量化，介绍下模型量化有几种方式，在项目中如何使用的"></a>6. 介绍下模型量化，介绍下模型量化有几种方式，在项目中如何使用的</h2><p>模型量化是将模型从高精度量化为低精度的一个过程，量化分为训练中量化，量化感知训练，训练后量化，动态量化和静态量化，在项目种使用的时动态量化，在模型训练后的激活float32量化为int8，与提前量化好的权重int8进行训练，输出的激活为int32，再将其反量化为float32输入到下一层。</p>
<ol start="7">
<li>说一下fasttext的负采样</li>
</ol>
<p>Fasttext负采样是为了平衡样本数据，正例样本保持不变，随机对负例样本进行随机采样来达到平衡样本数据。</p>
<ol start="8">
<li>深度学习有那些激活函数？</li>
</ol>
<p>Sigmoid函数，rule函数，tanh函数。</p>
<ol start="9">
<li>collect_fn函数是什么？</li>
</ol>
<p>该函数是将批次数据中的input_ids,attention_mask,label转换为词向量，能够直接让bert模型使用。</p>
<h2 id="3、项⽬架构"><a href="#3、项⽬架构" class="headerlink" title="3、项⽬架构"></a>3、项⽬架构</h2><p>• 第⼀阶段：基线模型（Random Forest）⾸先选择⽤TF-IDF特征 + Random Forest作为基线模型。它的⽬的是快速验证数据预处理的有效性，并为后续所有模型建⽴⼀个性能基准参照物。</p>
<p>• 第⼆阶段：捕捉词序特征（FastText）这个模型⾮常轻量⾼效，训练速度快，⽽且能捕捉件中的词序规律，并对拼写错误有很强的鲁棒性，达到了不错的效果，作为⽣产环境中的⼀个⾼性能备选⽅案。</p>
<p>• 第三阶段：深度语义理解（BERT）但前两个模型对解决语义模糊问题效果不好，必须选择具备深度的上下⽂理解能⼒的模型。因此，我采⽤了BERT模型进⾏微调（Fine-tuning）。它达到了最⾼93.5%的准确率。</p>
<p>• 第四阶段：为部署⽽优化（知识蒸馏）BERT效果虽好，但模型参数多速度慢，线上直接⽤成本太⾼。为了解决部署问题，决定使⽤BiLSTM作为学⽣模型来蒸馏bert模型。</p>
<p>• 第五阶段：项⽬部署：在部署阶段，我使⽤ Flask 框架将蒸馏后的BiLSTM模型封装成了API。这样做实现了模型服务</p>
<p>与业务系统的解耦，前端或其他服务只需通过HTTP请求传递邮件⽂本，即可实时获取分类结果。</p>
<h2 id="4、项⽬中遇到使⽤的问题"><a href="#4、项⽬中遇到使⽤的问题" class="headerlink" title="4、项⽬中遇到使⽤的问题"></a>4、项⽬中遇到使⽤的问题</h2><p>在这个项⽬中，我主要攻克了⼏个核⼼难题：</p>
<ol>
<li><p>专业术语导致的准确率波动：设计⾏业术语（如“⽔电位”、“软装”）极易被通⽤分词⼯具切错。我采取的策略是：深度分析业务⽂档，构建了⼀套领域词典，并集成到分词器中，从根本上提升了⽂本预处理的质量。</p>
</li>
<li><p>模型蒸馏过程中的稳定性问题：在蒸馏初期，学⽣模型难以学到教师模型的精髓。我通过调整损失函数权重（平衡软标签损失和硬标签损失） 和 精细化调优超参数，最终稳定了训练过程，成功实现了知识的⾼效迁移。</p>
</li>
</ol>
<p><code>2.你怎么做的这个项目？用的啥？</code></p>
<p>基线模型：随机森林</p>
<p>迭代模型：Fasttext</p>
<p>优化模型：Bert</p>
<p>上线模型：BiLSTM，以BERT作为教师模型蒸馏</p>
<p><code>3.随机森林中，怎么将句子文本转化为词向量的？</code></p>
<p>先使用结巴分词器对句子进行拆分，然后通过TF-IDF对 拆分后的词 进行加权处理。</p>
<h2 id="4-讲解一下Fasttext中的霍夫曼树？"><a href="#4-讲解一下Fasttext中的霍夫曼树？" class="headerlink" title="4.讲解一下Fasttext中的霍夫曼树？"></a><code>4.讲解一下Fasttext中的霍夫曼树？</code></h2><p>核心思想：</p>
<ul>
<li>根据词频构建：<ul>
<li>高频词离根节点近（路径短）</li>
<li>低频词离根节点远（路径长）</li>
</ul>
</li>
<li>路径即编码：每个词对应树中一条唯一路径（一串0&#x2F;1编码）。</li>
<li>预测概率变为路径概率乘积：<ul>
<li>传统 Softmax 需要计算所有节点概率</li>
<li>分层 Softmax 只需计算路径上每个节点的二分类概率（左&#x2F;右子树）</li>
</ul>
</li>
</ul>
<h2 id="5-讲解一下Bert模型？"><a href="#5-讲解一下Bert模型？" class="headerlink" title="5.讲解一下Bert模型？"></a><code>5.讲解一下Bert模型？</code></h2><p>5.1 核心双向 Self-Attention：</p>
<ul>
<li>传统模型的局限：过去模型（如 ELMo、GPT）只能从左到右或从右到左单向编码上下文。</li>
<li>BERT 的突破：通过双向Self-Attention 机制，同时学习文本左右两侧的上下文信息。</li>
</ul>
<p>5.2 预训练+微调模型：</p>
<p>预训练：用 “无标注数据” 学通用语言规律</p>
<p>这一步是 BERT 的核心优势：它先用<u>海量无标注文本</u>，让模型 “自学” 通用的语言逻辑。</p>
<ul>
<li>理解词语的上下文关联（如 “降噪” 和 “耳机” 常搭配，“快充” 和 “手机” 强相关）；</li>
<li>掌握语法、语义甚至常识（如 “苹果” 在 “吃苹果” 中是水果，在 “苹果手机” 中是品牌）。</li>
</ul>
<p>这一步<u>完全不需要任何标注</u>（不需要人工给文本打标签），模型靠 “无监督学习” 就能掌握这些通用语言能力 —— 相当于让模型先 “学会说人话、理解人话”，打下扎实的 “语言基础”。</p>
<p>微调：用 “少量标注数据” 适配具体任务</p>
<p>当模型已经通过预训练具备 “通用语言理解能力” 后，再针对具体任务（比如你的 “商品描述→类目分类”）进行微调：</p>
<ul>
<li>只需要少量任务相关的标注数据（比如几百条、上千条 “商品描述 + 类目标签”，远少于传统模型需要的几万 &#x2F; 几十万条）；</li>
<li>不需要重新训练整个模型，只需在预训练好的 BERT 基础上，加一个简单的 “分类层”，用少量标注数据 “微调” 模型参数 —— 本质是让模型把已有的 “通用语言知识”，快速适配到 “商品分类” 这个具体任务上。</li>
</ul>
<h2 id="6-Bert两个预训练任务是啥？"><a href="#6-Bert两个预训练任务是啥？" class="headerlink" title="6.Bert两个预训练任务是啥？"></a><code>6.Bert两个预训练任务是啥？</code></h2><ul>
<li>MLM（Masked Language Model）：随机遮盖 15% 的单词，让模型预测被遮盖的词（如 “我 [MASK] 苹果” → “吃”）。</li>
<li>NSP（Next Sentence Prediction）：判断两个句子是否连续（如 “天气真好” 和 “我们去公园” → 是连续的）。</li>
</ul>
<p><code>7.Bert模型输入的话输入哪三个值？</code></p>
<p>BERT 的输入由三部分组成：</p>
<ul>
<li>Token Embeddings：单词本身的向量（含特殊符号如 <code>[CLS]</code>、<code>[SEP]</code>）。</li>
<li>Segment Embeddings：区分句子 A 和 B（用于 NSP 任务）。</li>
<li>Position Embeddings：标记单词的位置信息。</li>
</ul>
<h2 id="8-Bert中的位置编码和Transformer中的位置编码有什么区别？"><a href="#8-Bert中的位置编码和Transformer中的位置编码有什么区别？" class="headerlink" title="8.Bert中的位置编码和Transformer中的位置编码有什么区别？"></a><code>8.Bert中的位置编码和Transformer中的位置编码有什么区别？</code></h2><p>原始 Transformer采用的是固定的正弦余弦位置编码</p>
<p>BERT 的位置编码采用可学习的位置嵌入。</p>
<p>BERT 预先定义一个 “位置嵌入矩阵”，其形状为 <code>[max_seq_len, d_model]</code>：</p>
<ul>
<li><code>max_seq_len</code>：BERT 支持的最大序列长度（如基础版 BERT 是 512），即模型最多能处理 512 个词的句子；</li>
<li><code>d_model</code>：与 BERT 的词嵌入维度一致（如 768）。</li>
</ul>
<p>这个矩阵中的每个元素都是可训练的参数，而非预定义的常数。在模型训练（包括预训练和微调阶段）过程中，位置嵌入矩阵会与其他参数（如自注意力层权重）一起，通过梯度下降不断优化更新。</p>
<h2 id="9-讲解一下注意力机制的计算公式？"><a href="#9-讲解一下注意力机制的计算公式？" class="headerlink" title="9.讲解一下注意力机制的计算公式？"></a><code>9.讲解一下注意力机制的计算公式？</code></h2><p>$$<br>\text{Attention}(Q, K, V) &#x3D; \text{Softmax}\left( \frac{Q \cdot K^T}{\sqrt{d_k}} \right) \cdot V<br>$$</p>
<p>$$<br>Q \cdot K^T：计算Q和K的相似度，分越高，相关性越强<br>$$</p>
<p>$$<br>\sqrt{d_k}：缩放系数：d_k是K的维度。<br>目的：防止点积结果过大。导致softmax后概率极端<br>$$</p>
<h2 id="10-讲解项目中用到的蒸馏，使用了什么损失函数并介绍公式"><a href="#10-讲解项目中用到的蒸馏，使用了什么损失函数并介绍公式" class="headerlink" title="10.讲解项目中用到的蒸馏，使用了什么损失函数并介绍公式"></a><code>10.讲解项目中用到的蒸馏，使用了什么损失函数并介绍公式</code></h2><p>项目中采用训练好的Bert模型作为教师模型，负责传授知识，采用BiLSTM模型作为学生模型，学习参数。</p>
<p>使用了KL散度损失函数来衡量两个模型的预测概率差值。</p>
<p>公式如下：<br>$$<br>D_{KL}(P \parallel Q) &#x3D; \sum_{x} P(x) \log \frac{P(x)}{Q(x)}<br>$$</p>
<h2 id="11-如何解决过拟合问题？"><a href="#11-如何解决过拟合问题？" class="headerlink" title="11.如何解决过拟合问题？"></a><code>11.如何解决过拟合问题？</code></h2><p>机器学习</p>
<ol>
<li>正则化：L1 正则、L2 正则</li>
<li>减少特征维度：<ul>
<li>去除冗余特征</li>
<li>降维：pca、奇异矩阵分解</li>
</ul>
</li>
<li>减枝（树模型）：预减枝、后减枝</li>
<li>数据增强</li>
<li>集成学习</li>
</ol>
<p>深度学习</p>
<ol>
<li>正则化：L1 正则、L2 正则</li>
<li>Dropout</li>
<li>early stop</li>
<li>归一化操作：批归一化、层归一化</li>
<li>降低模型复杂度：减少网络深度和宽度</li>
<li>数据增强：同义词替换、回译法、大模型生成</li>
</ol>
<p><code>12.你知道哪些激活函数？你项目中的Bert模型里面用的什么激活函数？</code></p>
<p>激活函数：ReLU,Sigmoid,Tanh,GELU,Softmax</p>
<p>Bert模型中用的：GELU激活函数</p>
<h2 id="13-介绍一下LSTM"><a href="#13-介绍一下LSTM" class="headerlink" title="13.介绍一下LSTM"></a><code>13.介绍一下LSTM</code></h2><p>LSTM 通过设计细胞状态（Cell State） 和三个门控单元，实现了对信息的 “选择性记忆” 和 “选择性遗忘”，从而有效捕捉长序列中的长期依赖。</p>
<ol>
<li>细胞状态</li>
</ol>
<ul>
<li>类似一条 “信息传送带”，贯穿整个序列过程，直接传递基本不变的信息，避免了梯度在长序列中快速衰减。</li>
<li>细胞状态的更新由门控机制控制，仅在必要时修改，保持了信息的长期稳定性。</li>
</ul>
<ol start="2">
<li>三个门控单元</li>
</ol>
<ul>
<li>遗忘门 决定 “细胞状态中哪些历史信息需要被遗忘”</li>
<li>输入门 决定 “哪些新信息需要被存入细胞状态”。 </li>
<li>细胞状态更新 结合遗忘门和输入门的结果，更新当前细胞状态：“先遗忘部分历史信息，再加入筛选后的新信息”。</li>
<li>输出门 决定 “基于当前细胞状态，输出哪些信息作为隐藏状态”。</li>
</ul>
<h2 id="问题：介绍一下BERT模型，模型输入是啥？经过哪些步骤？输出是啥？"><a href="#问题：介绍一下BERT模型，模型输入是啥？经过哪些步骤？输出是啥？" class="headerlink" title="问题：介绍一下BERT模型，模型输入是啥？经过哪些步骤？输出是啥？"></a>问题：介绍一下BERT模型，模型输入是啥？经过哪些步骤？输出是啥？</h2><p>答案：BERT模型主要用Transformer的encoder层（编码层）。输入部分是经过词嵌入层和位置编码的词向量；进入模型后，会经过多头注意力，再进行残差连接，接着进行前馈网络，最后再进行残差连接；输出是CLS的词向量，将其作为最后全分类的向量用于分类任务。BERT内部用的激活函数是Tanh。</p>
<h2 id="问题：残差连接是怎么做的？为什么要做残差连接？"><a href="#问题：残差连接是怎么做的？为什么要做残差连接？" class="headerlink" title="问题：残差连接是怎么做的？为什么要做残差连接？"></a>问题：残差连接是怎么做的？为什么要做残差连接？</h2><p>答案：残差连接是在多头注意力机制等操作后，将该操作的输入加上输出，再进行标准化，得到结果输出到下一层。作用是防止梯度消失和梯度爆炸，还能加速收敛、保留原始特征信息。</p>
<h2 id="问题：进行BERT模型蒸馏时，双向LSTM是怎么做蒸馏的？具体的。"><a href="#问题：进行BERT模型蒸馏时，双向LSTM是怎么做蒸馏的？具体的。" class="headerlink" title="问题：进行BERT模型蒸馏时，双向LSTM是怎么做蒸馏的？具体的。"></a>问题：进行BERT模型蒸馏时，双向LSTM是怎么做蒸馏的？具体的。</h2><p>答案：通过软标签加硬标签的组合进行模型训练。硬标签是双向LSTM模型的输出经过Softmax层后的输出概率分布，对比真实标签用交叉熵损失计算；还用到了KL散度损失（表述不完整）。</p>
<h2 id="问题：软标签相关的参数T（温度）的作用是什么？"><a href="#问题：软标签相关的参数T（温度）的作用是什么？" class="headerlink" title="问题：软标签相关的参数T（温度）的作用是什么？"></a>问题：软标签相关的参数T（温度）的作用是什么？</h2><p>答案：有点忘了。（面试官提示需回去查看）</p>
<h2 id="问题：LSTM每个细胞由什么组成？"><a href="#问题：LSTM每个细胞由什么组成？" class="headerlink" title="问题：LSTM每个细胞由什么组成？"></a>问题：LSTM每个细胞由什么组成？</h2><p>答案：由三门一细胞组成，分别是输入门、输出门、遗忘门和细胞状态这四部分。</p>
<h2 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h2><p><a target="_blank" rel="noopener" href="https://kduk730tiw.feishu.cn/docx/UO8PdUywpofPoyxurFKcpkSInYb">https://kduk730tiw.feishu.cn/docx/UO8PdUywpofPoyxurFKcpkSInYb</a></p>
<h1 id="机器学习题库"><a href="#机器学习题库" class="headerlink" title="机器学习题库"></a>机器学习题库</h1><p>1：什么是样本、特征、标签？请举例说明。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">样本：数据集中的单个数据个体，是模型学习的基本单位。</span><br><span class="line">特征：描述样本的属性或变量，是模型的输入。</span><br><span class="line">标签：样本的目标输出（在有监督学习中存在），用于指导模型学习</span><br></pre></td></tr></table></figure>

<p>2：解释 x_train、y_train、x_test、y_test 的含义及用途。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_train：训练集的特征数据，是模型学习的输入。</span><br><span class="line">y_train：训练集的标签数据，与 x_train 对应，用于指导模型参数学习。</span><br><span class="line">x_test：测试集的特征数据，用于评估模型在新数据上的表现。</span><br><span class="line">y_test：测试集的标签数据，与 x_test 对应，用于计算模型预测误差（评估泛化能力）。</span><br><span class="line">用途：x_train 和 y_train 用于模型训练（拟合参数）；x_test 和 y_test 用于验证模型是否过度拟合（泛化能力）。</span><br></pre></td></tr></table></figure>

<p>4：什么是回归问题和分类问题？如何区分？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">回归问题：目标变量是连续值（如温度、价格），模型预测具体数值。</span><br><span class="line">分类问题：目标变量是离散类别（如 “男 / 女”“正 / 负”），模型预测类别标签。</span><br></pre></td></tr></table></figure>

<p>5：简述机器学习建模的基本流程（从数据到模型评估）。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753183414677.png" alt="1753183414677"></p>
<h2 id="6：【面试重点】什么是特征工程？为什么它在机器学习中至关重要？"><a href="#6：【面试重点】什么是特征工程？为什么它在机器学习中至关重要？" class="headerlink" title="6：【面试重点】什么是特征工程？为什么它在机器学习中至关重要？"></a>6：【面试重点】什么是特征工程？为什么它在机器学习中至关重要？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753183439023.png" alt="1753183439023"></p>
<h2 id="7：【面试重点】解释过拟合和欠拟合的定义、原因及解决方案。"><a href="#7：【面试重点】解释过拟合和欠拟合的定义、原因及解决方案。" class="headerlink" title="7：【面试重点】解释过拟合和欠拟合的定义、原因及解决方案。"></a>7：【面试重点】解释过拟合和欠拟合的定义、原因及解决方案。</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753183456788.png" alt="1753183456788"></p>
<p>8：奥卡姆剃刀原则在机器学习中的体现是什么？</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753183483041.png" alt="1753183483041"></p>
<p>9：请简述 KNN 算法的核心思想</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753183539332.png" alt="1753183539332"></p>
<p>10：KNN 算法处理分类问题和回归问题的流程分别是什么？</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753183549160.png" alt="1753183549160"></p>
<p>11：KNN 算法中，K 值的选择对模型有什么影响？</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753183566972.png" alt="1753183566972"></p>
<p>12：为什么需要归一化和标准化的区别是什么？应用场景</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753183624281.png" alt="1753183624281"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753183638711.png" alt="1753183638711"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753183649416.png" alt="1753183649416"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250723173346103.png" alt="image-20250723173346103"></p>
<p>13：交叉验证和网格搜索的作用是什么？两者结合的优势是什么？</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250723173332995.png" alt="image-20250723173332995"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753183710834.png" alt="1753183710834"></p>
<p>14：什么是损失函数？</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753183796288.png" alt="1753183796288"></p>
<h2 id="15：【面试重点】MSE、MAE、RMSE-的定义及区别是什么？"><a href="#15：【面试重点】MSE、MAE、RMSE-的定义及区别是什么？" class="headerlink" title="15：【面试重点】MSE、MAE、RMSE 的定义及区别是什么？"></a>15：【面试重点】MSE、MAE、RMSE 的定义及区别是什么？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753183884109.png" alt="1753183884109"></p>
<p>16：梯度下降的常见类型有哪些？各自的特点是什么？</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753183898175.png" alt="1753183898175"></p>
<p>17：梯度下降中，学习率（步长）的作用是什么？设置不当会有什么问题？</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753183912187.png" alt="1753183912187"></p>
<p>18：L1 正则化（Lasso 回归）和 L2 正则化（Ridge 回归)的区别是什么？</p>
<p>​	L1（Lasso）：使部分权重为 0，可用于特征选择。</p>
<p>​	L2（Ridge）：使权重趋近于 0，减少权重绝对值，避免过拟合。</p>
<h2 id="19：【面试重点】逻辑回归主要解决什么类型的问题？逻辑回归的流程是什么-？"><a href="#19：【面试重点】逻辑回归主要解决什么类型的问题？逻辑回归的流程是什么-？" class="headerlink" title="19：【面试重点】逻辑回归主要解决什么类型的问题？逻辑回归的流程是什么 ？"></a>19：【面试重点】逻辑回归主要解决什么类型的问题？逻辑回归的流程是什么 ？</h2><p>逻辑回归(Logistic Regression)是一种用于二分类问题的机器学习算法，其核心原理是通过逻辑函数(sigmoid 函数)将线性回归模型的输出映射到概率空间(0 到 1 之间)，从而实现分类。</p>
<p>逻辑回归的核心原理:</p>
<p>线性组合 + 非线性变换:通过线性回归建模特征与标签的关系，用 sigmoid 函数将其转换为概率;</p>
<p>概率建模:基于最大似然估计构建对数损失函数，衡量预测概率与真实标签的差距;</p>
<p>优化求解:通过梯度下降等算法最小化损失函数，得到最优参数;</p>
<p>正则化防过拟合:通过L1&#x2F;L2 正则化约束参数复杂度，提升泛化能力</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250723194920910.png" alt="image-20250723194920910"></p>
<h2 id="20：【面试重点】AUC-指标的含义是什么？其取值范围是多少？AUC-0-5、AUC-1-、AUC-0分别说明模型的什么性能？"><a href="#20：【面试重点】AUC-指标的含义是什么？其取值范围是多少？AUC-0-5、AUC-1-、AUC-0分别说明模型的什么性能？" class="headerlink" title="20：【面试重点】AUC 指标的含义是什么？其取值范围是多少？AUC&#x3D;0.5、AUC&#x3D;1 、AUC&#x3D;0分别说明模型的什么性能？"></a>20：【面试重点】AUC 指标的含义是什么？其取值范围是多少？AUC&#x3D;0.5、AUC&#x3D;1 、AUC&#x3D;0分别说明模型的什么性能？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753184233517.png" alt="1753184233517"></p>
<p>21：CART 决策树与 ID3、C4.5 有哪些分支方式 ？</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753184256974.png" alt="1753184256974">22：为什么需要对决策树进行剪枝？常用的剪枝方法有哪些？</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753184289940.png" alt="1753184289940"></p>
<p>23：集成学习主要分为哪两类？请分别列举至少两种代表性算法。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753184374803.png" alt="1753184374803"></p>
<p>24：K-means 算法的具体实现流程是什么？</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1753184568824.png" alt="1753184568824"></p>
<h1 id="机器学习补充"><a href="#机器学习补充" class="headerlink" title="机器学习补充"></a>机器学习补充</h1><h2 id="有监督学习和无监督学习"><a href="#有监督学习和无监督学习" class="headerlink" title="有监督学习和无监督学习"></a>有监督学习和无监督学习</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/09.png" alt="09"></p>
<h2 id="什么是特征工程"><a href="#什么是特征工程" class="headerlink" title="什么是特征工程"></a>什么是特征工程</h2><p>特征工程，<code>是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。</code></p>
<p>特征工程包括：<code>特征提取、特征预处理、特征缩放</code></p>
<ul>
<li>特征提取：将任意数据（如文本或图像）转换为可用于机器学习的数字特征</li>
<li>特征预处理：通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程<ul>
<li>最值归一化（Min-Max Scaling）也就是归一化</li>
<li>均值方差归一化（Z-Score标准化）也就是标准化</li>
</ul>
</li>
<li>特征缩放：指在某些限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量的过程</li>
</ul>
<h2 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h2><p><code>过拟合</code></p>
<ul>
<li>训练集表现较好，但是测试集表现不好</li>
<li>产生原因：模型复杂度过高、训练数据量不足</li>
<li>可以通过剪枝（减少特征列）</li>
</ul>
<p><code>欠拟合</code></p>
<ul>
<li><p>训练集表现不好，测试集表现不好</p>
</li>
<li><p>产生原因： 学习到数据的特征过少</p>
</li>
<li><p>通过增加特征列解决</p>
</li>
</ul>
<p>注意：<code>正则化可以解决过拟合</code></p>
<h2 id="KNN-算法的核心思想"><a href="#KNN-算法的核心思想" class="headerlink" title="KNN 算法的核心思想"></a>KNN 算法的核心思想</h2><p>K近邻算法，给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例（也就是上面所说的K个邻居），这K个实例的多数属于某个类，就把该输入实例分类到这个类中。</p>
<p>KNN算法可以用于分类问题，也可以用于回归问题：</p>
<ul>
<li>分类问题，取前n个最近的实例，选择前n个最近的实例标签中<code>最多的</code>即为预测标签</li>
<li>回归问题，取前n个最近的实例，选择前n个最近的实例标签的<code>均值</code>作为预测标签</li>
</ul>
<h2 id="KNN分类问题和回归问题"><a href="#KNN分类问题和回归问题" class="headerlink" title="KNN分类问题和回归问题"></a>KNN分类问题和回归问题</h2><ol>
<li>计算测试样本和训练样本中每个样本点的距离（常见的距离度量有<code>欧式距离，曼哈顿距离</code>等）；</li>
<li><code>对上面所有的距离值进行排序；</code></li>
<li><code>选前 k 个最小距离的样本；</code></li>
<li>根据这 k 个样本的标签<code>进行投票</code>，得到最后的分类类别；</li>
</ol>
<table>
<thead>
<tr>
<th align="center">任务类型</th>
<th align="center">流程步骤</th>
<th align="center">输出决策方式</th>
</tr>
</thead>
<tbody><tr>
<td align="center">分类问题</td>
<td align="center">1. 计算距离 → 2. 选取K近邻 → 3. 统计K个邻居的类别频率 → 4. 最高票类别作为预测结果</td>
<td align="center">多数表决（Majority Vote）</td>
</tr>
<tr>
<td align="center">回归问题</td>
<td align="center">1. 计算距离 → 2. 选取K近邻 → 3. 计算K个邻居目标值的均值 → 4. 输出均值</td>
<td align="center">加权平均（可距离倒数加权）</td>
</tr>
</tbody></table>
<h2 id="KNN算法K值的选择"><a href="#KNN算法K值的选择" class="headerlink" title="KNN算法K值的选择"></a>KNN算法K值的选择</h2><ol>
<li><code>K值过小就意味着整体模型变得复杂，模型对噪声敏感，容易发生过拟合</code>。</li>
<li><code>K值过大就意味着整体的模型变得简单，模型忽略局部特征，可能欠拟合</code>。</li>
<li>K&#x3D;N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。</li>
</ol>
<p>在实际应用中，<code>K值一般取一个比较小的数值</code>，例如<code>采用交叉验证法</code>（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。</p>
<h2 id="特征预处理-归一化-标准化区别"><a href="#特征预处理-归一化-标准化区别" class="headerlink" title="特征预处理(归一化,标准化区别)"></a>特征预处理(归一化,标准化区别)</h2><p>特征预处理：通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程</p>
<p>归一化：<code>将特征值转换为0-1之间数据（也可以自定义区间范围）</code></p>
<p>标准化：<code>将特征转换为服从均值为0,方差为1的正态分布的数据</code></p>
<ul>
<li>最值归一化（Min-Max Scaling）也就是归一化</li>
</ul>
<p>$$<br>X_{\text{norm}} &#x3D; \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}<br>$$</p>
<p>$$<br>X’’ &#x3D; X’ \times (mx - mi) + mi<br>$$</p>
<ul>
<li>均值方差归一化（Z-Score标准化）也就是标准化</li>
</ul>
<p>$$<br>X_{\text{std}} &#x3D; \frac{X - \mu}{\sigma}<br>$$</p>
<p>Latex语法：<a href="https://liamjohnson-w.github.io/2024/08/08/2024.08.08/">传送门</a></p>
<h2 id="交叉验证和网格搜索"><a href="#交叉验证和网格搜索" class="headerlink" title="交叉验证和网格搜索"></a>交叉验证和网格搜索</h2><ul>
<li>交叉验证解决模型的数据输入问题(也就是<code>数据集划分</code>)，目的是得到更可靠的模型</li>
<li>网格搜索<code>解决超参数的组合问题</code>，目的是选择最优超参</li>
<li>两个组合再一起形成一个模型参数调优的解决方案</li>
</ul>
<p>API：GridSearchCV</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">es_model = KNeighborsClassifier()</span><br><span class="line">param_dict = &#123;<span class="string">&#x27;n_neighbors&#x27;</span>: [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>)]&#125;</span><br><span class="line">es = GridSearchCV(es_model, param_dict, cv=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<h2 id="损失函数是什么"><a href="#损失函数是什么" class="headerlink" title="损失函数是什么"></a>损失函数是什么</h2><p>损失函数是衡量预测值和真实值误差的一种函数。</p>
<p>线性回归的损失函数有MSE、MAE、RMSE<br>$$<br>\text{MSE} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} \left(Y_i - \hat{Y}_i\right)^2<br>$$</p>
<p>$$<br>\text{MAE} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} \left| Y_i - \hat{Y}_i \right|<br>$$</p>
<p>$$<br>RMSE &#x3D; \sqrt{\frac{1}{n}\sum_{i&#x3D;1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}}<br>$$</p>
<p>逻辑回归的损失函数为似然函数<br>$$<br>Loss(L) &#x3D; -\sum_{i&#x3D;1}^{m} \left( y_{i}\log(p_{i}) + (1 - y_{i})\log(1 - p_{i}) \right)<br>$$<br>其中：$ p_{i} &#x3D; \operatorname{sigmoid}(w^{T}x + b) $</p>
<h2 id="线性回归中最优参数求解"><a href="#线性回归中最优参数求解" class="headerlink" title="线性回归中最优参数求解"></a>线性回归中最优参数求解</h2><p>对于小数据集，使用<code>最小二乘法</code>(求导，求偏导联立方程)，并且要求特征矩阵$X$($Y&#x3D;X\cdot\beta$)可逆</p>
<p>对于大数据集，且$X$特征矩阵不可逆的情况下使用<code>梯度下降方法</code></p>
<p>特征共线性或高维数据，使用<code>正则化</code></p>
<h2 id="梯度下降的常见类型"><a href="#梯度下降的常见类型" class="headerlink" title="梯度下降的常见类型"></a>梯度下降的常见类型</h2><blockquote>
<p>在使用梯度下降时，需要进行调优</p>
<ul>
<li><code>算法的步长选择。步长太大，会导致迭代过快，有可能错过最优解。步长太小，收敛速度过慢</code>。需要多次运行后才能得到一个较为优的值。</li>
<li><code>算法参数的初始值选择</code>。由于有局部最优解的风险，需要多次用不同初始值运行算法，选择损失函数最小化的初值。</li>
<li>归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化。新期望为0，新方差为1，迭代速度可以大大加快。</li>
</ul>
</blockquote>
<p>$$<br>\theta_{i+1} &#x3D; \theta_{i} - \alpha \frac{\partial}{\partial \theta_{i}} J(\theta)<br>$$</p>
<p>步长：$θ_0$，</p>
<p>学习率：$α$，</p>
<p>$\frac{\partial}{\partial \theta_{i} } J(\theta)$为回归函数的对变量的导数。</p>
<h3 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h3><p>（Batch Gradient Descent, BGD）</p>
<p>每次迭代使用 全部训练数据 计算损失函数的梯度。</p>
<p>梯度计算稳定，但<code>可能陷入局部最优</code>且内存消耗大<br>$$<br>\theta_{t+1} &#x3D; \theta_t - \eta \cdot \nabla_\theta J(\theta; X, y)<br>$$</p>
<ul>
<li>$\eta$：学习率</li>
<li>$\nabla_\theta J(\theta)$：损失函数对参数 $\theta$ 的梯度</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">batch_gradient_descent</span>(<span class="params">X, y, lr=<span class="number">0.01</span>, epochs=<span class="number">100</span></span>):</span><br><span class="line">    theta = np.zeros(X.shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        grad = X.T @ (X @ theta - y) / <span class="built_in">len</span>(y)  <span class="comment"># 计算全量梯度</span></span><br><span class="line">        theta -= lr * grad</span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>



<h3 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><p>（Stochastic Gradient Descent, SGD）</p>
<p>每次迭代 随机选择一个样本 计算梯度并更新参数。<br>$$<br>\theta_{t+1} &#x3D; \theta_t - \eta \cdot \nabla_\theta J(\theta; x_i, y_i)<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">stochastic_gradient_descent</span>(<span class="params">X, y, lr=<span class="number">0.01</span>, epochs=<span class="number">100</span></span>):</span><br><span class="line">    theta = np.zeros(X.shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y)):</span><br><span class="line">            rand_idx = np.random.randint(<span class="built_in">len</span>(y))  <span class="comment"># 随机选样本</span></span><br><span class="line">            grad = X[rand_idx] * (X[rand_idx] @ theta - y[rand_idx])</span><br><span class="line">            theta -= lr * grad</span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>



<h3 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h3><p>（Mini-Batch Gradient Descent, MBGD）</p>
<p>折中方案，每次迭代使用 一小批样本（Batch） 计算梯度。<br>$$<br>\theta_{t+1} &#x3D; \theta_t - \eta \cdot \nabla_\theta J(\theta; X_{batch}, y_{batch})<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mini_batch_gradient_descent</span>(<span class="params">X, y, batch_size=<span class="number">32</span>, lr=<span class="number">0.01</span>, epochs=<span class="number">100</span></span>):</span><br><span class="line">    theta = np.zeros(X.shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(y), batch_size):</span><br><span class="line">            batch_X = X[i:i+batch_size]</span><br><span class="line">            batch_y = y[i:i+batch_size]</span><br><span class="line">            grad = batch_X.T @ (batch_X @ theta - batch_y) / batch_size</span><br><span class="line">            theta -= lr * grad</span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>



<table>
<thead>
<tr>
<th>类型</th>
<th>数据使用</th>
<th>内存消耗</th>
<th>收敛速度</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>BGD</td>
<td>全量数据</td>
<td>高</td>
<td>慢</td>
<td>小型数据集</td>
</tr>
<tr>
<td>SGD</td>
<td>单样本</td>
<td>低</td>
<td>快但不稳定</td>
<td>在线学习、非凸优化</td>
</tr>
<tr>
<td>MBGD</td>
<td>小批量（Batch）</td>
<td>中</td>
<td>中</td>
<td>深度学习（默认选择）</td>
</tr>
<tr>
<td>Momentum&#x2F;Adam</td>
<td>小批量 + 历史梯度</td>
<td>中</td>
<td>快且稳定</td>
<td>复杂非凸优化（如神经网络）</td>
</tr>
</tbody></table>
<h2 id="L1-L2-正则化的区别"><a href="#L1-L2-正则化的区别" class="headerlink" title="L1, L2 正则化的区别"></a>L1, L2 正则化的区别</h2><p>正则化项，也就是给损失函数loss function加上一个参数项，由于参数项的不同，产生了L1和L2正则化两种方式。</p>
<p>L1正则化(Lasso回归)：<code>将不重要的特征的参数为0</code></p>
<p>在原始损失函数中添加权重的<code>绝对值惩罚</code>迫使部分参数精确为零<br>$$<br>J(\mathbf{w}) &#x3D; \text{Loss}(\mathbf{y}, \hat{\mathbf{y}}) + \lambda \sum_{i&#x3D;1}^n |w_i|<br>$$</p>
<p>L2正则化（Ridge岭回归）：<code>将不重要的特征参数趋向于0</code></p>
<p><code>在原始损失函数中添加权重的平方和作为惩罚项</code><br>$$<br>J(\mathbf{w}) &#x3D; \text{Loss}(\mathbf{y}, \hat{\mathbf{y}}) + \lambda \sum_{i&#x3D;1}^n w_i^2<br>$$</p>
<h2 id="逻辑回归流程及解决了什么"><a href="#逻辑回归流程及解决了什么" class="headerlink" title="逻辑回归流程及解决了什么"></a>逻辑回归流程及解决了什么</h2><ol>
<li>获取数据</li>
<li>基本数据处理 <ol>
<li>缺失值处理</li>
<li>确定特征值,目标值 </li>
<li>分割数据</li>
</ol>
</li>
<li>特征工程(标准化) </li>
<li>机器学习(逻辑回归) </li>
<li>模型评估</li>
</ol>
<p>解决了二分类问题（可扩展至多分类）：</p>
<ul>
<li>垃圾邮件检测（是&#x2F;否）</li>
<li>疾病诊断（患病&#x2F;健康）</li>
<li>信用评分（违约&#x2F;不违约）</li>
</ul>
<h2 id="混淆矩阵四象限"><a href="#混淆矩阵四象限" class="headerlink" title="混淆矩阵四象限"></a>混淆矩阵四象限</h2><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250609202123066.png" alt="image-20250609202123066" style="zoom:67%;" />

<p>混淆矩阵作用在测试集样本集中：</p>
<ol>
<li>真实值是 正例 的样本中，被分类为 正例 的样本数量，叫做真正例（TP，True Positive）</li>
<li>真实值是 正例 的样本中，被分类为 假例 的样本数量，叫做伪反例（FN，False Negative）</li>
<li>真实值是 假例 的样本中，被分类为 正例 的样本数量，叫做伪正例（FP，False Positive）</li>
<li>真实值是 假例 的样本中，被分类为 假例 的样本数量，叫做真反例（TN，True Negative）</li>
</ol>
<h2 id="精确率，召回率反映什么"><a href="#精确率，召回率反映什么" class="headerlink" title="精确率，召回率反映什么"></a>精确率，召回率反映什么</h2><p>精确率也叫做查准率，指的是对正例样本的预测准确率。<br>$$<br>P &#x3D; \frac{TP}{TP + FP}<br>$$<br>召回率也叫做查全率，指的是预测为真正例样本占所有真实正例样本的比重。<br>$$<br>P &#x3D; \frac{TP}{TP + FN}<br>$$<br>F1-score 指标：模型在精度、召回率这两个方向的综合评估预测能力<br>$$<br>F1 &#x3D; \frac{2TP}{2TP + FN + FP} &#x3D; \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}<br>$$</p>
<h2 id="ROC曲线横纵轴-每个点"><a href="#ROC曲线横纵轴-每个点" class="headerlink" title="ROC曲线横纵轴,每个点"></a>ROC曲线横纵轴,每个点</h2><p>横纵：TPR （True Positive Rate）：正样本中被预测为正样本的概率 (召回率)</p>
<p>纵轴：FPR （False Positive Rate）：负样本中被预测为正样本的概率</p>
<p>ROC曲线是评估二分类模型性能的重要工具，尤其在样本不平衡的场景下表现优异。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250711213215389.png" alt="image-20250711213215389"></p>
<h2 id="AUC-指标"><a href="#AUC-指标" class="headerlink" title="AUC 指标"></a>AUC 指标</h2><p>AUC 是 ROC 曲线下面的面积，<code>该值越大，则模型的辨别能力就越强</code></p>
<p><code>AUC 值主要评估模型对正例样本、负例样本的辨别能力</code></p>
<blockquote>
<p>AUC&#x3D;0.5、AUC&#x3D;1 、AUC&#x3D;0分别说明模型的什么性能</p>
</blockquote>
<p>AUC&#x3D;0.5说明模型无判别能力（等同于随机猜测）</p>
<p>AUC&#x3D;1说明模型完美分类</p>
<p>AUC&#x3D;0说明模型完全反向预测</p>
<h2 id="决策树？基本结构"><a href="#决策树？基本结构" class="headerlink" title="决策树？基本结构"></a>决策树？基本结构</h2><blockquote>
<p>树中每个内部节点表示<code>一个特征上的判断</code></p>
<p>每个分支代表<code>一个判断结果的输出</code></p>
<p>每个叶子节点代表<code>一种分类结果</code></p>
</blockquote>
<p>构建决策树包括三个步骤：</p>
<ul>
<li>特征选择：选取有较强分类能力的特征。</li>
<li>决策树生成：根据选择的特征生成决策树。典型的算法有ID3、C4.5、CART，它们生成决策树过程相似，ID3是采用<code>信息增益</code>作为特征选择度量，而C4.5采用<code>信息增益率</code>、CART<code>基尼指数</code>。</li>
<li>决策树剪枝：决策树也易过拟合，采用剪枝的方法缓解过拟合。剪枝原因是决策树生成算法生成的树对训练数据的预测很准确，但是对于未知数据分类很差，这就产生了<code>过拟合</code>的现象。</li>
</ul>
<h2 id="熵-Entropy-在决策树中作用"><a href="#熵-Entropy-在决策树中作用" class="headerlink" title="熵(Entropy)在决策树中作用"></a>熵(Entropy)在决策树中作用</h2><p>熵(Entropy)：信息论中代表随机变量不确定度的度量</p>
<p>熵越大，数据的不确定性度越高，信息就越多</p>
<p>熵越小，数据的不确定性越低</p>
<h2 id="ID3-C4-5-CART-决策树"><a href="#ID3-C4-5-CART-决策树" class="headerlink" title="ID3,C4.5,CART 决策树"></a>ID3,C4.5,CART 决策树</h2><p>ID3是采用<code>信息增益</code>作为特征选择度量，而C4.5采用<code>信息增益率</code>、CART<code>基尼指数</code>。</p>
<p>信息增益（ID3）、信息增益率值越大（C4.5），则说明优先选择该特征。</p>
<p>基尼指数值越小（cart），则说明优先选择该特征。</p>
<h2 id="决策树节点切分依据"><a href="#决策树节点切分依据" class="headerlink" title="决策树节点切分依据"></a>决策树节点切分依据</h2><p>不同类型的决策树有不同的特征度量指标</p>
<p>例如ID3是采用<code>信息增益</code>作为特征选择度量，而C4.5采用<code>信息增益率</code>、CART<code>基尼指数</code>。</p>
<p>信息熵：<br>$$<br>\large<br>H &#x3D; -\sum_{i&#x3D;1}^{k}p_i\log_{b}(p_i)<br>$$<br>信息增益(信息熵 - 条件熵)：<br>$$<br>\large<br>g(D,A)&#x3D;H(D)-H(D|A)<br>$$<br>信息增益率 （信息增益&#x2F;特征熵）：<br>$$<br>\begin{aligned}<br>\text{Gain_Ratio}(D, a) &amp;&#x3D; \frac{\text{Gain}(D, a)}{IV(a)} \<br>\end{aligned}<br>$$<br>基尼指数（分类树）：<br>$$<br>Gini(D) &#x3D; \sum_{k&#x3D;1}^{|y|} \sum_{k’ \neq k} p_k p_{k’} &#x3D; 1 - \sum_{k&#x3D;1}^{|y|} p_k^2<br>$$<br>Cart回归树：<br>$$<br>\operatorname{Loss}(y, f(x))&#x3D;(f(x)-y)^{2}<br>$$</p>
<h2 id="决策树剪枝"><a href="#决策树剪枝" class="headerlink" title="决策树剪枝"></a>决策树剪枝</h2><p>决策树的剪枝基本策略有 预剪枝 (Pre-Pruning) 和 后剪枝 (Post-Pruning)。</p>
<ul>
<li>预剪枝：其中的核心思想就是，<code>在每一次实际对结点进行进一步划分之前，先采用验证集的数据来验证如果划分是否能提高划分的准确性。</code>如果不能，就把结点标记为叶结点并退出进一步划分；如果可以就继续递归生成节点。</li>
<li>后剪枝：后剪枝则是<code>先从训练集生成一颗完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来泛化性能提升，则将该子树替换为叶结点</code>。</li>
</ul>
<h2 id="Bagging-与-Boosting区别"><a href="#Bagging-与-Boosting区别" class="headerlink" title="Bagging 与 Boosting区别"></a>Bagging 与 Boosting区别</h2><blockquote>
<p>Baggging 框架通过有<code>放回的抽样</code>产生不同的训练集，从而训练具有差异性的弱学习器，然后<code>通过平权投票、多数表决的方式决定预测结果</code>。</p>
<p>Boosting 体现了<code>提升思想，每一个训练器重点关注前一个训练器不足的地方进行训练，通过加权投票的方式，得出预测结果。</code></p>
<p><code>只有随机森林算法是基于Bagging，Adaboost，GDBT，XGBoost都是基于Boosting</code>。</p>
</blockquote>
<p>区别一:数据方面</p>
<ul>
<li>Bagging：<code>有放回采样</code></li>
<li>Boosting：<code>全部数据集, 重点关注前一个弱学习器不足</code></li>
</ul>
<p>区别二:投票方面</p>
<ul>
<li>Bagging：<code>平权投票</code></li>
<li>Boosting：<code>加权投票</code></li>
</ul>
<p>区别三:学习顺序</p>
<ul>
<li>Bagging的<code>学习是并行的，每个学习器没有依赖关系</code></li>
<li>Boosting<code>学习是串行，学习有先后顺序</code></li>
</ul>
<h2 id="Adaboost-的完整构建流程"><a href="#Adaboost-的完整构建流程" class="headerlink" title="Adaboost 的完整构建流程"></a>Adaboost 的完整构建流程</h2><p>1 初始化弱学习器（目标值的均值作为预测值）</p>
<p>2 迭代构建学习器，每一个学习器拟合上一个学习器的负梯度</p>
<p>3 直到达到指定的学习器个数</p>
<p>4 当输入未知样本时，将所有弱学习器的输出结果组合起来作为强学习器的输出</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250716151739736.png" alt="image-20250716151739736"></p>
<h2 id="梯度提升树与随机森林区别"><a href="#梯度提升树与随机森林区别" class="headerlink" title="梯度提升树与随机森林区别"></a>梯度提升树与随机森林区别</h2><p>梯度提升树（GDBT）</p>
<table>
<thead>
<tr>
<th align="center">维度</th>
<th align="center">梯度提升树 (GBDT)</th>
<th align="center">随机森林 (RF)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">集成策略</td>
<td align="center">Boosting：串行训练，每棵树纠正前一棵的错误</td>
<td align="center">Bagging：并行训练，每棵树独立投票</td>
</tr>
<tr>
<td align="center">依赖关系</td>
<td align="center">树之间有强依赖（残差拟合）</td>
<td align="center">树之间完全独立</td>
</tr>
<tr>
<td align="center">目标</td>
<td align="center">最小化损失函数的梯度方向</td>
<td align="center">通过投票&#x2F;平均降低方差</td>
</tr>
</tbody></table>
<h2 id="XGBoost推导过程概述"><a href="#XGBoost推导过程概述" class="headerlink" title="XGBoost推导过程概述"></a>XGBoost推导过程概述</h2><ul>
<li><code>在损失函数的基础上+正则化项</code>，(使用泰勒展开二项式进行展开)</li>
<li><code>基于泰勒展开二项式进行转换，转换为近似函数</code> 。(把问题从样本的角度转换为叶子节点角度!!!)</li>
<li><code>把问题从样本角度-》叶子节点的角度进行分析</code></li>
<li>得出最终结果 ，打分函数<ul>
<li>Gain值 &#x3D; 拆分前的分-(拆分后的左子树的分+拆分后的右子树的分)</li>
<li><code>Gain值越小越好</code></li>
<li>如果gain &gt; 0，则分类之后树的损失更小，会考虑此次分裂</li>
<li>如果gain &lt; 0，说明分裂后的分数比分裂之前的分数大，此时不建议分裂</li>
</ul>
</li>
</ul>
<p>参考链接：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://showmeai.blog.csdn.net/article/details/123402980">图解机器学习算法(10) | XGBoost模型最全解析</a> </li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/143936295">关于XGBoost我所知道的一切</a>（ID3，C4.5，CART推导过程）</li>
</ul>
<h1 id="NLP面试题库"><a href="#NLP面试题库" class="headerlink" title="NLP面试题库"></a>NLP面试题库</h1><h2 id="Transformer架构"><a href="#Transformer架构" class="headerlink" title="Transformer架构"></a>Transformer架构</h2><p>提示:输入部分、输出部分、编码器部分、解码器部分<br>Transformer架构可以分为四个部分:<code>输入部分、编码器部分、解码器部分和输出部分</code>。</p>
<ol>
<li>输入部分:包括源文本嵌入层及其位置编码器，目标文本嵌入层及其位置编码器。源文本嵌入层将源语言文本转化为向量表示，目标文本嵌入层将目标语言文本转化为向量表示。位置编码器则将位置信息转化为向量表示，以用于Transformer模型中的自注意力机制。</li>
<li>编码器部分:由N个编码器层堆叠而成。每个编码器层由两个子层连接结构组成。第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接。第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接。编码器部分可以有效地捕捉输入序列中的上下文信息。</li>
<li>解码器部分:由N个解码器层堆叠而成。每个解码器层由三个子层连接结构组成。第一个子层连接结构包括一个带掩码的-多头自注意力子层和规范化层以及一个残差连接。第二个子层连接结构包括一个多头注意力子层(编码器到解码器)和规范化层以及一个残差连接。第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接。解码器部分可以有效地生成目标序列。</li>
<li>输出部分:包括线性层和Softmax层。线性层将解码器部分的输出转化为目标语言文本的向量表示，Softmax层则将这个向量表示转化为预测的目标语言文本。</li>
</ol>
<p>总的来说，Transformer架构通过使用自注意力机制和残差连接等方法，有效地捕捉输入序列中的上下文信息，并生成目标序列。同时，其预训练模型可以用于不同的NLP任务，如机器翻译、文本生成等，表现出较强的泛化能力。</p>
<p>上面看看就行：</p>
<blockquote>
<p>Transformer架构：分为Encoder和Decoder两个部分</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250728144342720.png" alt="image-20250728144342720"></p>
<p><code>Encoder部分主要包括：</code></p>
<ul>
<li>Input Embedding （输入嵌入）:<code>将输入的单词或者符号转换成固定维度的向量表示，使其能够被模型处理。</code></li>
<li>Positional Encoding（位置编码）：因为在LSTM中，每个隐含层的节点，都是要接收上一个隐含层的输出，所以他是有天然的时序顺序在里面的。但是<code>Transformer中，没有使用RNN，所以就需要给他的词向量中加入位置信息</code>。</li>
<li><code>Multi-Head Attention 多头注意力机制</code>：所谓多头，其实在底层就是多个权重矩阵。</li>
<li>Feed Forward 前馈网络：<ul>
<li>前馈网络包含两个全连接层：<ul>
<li>第一个全连接层将输入的维度扩展（例如，从512维扩展到2048维），接着是一个激活函数（通常是ReLU或GELU）</li>
<li>第二个全连接层，将维度从扩展的维度缩减回原始维度（例如，从2048维缩减回512维）。</li>
<li>前馈网络处理完后，先对其进行一个残差连接，再进行层归一化处理。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><code>Decoder部分主要包括：</code></p>
<ul>
<li>Masked Multi-Head Attention 具有掩码的多头注意力机制</li>
<li>Feed Forward 前馈网络</li>
<li>分类器</li>
</ul>
<h2 id="注意力机制QKV"><a href="#注意力机制QKV" class="headerlink" title="注意力机制QKV"></a>注意力机制QKV</h2><p>注意力机制中的Q、K和V分别表示查询(Query)、键(Key)和值 (Value)。它们是用于计算注意力分数的必要元素<br>在注意力机制中，QKV的计算可以帮助模型将输入序列映射到输出序列，特别是在处理自然语言处理任务时，比如机器翻译、语音识别等。<br>QKV的基本计算规则是在输入序列上进行逐元素地计算，具体公式为:<br>$$<br>score &#x3D; Softmax(\frac{Q K^T}{\sqrt{d}}) * V<br>$$</p>
<p>其中，q表示查询向量，k表示键向量，v表示值向量，$\sqrt{d}$表示是一个关键的缩放因子，避免其绝对值过大导致Softmax输出梯度消失。</p>
<p>这个公式主要用于计算查询(Query)和键(Key)之间的相似度，并通过Softmax函数得到一个权重分布，最后用这个权重分布去获取对应的值(Value)。</p>
<h2 id="自注意力机制的计算过程"><a href="#自注意力机制的计算过程" class="headerlink" title="自注意力机制的计算过程"></a>自注意力机制的计算过程</h2><p>自注意力机制的计算过程如下:</p>
<ol>
<li>输入句子中的每个词向量依次被用作査询(Q)、键(K)和值(V)。在这个过程中，每个词向量都与其他所有词向量进行计算，从而得到一个注意力分数。</li>
<li>注意力分数被用于计算一个新的表示向量，这个向量将取代原来的词向量。具体来说，每个词向量都被乘以对应的其他词向量的权重(由注意力分数决定)，并将这些乘积相加，得到一个新的表示向量。</li>
<li>这个新的表示向量可以捕捉到句子中的长依赖关系，因为它包含了其他词向量的信息。然而，这种计算方式可能会在计算过程中产生梯度消失或爆炸的问题。为了解决这个问题，可以使用点积注意力机制或加性注意力机制来进行改进。</li>
<li>点积注意力机制的计算过程与上述过程类似，但使用了点积操作来计算注意力分数。具体来说，每个词向量与其他所有词向量进行点积计算，得到一个分数矩阵。这个短阵被用于计算新的表示向量。</li>
<li>加性注意力机制的计算过程也类似，但使用了加法操作来计算注意力分数。具体来说，每个词向量与其他所有词向量进行加法计算，得到一个加法结果。这个结果被用于计算新的表示向量。需要注意的是，在实际应用中，自注意力机制的计算可能会根据具体的任务和模型结构进行调整和优化。</li>
</ol>
<h2 id="CBOW模式和skipgram模式-FASTTEXT"><a href="#CBOW模式和skipgram模式-FASTTEXT" class="headerlink" title="CBOW模式和skipgram模式(FASTTEXT)"></a>CBOW模式和skipgram模式(FASTTEXT)</h2><p>Word2Vec是一个用于生成词向量的工具，它通过两种模型–跳字模型(Skipgram)和连续词袋模型(Continuous Bag ofWords，简称CBOW)-一以及两种高效训练方法–负采样和层序softmax–来实现词向量的训练。<br>Skip·gram模型将当前词作为输入，预测其上下文作为输出-。CBOW模型则相反，它预测当前词作为输出，上下文作为输入。这两种模型都可以理解为两种实现方式，而不是Word2Vec包含的两个模型。<br>对于CBOW模型，它通过训练一个神经网络来预测一个词，给定其上下文。这个神经网络输入上下文向量，输出一个向量，这个向量再通过一个softmax层得到一个概率分布，最后选择概率最大的词作为预测结果。CBOW模型的特点是它关注的是局部上下文，即相邻的词。<br>Skip·gram模型则通过训练一个神经网络来预测上下文中的词，给定当前词。这个神经网络输入当前词向量，输出一个向量，这个向量再通过一个softmax层得到一个概率分布，最后选择概率最大的词作为预测结果。Skip·gram模型的特点是它关注的是全局上下文，即整个句子的语义。<br>这两种模型各有优劣。CBOW模型在训练速度上更快，因为它只需要预测一个词，而Skip·gram模型需要预测上下文中的每个词。但是Skip-gram模型在捕捉全局语义信息上表现更好。<br>总的来说，Word2vec的这两种模型都是基于神经网络的，它们通过训练大量的语料库学习词的表示，使得词向量可以定量地衡量词与词之间的关系，挖掘词之间的联系。这些词向量可以被用作预训练模型，然后放入另一个神经网络(比如RNN)中，从而使得相似的文本在新的向量空间中组合在一起。</p>
<h2 id="NLP中词向量的表示方法"><a href="#NLP中词向量的表示方法" class="headerlink" title="NLP中词向量的表示方法"></a>NLP中词向量的表示方法</h2><p>提示关键词:one-hot、word2vec，nn.Embedding，动态词向量、静态词向量<br>在NLP中，词的表示方法有多种，其中较为常见的是one-hot编码、word2vec、nn.Embedding以及静态词向量和动态词向量，</p>
<ol>
<li>one·hot编码:这是一种将离散变量转换为连续向量的方法。在NLP中，每个单词被分配一个唯一的整数，然后使用一个非常大的向量(通常是一维的)来表示这个单词。在这个向量中，只有对应单词编号的位置是1，其余位置都是0。这种表示方法不能很好地捕捉单词之间的语义和语法关系，因为它们在向量空间中是孤立的。</li>
<li>word2vec:这是由Google开发的一种用于学习词向量的神经网络模型。它通过训练语料库学习单词的表示，使得相似的单词在向量空间中相互靠近。Word2Vec有两种模型:CBOW(ContinuousBag ofWords)和Skip-gram。CBOW模型通过训练一个神经网络来预测一个词，给定其上下文;Skip-gram模型则通过训练一个神经网络来预测上下文中的词，给定当前词。Word2Vec的词向量可以捕捉到单词之间的语义和语法关系，使得相似的单词在向量空间中聚集在一起。</li>
<li>nn.Embedding:这是PyTorch等深度学习框架提供的一种用于学习词向量的方法。它可以将离散的单词映射到一个连续的向量空间中。nn.Embedding层可以接受一个单词的索引作为输入，并返回对应的词向量作为输出。与one·hot编码不同，nn.Embedding可以学习到单词之间的语义关系，并且可以在句子中捕捉到上下文信息。</li>
<li>静态词向量和动态词向量:静态词向量是指在训练时固定不变的词向量，如Word2Vec和nn.Embedding生成的词向量。而动态词向量则是指在推理时根据上下文动态调整的词向量。动态词向量的代表模型有BERT、GPT等。这些模型在训练时会对每个单词生成一个固定的词向量，但在推理时可以根据上下文动态调整词向量的权重，从而更好地捕捉当前单词的语义信息。<br>总的来说，不同的词向量表示方法各有优劣，选择哪种方法取决于具体的应用场景和任务需求</li>
</ol>
<h2 id="BERT是双向语言模型-如何理解"><a href="#BERT是双向语言模型-如何理解" class="headerlink" title="BERT是双向语言模型, 如何理解"></a>BERT是双向语言模型, 如何理解</h2><ul>
<li>对于输入BERT的语言的序列, 站在任意位置, 即可以看见前面的信息又可以看见后面的信息, 所以是双向语言模型. 区别于GPT系列的生成式语言模型, 只能look-ahead, 不能获取后面的信息.</li>
<li>追问: BERT为啥能解决一词多意的问题?<ul>
<li>上下文不同, 经过多层计算后的输出层词张量自然不同, 一词就有多个张量, 对应不同的语义</li>
</ul>
</li>
<li>追问: BERT提取的词向量, 同一个词在不同句子获取的词向量一致吗?<ul>
<li>不一样</li>
</ul>
</li>
</ul>
<h2 id="BERT多头注意力机制代码层面"><a href="#BERT多头注意力机制代码层面" class="headerlink" title="BERT多头注意力机制代码层面"></a>BERT多头注意力机制代码层面</h2><p>源代码中将Q, K, V三个矩阵的最后一个维度768进行了除法操作, 变成了4维张量. 在进行完多头注意力的各自独立操作后, 又重新view()操作回到了3维张量.</p>
<h2 id="为什么在Transformer用LayerNorm"><a href="#为什么在Transformer用LayerNorm" class="headerlink" title="为什么在Transformer用LayerNorm"></a>为什么在Transformer用LayerNorm</h2><ul>
<li>BatchNorm - 计算每个批次每层的平均值和方差.</li>
<li>LayerNorm - 独立计算每层每个样本的均值和方差.</li>
<li>因为LayerNorm对于批量大小是鲁棒的, 而且不会受到批次中不同长度样本的影响, 并且工作的更好. 因为LayerNorm在样本级别而不是批量级别工作!!!</li>
</ul>
<h2 id="view和transpose先后问题"><a href="#view和transpose先后问题" class="headerlink" title="view和transpose先后问题"></a>view和transpose先后问题</h2><ul>
<li>可以先view(), 再算transpose()</li>
<li>如果先transpose(), 必须加上contiguous(), 才能再view()</li>
</ul>
<h2 id="预训练模型序列长度为1000"><a href="#预训练模型序列长度为1000" class="headerlink" title="预训练模型序列长度为1000+"></a>预训练模型序列长度为1000+</h2><ul>
<li>推理时碰到sequence_length比训练时更长, 会出现碰见没有见过的position encoding, 造成训练和预测的不一致.</li>
<li>BERT训练时有MASK, 预测时没有MASK, 这个不一致怎么办?</li>
</ul>
<h2 id="MacBERT在优化MLM预训练任务"><a href="#MacBERT在优化MLM预训练任务" class="headerlink" title="MacBERT在优化MLM预训练任务"></a>MacBERT在优化MLM预训练任务</h2><ul>
<li>1: 采用了n-gram的模式, 对1-gram, 2-gram, 3-gram, 4-gram的词汇依次按照40%, 30%, 20%, 10%的比例进行掩码.</li>
<li>2: 不用[MASK]来遮掩, 而是用word2vec的近义词来遮掩, 选用同样字符数的词来mask, 如果找不到则进行规则降级, 用随机字来遮掩.</li>
</ul>
<h2 id="多分类问题数据样本不均衡"><a href="#多分类问题数据样本不均衡" class="headerlink" title="多分类问题数据样本不均衡"></a>多分类问题数据样本不均衡</h2><ul>
<li>可以采用单一检测模型先进行一轮过滤, 把难分类的或者样本少的类别先分出来.</li>
<li>损失函数可以对少样本的类别加权重, 采用FocalLoss</li>
</ul>
<h2 id="对比学习最重要的两个衡量指标"><a href="#对比学习最重要的两个衡量指标" class="headerlink" title="对比学习最重要的两个衡量指标"></a>对比学习最重要的两个衡量指标</h2><ul>
<li>Alignment: 对齐性, 对于相似的样本应该更接近, 也就是越相似的样本特征越接近.</li>
<li>Uniformity: 均匀性, 映射到单位超球面的特征应该尽量均匀分布, 意味着不同的样本要有差异.</li>
<li>追问: 具体用过哪些模型?<ul>
<li>SimCSE模型在项目中用过, 做用户关于搜索语义匹配的模块.</li>
</ul>
</li>
</ul>
<h2 id="Transformer和BERT中的位置编码"><a href="#Transformer和BERT中的位置编码" class="headerlink" title="Transformer和BERT中的位置编码"></a>Transformer和BERT中的位置编码</h2><ul>
<li>Transformer中是三角函数算完后, 就固定不变了.</li>
<li>BERT中是三角函数算(初始化的时候不变), 但是后续随着模型一起训练就会变化. <ul>
<li>追问: BERT的位置编码在后续推理中会变化吗?<ul>
<li>会的!!!</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="模型在工业界的加速部署的问题-一般部署中比较喜欢的模型有哪些"><a href="#模型在工业界的加速部署的问题-一般部署中比较喜欢的模型有哪些" class="headerlink" title="模型在工业界的加速部署的问题? 一般部署中比较喜欢的模型有哪些?"></a>模型在工业界的加速部署的问题? 一般部署中比较喜欢的模型有哪些?</h2><ul>
<li>首先肯定是选取简单模型, 比如做NER任务的IDCNN, 左文本分类任务的TextCNN, 或者FastText. 当然了现实中BERT系列的直接部署应用很广泛.</li>
<li>追问: 有用过哪些更高级的方案吗? 不是模型层面的?<ul>
<li>其次就是模型量化, 剪枝, 或者知识蒸馏, 达成模型的小型化目标</li>
<li>工程方面onnxruntime 🥭格式, 可以极大的加速 — 2022年已经成为工业界事实上的推理标准!!! ⭕️</li>
<li>土豪公司可以考虑TensorRT的GPU优化方案</li>
<li>底层的C++改写, 尤其在移动端部署的时候</li>
<li>FlashAttention 🥭加速技术—2023年已经成为工业界事实上的训练标准!!! ⭕️</li>
</ul>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://liamjohnson-w.github.io">李俊泽</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://liamjohnson-w.github.io/2025/09/14/TrafficDefenceDetection/">https://liamjohnson-w.github.io/2025/09/14/TrafficDefenceDetection/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/doc/">doc</a><a class="post-meta__tags" href="/tags/AI/">AI</a></div><div class="post_share"><div class="social-share" data-image="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen132.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> Donate</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/09/28/LLM_Base/" title="LLM大模型基础"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen112.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">LLM大模型基础</div></div></a></div><div class="next-post pull-right"><a href="/2025/07/18/Project/" title="Jason Project Demo"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Jason Project Demo</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2025/06/29/InterviewQuestions/" title="Jason Interview Note"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen121.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-29</div><div class="title">Jason Interview Note</div></div></a></div><div><a href="/2025/07/18/Project/" title="Jason Project Demo"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="title">Jason Project Demo</div></div></a></div><div><a href="/2025/10/21/RAG/" title="RAG"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen3.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-21</div><div class="title">RAG</div></div></a></div><div><a href="/2024/05/01/2024.05.01/" title="Document"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-01</div><div class="title">Document</div></div></a></div><div><a href="/2025/10/25/Agent/" title="Agent-Note"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/Pusheen1121.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-25</div><div class="title">Agent-Note</div></div></a></div><div><a href="/2025/06/03/2025.06.03/" title="OLLAMA"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250603132906789.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-03</div><div class="title">OLLAMA</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/tx.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">李俊泽</div><div class="author-info__description">机器都在学习,你有什么理由不学习?</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">239</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">58</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/weiswift/"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/weiswift" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:1265019024@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">博客为本人搭建 Github托管 仅记录学习过程 不做引流 不做排名 不打广告！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Day01"><span class="toc-number">1.</span> <span class="toc-text">Day01</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%EF%BC%9F"><span class="toc-number">1.1.</span> <span class="toc-text">1、什么是知识图谱？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E9%A1%B9%E7%9B%AE%E7%9A%84%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84%E5%9B%BE%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">1.2.</span> <span class="toc-text">2、项目的技术架构图是怎样的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E9%A1%B9%E7%9B%AE%E7%94%A8%E5%88%B0%E4%BA%86%E5%93%AA%E4%BA%9B%E5%B7%A5%E5%85%B7%EF%BC%9F"><span class="toc-number">1.3.</span> <span class="toc-text">3、项目用到了哪些工具？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%94%A8MySQL%E6%9D%A5%E5%AD%98%E5%82%A8%E4%B8%89%E5%85%83%E7%BB%84%E6%95%B0%E6%8D%AE%EF%BC%9F"><span class="toc-number">1.4.</span> <span class="toc-text">4、为什么不用MySQL来存储三元组数据？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E5%AE%9E%E4%BD%93%E5%92%8CNER%EF%BC%9F"><span class="toc-number">1.5.</span> <span class="toc-text">5、什么是实体和NER？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%9C%89%E5%93%AA%E4%BA%9B%E6%96%B9%E6%B3%95%EF%BC%9F"><span class="toc-number">1.6.</span> <span class="toc-text">6、命名实体识别有哪些方法？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81%E4%B8%BE%E4%B8%AA%E4%BE%8B%E5%AD%90%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%8B%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E8%A7%84%E5%88%99%E7%9A%84%E6%96%B9%E6%B3%95%E6%8A%BD%E5%8F%96%E5%AE%9E%E4%BD%93%EF%BC%9F"><span class="toc-number">1.7.</span> <span class="toc-text">7、举个例子描述一下如何使用规则的方法抽取实体？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day02"><span class="toc-number">2.</span> <span class="toc-text">Day02</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81LSTM%E9%9D%A2%E8%AF%95%E9%A2%98"><span class="toc-number">2.1.</span> <span class="toc-text">1、LSTM面试题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E7%BA%BF%E6%80%A7%E9%93%BE%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%EF%BC%88Linear-chain-CRF%EF%BC%89"><span class="toc-number">2.2.</span> <span class="toc-text">2、什么是线性链条件随机场（Linear-chain-CRF）?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%8BBiLSTM-CRF%E6%9E%B6%E6%9E%84%EF%BC%9F"><span class="toc-number">2.3.</span> <span class="toc-text">3、描述一下BiLSTM+CRF架构？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81CRF%E4%B8%AD%E7%9A%84%E5%8F%91%E5%B0%84%E5%88%86%E6%95%B0%E5%92%8C%E8%BD%AC%E7%A7%BB%E5%88%86%E6%95%B0%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">2.4.</span> <span class="toc-text">4、CRF中的发射分数和转移分数是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E8%AF%B4%E4%B8%80%E4%B8%8BCRF%E5%BB%BA%E6%A8%A1%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">2.5.</span> <span class="toc-text">5、说一下CRF建模的损失函数是怎样的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E5%89%8D%E5%90%91%E7%AE%97%E6%B3%95%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">2.6.</span> <span class="toc-text">6、前向算法是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81Viterbi%E8%A7%A3%E7%A0%81%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">2.7.</span> <span class="toc-text">7、Viterbi解码是什么？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day03"><span class="toc-number">3.</span> <span class="toc-text">Day03</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E5%9C%A8%E9%A1%B9%E7%9B%AE%E4%B8%AD%EF%BC%8C%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AE%E8%B7%AF%E5%BE%84%EF%BC%9F"><span class="toc-number">3.1.</span> <span class="toc-text">1、在项目中，应该如何设置路径？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E7%AE%80%E5%8D%95%E8%AF%B4%E4%B8%80%E4%B8%8B%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%9A%84%E6%9C%80%E7%BB%88%E6%A0%BC%E5%BC%8F%E8%A6%81%E6%B1%82%EF%BC%9F"><span class="toc-number">3.2.</span> <span class="toc-text">2、简单说一下数据处理的最终格式要求？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E5%9C%A8%E5%B0%86%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%88%90%E6%9C%80%E7%BB%88%E7%9A%84%E6%A0%BC%E5%BC%8F%E8%A6%81%E6%B1%82%E6%97%B6%EF%BC%8C%E4%B8%80%E8%88%AC%E5%8F%AF%E4%BB%A5%E5%9C%A8%E5%93%AA%E4%BA%9B%E5%9C%B0%E6%96%B9%E5%81%9A%E5%A4%84%E7%90%86%EF%BC%9F"><span class="toc-number">3.3.</span> <span class="toc-text">3、在将原始数据处理成最终的格式要求时，一般可以在哪些地方做处理？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E5%9C%A8%E6%9E%84%E9%80%A0%E6%95%B0%E6%8D%AE%E8%BF%AD%E4%BB%A3%E5%99%A8%EF%BC%88Dataloader%EF%BC%89%E6%97%B6%EF%BC%8C%E6%9C%89%E5%93%AA%E4%BA%9B%E6%AD%A5%E9%AA%A4%EF%BC%9F"><span class="toc-number">3.4.</span> <span class="toc-text">4、在构造数据迭代器（Dataloader）时，有哪些步骤？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E7%BB%9F%E4%B8%80%E6%A0%B7%E6%9C%AC%E9%95%BF%E5%BA%A6%E6%9C%89%E5%93%AA%E4%BA%9B%E6%96%B9%E6%B3%95%EF%BC%9F"><span class="toc-number">3.5.</span> <span class="toc-text">5、统一样本长度有哪些方法？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%8BBiLSTM-CRF%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9E%B6%E6%9E%84%EF%BC%9F"><span class="toc-number">3.6.</span> <span class="toc-text">6、描述一下BiLSTM_CRF模型的架构？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day04"><span class="toc-number">4.</span> <span class="toc-text">Day04</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">4.1.</span> <span class="toc-text">1、训练函数基本步骤是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E9%AA%8C%E8%AF%81%E5%87%BD%E6%95%B0%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">4.2.</span> <span class="toc-text">2、验证函数基本步骤是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81BiLSTM-CRF%E6%A8%A1%E5%9E%8B%E5%9C%A8%E8%AE%AD%E7%BB%83%E5%AE%8C%E5%90%8E%EF%BC%8C%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%93%AA%E4%BA%9B%E4%BC%98%E5%8C%96%E6%9D%A5%E6%94%B9%E5%96%84%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%EF%BC%9F"><span class="toc-number">4.3.</span> <span class="toc-text">3、BiLSTM_CRF模型在训练完后，可以做哪些优化来改善模型性能？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81precision%E3%80%81recall%E3%80%81f1%E3%80%81report%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">4.4.</span> <span class="toc-text">4、precision、recall、f1、report的使用方式是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">4.5.</span> <span class="toc-text">5、模型预测基本步骤是什么？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day05"><span class="toc-number">5.</span> <span class="toc-text">Day05</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%EF%BC%9F%E6%9C%AC%E8%B4%A8%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">5.1.</span> <span class="toc-text">1、什么是关系抽取？本质是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">5.2.</span> <span class="toc-text">关系抽取的常用方法有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E4%BB%BB%E5%8A%A1%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">5.3.</span> <span class="toc-text">3、关系抽取任务常见问题有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84%E6%96%B9%E6%B3%95%E5%AE%9E%E7%8E%B0%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">5.4.</span> <span class="toc-text">4、基于规则的方法实现关系抽取的优缺点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%8BBiLSTM-Attention%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9E%B6%E6%9E%84%EF%BC%9F"><span class="toc-number">5.5.</span> <span class="toc-text">5、描述一下BiLSTM+Attention模型的架构？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">5.6.</span> <span class="toc-text">6、注意力机制是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%8BBiLSTM-Attention%E6%A8%A1%E5%9E%8B%E4%B8%AD%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%98%AF%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%9A%84%EF%BC%9F"><span class="toc-number">5.7.</span> <span class="toc-text">7、描述一下BiLSTM+Attention模型中注意力机制是如何实现的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8%E3%80%81BiLSTM-Attentiom%E6%A8%A1%E5%9E%8B%E4%B8%AD%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%9A%84%E6%95%B4%E4%BD%93%E6%80%9D%E8%B7%AF%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">5.8.</span> <span class="toc-text">8、BiLSTM+Attentiom模型中数据处理的整体思路是什么？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day06"><span class="toc-number">6.</span> <span class="toc-text">Day06</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81BERT%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%89%80%E6%8E%A5%E6%94%B6%E7%9A%84%E6%9C%80%E5%A4%A7sequence%E9%95%BF%E5%BA%A6%E6%98%AF%E5%A4%9A%E5%B0%91%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%BE%E7%BD%AE%E6%9C%80%E5%A4%A7%E9%95%BF%E5%BA%A6%EF%BC%9F"><span class="toc-number">6.1.</span> <span class="toc-text">1、BERT预训练模型所接收的最大sequence长度是多少，为什么设置最大长度？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E5%AF%B9%E4%BA%8E%E9%95%BF%E6%96%87%E6%9C%AC-%E6%96%87%E6%9C%AC%E9%95%BF%E5%BA%A6%E8%B6%85%E8%BF%87512%E7%9A%84%E5%8F%A5%E5%AD%90-%E5%9C%A8%E4%BD%BF%E7%94%A8BERT%E6%97%B6-%E5%A6%82%E4%BD%95%E6%9D%A5%E6%9E%84%E9%80%A0%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%EF%BC%9F"><span class="toc-number">6.2.</span> <span class="toc-text">2、对于长文本(文本长度超过512的句子)在使用BERT时, 如何来构造训练样本？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81BiLSTM-Attention%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9E%B6%E6%9E%84%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">6.3.</span> <span class="toc-text">3、BiLSTM+Attention模型的架构是怎样的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E5%9C%A8BERT%E4%B8%AD%EF%BC%8C%E6%98%AF%E5%A6%82%E4%BD%95%E5%B0%86Token-Embedding%E3%80%81Segment-Embedding-%E5%92%8C-Position-Embedding%E7%BB%84%E5%90%88%E5%9C%A8%E4%B8%80%E8%B5%B7%E7%84%B6%E5%90%8E%E9%80%81%E5%88%B0encoder%E4%B8%AD%E7%9A%84%EF%BC%9F"><span class="toc-number">6.4.</span> <span class="toc-text">4、在BERT中，是如何将Token Embedding、Segment Embedding 和 Position Embedding组合在一起然后送到encoder中的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E4%B8%A4%E4%B8%AA%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98%E6%97%B6%EF%BC%8Cshape%E4%B8%8D%E7%AC%A6%E5%90%88%E8%A6%81%E6%B1%82%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F"><span class="toc-number">6.5.</span> <span class="toc-text">5、两个矩阵相乘时，shape不符合要求怎么办？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day07"><span class="toc-number">7.</span> <span class="toc-text">Day07</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81BiLSTM-Attention%E6%A8%A1%E5%9E%8B%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%93%AA%E4%BA%9B%E4%BC%98%E5%8C%96%E6%9D%A5%E6%94%B9%E5%96%84%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%EF%BC%9F"><span class="toc-number">7.1.</span> <span class="toc-text">1、BiLSTM+Attention模型可以做哪些优化来改善模型性能？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81Pipeline%E6%96%B9%E6%B3%95%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">7.2.</span> <span class="toc-text">2、Pipeline方法的优缺点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81Joint%E6%96%B9%E6%B3%95%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E6%9C%89%E5%93%AA%E4%B8%A4%E7%A7%8D%E7%B1%BB%E5%9E%8B%EF%BC%9F"><span class="toc-number">7.3.</span> <span class="toc-text">3、Joint方法是什么？有哪两种类型？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81Casrel%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9E%B6%E6%9E%84%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">7.4.</span> <span class="toc-text">4、Casrel模型的架构是怎样的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E8%AF%B4%E4%B8%80%E4%B8%8BCasrel%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">7.5.</span> <span class="toc-text">5、说一下Casrel模型的输入输出是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%95%B4%E4%BD%93%E6%80%9D%E8%B7%AF%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">7.6.</span> <span class="toc-text">6、数据处理整体思路是怎样的？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day08"><span class="toc-number">8.</span> <span class="toc-text">Day08</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81Casrel%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%9A%84%E6%95%B4%E4%BD%93%E6%80%9D%E8%B7%AF%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">8.1.</span> <span class="toc-text">1、Casrel模型数据处理的整体思路是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E4%BD%BF%E7%94%A8Casrel%E6%A8%A1%E5%9E%8B%E6%97%B6%EF%BC%8C%E9%81%87%E5%88%B0%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%8C%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%9A%84%EF%BC%9F"><span class="toc-number">8.2.</span> <span class="toc-text">2、使用Casrel模型时，遇到什么问题，如何解决的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81Casrel%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%93%E6%9E%84%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">8.3.</span> <span class="toc-text">3、Casrel模型的结构是怎样的？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day09"><span class="toc-number">9.</span> <span class="toc-text">Day09</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81Casrel%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%80%8E%E4%B9%88%E8%AE%A1%E7%AE%97%E7%9A%84%EF%BC%9F"><span class="toc-number">9.1.</span> <span class="toc-text">1、Casrel模型的损失函数怎么计算的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81AdamW%E7%9B%B8%E5%85%B3%E9%9D%A2%E8%AF%95%E9%A2%98"><span class="toc-number">9.2.</span> <span class="toc-text">2、AdamW相关面试题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81Casrel%E6%A8%A1%E5%9E%8B%E4%B8%AD%EF%BC%8CBert%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%8F%82%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%9B%E8%A1%8C%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%EF%BC%9F"><span class="toc-number">9.3.</span> <span class="toc-text">3、Casrel模型中，Bert为什么要参与反向传播进行参数更新？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81Casrel%E6%A8%A1%E5%9E%8B%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%93%AA%E4%BA%9B%E4%BC%98%E5%8C%96%EF%BC%9F"><span class="toc-number">9.4.</span> <span class="toc-text">4、Casrel模型可以做哪些优化？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day10"><span class="toc-number">10.</span> <span class="toc-text">Day10</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81Casrel%E6%A8%A1%E5%9E%8B%E5%9C%A8%E9%A2%84%E6%B5%8B%E6%97%B6%EF%BC%8C%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">10.1.</span> <span class="toc-text">1、Casrel模型在预测时，需要注意什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E7%9F%A5%E8%AF%86%E8%9E%8D%E5%90%88%EF%BC%9F"><span class="toc-number">10.2.</span> <span class="toc-text">2、什么是知识融合？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E4%B8%BB%E8%A6%81%E6%9C%89%E5%93%AA%E4%BA%9B%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">10.3.</span> <span class="toc-text">3、主要有哪些问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E5%AE%9E%E4%BD%93%E6%B6%88%E5%B2%90%EF%BC%88%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8E%A5%EF%BC%89%EF%BC%9F%E6%80%8E%E4%B9%88%E5%A4%84%E7%90%86%EF%BC%9F"><span class="toc-number">10.4.</span> <span class="toc-text">4、什么是实体消岐（实体链接）？怎么处理？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81-%E4%BB%80%E4%B9%88%E6%98%AF%E5%AE%9E%E4%BD%93%E7%BB%9F%E4%B8%80%EF%BC%88%E5%AE%9E%E4%BD%93%E5%AF%B9%E9%BD%90%EF%BC%89%EF%BC%9F%E6%80%8E%E4%B9%88%E5%A4%84%E7%90%86%EF%BC%9F"><span class="toc-number">10.5.</span> <span class="toc-text">5、 什么是实体统一（实体对齐）？怎么处理？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E5%85%B3%E7%B3%BB%E5%AF%B9%E9%BD%90%EF%BC%88%E5%85%B3%E7%B3%BB%E7%BB%9F%E4%B8%80%EF%BC%89%EF%BC%9F%E6%80%8E%E4%B9%88%E5%A4%84%E7%90%86%EF%BC%9F"><span class="toc-number">10.6.</span> <span class="toc-text">6、什么是关系对齐（关系统一）？怎么处理？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8TF-IDF%E6%9D%A5%E8%BF%9B%E8%A1%8C%E5%AE%9E%E4%BD%93%E6%B6%88%E6%AD%A7%EF%BC%9F"><span class="toc-number">10.7.</span> <span class="toc-text">7、如何使用TF-IDF来进行实体消歧？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8%E3%80%81%E4%BD%A0%E4%BB%AC%E9%A1%B9%E7%9B%AE%E9%80%89%E7%94%A8%E7%9A%84%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">10.8.</span> <span class="toc-text">8、你们项目选用的图数据库是什么？为什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9%E3%80%81NEO4J%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A6%82%E5%BF%B5%EF%BC%9F%E5%92%8Cspo%E4%B8%89%E5%85%83%E7%BB%84%E6%9C%89%E4%BB%80%E4%B9%88%E5%85%B3%E7%B3%BB%EF%BC%9F"><span class="toc-number">10.9.</span> <span class="toc-text">9、NEO4J数据库中有哪些概念？和spo三元组有什么关系？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10%E3%80%81%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9B%B8%E6%AF%94%E4%BC%A0%E7%BB%9F%E7%9A%84MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%8C%E6%9C%89%E5%93%AA%E4%BA%9B%E4%BC%98%E5%8A%BF%EF%BC%9F"><span class="toc-number">10.10.</span> <span class="toc-text">10、图数据库相比传统的MySQL数据库，有哪些优势？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day11"><span class="toc-number">11.</span> <span class="toc-text">Day11</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E6%9C%80%E7%BB%88%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%84%E7%BB%87%E5%BD%A2%E5%BC%8F%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.1.</span> <span class="toc-text">1、最终导入数据库的数据组织形式是怎样的？为什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%BA%93%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%95%B0%E6%8D%AE%EF%BC%9F"><span class="toc-number">11.2.</span> <span class="toc-text">2、导入数据库都有哪些数据？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E5%88%86%E4%B8%BA%E5%93%AA%E5%87%A0%E4%B8%AA%E9%83%A8%E5%88%86%EF%BC%8C%E5%88%86%E5%88%AB%E6%98%AF%E5%81%9A%E4%BB%80%E4%B9%88%E7%9A%84%EF%BC%9F"><span class="toc-number">11.3.</span> <span class="toc-text">3、问答系统分为哪几个部分，分别是做什么的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E6%84%8F%E5%9B%BE%E8%AF%86%E5%88%AB%EF%BC%9F"><span class="toc-number">11.4.</span> <span class="toc-text">4、什么是意图识别？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E6%A7%BD%E4%BD%8D%E5%A1%AB%E5%85%85%EF%BC%9F"><span class="toc-number">11.5.</span> <span class="toc-text">5、什么是槽位填充？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E8%AF%AD%E4%B9%89%E6%A7%BD%EF%BC%9F"><span class="toc-number">11.6.</span> <span class="toc-text">6、如何设计语义槽？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81%E5%8C%BB%E7%96%97KBQA%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">11.7.</span> <span class="toc-text">7、医疗KBQA系统架构是怎样的？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day12"><span class="toc-number">12.</span> <span class="toc-text">Day12</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E5%8C%BB%E7%96%97KBQA%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0%E7%9A%84%E6%AD%A5%E9%AA%A4%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">12.1.</span> <span class="toc-text">1、医疗KBQA系统实现的步骤是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%84%8F%E5%9B%BE%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6%E6%98%AF%E9%97%B2%E8%81%8A%E7%B1%BB%E7%9A%84%E6%84%8F%E5%9B%BE%EF%BC%89%EF%BC%9F"><span class="toc-number">12.2.</span> <span class="toc-text">2、如何实现第一个意图识别模型（判断是否是闲聊类的意图）？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%AC%AC%E4%BA%8C%E4%B8%AA%E6%84%8F%E5%9B%BE%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%8C%85%E6%8B%AC13%E4%B8%AA%E5%8C%BB%E7%96%97%E7%B1%BB%E7%9A%84%E6%84%8F%E5%9B%BE%EF%BC%89%EF%BC%9F"><span class="toc-number">12.3.</span> <span class="toc-text">3、如何实现第二个意图识别模型（包括13个医疗类的意图）？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%AC%AC%E4%B8%89%E4%B8%AA%E6%A7%BD%E4%BD%8D%E5%A1%AB%E5%85%85%EF%BC%88NER%EF%BC%89%E6%A8%A1%E5%9E%8B%EF%BC%88%E7%94%A8%E6%9D%A5%E8%AF%86%E5%88%AB%E7%94%A8%E6%88%B7%E8%AF%9D%E8%AF%AD%E4%B8%AD%E7%9A%84%E5%AE%9E%E4%BD%93%EF%BC%89%EF%BC%9F"><span class="toc-number">12.4.</span> <span class="toc-text">4、如何实现第三个槽位填充（NER）模型（用来识别用户话语中的实体）？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#NER%E4%BB%BB%E5%8A%A1"><span class="toc-number">13.</span> <span class="toc-text">NER任务</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#BiLSTM-CRF%E9%A1%B9%E7%9B%AE%E5%AE%8C%E6%95%B4%E5%AE%9E%E7%8E%B0"><span class="toc-number">13.1.</span> <span class="toc-text">BiLSTM+CRF项目完整实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">13.2.</span> <span class="toc-text">数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5-%E6%9F%A5%E7%9C%8B%E9%A1%B9%E7%9B%AE%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">13.2.1.</span> <span class="toc-text">第一步: 查看项目数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5-%E6%9E%84%E9%80%A0%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E6%95%B0%E6%8D%AE"><span class="toc-number">13.2.2.</span> <span class="toc-text">第二步: 构造序列标注数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5-%E7%BC%96%E5%86%99Config%E7%B1%BB%E9%A1%B9%E7%9B%AE%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%A0%81"><span class="toc-number">13.2.3.</span> <span class="toc-text">第三步: 编写Config类项目文件配置代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E6%AD%A5-%E6%9E%84%E5%BB%BADataset%E7%B1%BB%E4%B8%8Edataloader%E5%87%BD%E6%95%B0"><span class="toc-number">13.2.4.</span> <span class="toc-text">第四步: 构建Dataset类与dataloader函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BiLSTM-CRF%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA"><span class="toc-number">13.3.</span> <span class="toc-text">BiLSTM+CRF模型搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5-%E7%BC%96%E5%86%99%E6%A8%A1%E5%9E%8B%E7%B1%BB%E7%9A%84%E4%BB%A3%E7%A0%81"><span class="toc-number">13.3.1.</span> <span class="toc-text">第一步: 编写模型类的代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5-%E7%BC%96%E5%86%99%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0"><span class="toc-number">13.3.2.</span> <span class="toc-text">第二步: 编写训练函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5-%E7%BC%96%E5%86%99%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E5%87%BD%E6%95%B0"><span class="toc-number">13.3.3.</span> <span class="toc-text">第三步: 编写模型预测函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E4%BB%BB%E5%8A%A1%E4%BB%A3%E7%A0%81"><span class="toc-number">14.</span> <span class="toc-text">关系抽取任务代码</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96"><span class="toc-number">15.</span> <span class="toc-text">基于规则方式实现关系抽取</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Pipline%E6%96%B9%E6%B3%95%E5%AE%9E%E7%8E%B0%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96"><span class="toc-number">16.</span> <span class="toc-text">Pipline方法实现关系抽取</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Pipeline%E6%96%B9%E6%B3%95%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-number">16.1.</span> <span class="toc-text">Pipeline方法的原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BiLSTM-Attention%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E2%AD%90%EF%B8%8F"><span class="toc-number">16.2.</span> <span class="toc-text">BiLSTM+Attention模型架构⭐️</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%80%90%E5%AE%9E%E7%8E%B0%E3%80%91%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E6%A6%82%E8%A7%88"><span class="toc-number">16.3.</span> <span class="toc-text">【实现】代码实现概览</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%80%90%E5%AE%9E%E7%8E%B0%E3%80%91%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">16.4.</span> <span class="toc-text">【实现】数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5-%E6%9F%A5%E7%9C%8B%E9%A1%B9%E7%9B%AE%E6%95%B0%E6%8D%AE%E9%9B%86-1"><span class="toc-number">16.4.1.</span> <span class="toc-text">第一步: 查看项目数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5-%E7%BC%96%E5%86%99Config%E7%B1%BB%E9%A1%B9%E7%9B%AE%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%A0%81"><span class="toc-number">16.4.2.</span> <span class="toc-text">第二步: 编写Config类项目文件配置代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5-%E7%BC%96%E5%86%99%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E5%87%BD%E6%95%B0"><span class="toc-number">16.4.3.</span> <span class="toc-text">第三步: 编写数据处理相关函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E6%AD%A5-%E6%9E%84%E5%BB%BADataset%E7%B1%BB%E4%B8%8Edataloader%E5%87%BD%E6%95%B0-1"><span class="toc-number">16.4.4.</span> <span class="toc-text">第四步: 构建Dataset类与dataloader函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BiLSTM-Attention%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA"><span class="toc-number">16.5.</span> <span class="toc-text">BiLSTM+Attention模型搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5-%E7%BC%96%E5%86%99%E6%A8%A1%E5%9E%8B%E7%B1%BB%E7%9A%84%E4%BB%A3%E7%A0%81-1"><span class="toc-number">16.5.1.</span> <span class="toc-text">第一步: 编写模型类的代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5-%E7%BC%96%E5%86%99%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0-1"><span class="toc-number">16.5.2.</span> <span class="toc-text">第二步: 编写训练函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5-%E7%BC%96%E5%86%99%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E5%87%BD%E6%95%B0-1"><span class="toc-number">16.5.3.</span> <span class="toc-text">第三步: 编写模型预测函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BiLSTM-Attention%E6%A8%A1%E5%9E%8B%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%93%AA%E4%BA%9B%E4%BC%98%E5%8C%96%E6%9D%A5%E6%94%B9%E5%96%84%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%EF%BC%9F"><span class="toc-number">16.6.</span> <span class="toc-text">BiLSTM+Attention模型可以做哪些优化来改善模型性能？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pipeline%E6%96%B9%E6%B3%95%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">16.7.</span> <span class="toc-text">Pipeline方法的优缺点</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Joint%E6%96%B9%E6%B3%95%E5%AE%9E%E7%8E%B0%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96"><span class="toc-number">17.</span> <span class="toc-text">Joint方法实现关系抽取</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Joint%E6%96%B9%E6%B3%95%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-number">17.1.</span> <span class="toc-text">Joint方法的原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Casrel%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-number">17.2.</span> <span class="toc-text">Casrel模型架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E3%80%90%E5%AE%9E%E7%8E%B0%E3%80%91%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E6%A6%82%E8%A7%88-1"><span class="toc-number">17.2.1.</span> <span class="toc-text">【实现】代码实现概览</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E3%80%90%E5%AE%9E%E7%8E%B0%E3%80%91%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-1"><span class="toc-number">17.2.2.</span> <span class="toc-text">【实现】数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E3%80%90%E6%8E%8C%E6%8F%A1%E3%80%91Casrel%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA"><span class="toc-number">17.2.3.</span> <span class="toc-text">【掌握】Casrel模型搭建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5-%E7%BC%96%E5%86%99%E6%A8%A1%E5%9E%8B%E7%B1%BB%E7%9A%84%E4%BB%A3%E7%A0%81-2"><span class="toc-number">17.2.4.</span> <span class="toc-text">第一步: 编写模型类的代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5-%E7%BC%96%E5%86%99%E5%B7%A5%E5%85%B7%E7%B1%BB%E5%87%BD%E6%95%B0-%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0-%E9%AA%8C%E8%AF%81%E5%87%BD%E6%95%B0-%E6%B5%8B%E8%AF%95%E5%87%BD%E6%95%B0"><span class="toc-number">17.2.5.</span> <span class="toc-text">第二步: 编写工具类函数,训练函数,验证函数,测试函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5-%E7%BC%96%E5%86%99%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E5%87%BD%E6%95%B0-2"><span class="toc-number">17.2.6.</span> <span class="toc-text">第三步: 编写模型预测函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1"><span class="toc-number">18.</span> <span class="toc-text">文本分类任务</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-bert%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8E%9F%E7%90%86%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E7%9A%84%EF%BC%9F"><span class="toc-number">18.1.</span> <span class="toc-text">2. bert模型的原理是什么？如何构建的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F%E6%9C%89%E5%93%AA%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%EF%BC%9F%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E8%BD%AF%E6%A0%87%E7%AD%BE%E8%92%B8%E9%A6%8F%E7%9A%84%EF%BC%9F"><span class="toc-number">18.2.</span> <span class="toc-text">5. 模型蒸馏有哪几种方式？如何进行软标签蒸馏的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E4%BB%8B%E7%BB%8D%E4%B8%8B%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%EF%BC%8C%E4%BB%8B%E7%BB%8D%E4%B8%8B%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E6%9C%89%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%EF%BC%8C%E5%9C%A8%E9%A1%B9%E7%9B%AE%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E7%9A%84"><span class="toc-number">18.3.</span> <span class="toc-text">6. 介绍下模型量化，介绍下模型量化有几种方式，在项目中如何使用的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E9%A1%B9%E2%BD%AC%E6%9E%B6%E6%9E%84"><span class="toc-number">18.4.</span> <span class="toc-text">3、项⽬架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E9%A1%B9%E2%BD%AC%E4%B8%AD%E9%81%87%E5%88%B0%E4%BD%BF%E2%BD%A4%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">18.5.</span> <span class="toc-text">4、项⽬中遇到使⽤的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E8%AE%B2%E8%A7%A3%E4%B8%80%E4%B8%8BFasttext%E4%B8%AD%E7%9A%84%E9%9C%8D%E5%A4%AB%E6%9B%BC%E6%A0%91%EF%BC%9F"><span class="toc-number">18.6.</span> <span class="toc-text">4.讲解一下Fasttext中的霍夫曼树？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E8%AE%B2%E8%A7%A3%E4%B8%80%E4%B8%8BBert%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="toc-number">18.7.</span> <span class="toc-text">5.讲解一下Bert模型？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Bert%E4%B8%A4%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1%E6%98%AF%E5%95%A5%EF%BC%9F"><span class="toc-number">18.8.</span> <span class="toc-text">6.Bert两个预训练任务是啥？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Bert%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E5%92%8CTransformer%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">18.9.</span> <span class="toc-text">8.Bert中的位置编码和Transformer中的位置编码有什么区别？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E8%AE%B2%E8%A7%A3%E4%B8%80%E4%B8%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F%EF%BC%9F"><span class="toc-number">18.10.</span> <span class="toc-text">9.讲解一下注意力机制的计算公式？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-%E8%AE%B2%E8%A7%A3%E9%A1%B9%E7%9B%AE%E4%B8%AD%E7%94%A8%E5%88%B0%E7%9A%84%E8%92%B8%E9%A6%8F%EF%BC%8C%E4%BD%BF%E7%94%A8%E4%BA%86%E4%BB%80%E4%B9%88%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%B9%B6%E4%BB%8B%E7%BB%8D%E5%85%AC%E5%BC%8F"><span class="toc-number">18.11.</span> <span class="toc-text">10.讲解项目中用到的蒸馏，使用了什么损失函数并介绍公式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">18.12.</span> <span class="toc-text">11.如何解决过拟合问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8BLSTM"><span class="toc-number">18.13.</span> <span class="toc-text">13.介绍一下LSTM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%EF%BC%9A%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8BBERT%E6%A8%A1%E5%9E%8B%EF%BC%8C%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%85%A5%E6%98%AF%E5%95%A5%EF%BC%9F%E7%BB%8F%E8%BF%87%E5%93%AA%E4%BA%9B%E6%AD%A5%E9%AA%A4%EF%BC%9F%E8%BE%93%E5%87%BA%E6%98%AF%E5%95%A5%EF%BC%9F"><span class="toc-number">18.14.</span> <span class="toc-text">问题：介绍一下BERT模型，模型输入是啥？经过哪些步骤？输出是啥？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%EF%BC%9A%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E6%98%AF%E6%80%8E%E4%B9%88%E5%81%9A%E7%9A%84%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%81%9A%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%EF%BC%9F"><span class="toc-number">18.15.</span> <span class="toc-text">问题：残差连接是怎么做的？为什么要做残差连接？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%EF%BC%9A%E8%BF%9B%E8%A1%8CBERT%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F%E6%97%B6%EF%BC%8C%E5%8F%8C%E5%90%91LSTM%E6%98%AF%E6%80%8E%E4%B9%88%E5%81%9A%E8%92%B8%E9%A6%8F%E7%9A%84%EF%BC%9F%E5%85%B7%E4%BD%93%E7%9A%84%E3%80%82"><span class="toc-number">18.16.</span> <span class="toc-text">问题：进行BERT模型蒸馏时，双向LSTM是怎么做蒸馏的？具体的。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%EF%BC%9A%E8%BD%AF%E6%A0%87%E7%AD%BE%E7%9B%B8%E5%85%B3%E7%9A%84%E5%8F%82%E6%95%B0T%EF%BC%88%E6%B8%A9%E5%BA%A6%EF%BC%89%E7%9A%84%E4%BD%9C%E7%94%A8%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">18.17.</span> <span class="toc-text">问题：软标签相关的参数T（温度）的作用是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%EF%BC%9ALSTM%E6%AF%8F%E4%B8%AA%E7%BB%86%E8%83%9E%E7%94%B1%E4%BB%80%E4%B9%88%E7%BB%84%E6%88%90%EF%BC%9F"><span class="toc-number">18.18.</span> <span class="toc-text">问题：LSTM每个细胞由什么组成？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%94%E8%AE%B0"><span class="toc-number">18.19.</span> <span class="toc-text">笔记</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A2%98%E5%BA%93"><span class="toc-number">19.</span> <span class="toc-text">机器学习题库</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6%EF%BC%9A%E3%80%90%E9%9D%A2%E8%AF%95%E9%87%8D%E7%82%B9%E3%80%91%E4%BB%80%E4%B9%88%E6%98%AF%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AE%83%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E8%87%B3%E5%85%B3%E9%87%8D%E8%A6%81%EF%BC%9F"><span class="toc-number">19.1.</span> <span class="toc-text">6：【面试重点】什么是特征工程？为什么它在机器学习中至关重要？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%EF%BC%9A%E3%80%90%E9%9D%A2%E8%AF%95%E9%87%8D%E7%82%B9%E3%80%91%E8%A7%A3%E9%87%8A%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88%E7%9A%84%E5%AE%9A%E4%B9%89%E3%80%81%E5%8E%9F%E5%9B%A0%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E3%80%82"><span class="toc-number">19.2.</span> <span class="toc-text">7：【面试重点】解释过拟合和欠拟合的定义、原因及解决方案。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15%EF%BC%9A%E3%80%90%E9%9D%A2%E8%AF%95%E9%87%8D%E7%82%B9%E3%80%91MSE%E3%80%81MAE%E3%80%81RMSE-%E7%9A%84%E5%AE%9A%E4%B9%89%E5%8F%8A%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">19.3.</span> <span class="toc-text">15：【面试重点】MSE、MAE、RMSE 的定义及区别是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19%EF%BC%9A%E3%80%90%E9%9D%A2%E8%AF%95%E9%87%8D%E7%82%B9%E3%80%91%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%BB%E8%A6%81%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E7%B1%BB%E5%9E%8B%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%9F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E6%B5%81%E7%A8%8B%E6%98%AF%E4%BB%80%E4%B9%88-%EF%BC%9F"><span class="toc-number">19.4.</span> <span class="toc-text">19：【面试重点】逻辑回归主要解决什么类型的问题？逻辑回归的流程是什么 ？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20%EF%BC%9A%E3%80%90%E9%9D%A2%E8%AF%95%E9%87%8D%E7%82%B9%E3%80%91AUC-%E6%8C%87%E6%A0%87%E7%9A%84%E5%90%AB%E4%B9%89%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E5%85%B6%E5%8F%96%E5%80%BC%E8%8C%83%E5%9B%B4%E6%98%AF%E5%A4%9A%E5%B0%91%EF%BC%9FAUC-0-5%E3%80%81AUC-1-%E3%80%81AUC-0%E5%88%86%E5%88%AB%E8%AF%B4%E6%98%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BB%80%E4%B9%88%E6%80%A7%E8%83%BD%EF%BC%9F"><span class="toc-number">19.5.</span> <span class="toc-text">20：【面试重点】AUC 指标的含义是什么？其取值范围是多少？AUC&#x3D;0.5、AUC&#x3D;1 、AUC&#x3D;0分别说明模型的什么性能？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%A1%A5%E5%85%85"><span class="toc-number">20.</span> <span class="toc-text">机器学习补充</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">20.1.</span> <span class="toc-text">有监督学习和无监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-number">20.2.</span> <span class="toc-text">什么是特征工程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-number">20.3.</span> <span class="toc-text">过拟合和欠拟合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KNN-%E7%AE%97%E6%B3%95%E7%9A%84%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-number">20.4.</span> <span class="toc-text">KNN 算法的核心思想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KNN%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E5%92%8C%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98"><span class="toc-number">20.5.</span> <span class="toc-text">KNN分类问题和回归问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KNN%E7%AE%97%E6%B3%95K%E5%80%BC%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">20.6.</span> <span class="toc-text">KNN算法K值的选择</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86-%E5%BD%92%E4%B8%80%E5%8C%96-%E6%A0%87%E5%87%86%E5%8C%96%E5%8C%BA%E5%88%AB"><span class="toc-number">20.7.</span> <span class="toc-text">特征预处理(归一化,标准化区别)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E5%92%8C%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2"><span class="toc-number">20.8.</span> <span class="toc-text">交叉验证和网格搜索</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">20.9.</span> <span class="toc-text">损失函数是什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%AD%E6%9C%80%E4%BC%98%E5%8F%82%E6%95%B0%E6%B1%82%E8%A7%A3"><span class="toc-number">20.10.</span> <span class="toc-text">线性回归中最优参数求解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%B8%B8%E8%A7%81%E7%B1%BB%E5%9E%8B"><span class="toc-number">20.11.</span> <span class="toc-text">梯度下降的常见类型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">20.11.1.</span> <span class="toc-text">批量梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">20.11.2.</span> <span class="toc-text">随机梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">20.11.3.</span> <span class="toc-text">小批量梯度下降</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#L1-L2-%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">20.12.</span> <span class="toc-text">L1, L2 正则化的区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%B5%81%E7%A8%8B%E5%8F%8A%E8%A7%A3%E5%86%B3%E4%BA%86%E4%BB%80%E4%B9%88"><span class="toc-number">20.13.</span> <span class="toc-text">逻辑回归流程及解决了什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5%E5%9B%9B%E8%B1%A1%E9%99%90"><span class="toc-number">20.14.</span> <span class="toc-text">混淆矩阵四象限</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B2%BE%E7%A1%AE%E7%8E%87%EF%BC%8C%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%8F%8D%E6%98%A0%E4%BB%80%E4%B9%88"><span class="toc-number">20.15.</span> <span class="toc-text">精确率，召回率反映什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ROC%E6%9B%B2%E7%BA%BF%E6%A8%AA%E7%BA%B5%E8%BD%B4-%E6%AF%8F%E4%B8%AA%E7%82%B9"><span class="toc-number">20.16.</span> <span class="toc-text">ROC曲线横纵轴,每个点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AUC-%E6%8C%87%E6%A0%87"><span class="toc-number">20.17.</span> <span class="toc-text">AUC 指标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%9F%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84"><span class="toc-number">20.18.</span> <span class="toc-text">决策树？基本结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%86%B5-Entropy-%E5%9C%A8%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%AD%E4%BD%9C%E7%94%A8"><span class="toc-number">20.19.</span> <span class="toc-text">熵(Entropy)在决策树中作用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ID3-C4-5-CART-%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">20.20.</span> <span class="toc-text">ID3,C4.5,CART 决策树</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E8%8A%82%E7%82%B9%E5%88%87%E5%88%86%E4%BE%9D%E6%8D%AE"><span class="toc-number">20.21.</span> <span class="toc-text">决策树节点切分依据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%89%AA%E6%9E%9D"><span class="toc-number">20.22.</span> <span class="toc-text">决策树剪枝</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bagging-%E4%B8%8E-Boosting%E5%8C%BA%E5%88%AB"><span class="toc-number">20.23.</span> <span class="toc-text">Bagging 与 Boosting区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adaboost-%E7%9A%84%E5%AE%8C%E6%95%B4%E6%9E%84%E5%BB%BA%E6%B5%81%E7%A8%8B"><span class="toc-number">20.24.</span> <span class="toc-text">Adaboost 的完整构建流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%8C%BA%E5%88%AB"><span class="toc-number">20.25.</span> <span class="toc-text">梯度提升树与随机森林区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#XGBoost%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B%E6%A6%82%E8%BF%B0"><span class="toc-number">20.26.</span> <span class="toc-text">XGBoost推导过程概述</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#NLP%E9%9D%A2%E8%AF%95%E9%A2%98%E5%BA%93"><span class="toc-number">21.</span> <span class="toc-text">NLP面试题库</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer%E6%9E%B6%E6%9E%84"><span class="toc-number">21.1.</span> <span class="toc-text">Transformer架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6QKV"><span class="toc-number">21.2.</span> <span class="toc-text">注意力机制QKV</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="toc-number">21.3.</span> <span class="toc-text">自注意力机制的计算过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CBOW%E6%A8%A1%E5%BC%8F%E5%92%8Cskipgram%E6%A8%A1%E5%BC%8F-FASTTEXT"><span class="toc-number">21.4.</span> <span class="toc-text">CBOW模式和skipgram模式(FASTTEXT)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NLP%E4%B8%AD%E8%AF%8D%E5%90%91%E9%87%8F%E7%9A%84%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95"><span class="toc-number">21.5.</span> <span class="toc-text">NLP中词向量的表示方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BERT%E6%98%AF%E5%8F%8C%E5%90%91%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3"><span class="toc-number">21.6.</span> <span class="toc-text">BERT是双向语言模型, 如何理解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BERT%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%A3%E7%A0%81%E5%B1%82%E9%9D%A2"><span class="toc-number">21.7.</span> <span class="toc-text">BERT多头注意力机制代码层面</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E5%9C%A8Transformer%E7%94%A8LayerNorm"><span class="toc-number">21.8.</span> <span class="toc-text">为什么在Transformer用LayerNorm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#view%E5%92%8Ctranspose%E5%85%88%E5%90%8E%E9%97%AE%E9%A2%98"><span class="toc-number">21.9.</span> <span class="toc-text">view和transpose先后问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%BA%8F%E5%88%97%E9%95%BF%E5%BA%A6%E4%B8%BA1000"><span class="toc-number">21.10.</span> <span class="toc-text">预训练模型序列长度为1000+</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MacBERT%E5%9C%A8%E4%BC%98%E5%8C%96MLM%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1"><span class="toc-number">21.11.</span> <span class="toc-text">MacBERT在优化MLM预训练任务</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E6%95%B0%E6%8D%AE%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1"><span class="toc-number">21.12.</span> <span class="toc-text">多分类问题数据样本不均衡</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%B8%A4%E4%B8%AA%E8%A1%A1%E9%87%8F%E6%8C%87%E6%A0%87"><span class="toc-number">21.13.</span> <span class="toc-text">对比学习最重要的两个衡量指标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer%E5%92%8CBERT%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">21.14.</span> <span class="toc-text">Transformer和BERT中的位置编码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E5%8A%A0%E9%80%9F%E9%83%A8%E7%BD%B2%E7%9A%84%E9%97%AE%E9%A2%98-%E4%B8%80%E8%88%AC%E9%83%A8%E7%BD%B2%E4%B8%AD%E6%AF%94%E8%BE%83%E5%96%9C%E6%AC%A2%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%9C%89%E5%93%AA%E4%BA%9B"><span class="toc-number">21.15.</span> <span class="toc-text">模型在工业界的加速部署的问题? 一般部署中比较喜欢的模型有哪些?</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/10/30/Agent_all/" title="Agent"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/Pusheen12222.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Agent"/></a><div class="content"><a class="title" href="/2025/10/30/Agent_all/" title="Agent">Agent</a><time datetime="2025-10-29T16:00:00.000Z" title="Created 2025-10-30 00:00:00">2025-10-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/25/Agent/" title="Agent-Note"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/Pusheen1121.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Agent-Note"/></a><div class="content"><a class="title" href="/2025/10/25/Agent/" title="Agent-Note">Agent-Note</a><time datetime="2025-10-24T16:00:00.000Z" title="Created 2025-10-25 00:00:00">2025-10-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/21/RAG/" title="RAG"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="RAG"/></a><div class="content"><a class="title" href="/2025/10/21/RAG/" title="RAG">RAG</a><time datetime="2025-10-20T16:00:00.000Z" title="Created 2025-10-21 00:00:00">2025-10-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/28/LLM_Base/" title="LLM大模型基础"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen112.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM大模型基础"/></a><div class="content"><a class="title" href="/2025/09/28/LLM_Base/" title="LLM大模型基础">LLM大模型基础</a><time datetime="2025-09-27T16:00:00.000Z" title="Created 2025-09-28 00:00:00">2025-09-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/14/TrafficDefenceDetection/" title="TQ System"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen132.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="TQ System"/></a><div class="content"><a class="title" href="/2025/09/14/TrafficDefenceDetection/" title="TQ System">TQ System</a><time datetime="2025-09-13T16:00:00.000Z" title="Created 2025-09-14 00:00:00">2025-09-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 李俊泽</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Welcome to 李俊泽 の <a target="_blank" rel="noopener" href="https://www.cnblogs.com/liam-sliversucks/">Blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? '' : ''

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>