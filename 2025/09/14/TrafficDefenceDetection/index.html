<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>TQ System | All wisdom begins with memory.</title><meta name="author" content="李俊泽"><meta name="copyright" content="李俊泽"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Day011、什么是知识图谱？概念：知识图谱是以图的形式来表示实体和实体之间关系的语义网络。  节点：实体、概念 边：关系（外部）、属性（内部）    类型有两种：  实体-关系-实体【通常的说法！！】 实体-属性-属性值  2、项目的技术架构图是怎样的？  数据获取 业务数据：比较规范，一般可以直"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://liamjohnson-w.github.io/2025/09/14/TrafficDefenceDetection/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'TQ System',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-09-20 11:53:55'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css"> <script src="/live2d-widget/autoload.js"></script><script src="/live2d-widget/autoload.js"> </script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/tx.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">235</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">58</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-gamepad"></i><span> Games</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/mikutap/"><i class="fa-fw fa fa-music"></i><span> MikuTap 初音未来</span></a></li><li><a class="site-page child" href="/starbattle/"><i class="fa-fw fa fa-space-shuttle"></i><span> StartBattle 星际大战</span></a></li><li><a class="site-page child" href="/2048/"><i class="fa-fw fa fa-flag"></i><span> 2048 经典游戏</span></a></li><li><a class="site-page child" href="/battlecity/"><i class="fa-fw fa fa-arrow-circle-left"></i><span> BattleCity 坦克大战</span></a></li><li><a class="site-page child" href="/pacman/"><i class="fa-fw fa fa-bolt"></i><span> PacMan  吃豆人</span></a></li><li><a class="site-page child" href="/tetris/"><i class="fa-fw fa fa-arrows-alt"></i><span> Tetris 俄罗斯方块</span></a></li><li><a class="site-page child" href="/smallcat/"><i class="fa-fw fa fa-paw"></i><span> CatchCat 困住小猫</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-leaf"></i><span> Moments</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/diary/"><i class="fa-fw fas fa-bookmark"></i><span> Diary</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fa fa-hourglass-half"></i><span> Gallery</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-podcast"></i><span> More</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags标签</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About关于</span></a></li><li><a class="site-page child" href="/messageboard/"><i class="fa-fw fas fa-bookmark"></i><span> Messageboard留言板</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/%E8%83%8C%E6%99%AF%20(2).png')"><nav id="nav"><span id="blog-info"><a href="/" title="All wisdom begins with memory."><span class="site-name">All wisdom begins with memory.</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-gamepad"></i><span> Games</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/mikutap/"><i class="fa-fw fa fa-music"></i><span> MikuTap 初音未来</span></a></li><li><a class="site-page child" href="/starbattle/"><i class="fa-fw fa fa-space-shuttle"></i><span> StartBattle 星际大战</span></a></li><li><a class="site-page child" href="/2048/"><i class="fa-fw fa fa-flag"></i><span> 2048 经典游戏</span></a></li><li><a class="site-page child" href="/battlecity/"><i class="fa-fw fa fa-arrow-circle-left"></i><span> BattleCity 坦克大战</span></a></li><li><a class="site-page child" href="/pacman/"><i class="fa-fw fa fa-bolt"></i><span> PacMan  吃豆人</span></a></li><li><a class="site-page child" href="/tetris/"><i class="fa-fw fa fa-arrows-alt"></i><span> Tetris 俄罗斯方块</span></a></li><li><a class="site-page child" href="/smallcat/"><i class="fa-fw fa fa-paw"></i><span> CatchCat 困住小猫</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-leaf"></i><span> Moments</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/diary/"><i class="fa-fw fas fa-bookmark"></i><span> Diary</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fa fa-hourglass-half"></i><span> Gallery</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-podcast"></i><span> More</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags标签</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About关于</span></a></li><li><a class="site-page child" href="/messageboard/"><i class="fa-fw fas fa-bookmark"></i><span> Messageboard留言板</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">TQ System</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-09-13T16:00:00.000Z" title="Created 2025-09-14 00:00:00">2025-09-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-09-20T03:53:55.230Z" title="Updated 2025-09-20 11:53:55">2025-09-20</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="TQ System"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Day01"><a href="#Day01" class="headerlink" title="Day01"></a>Day01</h1><h2 id="1、什么是知识图谱？"><a href="#1、什么是知识图谱？" class="headerlink" title="1、什么是知识图谱？"></a>1、什么是知识图谱？</h2><p>概念：<strong><code>知识图谱是以图的形式来表示实体和实体之间关系的语义网络。</code></strong></p>
<ul>
<li>节点：<strong>实体</strong>、概念</li>
<li>边：<strong>关系</strong>（外部）、属性（内部）</li>
</ul>
<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250629194951478.png" alt="image-20250629194951478" style="zoom:67%;" />

<p>类型有两种：</p>
<ul>
<li><strong>实体-关系-实体【通常的说法！！】</strong></li>
<li>实体-属性-属性值</li>
</ul>
<h2 id="2、项目的技术架构图是怎样的？"><a href="#2、项目的技术架构图是怎样的？" class="headerlink" title="2、项目的技术架构图是怎样的？"></a>2、项目的技术架构图是怎样的？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250629195102287.png" alt="image-20250629195102287"></p>
<ul>
<li>数据获取<ul>
<li>业务数据：比较规范，一般可以直接使用构建知识图谱</li>
<li>采集数据：形式各异，需要进行清洗和信息抽取工作</li>
</ul>
</li>
<li>信息抽取【核心】<ul>
<li>工作：实体抽取、关系抽取、属性抽取</li>
<li>方法：规则匹配、机器学习、深度学习</li>
</ul>
</li>
<li>知识融合<ul>
<li>任务：消除冗余、解决冲突、统一表达、知识扩展</li>
<li>技术：指代消解、实体消岐、知识融合（实体对齐、关系对齐）</li>
</ul>
</li>
<li>知识加工<ul>
<li>工作：质量评估</li>
</ul>
</li>
<li>图谱搭建<ul>
<li>工作：将三元组导入到数据库中</li>
</ul>
</li>
<li>图谱应用<ul>
<li>工作：搭建问答系统</li>
</ul>
</li>
</ul>
<h2 id="3、项目用到了哪些工具？"><a href="#3、项目用到了哪些工具？" class="headerlink" title="3、项目用到了哪些工具？"></a>3、项目用到了哪些工具？</h2><ul>
<li>Doccano（多卡诺）是一种用于文本标注的开源工具，支持多种常见的文本标注任务，如命名实体识别、文本分类、关系抽取等。</li>
<li>Flask 是一个轻量级的 Python Web 框架，它的核心作用是帮助开发者快速构建 Web 应用程序和 API，实现使用URL对函数进行调用 。</li>
<li>Gunicorn是一个被广泛使用的高性能的Python WSGI UNIX HTTP服务组件(WSGI: Web Server Gateway Interface)<ul>
<li>核心作用是为 Python Web 应用（如 Flask、Django）提供生产级并发、稳定性等。</li>
<li>具有使用非常简单，轻量级的资源消耗，以及高性能等特点。</li>
</ul>
</li>
<li>Neo4j是一个高性能的图数据库，作为核心的知识存储和查询数据库。</li>
</ul>
<h2 id="4、为什么不用MySQL来存储三元组数据？"><a href="#4、为什么不用MySQL来存储三元组数据？" class="headerlink" title="4、为什么不用MySQL来存储三元组数据？"></a>4、为什么不用MySQL来存储三元组数据？</h2><ul>
<li>多跳关联查询需要多表连接，效率低</li>
<li>MySQL 是面向关系表结构设计的，缺乏对三元组语义和图结构的原生支持</li>
</ul>
<h2 id="5、什么是实体和NER？"><a href="#5、什么是实体和NER？" class="headerlink" title="5、什么是实体和NER？"></a>5、什么是实体和NER？</h2><ul>
<li>实体：<code>文本之中承载信息的语义单元</code>。如人名、地名、机构名等。</li>
<li>实体抽取：又称为命名实体识别（named entity recognition，NER），<code>指的是从文本之中抽取出命名性实体，并把这些实体划分到指定的类别。</code></li>
</ul>
<h2 id="6、命名实体识别有哪些方法？"><a href="#6、命名实体识别有哪些方法？" class="headerlink" title="6、命名实体识别有哪些方法？"></a>6、命名实体识别有哪些方法？</h2><p><strong>（1）基于规则的方法</strong></p>
<ul>
<li>针对有特殊上下文的实体，或实体本身有很多特征的文本，使用规则的方法简单且有效。比较适合半结构化或比较规范的文本中的进行抽取任务。</li>
<li>方法：<ul>
<li><code>【设计规则的模版（词典+正则表达式）再去进行匹配】</code></li>
</ul>
</li>
<li>优缺点<ul>
<li>优点：简单，快速。</li>
<li>缺点：适用性差，维护成本高后期甚至不能维护。</li>
</ul>
</li>
</ul>
<p><strong>（2）基于传统机器学习的方法</strong></p>
<ul>
<li>一般使用统计模型是把实体抽取任务转化为<code>【序列标注问题】</code>，使用IO、<code>BIO</code>、BIOES等标注方法对实体进行标注。对于文本之中的每个词，或者汉语之中的每个字，都有若干候选的标签</li>
</ul>
<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610094314077.png" alt="image-20250610094314077" style="zoom:50%;" />

<ul>
<li>基于序列标注方法的统计模型，常见的包括：支持向量机（SVM）、隐马尔科夫模型（HMM）、条件随机场（CRF）等。在实际研究之中，研究人员往往把这些模型和其他方法结合在一起。</li>
<li>优缺点<ul>
<li>优点：统计学习方法较之基于规则的方法，更加灵活和健壮，可以移植到其他领域。</li>
<li>缺点：特征的选择是至关重要的。这些模型依赖人工设计的特征和现有的自然语言处理工具（如分词工具）。<ul>
<li>常见的特征可以分为形态、词汇、句法、全局特征、外部信息等。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>（3）基于深度学习的方法</strong></p>
<ul>
<li>大量的深度学习模型被使用到实体抽取任务之中。</li>
<li>方法：基于深度学习的方法主要使用神经网络模型，结合条件随机场模型。<ul>
<li>常用的神经网络模型包括卷积神经网络（CNN）、循环神经网络（RNN）、长短期记忆网络（LSTM）等，其中<code>【BiLSTM+CRF】</code>是目前最为常用的命名实体识别模型.</li>
</ul>
</li>
<li><code>优缺点</code><ul>
<li><code>优点：不需要人工来设计特征，同时能够取得较高的准确率和召回率。</code></li>
<li><code>缺点：这些模型十分依赖人工标注数据，标注语料的缺乏为模型的训练带来了极大的困难。</code></li>
</ul>
</li>
</ul>
<h2 id="7、举个例子描述一下如何使用规则的方法抽取实体？"><a href="#7、举个例子描述一下如何使用规则的方法抽取实体？" class="headerlink" title="7、举个例子描述一下如何使用规则的方法抽取实体？"></a>7、举个例子描述一下如何使用规则的方法抽取实体？</h2><p>例子：比如要从一段新闻报道中识别出机构名。</p>
<p>规则：</p>
<ul>
<li><p>如果一个词语以“北京”、“中央”等地名词开头，那么它可能是一个机构名的开始。</p>
</li>
<li><p>如果一个词语后面紧跟着“公司”、“集团”、“局”、“部”等词，那么它可能是一个机构名的结束。</p>
</li>
</ul>
<p>思路：</p>
<ul>
<li>1）先构建词典，用于定位结构名的结束位置</li>
<li>2）使用jieba的词性标注对文本进行序列标注，获取分词结果及对应的词性</li>
<li>3）根据规则将词标注为B、E或O<ul>
<li>其中词性为ns的，即地名的，标注为B</li>
<li>词在词典中，标注成E</li>
<li>其余标注为O</li>
</ul>
</li>
<li>4）然后使用正则表达式从标注序列中取出机构名</li>
</ul>
<h1 id="Day02"><a href="#Day02" class="headerlink" title="Day02"></a>Day02</h1><h2 id="1、LSTM面试题"><a href="#1、LSTM面试题" class="headerlink" title="1、LSTM面试题"></a>1、LSTM面试题</h2><ul>
<li>传统RNN结构为什么会出现梯度消失和爆炸问题？</li>
</ul>
<p>因为在RNN的反向传播时，梯度要经过多个时间步的链式相乘，而每个时间步使用的是相同的权重矩阵，就会造成梯度消失或爆炸！——当权重矩阵的特征值小于 1 时，梯度会指数级衰减（梯度消失）；而当特征值大于 1 时，则会指数级增长（梯度爆炸）</p>
<ul>
<li>LSTM相比RNN有什么优势？</li>
</ul>
<p>LSTM的门控机制使得LSTM可以“选择性地”记忆和遗忘信息，从而有效缓解了梯度消失和梯度爆炸的问题，能够更好地捕捉序列中的长时间依赖关系。因此，LSTM相较于普通RNN在处理长序列任务（如文本生成、语音识别、时间序列预测等）中表现更为出色。</p>
<ul>
<li>BiLSTM相比LSTM有什么特点？</li>
</ul>
<p>Bi-LSTM相比LSTM能同时捕捉前后文信息，提升序列建模效果，但计算成本更高、训练时间更长。</p>
<h2 id="2、什么是线性链条件随机场（Linear-chain-CRF）"><a href="#2、什么是线性链条件随机场（Linear-chain-CRF）" class="headerlink" title="2、什么是线性链条件随机场（Linear-chain-CRF）?"></a>2、什么是线性链条件随机场（Linear-chain-CRF）?</h2><p><code>线性链条件随机场是一类给定线性输入序列 𝑋 的条件下，输出线性标签序列 𝑌 的概率分布 𝑃(𝑌∣𝑋)的概率模型。其中每个位置的标签只依赖于它前后相邻的标签以及线性序列 𝑋 ，而不依赖于更远处的标签。</code></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912071120914.png" alt="image-20250912071120914"></p>
<h2 id="3、描述一下BiLSTM-CRF架构？"><a href="#3、描述一下BiLSTM-CRF架构？" class="headerlink" title="3、描述一下BiLSTM+CRF架构？"></a>3、描述一下BiLSTM+CRF架构？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912071145607.png" alt="image-20250912071145607"></p>
<h2 id="4、CRF中的发射分数和转移分数是什么？"><a href="#4、CRF中的发射分数和转移分数是什么？" class="headerlink" title="4、CRF中的发射分数和转移分数是什么？"></a>4、CRF中的发射分数和转移分数是什么？</h2><ul>
<li>发射分数：</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912071203105.png" alt="image-20250912071203105"></p>
<ul>
<li>转移分数：</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912071224807.png" alt="image-20250912071224807"></p>
<h2 id="5、说一下CRF建模的损失函数是怎样的？"><a href="#5、说一下CRF建模的损失函数是怎样的？" class="headerlink" title="5、说一下CRF建模的损失函数是怎样的？"></a>5、说一下CRF建模的损失函数是怎样的？</h2><p>首先计算出真实路径的概率，然后让该概率值越大越好！！也就是让真实路径概率值最大时，估计未知参数的值，从而将问题转变成极大似然估计问题。</p>
<p>在问题求解中通过加负数，将求最大转换成求最小，通过求对数，将连除形式转换成对数减法形式，即<code>**负对数似然损失**</code> ！</p>
<p>最终损失函数有两部分组成，一部分是归一化项，一部分真实路径的分数。求解归一化项时使用的方法是<code>前向算法的动态规划</code>！</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912071251574.png" alt="image-20250912071251574"></p>
<p><code>说一下什么叫极大似然估计？</code></p>
<p><strong>找到一组参数，使得在这些参数下，观察到的数据出现的概率最大</strong>。</p>
<h2 id="6、前向算法是什么？"><a href="#6、前向算法是什么？" class="headerlink" title="6、前向算法是什么？"></a>6、前向算法是什么？</h2><p><code>首先：单条路径的分数怎么算的？</code></p>
<p>每条路径的分数就是由对应的发射分数和转移分数组合而成的。</p>
<p>背景：如果标签数量是𝑘，文本长度是𝑛，那么有k^n条路径，不能遍历每条路径获得所有路径的分数。</p>
<p>办法：使用<code>前向算法的动态规划</code></p>
<ul>
<li>目的：计算给定观测序列的概率总和。<ul>
<li><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250630163358692.png" alt="image-20250630163358692"></li>
</ul>
</li>
<li>过程：<code>通过动态规划的方法，逐步计算每个时刻每个状态的累加概率，以得到最终的观测序列概率总和。</code></li>
<li>特点：关注所有状态路径的累加值，计算的是所有路径的概率总和。</li>
</ul>
<p>递推公式：<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250701160500169.png" alt="image-20250701160500169"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250701155652771.png" alt="image-20250701155652771"></p>
<h2 id="7、Viterbi解码是什么？"><a href="#7、Viterbi解码是什么？" class="headerlink" title="7、Viterbi解码是什么？"></a>7、Viterbi解码是什么？</h2><p>目的：寻找给定观测序列下最可能的状态序列。</p>
<p>过程：同样<code>使用动态规划，但在每一步中只保留最优路径（即最大概率路径），而不是所有路径。</code></p>
<p>特点：关注最优路径，只保留最大概率路径，而非所有路径。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912071434296.png" alt="image-20250912071434296"></p>
<h1 id="Day03"><a href="#Day03" class="headerlink" title="Day03"></a>Day03</h1><h2 id="1、在项目中，应该如何设置路径？"><a href="#1、在项目中，应该如何设置路径？" class="headerlink" title="1、在项目中，应该如何设置路径？"></a>1、在项目中，应该如何设置路径？</h2><p>1）为了项目可移植性，需要配置成相对路径</p>
<p>2）为了避免文件在调用时，路径随着调用位置变化而变化，需要使用如下的方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如何设计，让调用时不随着调用的位置变化，而路径发生变化</span></span><br><span class="line">base_dir = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;base_dir--&gt;<span class="subst">&#123;base_dir&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 路径拼接</span></span><br><span class="line">path = os.path.join(base_dir, <span class="string">&#x27;../data/labels.json&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;拼接后的path--&gt;<span class="subst">&#123;path&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>



<h2 id="2、简单说一下数据处理的最终格式要求？"><a href="#2、简单说一下数据处理的最终格式要求？" class="headerlink" title="2、简单说一下数据处理的最终格式要求？"></a>2、简单说一下数据处理的最终格式要求？</h2><p>1）分样本的，每个样本是一个句子，并且是一个X,Y数据对</p>
<p>2）训练数据为id，而不是文字或者标签值</p>
<p>3）拆分出训练集和验证集，封装到DataLoder中</p>
<p>4）每个批次中样本的长度是一样的</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250702195148758.png" alt="image-20250702195148758"></p>
<h2 id="3、在将原始数据处理成最终的格式要求时，一般可以在哪些地方做处理？"><a href="#3、在将原始数据处理成最终的格式要求时，一般可以在哪些地方做处理？" class="headerlink" title="3、在将原始数据处理成最终的格式要求时，一般可以在哪些地方做处理？"></a>3、在将原始数据处理成最终的格式要求时，一般可以在哪些地方做处理？</h2><p>1）直接读取数据文件，然后将数据加工成想要的格式。一般用于比较复杂的数据清洗和转换操作。</p>
<p>2）在构造Dataset类时做处理。可以做一些x,y的封装、数据类型转换等。</p>
<p>3）在创建Dataloader时，在自定义函数collate_fn()中做处理。可以做一些id的转换、数据类型转换、长度对齐、生成掩码张量等。</p>
<h2 id="4、在构造数据迭代器（Dataloader）时，有哪些步骤？"><a href="#4、在构造数据迭代器（Dataloader）时，有哪些步骤？" class="headerlink" title="4、在构造数据迭代器（Dataloader）时，有哪些步骤？"></a>4、在构造数据迭代器（Dataloader）时，有哪些步骤？</h2><p>1）构建Dataset类<br>2）构建自定义函数collate_fn()<br>3）构建get_data函数，获得数据迭代器</p>
<h2 id="5、统一样本长度有哪些方法？"><a href="#5、统一样本长度有哪些方法？" class="headerlink" title="5、统一样本长度有哪些方法？"></a>5、统一样本长度有哪些方法？</h2><p>1）使用sequence来处理列表</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> sequence</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">x_train: 文本的张量表示</span></span><br><span class="line"><span class="string">max_len：最大的句子长度</span></span><br><span class="line"><span class="string">可以通过padding和truncating设置补齐或截断的方向，默认是pre</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">return</span> sequence.pad_sequences(x_train, max_len, padding=<span class="string">&quot;post&quot;</span>, truncating=<span class="string">&quot;pre&quot;</span>, value=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>2）使用pad_sequence来处理张量或者张量列表</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">pad_sequence:可以对一个批次的样本进行统一长度，统一长度的方式是以该批次中最长的样本为基准</span></span><br><span class="line"><span class="string">batch_first=True,则返回的数据形状为[batch_size, max_seq_len]  padding_value是指用什么补齐</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">input_ids_padded = pad_sequence(x_train, batch_first=<span class="literal">True</span>, padding_value=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>3）使用BertTokenizer的batch_encode_plus来处理列表</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">my_tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line"><span class="comment"># 编码text2id 对多句话进行编码用batch_encode_plus函数</span></span><br><span class="line">data = my_tokenizer.batch_encode_plus(batch_text_or_text_pairs=sents,</span><br><span class="line">                                      truncation=<span class="literal">True</span>,</span><br><span class="line">                                      padding=<span class="string">&#x27;max_length&#x27;</span>,</span><br><span class="line">                                      max_length=<span class="number">500</span>,</span><br><span class="line">                                      return_tensors=<span class="string">&#x27;pt&#x27;</span>,</span><br><span class="line">                                      return_length=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>4）使用自定义的方法</p>
<h2 id="6、描述一下BiLSTM-CRF模型的架构？"><a href="#6、描述一下BiLSTM-CRF模型的架构？" class="headerlink" title="6、描述一下BiLSTM_CRF模型的架构？"></a>6、描述一下BiLSTM_CRF模型的架构？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250913074306242.png" alt="image-20250913074306242"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250913083014466.png" alt="image-20250913083014466"></p>
<h1 id="Day04"><a href="#Day04" class="headerlink" title="Day04"></a>Day04</h1><h2 id="1、训练函数基本步骤是什么？"><a href="#1、训练函数基本步骤是什么？" class="headerlink" title="1、训练函数基本步骤是什么？"></a>1、训练函数基本步骤是什么？</h2><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">1.构建数据迭代器Dataloader(包括数据处理与构建数据源Dataset)</span></span><br><span class="line"><span class="attr">2.实例化模型</span></span><br><span class="line"><span class="attr">3.实例化损失函数对象</span></span><br><span class="line"><span class="attr">4.实例化优化器对象</span></span><br><span class="line"><span class="attr">5.定义打印日志参数</span></span><br><span class="line"><span class="attr">6.开始训练</span></span><br><span class="line"><span class="attr">6.1</span> <span class="string">实现外层大循环epoch</span></span><br><span class="line">    <span class="attr">6.2</span> <span class="string">将模型设置为训练模式</span></span><br><span class="line">    <span class="attr">6.3</span> <span class="string">内部遍历数据迭代器dataloader</span></span><br><span class="line">        <span class="attr">1）将数据送入模型得到输出结果</span></span><br><span class="line">        <span class="attr">2）计算损失</span></span><br><span class="line">        <span class="attr">3）梯度清零</span>: <span class="string">optimizer.zero_grad()</span></span><br><span class="line">        <span class="attr">4）反向传播(计算梯度)</span>: <span class="string">loss.backward()</span></span><br><span class="line">        <span class="attr">5）梯度更新(参数更新)</span>: <span class="string">optimizer.step()</span></span><br><span class="line">        <span class="attr">6）打印内部训练日志</span></span><br><span class="line">    <span class="attr">6.4</span> <span class="string">使用验证集进行模型评估【将模型设置为评估模式】</span></span><br><span class="line">    <span class="attr">6.5</span> <span class="string">保存模型: torch.save(model.state_dict(), &quot;model_path&quot;)</span></span><br><span class="line"><span class="attr">6.6</span> <span class="string">打印外部训练日志</span></span><br></pre></td></tr></table></figure>

<h2 id="2、验证函数基本步骤是什么？"><a href="#2、验证函数基本步骤是什么？" class="headerlink" title="2、验证函数基本步骤是什么？"></a>2、验证函数基本步骤是什么？</h2><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">1.定义打印日志参数</span></span><br><span class="line"><span class="attr">2.将模型设置为评估模式</span></span><br><span class="line"><span class="attr">3.内部遍历数据迭代器dataloader</span></span><br><span class="line">  <span class="attr">3.1</span> <span class="string">将数据送入模型得到输出结果</span></span><br><span class="line">  <span class="attr">3.2</span> <span class="string">计算损失</span></span><br><span class="line">  <span class="attr">3.3</span> <span class="string">处理结果</span></span><br><span class="line">  <span class="attr">3.4</span> <span class="string">统计批次内指标</span></span><br><span class="line"><span class="attr">4.统计整体指标</span></span><br></pre></td></tr></table></figure>

<h2 id="3、BiLSTM-CRF模型在训练完后，可以做哪些优化来改善模型性能？"><a href="#3、BiLSTM-CRF模型在训练完后，可以做哪些优化来改善模型性能？" class="headerlink" title="3、BiLSTM_CRF模型在训练完后，可以做哪些优化来改善模型性能？"></a>3、BiLSTM_CRF模型在训练完后，可以做哪些优化来改善模型性能？</h2><p>1）模型优化</p>
<p>预训练词向量：使用预训练的词向量（如Word2Vec、GloVe、FastText）替代随机初始化的词嵌入，可以更好地捕捉词汇语义信息。</p>
<p>自注意力机制：在BiLSTM后加入自注意力层，增强模型对长距离依赖的捕捉能力。</p>
<p>调整随机失活层：可以在embedding层后添加随机失活层，也可以修改随机失活比例。</p>
<p>2）训练过程优化</p>
<ul>
<li>shuffles设置：注意真正训练时，需要将DataLoader中的shuffle设置为True</li>
<li>梯度裁剪：在反向传播时对梯度进行裁剪，防止梯度爆炸。</li>
<li>早停机制：监控验证集F1值，若连续多个epoch未提升则提前终止训练。</li>
</ul>
<p>3）训练数据优化</p>
<ul>
<li>如果训练集和验证集数据分布不同，也就是说使用的是差距很大的样本，会使模型的效果较差，所以可以将数据打散后再送到dataloader中</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>():</span><br><span class="line">    <span class="comment"># 为了能够让训练集和验证集的样本分布一致，需要先将数据集打乱，然后再去进行划分</span></span><br><span class="line">    random.seed(<span class="number">66</span>)</span><br><span class="line">    random.shuffle(datas)  <span class="comment"># 数据会原地修改</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建训练数据集：基于原始数据集，进行8:2拆分</span></span><br><span class="line">    train_dataset = NerDataset(datas[:<span class="number">6300</span>])</span><br></pre></td></tr></table></figure>

<ul>
<li>除了这种方式之外，也可以使用<code>分类采样</code>的方式。这种方式可以绝对类型上，训练集和验证集的分布是一致的。</li>
</ul>
<p>另外，还有以下方法——</p>
<p>更多数据：收集或标注更多数据，送到模型中进行训练。</p>
<p>随机替换：随机替换部分词为同义词或近义词，增强模型鲁棒性。</p>
<p>实体替换：保留实体边界，随机替换实体内容（如疾病名称、药品名称），提升实体识别泛化能力。</p>
<h2 id="4、precision、recall、f1、report的使用方式是什么？"><a href="#4、precision、recall、f1、report的使用方式是什么？" class="headerlink" title="4、precision、recall、f1、report的使用方式是什么？"></a>4、precision、recall、f1、report的使用方式是什么？</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># classification_report可以导出字典格式，修改参数：output_dict=True，可以将字典在保存为csv格式输出</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score, f1_score, classification_report</span><br><span class="line"></span><br><span class="line">precision = precision_score(golds, preds, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">recall = recall_score(golds, preds, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">f1 = f1_score(golds, preds, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">report = classification_report(golds, preds)</span><br></pre></td></tr></table></figure>

<p>其中golds和preds要求的格式要求为：</p>
<p>1）<strong>1D 数组</strong>（最常见）</p>
<ul>
<li>Python 列表：<code>[0, 1, 1, 0, 2]</code></li>
<li>NumPy 数组：<code>np.array([0, 1, 1, 0, 2])</code></li>
<li>Pandas Series：<code>pd.Series([0, 1, 1, 0, 2])</code></li>
</ul>
<p>适用于 多分类、二分类、单标签 情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]</span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<p>2）<strong>Label Indicator Array &#x2F; Sparse Matrix</strong></p>
<p>适用于 多标签分类（multi-label classification）：</p>
<p>label indicator array：二维数组，每一列表示一个类别，值为 0&#x2F;1，表示每个类别的有无。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y_true = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">          [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">          [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]]</span><br><span class="line">y_pred = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">          [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">          [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]]</span><br></pre></td></tr></table></figure>

<p>含义是：</p>
<ul>
<li><p>第一行：属于类别 0 和 2</p>
</li>
<li><p>第二行：属于类别 1</p>
</li>
<li><p>第三行：属于类别 0 和 1</p>
</li>
</ul>
<p>把数据组装成一维列表的方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将非padding位置的预测标签和真实标签保存起来，使用的方法是：通过input_ids进行非零判断，然后得到一个boolean的张量，然后直接对这个张量进行求和，就可以获取到每个句子的真实的长度，让让再通过这个长度，使用列表切片的方式，从标签中取出真实位置对应的标签</span></span><br><span class="line"><span class="comment"># print(f&#x27;每个样本的真实长度--&gt;&#123;(input_ids&gt;0).sum(-1).tolist()&#125;&#x27;)  # [11, 13, 10, 14, 55, 22, 39, 25]</span></span><br><span class="line">real_len = (input_ids&gt;<span class="number">0</span>).<span class="built_in">sum</span>(-<span class="number">1</span>).tolist()</span><br><span class="line"><span class="comment"># 根据真实的句子长度，获取预测的标签</span></span><br><span class="line"><span class="keyword">for</span> index, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(predict):</span><br><span class="line">    preds.extend(label[:real_len[index]])</span><br><span class="line"><span class="comment"># 根据真实的句子长度，获取真实的标签</span></span><br><span class="line"><span class="keyword">for</span> index, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(labels.tolist()):</span><br><span class="line">    golds.extend(label[:real_len[index]])</span><br></pre></td></tr></table></figure>



<h2 id="5、模型预测基本步骤是什么？"><a href="#5、模型预测基本步骤是什么？" class="headerlink" title="5、模型预测基本步骤是什么？"></a>5、模型预测基本步骤是什么？</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>实例化模型</span><br><span class="line"><span class="number">2.</span>加载训练好的模型参数</span><br><span class="line"><span class="number">3.</span>处理数据</span><br><span class="line"><span class="number">4.</span>模型预测</span><br><span class="line"><span class="number">5.</span>结果处理</span><br></pre></td></tr></table></figure>



<h1 id="Day05"><a href="#Day05" class="headerlink" title="Day05"></a>Day05</h1><h2 id="1、什么是关系抽取？本质是什么？"><a href="#1、什么是关系抽取？本质是什么？" class="headerlink" title="1、什么是关系抽取？本质是什么？"></a>1、什么是关系抽取？本质是什么？</h2><p>关系抽取就是从一段文本中抽取出 (主体，关系，客体) 这样的三元组</p>
<p>本质是：<code>文本分类问题</code></p>
<h2 id="关系抽取的常用方法有哪些？"><a href="#关系抽取的常用方法有哪些？" class="headerlink" title="关系抽取的常用方法有哪些？"></a>关系抽取的常用方法有哪些？</h2><ul>
<li>基于规则方式实现关系抽取<ul>
<li>人工定义规则</li>
</ul>
</li>
<li>基于机器学习<ul>
<li>决策树、随机森林、线性回归等</li>
</ul>
</li>
<li>基于深度学习<ul>
<li>基于Pipeline流水线方法实现关系抽取：在实体识别已经完成的基础上再进行实体之间关系的抽取<ul>
<li>如：BiLSTM+Attention模型</li>
</ul>
</li>
<li>基于Joint联合抽取方法实现关系抽取：修改标注方法和模型结构直接输出文本中包含的(ei ,rk, ej)三元组<ul>
<li>如：联合解码的联合模型、参数共享的联合模型</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="3、关系抽取任务常见问题有哪些？"><a href="#3、关系抽取任务常见问题有哪些？" class="headerlink" title="3、关系抽取任务常见问题有哪些？"></a>3、关系抽取任务常见问题有哪些？</h2><ul>
<li>正常关系 (Normal) 问题：数据中只有一个实体对及关系</li>
<li>单一实体关系重叠问题 (Single Entity Overlap (SEO) )：数据中一个实体参与到了多个关系中<ul>
<li>BiLSTM+Attention模型即可解决，一个句子中有几个三元组就构建几个样本即可</li>
</ul>
</li>
<li>实体对重叠(Entity Pair Overlap (EPO))：数据中一个实体对有两种不同的关系类型<ul>
<li>Casrel模型可以解决</li>
</ul>
</li>
</ul>
<h2 id="4、基于规则的方法实现关系抽取的优缺点是什么？"><a href="#4、基于规则的方法实现关系抽取的优缺点是什么？" class="headerlink" title="4、基于规则的方法实现关系抽取的优缺点是什么？"></a>4、基于规则的方法实现关系抽取的优缺点是什么？</h2><ul>
<li>优点：实现简单、无需训练，小规模数据集容易实现.</li>
<li>缺点：<ul>
<li>无法解决复杂的场景</li>
<li>对跨领域的可移植性较差、人工制作规则的成本较高以及召回率较低.</li>
</ul>
</li>
</ul>
<h2 id="5、描述一下BiLSTM-Attention模型的架构？"><a href="#5、描述一下BiLSTM-Attention模型的架构？" class="headerlink" title="5、描述一下BiLSTM+Attention模型的架构？"></a>5、描述一下BiLSTM+Attention模型的架构？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250916071419364.png" alt="image-20250916071419364"></p>
<h2 id="6、注意力机制是什么？"><a href="#6、注意力机制是什么？" class="headerlink" title="6、注意力机制是什么？"></a>6、注意力机制是什么？</h2><p>注意力机制是什么？</p>
<p>注意力机制（Attention）是一种动态加权的方法，它通过计算“查询”（query）与一组“键”（keys）之间的相似度来为对应的“值”（values）分配不同的重要性权重，从而使模型能够在处理序列或图像等输入时，重点关注与当前任务最相关的部分信息。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250802233021882.png" alt="image-20250802233021882"></p>
<p>优势：</p>
<p><strong>捕捉长距离依赖</strong>：不再依赖 RNN 的逐步传递，能直接建模序列中任意两位置的依赖关系。</p>
<p><strong>并行计算</strong>：尤其在 Transformer 中，注意力计算可以大范围并行，极大加速训练。</p>
<p><strong>可解释性</strong>：通过可视化注意力权重，可以了解到模型在处理时重点关注了哪些输入位置。</p>
<h2 id="7、描述一下BiLSTM-Attention模型中注意力机制是如何实现的？"><a href="#7、描述一下BiLSTM-Attention模型中注意力机制是如何实现的？" class="headerlink" title="7、描述一下BiLSTM+Attention模型中注意力机制是如何实现的？"></a>7、描述一下BiLSTM+Attention模型中注意力机制是如何实现的？</h2><p>首先对 BiLSTM 的输出进行非线性变换，得到初步的语义特征表示；然后通过一个可训练的权重向量和 softmax 函数，计算每个单词对整体语义的重要性权重；接着使用这些注意力权重对 BiLSTM 的输出进行加权求和，提取出句子的全局语义特征；最后通过非线性变换得到最终的上下文向量，用于后续的分类任务。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250916071359413.png" alt="image-20250916071359413"></p>
<h2 id="8、BiLSTM-Attentiom模型中数据处理的整体思路是什么？"><a href="#8、BiLSTM-Attentiom模型中数据处理的整体思路是什么？" class="headerlink" title="8、BiLSTM+Attentiom模型中数据处理的整体思路是什么？"></a>8、BiLSTM+Attentiom模型中数据处理的整体思路是什么？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250916071311082.png" alt="image-20250916071311082"></p>
<h1 id="Day06"><a href="#Day06" class="headerlink" title="Day06"></a>Day06</h1><h2 id="1、BERT预训练模型所接收的最大sequence长度是多少，为什么设置最大长度？"><a href="#1、BERT预训练模型所接收的最大sequence长度是多少，为什么设置最大长度？" class="headerlink" title="1、BERT预训练模型所接收的最大sequence长度是多少，为什么设置最大长度？"></a>1、BERT预训练模型所接收的最大sequence长度是多少，为什么设置最大长度？</h2><p>512</p>
<p>BERT 的输入除了 Token Embedding 之外，还要加上位置编码（position embeddings），以告诉模型“第 i 个 token 在序列中的位置”。它在进行embedding的时候，需要设置embedding层的 vocab_size（这里指的的是有多少个位置，而不是字符的数量），这个值会在构建模型时写死！所以，一旦写死之后，句子的最大长度就确定了，后续在使用时，就不能超过这个句子的最大长度，因为一旦超过之后，超出的位置编码就没有办法进行embedding查表了。训练BERT时，大部分语料都不超过512，所以最终指定句子的最大长度为512。模型训练好之后，在进行使用时，需要将样本统一成最大长度。</p>
<h2 id="2、对于长文本-文本长度超过512的句子-在使用BERT时-如何来构造训练样本？"><a href="#2、对于长文本-文本长度超过512的句子-在使用BERT时-如何来构造训练样本？" class="headerlink" title="2、对于长文本(文本长度超过512的句子)在使用BERT时, 如何来构造训练样本？"></a>2、对于长文本(文本长度超过512的句子)在使用BERT时, 如何来构造训练样本？</h2><p>核心就是如何进行截断。</p>
<ul>
<li>head-only方式: 这是只保留长文本头部信息的截断方式, 具体为保存前510个token (要留两个位置给[CLS]和[SEP]).</li>
<li>tail-only方式: 这是只保留长文本尾部信息的截断方式, 具体为保存最后510个token (要留两个位置给[CLS]和[SEP]).</li>
<li>head+only方式: 选择前128个token和最后382个token (文本总长度在510以内), 或者前256个token和最后254个token (文本总长度大于510).</li>
</ul>
<h2 id="3、BiLSTM-Attention模型的架构是怎样的？"><a href="#3、BiLSTM-Attention模型的架构是怎样的？" class="headerlink" title="3、BiLSTM+Attention模型的架构是怎样的？"></a>3、BiLSTM+Attention模型的架构是怎样的？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250917172341057.png" alt="image-20250917172341057"></p>
<h2 id="4、在BERT中，是如何将Token-Embedding、Segment-Embedding-和-Position-Embedding组合在一起然后送到encoder中的？"><a href="#4、在BERT中，是如何将Token-Embedding、Segment-Embedding-和-Position-Embedding组合在一起然后送到encoder中的？" class="headerlink" title="4、在BERT中，是如何将Token Embedding、Segment Embedding 和 Position Embedding组合在一起然后送到encoder中的？"></a>4、在BERT中，是如何将Token Embedding、Segment Embedding 和 Position Embedding组合在一起然后送到encoder中的？</h2><p>在 BERT 中，模型的输入表示由三部分同维度的向量按位相加得到，然后送入后续的 Transformer encoder。</p>
<p>网络模型：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250708203107793.png" alt="image-20250708203107793"></p>
<p>示意图：</p>
<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250708203422586.png" alt="image-20250708203422586" style="zoom:67%;" />

<p>示例：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250708203902969.png" alt="image-20250708203902969"></p>
<h2 id="5、两个矩阵相乘时，shape不符合要求怎么办？"><a href="#5、两个矩阵相乘时，shape不符合要求怎么办？" class="headerlink" title="5、两个矩阵相乘时，shape不符合要求怎么办？"></a>5、两个矩阵相乘时，shape不符合要求怎么办？</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">A = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">B = torch.randn(<span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;A--&gt;<span class="subst">&#123;A&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;B--&gt;<span class="subst">&#123;B&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># matmul 自动广播：</span></span><br><span class="line">result = torch.matmul(A, B)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 先手动进行广播，再进行矩阵乘法</span></span><br><span class="line">result = torch.bmm(A.expand(<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>),  B)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>

<h1 id="Day07"><a href="#Day07" class="headerlink" title="Day07"></a>Day07</h1><h2 id="1、BiLSTM-Attention模型可以做哪些优化来改善模型性能？"><a href="#1、BiLSTM-Attention模型可以做哪些优化来改善模型性能？" class="headerlink" title="1、BiLSTM+Attention模型可以做哪些优化来改善模型性能？"></a>1、BiLSTM+Attention模型可以做哪些优化来改善模型性能？</h2><p><strong>1）模型优化</strong></p>
<ul>
<li>句子嵌入方式：可以使用jieba分词得到词语，然后再使用词语的方式进行嵌入。</li>
<li>替换BiLSTM：将BiLSTM替换成BERT&#x2F;RoBERTa等这种预训练模型或BiGRU去做嵌入，看是否可以提供模型的语义表达能力。</li>
<li>多头注意力机制：借鉴Transformer中多头注意力机制，将单一注意力拆分到多个子空间，去捕捉不同维度的语义信息。</li>
<li>修改注意力机制的方式：使用transformer中注意力机制的计算方式或者先进行从concat再经过linear层的方式等，来计算注意力机制，看模型的性能效果。</li>
<li>调整随机失活层：调整随机失活层的位置、有无或随机失活比例，来观察模型的性能变化。</li>
</ul>
<p><strong>2）训练过程的优化</strong></p>
<ul>
<li>shuffle设置：注意在真正训练时，需要将dataloader中的shuffle设置为True</li>
<li>梯度裁剪：在反向传播时对梯度进行裁剪，防止梯度消失或爆炸。</li>
<li>早停机制：监控验证集上F1值或其他关键指标，如果连续多个epoch未提升或者开始下降，则提前终止训练。</li>
</ul>
<p><strong>3）训练数据优化</strong></p>
<ul>
<li>通过过采样或欠采样来解决样本不均衡问题</li>
<li>通过同义词替换、回译、实体替换等方法来扩充数据集。或者直接使用大模型进行训练样本的生成。</li>
</ul>
<h2 id="2、Pipeline方法的优缺点"><a href="#2、Pipeline方法的优缺点" class="headerlink" title="2、Pipeline方法的优缺点"></a>2、Pipeline方法的优缺点</h2><ul>
<li>优点：<ul>
<li>易于实现，实体模型和关系模型使用独立的数据集，不需要同时标注实体和关系的数据集.</li>
<li>两者相互独立，若关系抽取模型没训练好不会影响到实体抽取.</li>
</ul>
</li>
<li>缺点：<ul>
<li>关系和实体两者是紧密相连的，互相之间的联系没有捕捉到.</li>
<li>上游 NER 的错误会直接影响下游关系抽取，容易造成误差积累.</li>
<li>BiLSTM_Attention难以处理EPO问题</li>
</ul>
</li>
</ul>
<h2 id="3、Joint方法是什么？有哪两种类型？"><a href="#3、Joint方法是什么？有哪两种类型？" class="headerlink" title="3、Joint方法是什么？有哪两种类型？"></a>3、Joint方法是什么？有哪两种类型？</h2><p>（1）概念</p>
<p>通过修改模型结构或标注方法， 直接输出文本中包含的SPO三元组</p>
<p>（2）类型</p>
<ul>
<li>参数共享的联合模型【修改模型结构】<ul>
<li>主体、客体和关系的抽取不是严格同步进行的 (<strong>通常是依次执行，但是某些情况下也可以其中两个任务一起进行</strong>) ，各个过程都可以得到一个loss值，<code>整个模型的loss是各过程loss值之和.</code></li>
</ul>
</li>
</ul>
<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/4-1-1.png" alt="img" style="zoom:67%;" />

<ul>
<li><p>联合解码的联合模型【修改标注方法】</p>
<ul>
<li>主体、客体和关系的抽取是同时进行的，通过一个模型直接得到SPO三元组.</li>
</ul>
</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250918193641102.png" alt="image-20250918193641102"></p>
<h2 id="4、Casrel模型的架构是怎样的？"><a href="#4、Casrel模型的架构是怎样的？" class="headerlink" title="4、Casrel模型的架构是怎样的？"></a>4、Casrel模型的架构是怎样的？</h2><p>第一步：识别出句子中的Subject</p>
<ul>
<li><p>（1）<code>两个线性层+sigmoid去分类任务</code>：一个判断每个token是不是头实体的开始索引；一个判断每个token是不是头实体的结束索引。</p>
</li>
<li><p>（2）利用最近匹配原则将识别到的start和end配对，获得候选头实体集合</p>
</li>
</ul>
<p><code>第二步：根据识别出的Subject，识别出所有有可能的Relation及对应的Object</code></p>
<ul>
<li><code>（1）bert隐藏层输出+所取的Subject特征向量作为输入【若Subject存在多个字，则取平均向量】</code></li>
<li>（2）对于识别出来的每一个Subject，对应的每一种关系会解码出其Object的Start和End索引位置，与Subject类似</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250918193657010.png" alt="image-20250918193657010"></p>
<h2 id="5、说一下Casrel模型的输入输出是什么？"><a href="#5、说一下Casrel模型的输入输出是什么？" class="headerlink" title="5、说一下Casrel模型的输入输出是什么？"></a>5、说一下Casrel模型的输入输出是什么？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250918193827404.png" alt="image-20250918193827404"></p>
<h2 id="6、数据处理整体思路是怎样的？"><a href="#6、数据处理整体思路是怎样的？" class="headerlink" title="6、数据处理整体思路是怎样的？"></a>6、数据处理整体思路是怎样的？</h2><p>原始数据：<br>文件格式是json，每行都是样本<br>text为原始文本，spo_list为三元组列表【一个样本中可能有多个spo三元组】</p>
<ol>
<li>构建Dataset</li>
</ol>
<p>在构建Dataset时，直接使用json.loads()方法逐行去加载数据，存储到字典列表中，然后取出text中的原始文本和spo_list中的三元组列表进行返回</p>
<ol start="2">
<li>构建自定义函数collect_fn()<ul>
<li><code>使用bert的分词器对原始文本进行处理，获取input_ids和attention_mask</code></li>
<li>基于每个样本的input_ids和spo_list去获取训练数据的其他输入和输出</li>
<li><code>对每个样本的结果数据进行拼接，再转成tensor作为最终模型训练的数据</code></li>
</ul>
</li>
<li>构建get_data_loader()函数，获取数据迭代器<ul>
<li>分别使用train&#x2F;dev&#x2F;test.json文件构造不同的Dataloader对象即可</li>
</ul>
</li>
</ol>
<p>训练数据：</p>
<p><code>输入：input_ids, attention_mask, 所取头实体从头到尾的位置信息，所取头实体的长度</code></p>
<p><code>输出：主实体的开始、结束位置信息，客实体的开始、结束位置信息及关系信息</code></p>
<p>注意：</p>
<p>在使用每个样本的input_ids和spo_list获取 sub_head2tail、sub_len、obj_heads、obj_tails这四个值的时候，做法如下：</p>
<p>在模型训练时，取的主实体的信息是真实spo_list中的值，原因是:</p>
<p>1)如果我们取模型第一步预测出来的主实体，此时有可能这个主实体预测错了，那么它就没有对应的客实体及关系信息，此时则无法构造标签，无法计算损失!</p>
<p>2)使用teacher_forcing这种方法，使用真实的spo_list中主实体信息去训练模型，可以加快模型的收敛速度，提高训练的效率</p>
<p>而在模型预测时，只知道原始文本，不知道主实体信息，此时sub_head2tail和sub_len这两个信息则是由模型第一步预测出来的主实体。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250918193956400.png" alt="image-20250918193956400"></p>
<h1 id="Day08"><a href="#Day08" class="headerlink" title="Day08"></a>Day08</h1><h2 id="1、Casrel模型数据处理的整体思路是什么？"><a href="#1、Casrel模型数据处理的整体思路是什么？" class="headerlink" title="1、Casrel模型数据处理的整体思路是什么？"></a>1、Casrel模型数据处理的整体思路是什么？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250919191750171.png" alt="image-20250919191750171"></p>
<h2 id="2、使用Casrel模型时，遇到什么问题，如何解决的？"><a href="#2、使用Casrel模型时，遇到什么问题，如何解决的？" class="headerlink" title="2、使用Casrel模型时，遇到什么问题，如何解决的？"></a>2、使用Casrel模型时，遇到什么问题，如何解决的？</h2><p>遇到的问题：<br>如果一个样本中有多个主实体，按照Casrel模型的定义，需要先取出一个主实体，然后去预测该主实体的客实体及关系；然后用相同的方法再去处理其他主实体。这种处理方式在构建数据时比较复杂。</p>
<p>解决方案：<br>考虑到数据的情况，大部分的样本都是只有1个主实体，可以在每次训练构建训练数据集时，使用随机的方式抽取一个主实体，然后基于抽取到的这个主实体完成其客实体和关系的预测。因为训练是有多个轮次的，每次随机抽取，所以也相当于将所有的主实体都送到了模型中进行了训练。</p>
<p>在随机抽取时，每次随机抽取一个主实体，然后在客实体及关系信息中，记录该主实体所有的客实体开始位置及结束位置信息和关系。即sub_head2tail、sub_len是所取的主实体的信息，obj_heads、obj_tails为所取主实体对应的所有的客实体开始位置及结束位置信息和关系信息。</p>
<p>但是sub_heads和sub_tails需要记录一个样本的所有的主实体的开始位置信息及结束位置信息，这个不单单只记录抽取到的那个主实体，即sub_heads、sub_tails为该样本所有的主实体的开始位置和结束位置信息。</p>
<h2 id="3、Casrel模型的结构是怎样的？"><a href="#3、Casrel模型的结构是怎样的？" class="headerlink" title="3、Casrel模型的结构是怎样的？"></a>3、Casrel模型的结构是怎样的？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250919191736057.png" alt="image-20250919191736057"></p>
<h1 id="NER任务代码"><a href="#NER任务代码" class="headerlink" title="NER任务代码"></a>NER任务代码</h1><h2 id="BiLSTM-CRF项目完整实现"><a href="#BiLSTM-CRF项目完整实现" class="headerlink" title="BiLSTM+CRF项目完整实现"></a>BiLSTM+CRF项目完整实现</h2><p>（1）整体步骤</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">整体实现思路(1-4数据数据预处理，5-8模型部分)</span>: <span class="string"></span></span><br><span class="line"><span class="attr">1、获取数据，例如通过人工数据标注或者第三方数据等。</span></span><br><span class="line"><span class="attr">2、对数据进行处理，构造训练数据</span></span><br><span class="line"><span class="attr">3、构建DataSet类</span></span><br><span class="line"><span class="attr">4、加载数据集</span> <span class="string">DataLoader</span></span><br><span class="line"><span class="attr">5、定义模型（embedding、线性层、CRF层）</span></span><br><span class="line"><span class="attr">6、初始化模型、loss、优化器、前向传播、反向传播、梯度更新</span></span><br><span class="line"><span class="attr">7、模型训练、评估</span></span><br><span class="line"><span class="attr">8、模型加载、测试</span></span><br></pre></td></tr></table></figure>

<p>（2）代码架构图</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250614195327232.png" alt="image-20250614195327232"></p>
<h3 id="【实现】数据预处理"><a href="#【实现】数据预处理" class="headerlink" title="【实现】数据预处理"></a>【实现】数据预处理</h3><h5 id="第一步-查看项目数据集"><a href="#第一步-查看项目数据集" class="headerlink" title="第一步: 查看项目数据集"></a>第一步: 查看项目数据集</h5><p><strong>data_origin：原始数据</strong></p>
<ul>
<li><p>四类内容：一般项目、出院情况、病史特点、诊疗经过</p>
</li>
<li><p>每类中有两种文件</p>
<p>（1）.txt结尾：<code>标注好的数据，包括其位置和类型</code></p>
</li>
</ul>
<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250612095407502.png" alt="image-20250612095407502" style="zoom: 67%;" />



<p>​				（2）txtoriginal.txt结尾：<code>原始文档</code></p>
<p><strong>data：处理好的数据</strong></p>
<ul>
<li><p>labels.json  实体类型文件</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&#123;</span></span><br><span class="line">  <span class="attr">&quot;治疗&quot;</span>: <span class="string">&quot;TREATMENT&quot;,</span></span><br><span class="line">  <span class="attr">&quot;身体部位&quot;</span>: <span class="string">&quot;BODY&quot;,</span></span><br><span class="line">  <span class="attr">&quot;症状和体征&quot;</span>: <span class="string">&quot;SIGNS&quot;,</span></span><br><span class="line">  <span class="attr">&quot;检查和检验&quot;</span>: <span class="string">&quot;CHECK&quot;,</span></span><br><span class="line">  <span class="attr">&quot;疾病和诊断&quot;</span>: <span class="string">&quot;DISEASE&quot;</span></span><br><span class="line"><span class="attr">&#125;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>tag2id.json  标注标签及ID</p>
</li>
</ul>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&#123;</span></span><br><span class="line"> <span class="attr">&quot;O&quot;</span>: <span class="string">0,</span></span><br><span class="line"> <span class="attr">&quot;B-TREATMENT&quot;</span>: <span class="string">1,</span></span><br><span class="line"> <span class="attr">&quot;I-TREATMENT&quot;</span>: <span class="string">2,</span></span><br><span class="line"> <span class="attr">&quot;B-BODY&quot;</span>: <span class="string">3,</span></span><br><span class="line"> <span class="attr">&quot;I-BODY&quot;</span>: <span class="string">4,</span></span><br><span class="line"> <span class="attr">&quot;B-SIGNS&quot;</span>: <span class="string">5,</span></span><br><span class="line"> <span class="attr">&quot;I-SIGNS&quot;</span>: <span class="string">6,</span></span><br><span class="line"> <span class="attr">&quot;B-CHECK&quot;</span>: <span class="string">7,</span></span><br><span class="line"> <span class="attr">&quot;I-CHECK&quot;</span>: <span class="string">8,</span></span><br><span class="line"> <span class="attr">&quot;B-DISEASE&quot;</span>: <span class="string">9,</span></span><br><span class="line"> <span class="attr">&quot;I-DISEASE&quot;</span>: <span class="string">10</span></span><br><span class="line"><span class="attr">&#125;</span></span><br></pre></td></tr></table></figure>



<h5 id="第二步-构造序列标注数据"><a href="#第二步-构造序列标注数据" class="headerlink" title="第二步: 构造序列标注数据"></a>第二步: 构造序列标注数据</h5><p>（1）思路</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250910163559050.png" alt="image-20250910163559050"></p>
<p>（2）课堂知识补充</p>
<p>代码位置：P03_NER&#x2F;LSTM_CRF&#x2F;utils&#x2F;test.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取当前路径</span></span><br><span class="line">cur = os.getcwd()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;当前路径--&gt;<span class="subst">&#123;cur&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 切换路径</span></span><br><span class="line">os.chdir(<span class="string">&#x27;..&#x27;</span>)</span><br><span class="line">cur = os.getcwd()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;修改之后的路径--&gt;<span class="subst">&#123;cur&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 路径的拼接</span></span><br><span class="line">path = os.path.join(cur, <span class="string">&#x27;data/labels.json&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;拼接之后的路径--&gt;<span class="subst">&#123;path&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 读取json文件</span></span><br><span class="line"><span class="comment"># labels = json.load(open(path, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;))</span></span><br><span class="line"><span class="comment"># print(f&#x27;labels--&gt;&#123;labels&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如何设计，让这个代码在调用时，相对路径不随着调用位置变化而变化</span></span><br><span class="line">file_path = os.path.abspath(__file__)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;file_path--&gt;<span class="subst">&#123;file_path&#125;</span>&#x27;</span>)</span><br><span class="line">base_dir = os.path.dirname(file_path)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;base_dir--&gt;<span class="subst">&#123;base_dir&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 路径拼接</span></span><br><span class="line">path = os.path.join(base_dir, <span class="string">&#x27;../data/labels.json&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;拼接之后的路径2--&gt;<span class="subst">&#123;path&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 读取json文件</span></span><br><span class="line">labels = json.load(<span class="built_in">open</span>(path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;labels--&gt;<span class="subst">&#123;labels&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># os.walk的使用</span></span><br><span class="line">results = os.walk(os.path.join(base_dir, <span class="string">&#x27;../data_origin&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;results--&gt;<span class="subst">&#123;results&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> dir_path, dirs, files <span class="keyword">in</span> results:  <span class="comment"># 路径、文件夹（列表）、文件（列表）</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;*&#x27;</span>*<span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;dir_path--&gt;<span class="subst">&#123;dir_path&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;dirs--&gt;<span class="subst">&#123;dirs&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;files--&gt;<span class="subst">&#123;files&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>（3）代码</p>
<p>代码位置：P03_NER&#x2F;LSTM_CRF&#x2F;utils&#x2F;data_process.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取文件的绝对路径，然后根据这个路径去进行拼接</span></span><br><span class="line">base_dir = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;base_dir--&gt;<span class="subst">&#123;base_dir&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransferData</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 获取标签</span></span><br><span class="line">        self.lables_dict = json.load(<span class="built_in">open</span>(os.path.join(base_dir, <span class="string">&#x27;../data/labels.json&#x27;</span>), <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">        <span class="comment"># print(f&#x27;lables_dict--&gt;&#123;self.lables_dict&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># 原始数据路径</span></span><br><span class="line">        self.origin_path = os.path.join(base_dir, <span class="string">&#x27;../data_origin&#x27;</span>)</span><br><span class="line">        <span class="comment"># 处理后的数据路径</span></span><br><span class="line">        self.train_path = os.path.join(base_dir, <span class="string">&#x27;../data/train.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">transfer</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(self.train_path, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fw:</span><br><span class="line">            <span class="keyword">for</span> dirpath, dirnames, filenames <span class="keyword">in</span> os.walk(self.origin_path): <span class="comment"># 路径、文件夹、文件</span></span><br><span class="line">                <span class="comment"># print(f&#x27;dirpath--&gt;&#123;dirpath&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># print(f&#x27;dirnames--&gt;&#123;dirnames&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># print(f&#x27;filenames--&gt;&#123;filenames&#125;&#x27;)</span></span><br><span class="line">                <span class="keyword">for</span> filename <span class="keyword">in</span> filenames:</span><br><span class="line">                    <span class="keyword">if</span> <span class="string">&#x27;txtoriginal&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> filename:  <span class="comment"># 我们只处理包含txtoriginal文件</span></span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    <span class="comment"># print(f&#x27;filename--&gt;&#123;filename&#125;&#x27;)</span></span><br><span class="line">                    <span class="comment"># 获取原始文件路径</span></span><br><span class="line">                    file_path = os.path.join(dirpath, filename)</span><br><span class="line">                    <span class="comment"># print(f&#x27;file_path--&gt;&#123;file_path&#125;&#x27;)</span></span><br><span class="line">                    <span class="comment"># 获取标注文件路径</span></span><br><span class="line">                    label_file_path = file_path.replace(<span class="string">&#x27;.txtoriginal&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">                    <span class="comment"># print(f&#x27;label_file_path--&gt;&#123;label_file_path&#125;&#x27;)</span></span><br><span class="line">                    <span class="comment"># 调用封装的方法，处理标注数据，生成 索引和标签的字典</span></span><br><span class="line">                    label_dict = self.read_label_text(label_file_path)</span><br><span class="line">                    <span class="comment"># print(f&#x27;label_dict--&gt;&#123;label_dict&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 读取原始数据，然后进行遍历，给字符打上对应的标签</span></span><br><span class="line">                    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fr:</span><br><span class="line">                        content = fr.read().strip()</span><br><span class="line">                        <span class="comment"># 如果数据最后一位不是结束符号，则添加一个结束符号</span></span><br><span class="line">                        <span class="keyword">if</span> content[-<span class="number">1</span>] <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;。&#x27;</span>, <span class="string">&#x27;?&#x27;</span>, <span class="string">&#x27;!&#x27;</span>, <span class="string">&#x27;！&#x27;</span>, <span class="string">&#x27;？&#x27;</span>]:</span><br><span class="line">                            content += <span class="string">&#x27;。&#x27;</span></span><br><span class="line">                        <span class="comment"># 遍历原始数据，给字符打标签</span></span><br><span class="line">                        <span class="keyword">for</span> i, char <span class="keyword">in</span> <span class="built_in">enumerate</span>(content):</span><br><span class="line">                            label = label_dict.get(i, <span class="string">&#x27;O&#x27;</span>)</span><br><span class="line">                            final_str = char + <span class="string">&#x27;\t&#x27;</span> + label + <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">                            fw.write(final_str)</span><br><span class="line">                    <span class="comment"># print(&#x27;*&#x27;*50)</span></span><br><span class="line">                    <span class="comment"># break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">read_label_text</span>(<span class="params">self, label_file_path</span>):</span><br><span class="line">        label_dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(label_file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fr:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> fr:  <span class="comment"># 遍历每行数据</span></span><br><span class="line">                line = line.strip()</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="comment"># print(f&#x27;line--&gt;&#123;line&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># 获取索引和标签</span></span><br><span class="line">                line_list = line.split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">                <span class="comment"># print(f&#x27;line_list--&gt;&#123;line_list&#125;&#x27;)</span></span><br><span class="line">                start = <span class="built_in">int</span>(line_list[<span class="number">1</span>])</span><br><span class="line">                end = <span class="built_in">int</span>(line_list[<span class="number">2</span>])</span><br><span class="line">                label = self.lables_dict.get(line_list[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 进行for循环，生成索引和标签的字典</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(start, end+<span class="number">1</span>):</span><br><span class="line">                    <span class="keyword">if</span> i ` start:</span><br><span class="line">                        label_dict[i] = <span class="string">&#x27;B-&#x27;</span> + label</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        label_dict[i] = <span class="string">&#x27;I-&#x27;</span> + label</span><br><span class="line">                <span class="comment"># print(f&#x27;label_dict--&gt;&#123;label_dict&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># break</span></span><br><span class="line">        <span class="keyword">return</span> label_dict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    td = TransferData()</span><br><span class="line">    td.transfer()</span><br></pre></td></tr></table></figure>



<h5 id="第三步-编写Config类项目文件配置代码"><a href="#第三步-编写Config类项目文件配置代码" class="headerlink" title="第三步: 编写Config类项目文件配置代码"></a>第三步: 编写Config类项目文件配置代码</h5><p>（1）目的: 配置项目常用变量，一般这些变量属于不经常改变的，比如: 训练文件路径、模型训练次数、模型超参数等等</p>
<p>（2）代码</p>
<p>注意：可以修改成相对路径</p>
<p>代码位置：P03_NER&#x2F;LSTM_CRF&#x2F;config.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">base_dir = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line"><span class="comment"># print(f&#x27;base_dir--&gt;&#123;base_dir&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Config</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 如果是windows或者linux电脑（使用GPU）</span></span><br><span class="line">        self.device = <span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu:0&quot;</span></span><br><span class="line">        <span class="comment"># M1芯片及其以上的电脑（使用GPU）</span></span><br><span class="line">        <span class="comment"># self.device = &#x27;mps&#x27;</span></span><br><span class="line">        self.train_path = os.path.join(base_dir, <span class="string">&#x27;data/train.txt&#x27;</span>)</span><br><span class="line">        self.vocab_path = os.path.join(base_dir, <span class="string">&#x27;vocab/vocab.txt&#x27;</span>)</span><br><span class="line">        self.embedding_dim = <span class="number">300</span></span><br><span class="line">        self.epochs = <span class="number">5</span></span><br><span class="line">        self.batch_size = <span class="number">8</span></span><br><span class="line">        self.hidden_dim = <span class="number">256</span></span><br><span class="line">        self.lr = <span class="number">2e-3</span> <span class="comment"># crf的时候，lr可以小点，比如1e-3</span></span><br><span class="line">        self.dropout = <span class="number">0.2</span></span><br><span class="line">        self.model = <span class="string">&quot;BiLSTM_CRF&quot;</span> <span class="comment"># 可以只用&quot;BiLSTM&quot;</span></span><br><span class="line">        self.tag2id = json.load(<span class="built_in">open</span>(os.path.join(base_dir, <span class="string">&#x27;data/tag2id.json&#x27;</span>), <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = Config()</span><br><span class="line">    <span class="built_in">print</span>(conf.train_path)</span><br><span class="line">    <span class="built_in">print</span>(conf.tag2id)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h5 id="第四步-构建Dataset类与dataloader函数"><a href="#第四步-构建Dataset类与dataloader函数" class="headerlink" title="第四步: 构建Dataset类与dataloader函数"></a>第四步: 构建Dataset类与dataloader函数</h5><h6 id="（1）整体思路"><a href="#（1）整体思路" class="headerlink" title="（1）整体思路"></a>（1）整体思路</h6><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912093537529.png" alt="image-20250912093537529"></p>
<h6 id="（2）构造-x-y-样本对，以及获取vocabs"><a href="#（2）构造-x-y-样本对，以及获取vocabs" class="headerlink" title="（2）构造(x,y)样本对，以及获取vocabs"></a>（2）构造(x,y)样本对，以及获取vocabs</h6><p>代码：</p>
<p>代码位置：P03_NER&#x2F;LSTM_CRF&#x2F;utils&#x2F;common.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.config <span class="keyword">import</span> Config</span><br><span class="line"></span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造数据集：对train.txt进行处理，按照句子结束标点进行切分，得到x,y样本对，放到列表中</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_data</span>():</span><br><span class="line">    datas = []  <span class="comment"># 存储最终数据的三维列表</span></span><br><span class="line">    sample_x = []  <span class="comment"># 存储一个句子的文字</span></span><br><span class="line">    sample_y = []  <span class="comment"># 存储一个句子的标签</span></span><br><span class="line">    vocab_list = [<span class="string">&#x27;PAD&#x27;</span>, <span class="string">&#x27;UNK&#x27;</span>]  <span class="comment"># 存储所有的文字，默认添加PAD和UNK两个特殊字符</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历train.txt</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(conf.train_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>):</span><br><span class="line">        <span class="comment"># print(f&#x27;line--&gt;&#123;line&#125;&#x27;)</span></span><br><span class="line">        word_tag_list = line.strip(<span class="string">&#x27;\n&#x27;</span>).split(<span class="string">&#x27;\t&#x27;</span>)  <span class="comment"># 为了防止将数据中的空格处理掉，这里在使用strip的时候，需要指定删除的字符串</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(word_tag_list) != <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># print(f&#x27;word_tag_list--&gt;&#123;word_tag_list&#125;&#x27;)</span></span><br><span class="line">        word = word_tag_list[<span class="number">0</span>]</span><br><span class="line">        tag = word_tag_list[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 如果是空行，则跳过</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> word:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># 将文字和标签添加到列表中</span></span><br><span class="line">        sample_x.append(word)</span><br><span class="line">        sample_y.append(tag)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果遇到到一个句尾标点，则将sample_x和sample_y添加到datas中，并清空列表</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> [<span class="string">&#x27;。&#x27;</span>, <span class="string">&#x27;?&#x27;</span>, <span class="string">&#x27;!&#x27;</span>, <span class="string">&#x27;！&#x27;</span>, <span class="string">&#x27;？&#x27;</span>]:</span><br><span class="line">            datas.append([sample_x, sample_y])</span><br><span class="line">            <span class="comment"># print(f&#x27;datas--&gt;&#123;datas&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># 清空列表</span></span><br><span class="line">            sample_x = []</span><br><span class="line">            sample_y = []</span><br><span class="line">            <span class="comment"># break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果文字不在vocab_list中，则添加</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> vocab_list:</span><br><span class="line">            vocab_list.append(word)</span><br><span class="line">        <span class="comment"># print(f&#x27;vocab_list--&gt;&#123;vocab_list&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为了方便后续使用，需要将vocab_list保存到文件中</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(conf.vocab_path, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        fw.write(<span class="string">&#x27;\n&#x27;</span>.join(vocab_list))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将列表转成word2id的字典</span></span><br><span class="line">    word2id = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab_list)&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> datas, word2id</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    datas, word2id = build_data()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;datas--&gt;<span class="subst">&#123;datas[:<span class="number">5</span>]&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;word2id--&gt;<span class="subst">&#123;word2id&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;len(datas)--&gt;<span class="subst">&#123;<span class="built_in">len</span>(datas)&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;len(word2id)--&gt;<span class="subst">&#123;<span class="built_in">len</span>(word2id)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h6 id="（3）构造数据迭代器"><a href="#（3）构造数据迭代器" class="headerlink" title="（3）构造数据迭代器"></a>（3）构造数据迭代器</h6><p>步骤：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、构建Dataset类</span><br><span class="line">2、构建自定义函数collate<span class="built_in">_</span>fn()</span><br><span class="line">3、构建get<span class="built_in">_</span>data函数，获得数据迭代器</span><br></pre></td></tr></table></figure>

<p>代码：</p>
<p>调用过程：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912110823059.png" alt="image-20250912110823059"></p>
<p>代码位置：P03_NER&#x2F;LSTM_CRF&#x2F;utils&#x2F;data_loader.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence  <span class="comment"># 进行句子长度补齐或截断</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.utils.common <span class="keyword">import</span> build_data</span><br><span class="line"></span><br><span class="line">datas, word2id = build_data()</span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、构建Dataset类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NerDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, datas</span>):</span><br><span class="line">        <span class="built_in">super</span>(NerDataset, self).__init__()</span><br><span class="line">        self.datas = datas</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):  <span class="comment"># 获取数据集大小</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.datas)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):  <span class="comment"># 根据索引获取数据</span></span><br><span class="line">        <span class="comment"># print(f&#x27;index--&gt;&#123;index&#125;&#x27;)</span></span><br><span class="line">        sample = self.datas[index]</span><br><span class="line">        x = sample[<span class="number">0</span>]</span><br><span class="line">        y = sample[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_dataset</span>():</span><br><span class="line">    ner_dataset = NerDataset(datas)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;len(ner_dataset)--&gt;<span class="subst">&#123;ner_dataset.__len__()&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;len(ner_dataset)--&gt;<span class="subst">&#123;<span class="built_in">len</span>(ner_dataset)&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;ner_dataset[0]--&gt;<span class="subst">&#123;ner_dataset.__getitem__(<span class="number">0</span>)&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;ner_dataset[0]--&gt;<span class="subst">&#123;ner_dataset[<span class="number">0</span>]&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、构建自定义函数collate_fn()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch_data</span>):</span><br><span class="line">    <span class="comment"># print(f&#x27;batch_data--&gt;&#123;batch_data&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># 1)将文字和标签转成id</span></span><br><span class="line">    <span class="comment"># x_train = []</span></span><br><span class="line">    <span class="comment"># for data in batch_data:</span></span><br><span class="line">    <span class="comment">#     # 将文字转成id</span></span><br><span class="line">    <span class="comment">#     id_list = [word2id.get(word, 1) for word in data[0]]  # 如果这个文字不在字典中，则默认为1【UNK】</span></span><br><span class="line">    <span class="comment">#     # 转成tensor</span></span><br><span class="line">    <span class="comment">#     x_train.append(torch.tensor(id_list))</span></span><br><span class="line">    <span class="comment"># print(f&#x27;x_train--&gt;&#123;x_train&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># 简写</span></span><br><span class="line">    x_train = [torch.tensor([word2id.get(word, <span class="number">1</span>) <span class="keyword">for</span> word <span class="keyword">in</span> data[<span class="number">0</span>]]) <span class="keyword">for</span> data <span class="keyword">in</span> batch_data]</span><br><span class="line">    <span class="comment"># print(f&#x27;x_train--&gt;&#123;x_train&#125;&#x27;)</span></span><br><span class="line">    y_train = [torch.tensor([conf.tag2id.get(tag, <span class="number">0</span>) <span class="keyword">for</span> tag <span class="keyword">in</span> data[<span class="number">1</span>]]) <span class="keyword">for</span> data <span class="keyword">in</span> batch_data]</span><br><span class="line">    <span class="comment"># print(f&#x27;y_train--&gt;&#123;y_train&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2)统一样本长度</span></span><br><span class="line">    <span class="comment"># pad_sequence:可以对批次的样本进行统一长度处理， 统一长度的方式是以该批次中最长的样本为准，进行填充</span></span><br><span class="line">    <span class="comment"># batch_first=True，则返回的tensor的维度为[batch_size, max_len]</span></span><br><span class="line">    <span class="comment"># padding_value 当样本不足时，使用xx进行填充</span></span><br><span class="line">    input_ids = pad_sequence(x_train, batch_first=<span class="literal">True</span>, padding_value=<span class="number">0</span>)  <span class="comment"># 用PAD对应的0进行补齐</span></span><br><span class="line">    labels = pad_sequence(y_train, batch_first=<span class="literal">True</span>, padding_value=<span class="number">11</span>)  <span class="comment"># 用PAD对应的11进行补齐</span></span><br><span class="line">    <span class="comment"># print(f&#x27;input_ids--&gt;&#123;input_ids&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;labels--&gt;&#123;labels&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3）创建 attention_mask</span></span><br><span class="line">    attention_mask = (input_ids != <span class="number">0</span>).long()</span><br><span class="line">    <span class="comment"># print(f&#x27;attention_mask--&gt;&#123;attention_mask&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> input_ids, labels, attention_mask</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、构建get_data函数，获得数据迭代器</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>():</span><br><span class="line">    <span class="comment"># 构建训练数据集：基于原始数据集，进行8:2拆分</span></span><br><span class="line">    train_dataset = NerDataset(datas[:<span class="number">6300</span>])</span><br><span class="line">    <span class="comment"># 在写代码的时候，需要把shuffle设置为 Fasle; 在训练时，需要把shuffle设置为 True</span></span><br><span class="line">    train_dataloader = DataLoader(train_dataset,</span><br><span class="line">                                  batch_size=conf.batch_size,</span><br><span class="line">                                  shuffle=<span class="literal">False</span>,</span><br><span class="line">                                  collate_fn=collate_fn,</span><br><span class="line">                                  drop_last=<span class="literal">True</span></span><br><span class="line">                                  )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建验证数据集：基于原始数据集，进行8:2拆分</span></span><br><span class="line">    valid_dataset = NerDataset(datas[<span class="number">6300</span>:])</span><br><span class="line">    valid_dataloader = DataLoader(valid_dataset,</span><br><span class="line">                                  batch_size=conf.batch_size,</span><br><span class="line">                                  shuffle=<span class="literal">False</span>,</span><br><span class="line">                                  collate_fn=collate_fn,</span><br><span class="line">                                  drop_last=<span class="literal">True</span></span><br><span class="line">                                  )</span><br><span class="line">    <span class="keyword">return</span> train_dataloader, valid_dataloader</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># test_dataset()</span></span><br><span class="line">    train_dataloader, valid_dataloader = get_data()</span><br><span class="line">    <span class="comment"># for x in train_dataloader:</span></span><br><span class="line">    <span class="comment">#     print(f&#x27;x--&gt;&#123;x&#125;&#x27;)</span></span><br><span class="line">    <span class="comment">#     break</span></span><br><span class="line">    <span class="keyword">for</span> input_ids, labels, attention_mask <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;input_ids--&gt;<span class="subst">&#123;input_ids.shape&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;labels--&gt;<span class="subst">&#123;labels.shape&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;attention_mask--&gt;<span class="subst">&#123;attention_mask.shape&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<h2 id="BiLSTM-CRF模型搭建"><a href="#BiLSTM-CRF模型搭建" class="headerlink" title="BiLSTM+CRF模型搭建"></a>BiLSTM+CRF模型搭建</h2><h3 id="第一步-编写模型类的代码"><a href="#第一步-编写模型类的代码" class="headerlink" title="第一步: 编写模型类的代码"></a>第一步: 编写模型类的代码</h3><ul>
<li>构建BiLSTM模型</li>
</ul>
<p>（1）思路</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912150529230.png" alt="image-20250912150529230"></p>
<p>（2）代码</p>
<p>代码位置：P03_NER&#x2F;LSTM_CRF&#x2F;model&#x2F;BiLSTM.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.utils.data_loader <span class="keyword">import</span> word2id, get_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NERLSTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_dim, hidden_dim, dropout, tag2id, word2id</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        模型初始化</span></span><br><span class="line"><span class="string">        :param embedding_dim: 嵌入层维度 300</span></span><br><span class="line"><span class="string">        :param hidden_dim: 这里指的是BiLSTM模型输出时的维度，因为是双向LSTM，所以，隐藏层维度为 hidden_dim//2</span></span><br><span class="line"><span class="string">        :param dropout: 随机失活比例</span></span><br><span class="line"><span class="string">        :param tag2id: tag2id字典</span></span><br><span class="line"><span class="string">        :param word2id: word2id字典</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>(NERLSTM, self).__init__()</span><br><span class="line">        self.name = <span class="string">&#x27;BiLSTM&#x27;</span></span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.tag_size = <span class="built_in">len</span>(tag2id)</span><br><span class="line">        self.vocab_size = <span class="built_in">len</span>(word2id)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建词嵌入层</span></span><br><span class="line">        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)</span><br><span class="line">        <span class="comment"># 创建BiLSTM层</span></span><br><span class="line">        self.bilstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_dim // <span class="number">2</span>, bidirectional=<span class="literal">True</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 定义随机失活</span></span><br><span class="line">        self.dropout = nn.Dropout(self.dropout)</span><br><span class="line">        <span class="comment"># 定义线性层</span></span><br><span class="line">        self.linear = nn.Linear(self.hidden_dim, self.tag_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask</span>):</span><br><span class="line">        <span class="comment"># print(f&#x27;input_ids--&gt;&#123;input_ids&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># print(f&#x27;attention_mask--&gt;&#123;attention_mask&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># 送入词嵌入层</span></span><br><span class="line">        embedding = self.embedding(input_ids)</span><br><span class="line">        <span class="comment"># 送入BiLSTM层，这里直接输入embedding即可，模型会自动初始化(h0,c0)</span></span><br><span class="line">        bilstm_out, (h_n, c_n) = self.bilstm(embedding)</span><br><span class="line">        <span class="comment"># 送入随机失活层</span></span><br><span class="line">        dropout_out = self.dropout(bilstm_out)</span><br><span class="line">        <span class="comment"># 对位相乘，只保留有效位置的输出结果，将pad的部分置成0</span></span><br><span class="line">        attention_mask = attention_mask.unsqueeze(-<span class="number">1</span>)  <span class="comment"># 先对attention_mask进行升维</span></span><br><span class="line">        dropout_out = dropout_out * attention_mask</span><br><span class="line">        <span class="comment"># 送入线性层</span></span><br><span class="line">        linear_out = self.linear(dropout_out)</span><br><span class="line">        <span class="keyword">return</span> linear_out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = Config()</span><br><span class="line">    ner_lstm = NERLSTM(conf.embedding_dim, conf.hidden_dim, conf.dropout, conf.tag2id, word2id)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;ner_lstm--&gt;<span class="subst">&#123;ner_lstm&#125;</span>&#x27;</span>)</span><br><span class="line">    train_dataloader, valid_dataloader = get_data()</span><br><span class="line">    <span class="keyword">for</span> input_ids, labels, attention_mask <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        result = ner_lstm(input_ids, attention_mask)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;result--&gt;<span class="subst">&#123;result.shape&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>



<ul>
<li>构建BiLSTM_CRF模型</li>
</ul>
<p>（1）思路</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250912161936723.png" alt="image-20250912161936723"></p>
<p>（2）代码</p>
<p>需要提前装一下包</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install TorchCRF</span><br></pre></td></tr></table></figure>



<blockquote>
<p>注意：由于CRF是自定义的损失函数，所以这里不再需要使用交叉熵损失等，直接使用crf封装好的方法即可，计算损失的函数定义为log_likelihood()。而在forward方法不再用于计算概率值，而是通过viterbi解码得到概率最大的标签路径。</p>
</blockquote>
<p>代码位置：P03_NER&#x2F;LSTM_CRF&#x2F;model&#x2F;BiLSTM_CRF.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> TorchCRF <span class="keyword">import</span> CRF</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.utils.data_loader <span class="keyword">import</span> word2id, get_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NERLSTM_CRF</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_dim, hidden_dim, dropout, tag2id, word2id</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        模型初始化</span></span><br><span class="line"><span class="string">        :param embedding_dim: 嵌入层维度 300</span></span><br><span class="line"><span class="string">        :param hidden_dim: 这里指的是BiLSTM模型输出时的维度，因为是双向LSTM，所以，隐藏层维度为 hidden_dim//2</span></span><br><span class="line"><span class="string">        :param dropout: 随机失活比例</span></span><br><span class="line"><span class="string">        :param tag2id: tag2id字典</span></span><br><span class="line"><span class="string">        :param word2id: word2id字典</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>(NERLSTM_CRF, self).__init__()</span><br><span class="line">        self.name = <span class="string">&#x27;BiLSTM_CRF&#x27;</span></span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.tag_size = <span class="built_in">len</span>(tag2id)</span><br><span class="line">        self.vocab_size = <span class="built_in">len</span>(word2id)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建词嵌入层</span></span><br><span class="line">        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)</span><br><span class="line">        <span class="comment"># 创建BiLSTM层</span></span><br><span class="line">        self.bilstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_dim // <span class="number">2</span>, bidirectional=<span class="literal">True</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 定义随机失活</span></span><br><span class="line">        self.dropout = nn.Dropout(self.dropout)</span><br><span class="line">        <span class="comment"># 定义线性层</span></span><br><span class="line">        self.linear = nn.Linear(self.hidden_dim, self.tag_size)</span><br><span class="line">        <span class="comment"># 创建CRF层</span></span><br><span class="line">        self.crf = CRF(self.tag_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取发射分数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_emission_score</span>(<span class="params">self, input_ids, attention_mask</span>):</span><br><span class="line">        <span class="comment"># print(f&#x27;input_ids--&gt;&#123;input_ids&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># print(f&#x27;attention_mask--&gt;&#123;attention_mask&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># 送入词嵌入层</span></span><br><span class="line">        embedding = self.embedding(input_ids)</span><br><span class="line">        <span class="comment"># 送入BiLSTM层，这里直接输入embedding即可，模型会自动初始化(h0,c0)</span></span><br><span class="line">        bilstm_out, (h_n, c_n) = self.bilstm(embedding)</span><br><span class="line">        <span class="comment"># 送入随机失活层</span></span><br><span class="line">        dropout_out = self.dropout(bilstm_out)</span><br><span class="line">        <span class="comment"># 对位相乘，只保留有效位置的输出结果，将pad的部分置成0</span></span><br><span class="line">        attention_mask = attention_mask.unsqueeze(-<span class="number">1</span>)  <span class="comment"># 先对attention_mask进行升维</span></span><br><span class="line">        dropout_out = dropout_out * attention_mask</span><br><span class="line">        <span class="comment"># 送入线性层</span></span><br><span class="line">        linear_out = self.linear(dropout_out)</span><br><span class="line">        <span class="keyword">return</span> linear_out</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算损失的函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">log_likelihood</span>(<span class="params">self, input_ids, labels, attention_mask</span>):</span><br><span class="line">        <span class="comment"># 获取发射分数</span></span><br><span class="line">        emission_score = self.get_emission_score(input_ids, attention_mask)</span><br><span class="line">        <span class="comment"># 计算损失【直接调用模型封装好的方法，将发射分数、labels、attention_mask输入进去，即可得到对数似然损失】</span></span><br><span class="line">        loss = -self.crf(emission_score, labels, attention_mask.<span class="built_in">bool</span>())</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测的函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask</span>):</span><br><span class="line">        <span class="comment"># 获取发射分数</span></span><br><span class="line">        emission_score = self.get_emission_score(input_ids, attention_mask)</span><br><span class="line">        <span class="comment"># 获取路径</span></span><br><span class="line">        predict_result = self.crf.viterbi_decode(emission_score, attention_mask.<span class="built_in">bool</span>())</span><br><span class="line">        <span class="keyword">return</span> predict_result</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = Config()</span><br><span class="line">    ner_lstm_crf = NERLSTM_CRF(conf.embedding_dim, conf.hidden_dim, conf.dropout, conf.tag2id, word2id)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;ner_lstm_crf--&gt;<span class="subst">&#123;ner_lstm_crf&#125;</span>&#x27;</span>)</span><br><span class="line">    train_dataloader, valid_dataloader = get_data()</span><br><span class="line">    <span class="keyword">for</span> input_ids, labels, attention_mask <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        loss = ner_lstm_crf.log_likelihood(input_ids, labels, attention_mask)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;loss--&gt;<span class="subst">&#123;loss&#125;</span>&#x27;</span>)</span><br><span class="line">        predict_result = ner_lstm_crf(input_ids, attention_mask)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;predict_result--&gt;<span class="subst">&#123;predict_result&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>



<h3 id="第二步-编写训练函数"><a href="#第二步-编写训练函数" class="headerlink" title="第二步: 编写训练函数"></a>第二步: 编写训练函数</h3><p>（1）基本步骤</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">训练函数基本步骤——</span></span><br><span class="line"><span class="attr">1.构建数据迭代器Dataloader(包括数据处理与构建数据源Dataset)</span></span><br><span class="line"><span class="attr">2.实例化模型</span></span><br><span class="line"><span class="attr">3.实例化损失函数对象</span></span><br><span class="line"><span class="attr">4.实例化优化器对象</span></span><br><span class="line"><span class="attr">5.定义打印日志参数</span></span><br><span class="line"><span class="attr">6.开始训练</span></span><br><span class="line"><span class="attr">6.1</span> <span class="string">实现外层大循环epoch</span></span><br><span class="line">    <span class="attr">6.2</span> <span class="string">将模型设置为训练模式</span></span><br><span class="line">    <span class="attr">6.3</span> <span class="string">内部遍历数据迭代器dataloader</span></span><br><span class="line">        <span class="attr">1）将数据送入模型得到输出结果</span></span><br><span class="line">        <span class="attr">2）计算损失</span></span><br><span class="line">        <span class="attr">3）梯度清零</span>: <span class="string">optimizer.zero_grad()</span></span><br><span class="line">        <span class="attr">4）反向传播(计算梯度)</span>: <span class="string">loss.backward()</span></span><br><span class="line">        <span class="attr">5）梯度更新(参数更新)</span>: <span class="string">optimizer.step()</span></span><br><span class="line">        <span class="attr">6）打印内部训练日志</span></span><br><span class="line">    <span class="attr">6.4</span> <span class="string">使用验证集进行模型评估【将模型设置为评估模式】</span></span><br><span class="line">    <span class="attr">6.5</span> <span class="string">保存模型: torch.save(model.state_dict(), &quot;model_path&quot;)</span></span><br><span class="line"><span class="attr">6.6</span> <span class="string">打印外部训练日志</span></span><br><span class="line"></span><br><span class="line"><span class="attr">验证函数基本步骤——</span></span><br><span class="line"><span class="attr">1.定义打印日志参数</span></span><br><span class="line"><span class="attr">2.将模型设置为评估模式</span></span><br><span class="line"><span class="attr">3.内部遍历数据迭代器dataloader</span></span><br><span class="line">  <span class="attr">3.1</span> <span class="string">将数据送入模型得到输出结果</span></span><br><span class="line">  <span class="attr">3.2</span> <span class="string">计算损失</span></span><br><span class="line">  <span class="attr">3.3</span> <span class="string">处理结果</span></span><br><span class="line">  <span class="attr">3.4</span> <span class="string">统计批次内指标</span></span><br><span class="line"><span class="attr">4.统计整体指标</span></span><br></pre></td></tr></table></figure>

<p>（2）代码</p>
<blockquote>
<p>注意：使用BiLSTM_CRF模型时，使用自定义的损失函数，封装在了log_likelihood()方法中。而forward()方法可以直接获取预测的标签类型。</p>
</blockquote>
<p>代码位置：P03_NER&#x2F;LSTM_CRF&#x2F;train.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score, classification_report, f1_score</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.model.BiLSTM <span class="keyword">import</span> NERLSTM</span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.model.BiLSTM_CRF <span class="keyword">import</span> NERLSTM_CRF</span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.utils.data_loader <span class="keyword">import</span> get_data, word2id</span><br><span class="line"></span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model2dev</span>(<span class="params">valid_dataloader, model, criterion=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    使用验证集，评估模型的效果【同时支持 BiLSTM和 BiLSTM_CRF】</span></span><br><span class="line"><span class="string">    :param valid_dataloader: 验证集的dataloader</span></span><br><span class="line"><span class="string">    :param model: 需要评估的模型实例</span></span><br><span class="line"><span class="string">    :param criterion: 损失函数对象，因为BiLSTM需要使用交叉熵损失，所以需要用到损失函数对象，而BiLSTM_CRF是不需要的，所以，需要设置默认值为None</span></span><br><span class="line"><span class="string">    :return: 评估指标</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 1.定义打印日志参数</span></span><br><span class="line">    avg_loss = <span class="number">0</span>  <span class="comment"># 保存平均损失</span></span><br><span class="line">    preds = []  <span class="comment"># 保存非padding位置的预测标签</span></span><br><span class="line">    golds = []  <span class="comment"># 保存非padding位置的真实标签</span></span><br><span class="line">    <span class="comment"># 2.将模型设置为评估模式</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="comment"># 3.内部遍历数据迭代器dataloader</span></span><br><span class="line">    <span class="keyword">for</span> index, (input_ids, labels, attention_mask) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(valid_dataloader)):</span><br><span class="line">        <span class="comment">#   3.1 将数据送入模型得到输出结果</span></span><br><span class="line">        <span class="comment"># 把数据放到gpu上</span></span><br><span class="line">        input_ids = input_ids.to(conf.device)</span><br><span class="line">        labels = labels.to(conf.device)</span><br><span class="line">        attention_mask = attention_mask.to(conf.device)</span><br><span class="line">        <span class="keyword">if</span>  conf.model ` <span class="string">&#x27;BiLSTM&#x27;</span>:</span><br><span class="line">            output = model(input_ids, attention_mask)</span><br><span class="line">            <span class="comment"># print(f&#x27;output--&gt;&#123;output.shape&#125;&#x27;)</span></span><br><span class="line">            <span class="comment">#   3.2 计算损失</span></span><br><span class="line">            <span class="comment"># 计算损失之前，需要将output形状转换为(batch_size * seq_len, tag_size)</span></span><br><span class="line">            output2 = output.view(-<span class="number">1</span>, <span class="built_in">len</span>(conf.tag2id))</span><br><span class="line">            <span class="comment"># print(f&#x27;output2--&gt;&#123;output2.shape&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># 同时需要将标签转换为(batch_size * seq_len)</span></span><br><span class="line">            labels2 = labels.view(-<span class="number">1</span>)</span><br><span class="line">            loss = criterion(output2, labels2)</span><br><span class="line">            <span class="comment"># print(f&#x27;loss--&gt;&#123;loss&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># 对损失进行累积操作</span></span><br><span class="line">            avg_loss += loss</span><br><span class="line">            <span class="comment">#   3.3 处理结果</span></span><br><span class="line">            predict = output.argmax(dim=-<span class="number">1</span>).tolist()</span><br><span class="line">            <span class="comment"># print(f&#x27;predict--&gt;&#123;predict&#125;&#x27;)</span></span><br><span class="line">        <span class="keyword">elif</span> conf.model ` <span class="string">&#x27;BiLSTM_CRF&#x27;</span>:</span><br><span class="line">            loss = model.log_likelihood(input_ids, labels, attention_mask).mean()</span><br><span class="line">            <span class="comment"># print(f&#x27;loss--&gt;&#123;loss&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># 对损失进行累积操作</span></span><br><span class="line">            avg_loss += loss</span><br><span class="line">            <span class="comment"># 3.3 处理结果</span></span><br><span class="line">            <span class="comment"># 这里的预测结果是通过维特比算法解码之后的结果，所以本身就是标签id</span></span><br><span class="line">            predict = model(input_ids, attention_mask)</span><br><span class="line">            <span class="comment"># print(f&#x27;predict--&gt;&#123;predict&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#   3.4 统计批次内指标</span></span><br><span class="line">        <span class="comment"># 将非padding位置的预测标签和真实标签保存起来，使用的方法是：通过input_ids进行非零判断，然后得到一个boolean的张量，然后直接对这个张量进行求和，就可以获取到每个句子的真实的长度，让让再通过这个长度，使用列表切片的方式，从标签中取出真实位置对应的标签</span></span><br><span class="line">        <span class="comment"># print(f&#x27;每个样本的真实长度--&gt;&#123;(input_ids&gt;0).sum(-1).tolist()&#125;&#x27;)  # [11, 13, 10, 14, 55, 22, 39, 25]</span></span><br><span class="line">        real_len = (input_ids&gt;<span class="number">0</span>).<span class="built_in">sum</span>(-<span class="number">1</span>).tolist()</span><br><span class="line">        <span class="comment"># 根据真实的句子长度，获取预测的标签</span></span><br><span class="line">        <span class="keyword">for</span> index, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(predict):</span><br><span class="line">            preds.extend(label[:real_len[index]])</span><br><span class="line">            <span class="comment"># break</span></span><br><span class="line">        <span class="comment"># 根据真实的句子长度，获取真实的标签</span></span><br><span class="line">        <span class="keyword">for</span> index, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(labels.tolist()):</span><br><span class="line">            golds.extend(label[:real_len[index]])</span><br><span class="line">            <span class="comment"># break</span></span><br><span class="line">        <span class="comment"># print(f&#x27;preds--&gt;&#123;preds&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># print(f&#x27;golds--&gt;&#123;golds&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># break</span></span><br><span class="line">    <span class="comment"># 4.统计整体指标</span></span><br><span class="line">    avg_loss = avg_loss / <span class="built_in">len</span>(valid_dataloader)</span><br><span class="line">    precision = precision_score(golds, preds, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">    recall = recall_score(golds, preds, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">    f1 = f1_score(golds, preds, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">    report = classification_report(golds, preds)</span><br><span class="line">    <span class="comment"># print(f&#x27;avg_loss--&gt;&#123;avg_loss&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;precision--&gt;&#123;precision&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;recall--&gt;&#123;recall&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;f1--&gt;&#123;f1&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;report--&gt;&#123;report&#125;&#x27;)</span></span><br><span class="line">    <span class="keyword">return</span> avg_loss, precision, recall, f1, report</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model2train</span>():</span><br><span class="line">    <span class="comment"># 1.构建数据迭代器Dataloader(包括数据处理与构建数据源Dataset)</span></span><br><span class="line">    train_dataloader, valid_dataloader = get_data()</span><br><span class="line">    <span class="comment"># 2.实例化模型</span></span><br><span class="line">    models = &#123;<span class="string">&#x27;BiLSTM&#x27;</span>: NERLSTM,</span><br><span class="line">              <span class="string">&#x27;BiLSTM_CRF&#x27;</span>: NERLSTM_CRF&#125;</span><br><span class="line">    model = models[conf.model](conf.embedding_dim, conf.hidden_dim, conf.dropout, conf.tag2id, word2id).to(conf.device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;model--&gt;<span class="subst">&#123;model&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 3.实例化损失函数对象</span></span><br><span class="line">    <span class="comment"># 忽略索引为11的标签，即[PAD]，效果就是这个表情不会参与损失的计算，也就是不产生梯度，不计算损失</span></span><br><span class="line">    <span class="comment"># 【原因padding部分并不是真实的标签，不应该影响训练】</span></span><br><span class="line">    criterion = nn.CrossEntropyLoss(ignore_index=<span class="number">11</span>)</span><br><span class="line">    <span class="comment"># 4.实例化优化器对象</span></span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=conf.lr)</span><br><span class="line">    <span class="comment"># 5.定义打印日志参数</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6.开始训练</span></span><br><span class="line">    best_f1 = -<span class="number">1.0</span></span><br><span class="line">    <span class="keyword">if</span> conf.model ` <span class="string">&#x27;BiLSTM&#x27;</span>:</span><br><span class="line">        <span class="comment"># 6.1 实现外层大循环epoch</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(conf.epochs):</span><br><span class="line">            <span class="comment"># 6.2 将模型设置为训练模式</span></span><br><span class="line">            model.train()</span><br><span class="line">            <span class="comment"># 6.3 内部遍历数据迭代器dataloader</span></span><br><span class="line">            <span class="keyword">for</span> index, (input_ids, labels, attention_mask) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(train_dataloader)):</span><br><span class="line">                <span class="comment"># print(f&#x27;input_ids--&gt;&#123;input_ids.shape&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># print(f&#x27;labels--&gt;&#123;labels.shape&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># print(f&#x27;attention_mask--&gt;&#123;attention_mask.shape&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># 1）将数据送入模型得到输出结果</span></span><br><span class="line">                <span class="comment"># 把数据放到gpu上</span></span><br><span class="line">                input_ids = input_ids.to(conf.device)</span><br><span class="line">                labels = labels.to(conf.device)</span><br><span class="line">                attention_mask = attention_mask.to(conf.device)</span><br><span class="line">                output = model(input_ids, attention_mask)</span><br><span class="line">                <span class="comment"># print(f&#x27;output--&gt;&#123;output.shape&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># 2）计算损失</span></span><br><span class="line">                <span class="comment"># 计算损失之前，需要将output形状转换为(batch_size * seq_len, tag_size)</span></span><br><span class="line">                output = output.view(-<span class="number">1</span>, <span class="built_in">len</span>(conf.tag2id))</span><br><span class="line">                <span class="comment"># 同时需要将标签转换为(batch_size * seq_len)</span></span><br><span class="line">                labels = labels.view(-<span class="number">1</span>)</span><br><span class="line">                loss = criterion(output, labels)</span><br><span class="line">                <span class="comment"># print(f&#x27;loss--&gt;&#123;loss&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># 3）梯度清零: optimizer.zero_grad()</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">                <span class="comment"># 4）反向传播(计算梯度): loss.backward()</span></span><br><span class="line">                loss.backward()</span><br><span class="line">                <span class="comment"># 5）梯度更新(参数更新): optimizer.step()</span></span><br><span class="line">                optimizer.step()</span><br><span class="line">                <span class="comment"># 6）打印内部训练日志</span></span><br><span class="line">                <span class="keyword">if</span> (index + <span class="number">1</span>) % <span class="number">200</span> ` <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;epoch:%04d------------loss:%f&#x27;</span> % (epoch, loss.item()))</span><br><span class="line">                    <span class="comment"># break</span></span><br><span class="line">            <span class="comment"># 6.4 使用验证集进行模型评估【将模型设置为评估模式】</span></span><br><span class="line">            avg_loss, precision, recall, f1, report = model2dev(valid_dataloader, model, criterion)</span><br><span class="line">            <span class="comment"># 6.5 保存模型: torch.save(model.state_dict(), &quot;model_path&quot;)</span></span><br><span class="line">            <span class="keyword">if</span> f1 &gt; best_f1:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;当前轮次为<span class="subst">&#123;epoch&#125;</span>轮次, 获取到新的最佳f1为<span class="subst">&#123;best_f1&#125;</span>, 保存模型&#x27;</span>)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;report--&gt;<span class="subst">&#123;report&#125;</span>&#x27;</span>)</span><br><span class="line">                torch.save(model.state_dict(), <span class="string">&#x27;save_model/bilstm_best.pth&#x27;</span>)</span><br><span class="line">                <span class="comment"># 更新best_f1</span></span><br><span class="line">                best_f1 = f1</span><br><span class="line">            <span class="comment"># break</span></span><br><span class="line">    <span class="keyword">elif</span> conf.model ` <span class="string">&#x27;BiLSTM_CRF&#x27;</span>:</span><br><span class="line">        <span class="comment"># 6.1 实现外层大循环epoch</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(conf.epochs):</span><br><span class="line">            <span class="comment"># 6.2 将模型设置为训练模式</span></span><br><span class="line">            model.train()</span><br><span class="line">            <span class="comment"># 6.3 内部遍历数据迭代器dataloader</span></span><br><span class="line">            <span class="keyword">for</span> index, (input_ids, labels, attention_mask) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(train_dataloader)):</span><br><span class="line">                <span class="comment"># print(f&#x27;input_ids--&gt;&#123;input_ids.shape&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># print(f&#x27;labels--&gt;&#123;labels.shape&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># print(f&#x27;attention_mask--&gt;&#123;attention_mask.shape&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># 1）将数据送入模型得到输出结果</span></span><br><span class="line">                <span class="comment"># 把数据放到gpu上</span></span><br><span class="line">                input_ids = input_ids.to(conf.device)</span><br><span class="line">                labels = labels.to(conf.device)</span><br><span class="line">                attention_mask = attention_mask.to(conf.device)</span><br><span class="line">                <span class="comment"># 2）计算损失</span></span><br><span class="line">                <span class="comment"># 直接调用log_likelihood方法</span></span><br><span class="line">                loss = model.log_likelihood(input_ids, labels, attention_mask).mean()</span><br><span class="line">                <span class="comment"># print(f&#x27;loss--&gt;&#123;loss&#125;&#x27;)</span></span><br><span class="line">                <span class="comment"># 3）梯度清零: optimizer.zero_grad()</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">                <span class="comment"># 4）反向传播(计算梯度): loss.backward()</span></span><br><span class="line">                loss.backward()</span><br><span class="line">                <span class="comment"># 5）梯度更新(参数更新): optimizer.step()</span></span><br><span class="line">                optimizer.step()</span><br><span class="line">                <span class="comment"># 6）打印内部训练日志</span></span><br><span class="line">                <span class="keyword">if</span> (index + <span class="number">1</span>) % <span class="number">200</span> ` <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;epoch:%04d------------loss:%f&#x27;</span> % (epoch, loss.item()))</span><br><span class="line">                    <span class="comment"># break</span></span><br><span class="line">            <span class="comment"># 6.4 使用验证集进行模型评估【将模型设置为评估模式】</span></span><br><span class="line">            avg_loss, precision, recall, f1, report = model2dev(valid_dataloader, model)</span><br><span class="line">            <span class="comment"># 6.5 保存模型: torch.save(model.state_dict(), &quot;model_path&quot;)</span></span><br><span class="line">            <span class="keyword">if</span> f1 &gt; best_f1:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;当前轮次为<span class="subst">&#123;epoch&#125;</span>轮次, 获取到新的最佳f1为<span class="subst">&#123;best_f1&#125;</span>, 保存模型&#x27;</span>)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;report--&gt;<span class="subst">&#123;report&#125;</span>&#x27;</span>)</span><br><span class="line">                torch.save(model.state_dict(), <span class="string">&#x27;save_model/bilstm_crf_best.pth&#x27;</span>)</span><br><span class="line">                <span class="comment"># 更新best_f1</span></span><br><span class="line">                best_f1 = f1</span><br><span class="line">            <span class="comment"># break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6.6 打印外部训练日志</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;训练结束, 总耗时: %.2f&#x27;</span> % (time.time() - start_time))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model2train()</span><br></pre></td></tr></table></figure>

<p><code>结论：</code></p>
<p>使用CRF之后，效果会比之前稍微好一些，但是训练成本会变高。</p>
<p><code>优化点：</code></p>
<p><code>（1）在正在训练时，将dataloader中的shuffle设置成true</code></p>
<p><code>（2）为了能够让训练集和验证集的样本分布一致，需要先将数据集打乱，然后再去进行划分</code></p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>():</span><br><span class="line">    <span class="comment"># 为了能够让训练集和验证集的样本分布一致，需要先将数据集打乱，然后再去进行划分</span></span><br><span class="line">    random.seed(<span class="number">66</span>)</span><br><span class="line">    random.shuffle(datas)  <span class="comment"># 数据会原地修改</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建训练数据集：基于原始数据集，进行8:2拆分</span></span><br><span class="line">    train_dataset = NerDataset(datas[:<span class="number">6300</span>])</span><br></pre></td></tr></table></figure>

<p>除了这种方式之外，也可以使用<code>分类采样</code>的方式。这种方式可以绝对类型上，训练集和验证集的分布是一致的。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250913114311551.png" alt="image-20250913114311551"></p>
<p><code>（3）训练优化：梯度裁剪</code>，它的作用是防止参数过大带来训练不稳定或者梯度爆炸</p>
<p>它实现的方式，当参数的范数大于了设置的最大范数时，所有参数会乘以缩放比例进行变小，缩放比例&#x3D;max_norm&#x2F;total_norm</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5）梯度更新(参数更新): optimizer.step()</span></span><br><span class="line">            <span class="comment"># 梯度裁剪</span></span><br><span class="line">            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=<span class="number">10</span>)</span><br><span class="line">            optimizer.step()</span><br></pre></td></tr></table></figure>

<p><code>（4）增加标注数据</code></p>
<p><code>（5）和规则进行结合，去做结果后处理</code></p>
<p>（6）日志保存</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250913120133235.png" alt="image-20250913120133235"></p>
<h3 id="第三步-编写模型预测函数"><a href="#第三步-编写模型预测函数" class="headerlink" title="第三步: 编写模型预测函数"></a>第三步: 编写模型预测函数</h3><p>（1）思路</p>
<ul>
<li>基本步骤：</li>
</ul>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">1.实例化模型</span></span><br><span class="line"><span class="attr">2.加载训练好的模型参数</span></span><br><span class="line"><span class="attr">3.处理数据</span></span><br><span class="line"><span class="attr">4.模型预测</span></span><br><span class="line"><span class="attr">5.结果处理</span></span><br></pre></td></tr></table></figure>

<ul>
<li>整体思路</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250913145456728.png" alt="image-20250913145456728"></p>
<p>（2）代码</p>
<p>代码位置：P03_NER&#x2F;LSTM_CRF&#x2F;ner_predict.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.model.BiLSTM <span class="keyword">import</span> NERLSTM</span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.model.BiLSTM_CRF <span class="keyword">import</span> NERLSTM_CRF</span><br><span class="line"><span class="keyword">from</span> P03_NER.LSTM_CRF.utils.data_loader <span class="keyword">import</span> word2id</span><br><span class="line"></span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line">id2tag = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> conf.tag2id.items()&#125;</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;id2tag--&gt;<span class="subst">&#123;id2tag&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.实例化模型</span></span><br><span class="line">models = &#123;<span class="string">&#x27;BiLSTM&#x27;</span>: NERLSTM,</span><br><span class="line">          <span class="string">&#x27;BiLSTM_CRF&#x27;</span>: NERLSTM_CRF&#125;</span><br><span class="line">model = models[conf.model](conf.embedding_dim, conf.hidden_dim, conf.dropout, conf.tag2id, word2id).to(conf.device)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;model--&gt;<span class="subst">&#123;model&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 2.加载训练好的模型参数</span></span><br><span class="line"><span class="keyword">if</span> conf.model ` <span class="string">&#x27;BiLSTM&#x27;</span>:</span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">&#x27;save_model/bilstm_best.pth&#x27;</span>, weights_only=<span class="literal">True</span>))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">&#x27;save_model/bilstm_crf_best.pth&#x27;</span>, weights_only=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model2predict</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="comment"># 3.处理数据</span></span><br><span class="line">    <span class="comment"># 1)字符转id</span></span><br><span class="line">    text_id = [word2id.get(i, <span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> text]  <span class="comment"># 对于取不到的字符，用1代替</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;text_id--&gt;<span class="subst">&#123;text_id&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 2)转成张量</span></span><br><span class="line">    id_tensor = torch.tensor([text_id]).to(conf.device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;id_tensor--&gt;<span class="subst">&#123;id_tensor&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 3)构建 attention_mask</span></span><br><span class="line">    <span class="comment"># attention_mask = torch.tensor([[1] * len(text_id)]).to(conf.device)</span></span><br><span class="line">    attention_mask = (id_tensor != <span class="number">0</span>).long().to(conf.device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;attention_mask--&gt;<span class="subst">&#123;attention_mask&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 4.模型预测</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">if</span> conf.model ` <span class="string">&#x27;BiLSTM&#x27;</span>:</span><br><span class="line">            <span class="comment"># 送入模型</span></span><br><span class="line">            logits = model(id_tensor, attention_mask)</span><br><span class="line">            <span class="comment"># 通过argmax取到最大概率对应的索引</span></span><br><span class="line">            preds = logits.argmax(dim=-<span class="number">1</span>).squeeze(<span class="number">0</span>).tolist()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;preds--&gt;<span class="subst">&#123;preds&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 送入模型</span></span><br><span class="line">            preds = model(id_tensor, attention_mask)[<span class="number">0</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;preds--&gt;<span class="subst">&#123;preds&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 5.结果处理</span></span><br><span class="line">    <span class="comment"># 将id转成标签</span></span><br><span class="line">    predict_labels = [id2tag[i] <span class="keyword">for</span> i <span class="keyword">in</span> preds]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;predict_labels--&gt;<span class="subst">&#123;predict_labels&#125;</span>&#x27;</span>)</span><br><span class="line">    result_dict = extract_entities(text, predict_labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;result_dict--&gt;<span class="subst">&#123;result_dict&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result_dict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_entities</span>(<span class="params">text, tags</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    从带 BIO 标签的文本中提取实体。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        text (str): 原始文本</span></span><br><span class="line"><span class="string">        tags (List[str]): 对应文本的标签列表</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">        dict: 实体名称到类型的映射，如 &#123;&#x27;冠心病&#x27;: &#x27;DISEASE&#x27;, &#x27;糖尿病&#x27;: &#x27;DISEASE&#x27;&#125;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    entities = &#123;&#125;</span><br><span class="line">    current_entity = []</span><br><span class="line">    current_type = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> char, tag <span class="keyword">in</span> <span class="built_in">zip</span>(text, tags):</span><br><span class="line">        <span class="keyword">if</span> tag.startswith(<span class="string">&#x27;B-&#x27;</span>):</span><br><span class="line">            <span class="comment"># 开始一个新的实体</span></span><br><span class="line">            <span class="keyword">if</span> current_type <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 保存之前未完成的实体</span></span><br><span class="line">                entity_name = <span class="string">&#x27;&#x27;</span>.join(current_entity)</span><br><span class="line">                entities[entity_name] = current_type</span><br><span class="line">                current_entity = []</span><br><span class="line">                current_type = <span class="literal">None</span></span><br><span class="line">            current_type = tag[<span class="number">2</span>:]  <span class="comment"># 提取实体类型</span></span><br><span class="line">            current_entity.append(char)</span><br><span class="line">        <span class="keyword">elif</span> tag.startswith(<span class="string">&#x27;I-&#x27;</span>):</span><br><span class="line">            <span class="keyword">if</span> current_type <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> tag[<span class="number">2</span>:] ` current_type:</span><br><span class="line">                <span class="comment"># 继续当前实体</span></span><br><span class="line">                current_entity.append(char)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 结束当前实体</span></span><br><span class="line">            <span class="keyword">if</span> current_type <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                entity_name = <span class="string">&#x27;&#x27;</span>.join(current_entity)</span><br><span class="line">                entities[entity_name] = current_type</span><br><span class="line">                current_entity = []</span><br><span class="line">                current_type = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理最后可能未保存的实体</span></span><br><span class="line">    <span class="keyword">if</span> current_type <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        entity_name = <span class="string">&#x27;&#x27;</span>.join(current_entity)</span><br><span class="line">        entities[entity_name] = current_type</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> entities</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model2predict(<span class="string">&#x27;女性，88岁，农民，双滦区应营子村人，主因右髋部摔伤后疼痛肿胀，活动受限5小时于2016-10-29；11：12入院。&#x27;</span>)</span><br></pre></td></tr></table></figure>





<h1 id="关系抽取任务代码"><a href="#关系抽取任务代码" class="headerlink" title="关系抽取任务代码"></a>关系抽取任务代码</h1><h2 id="【实现】基于规则方式实现关系抽取"><a href="#【实现】基于规则方式实现关系抽取" class="headerlink" title="【实现】基于规则方式实现关系抽取"></a>【实现】基于规则方式实现关系抽取</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>基于规则实现关系抽取的原理 (主要分为三个步骤)</p>
<ul>
<li><p>第一步：定义需要抽取的关系集合，比如【夫妻关系，合作关系，，…】</p>
</li>
<li><p>第二步：遍历文章的每一句话，将每句话中非实体和非关系集合里面的词去掉</p>
</li>
<li><p>第三步：分别从实体集合和关系集合中，提取关系三元组</p>
</li>
</ul>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> pseg</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要进行关系抽取的样本数据</span></span><br><span class="line">samples = [<span class="string">&quot;2014年1月8日，杨幂与刘恺威的婚礼在印度尼西亚巴厘岛举行&quot;</span>,</span><br><span class="line">           <span class="string">&quot;周星驰和吴孟达在《逃学威龙》中合作出演&quot;</span>,</span><br><span class="line">           <span class="string">&#x27;成龙出演了《警察故事》等多部经典电影&#x27;</span>]</span><br><span class="line"><span class="comment"># 定义需要抽取的关系集合</span></span><br><span class="line">relations2dict = &#123;<span class="string">&#x27;夫妻关系&#x27;</span>:[<span class="string">&#x27;结婚&#x27;</span>, <span class="string">&#x27;领证&#x27;</span>, <span class="string">&#x27;婚礼&#x27;</span>],</span><br><span class="line">                  <span class="string">&#x27;合作关系&#x27;</span>: [<span class="string">&#x27;搭档&#x27;</span>, <span class="string">&#x27;合作&#x27;</span>, <span class="string">&#x27;签约&#x27;</span>],</span><br><span class="line">                  <span class="string">&#x27;演员关系&#x27;</span>: [<span class="string">&#x27;出演&#x27;</span>, <span class="string">&#x27;角色&#x27;</span>, <span class="string">&#x27;主演&#x27;</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过jieba词性识别抽取出nr的实体和带有关系的词组</span></span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> samples:</span><br><span class="line">    entities = []  <span class="comment"># 存储实体</span></span><br><span class="line">    relations = []  <span class="comment"># 存储关系</span></span><br><span class="line">    move_index = []  <span class="comment"># 用来存储《》的索引</span></span><br><span class="line">    <span class="keyword">for</span> word, flag <span class="keyword">in</span> pseg.lcut(text):</span><br><span class="line">        <span class="keyword">if</span> flag ` <span class="string">&#x27;nr&#x27;</span>:  <span class="comment"># 如果是人名</span></span><br><span class="line">            entities.append(word)</span><br><span class="line">        <span class="keyword">elif</span> flag ` <span class="string">&#x27;x&#x27;</span>:  <span class="comment"># 如果是非语素词，则认为是《 或 》</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(move_index) ` <span class="number">0</span>:</span><br><span class="line">                move_index.append(text.index(word))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                move_index.append(text.index(word))</span><br><span class="line">                entities.append(text[move_index[<span class="number">0</span>] + <span class="number">1</span>: move_index[<span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> key, value <span class="keyword">in</span> relations2dict.items():</span><br><span class="line">                <span class="keyword">if</span> word <span class="keyword">in</span> value:</span><br><span class="line">                    relations.append(key)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;entities--&gt;<span class="subst">&#123;entities&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;relations--&gt;<span class="subst">&#123;relations&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分别从实体集合和关系集合中，提取关系三元组</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(entities) &gt;= <span class="number">2</span> <span class="keyword">and</span> <span class="built_in">len</span>(relations) &gt;= <span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;原始文本：&quot;</span>, text)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;提取结果：&#x27;</span>, entities[<span class="number">0</span>] + <span class="string">&#x27;-&gt;&#x27;</span> + relations[<span class="number">0</span>] + <span class="string">&#x27;-&gt;&#x27;</span> + entities[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;原始文本：&quot;</span>, text)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;不好意思，暂时没能从文本中提取出关系结果&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;*&#x27;</span>*<span class="number">80</span>)</span><br><span class="line">    <span class="comment"># break</span></span><br></pre></td></tr></table></figure>



<h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><ul>
<li>优点：实现简单、无需训练，小规模数据集容易实现.</li>
<li>缺点：<ul>
<li>无法解决复杂的场景</li>
<li>对跨领域的可移植性较差、人工标注成本较高以及召回率较低.</li>
</ul>
</li>
</ul>
<h2 id="Pipline方法实现关系抽取"><a href="#Pipline方法实现关系抽取" class="headerlink" title="Pipline方法实现关系抽取"></a>Pipline方法实现关系抽取</h2><h3 id="【掌握】Pipeline方法的原理"><a href="#【掌握】Pipeline方法的原理" class="headerlink" title="【掌握】Pipeline方法的原理"></a>【掌握】Pipeline方法的原理</h3><ul>
<li>步骤<ul>
<li>先完成实体抽取</li>
<li>再进行关系分类</li>
</ul>
</li>
<li>方法<ul>
<li>CNN&#x2F;RNN及其变体</li>
<li>CNN多样性卷积核的特性有利于识别目标的结构特征，而RNN能充分考虑长距离词之间的依赖性，其记忆功能有利于识别序列</li>
</ul>
</li>
</ul>
<h3 id="【掌握】BiLSTM-Attention模型架构"><a href="#【掌握】BiLSTM-Attention模型架构" class="headerlink" title="【掌握】BiLSTM+Attention模型架构"></a>【掌握】BiLSTM+Attention模型架构</h3><p>（1）模型架构</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250915105309661.png" alt="image-20250915105309661"></p>
<p>（2）注意力机制</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250915114909468.png" alt="image-20250915114909468"></p>
<h3 id="【实现】代码实现概览"><a href="#【实现】代码实现概览" class="headerlink" title="【实现】代码实现概览"></a>【实现】代码实现概览</h3><p>（1）整体步骤</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">整体实现思路(1-4数据数据预处理，5-8模型部分)</span>: <span class="string"></span></span><br><span class="line"><span class="attr">1、获取数据，例如通过人工数据标注或者第三方数据等。</span></span><br><span class="line"><span class="attr">2、对数据进行处理，构造训练数据</span></span><br><span class="line"><span class="attr">3、构建DataSet类</span></span><br><span class="line"><span class="attr">4、加载数据集</span> <span class="string">DataLoader</span></span><br><span class="line"><span class="attr">5、定义模型（embedding、线性层、CRF层）</span></span><br><span class="line"><span class="attr">6、初始化模型、loss、优化器、前向传播、反向传播、梯度更新</span></span><br><span class="line"><span class="attr">7、模型训练、评估</span></span><br><span class="line"><span class="attr">8、模型加载、测试</span></span><br></pre></td></tr></table></figure>

<p>（2）整体代码架构图</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250617003704466.png" alt="image-20250617003704466"></p>
<h3 id="【实现】数据预处理-1"><a href="#【实现】数据预处理-1" class="headerlink" title="【实现】数据预处理"></a>【实现】数据预处理</h3><h4 id="第一步-查看项目数据集-1"><a href="#第一步-查看项目数据集-1" class="headerlink" title="第一步: 查看项目数据集"></a>第一步: 查看项目数据集</h4><p>存放在data文件夹中</p>
<ul>
<li>关系类型文件  data&#x2F;relation2id.txt</li>
</ul>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">导演 0</span><br><span class="line">歌手 1</span><br><span class="line">作曲 2</span><br><span class="line">作词 3</span><br><span class="line">主演 4</span><br></pre></td></tr></table></figure>

<blockquote>
<p>relation2id.txt中包含5个类别标签, 文件共分为两列，第一列是类别名称，第二列为类别序号，中间空格符号隔开</p>
</blockquote>
<ul>
<li>训练数据集 data&#x2F;train.txt</li>
</ul>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">今晚会在哪里醒来 黄家强 歌手 《今晚会在哪里醒来》是黄家强的一首粤语歌曲，由何启弘作词，黄家强作曲编曲并演唱，收录于2007年08月01日发行的专辑《她他》中</span><br><span class="line"></span><br><span class="line">似水流年 许晓杰 作曲 似水流年，由著名作词家闫肃作词，著名音乐人许晓杰作曲，张烨演唱</span><br></pre></td></tr></table></figure>

<blockquote>
<p>train.txt 中包含18267行样本, 每行分为4列元素，元素中间用空格隔开，第一列元素为实体1、第二列元素为实体2、第三列元素为关系类型、第四列元素是原始文本</p>
</blockquote>
<ul>
<li>测试数据集 data&#x2F;test.txt</li>
</ul>
<blockquote>
<p>test.txt中包含5873行样本，数据样式通训练数据集</p>
</blockquote>
<h4 id="第二步-编写Config类项目文件配置代码"><a href="#第二步-编写Config类项目文件配置代码" class="headerlink" title="第二步: 编写Config类项目文件配置代码"></a>第二步: 编写Config类项目文件配置代码</h4><p>（1）目的: 配置项目常用变量，一般这些变量属于不经常改变的，比如: 训练文件路径、模型训练次数、模型超参数等等</p>
<p>（2）代码</p>
<p>代码位置：P04_RE&#x2F;Bilstm_Attention_RE&#x2F;config.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">base_dir = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;base_dir--&gt;<span class="subst">&#123;base_dir&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Config</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">        <span class="comment"># self.device = &quot;mps&quot;</span></span><br><span class="line">        self.train_data_path = os.path.join(base_dir, <span class="string">&#x27;data/train.txt&#x27;</span>)</span><br><span class="line">        self.test_data_path = os.path.join(base_dir, <span class="string">&#x27;data/test.txt&#x27;</span>)</span><br><span class="line">        self.rel_data_path = os.path.join(base_dir, <span class="string">&#x27;data/relation2id.txt&#x27;</span>)</span><br><span class="line">        self.embedding_dim = <span class="number">128</span> <span class="comment"># 词嵌入维度</span></span><br><span class="line">        self.pos_dim = <span class="number">32</span>  <span class="comment"># 位置嵌入维度</span></span><br><span class="line">        self.hidden_dim = <span class="number">200</span></span><br><span class="line">        self.epochs = <span class="number">50</span></span><br><span class="line">        self.batch_size = <span class="number">32</span></span><br><span class="line">        self.max_len = <span class="number">70</span>  <span class="comment"># 指定输入句子的最大长度</span></span><br><span class="line">        self.learning_rate = <span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = Config()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;train_data_path--&gt;<span class="subst">&#123;conf.train_data_path&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>



<h4 id="第三步-编写数据处理相关函数"><a href="#第三步-编写数据处理相关函数" class="headerlink" title="第三步: 编写数据处理相关函数"></a>第三步: 编写数据处理相关函数</h4><p>（1）整体思路</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250915154107971.png" alt="image-20250915154107971"></p>
<p>（2）代码</p>
<p>代码位置：P04_RE&#x2F;Bilstm_Attention_RE&#x2F;utils&#x2F;process.py</p>
<p>方法：</p>
<p>1）获取关系类型字典</p>
<p>2）处理数据，获取训练、测试数据集格式</p>
<p>3）文本数字化表示处理，得到word2id, id2word</p>
<p>4）把句子 words 转为 id 形式，并自动补全或截断为 max_len 长度。</p>
<p>5）负值相对编码处理</p>
<p>6）将id进行数字转换，防止为负数，而且进行句子长度的补齐或者截断</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> P04_RE.Bilstm_Attention_RE.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1）获取关系类型字典</span></span><br><span class="line">relation2id = &#123;&#125;</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(conf.rel_data_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        line = line.strip().split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        relation2id[line[<span class="number">0</span>]] = <span class="built_in">int</span>(line[<span class="number">1</span>])</span><br><span class="line"><span class="comment"># print(f&#x27;relation2id--&gt;&#123;relation2id&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2）处理数据，获取训练、测试数据集格式</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_txt_data</span>(<span class="params">file_path</span>):</span><br><span class="line">    datas = []  <span class="comment"># 存储每个文本中的字符</span></span><br><span class="line">    labels = []  <span class="comment"># 存储每个文本中的标签id</span></span><br><span class="line">    positionE1 = []  <span class="comment"># 存储每个文本中相对于实体1的位置</span></span><br><span class="line">    positionE2 = []  <span class="comment"># 存储每个文本中相对于实体2的位置</span></span><br><span class="line">    entities = []  <span class="comment"># 存储每个文本中的实体对，便于后续使用</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 优化点：为了保证每种关系类型的数量均衡，需要统计每个关系类型的样本数量，让每种类型的数量不超过2000</span></span><br><span class="line">    count_dict = &#123;k: <span class="number">0</span> <span class="keyword">for</span> k <span class="keyword">in</span> relation2id&#125;  <span class="comment"># 定义一个计数器，用于统计每种关系类型的数量，初始数量为0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            <span class="comment"># 1）对每个样本进行处理，按照空格进行切分，获取主实体、客实体、关系和原始文本。</span></span><br><span class="line">            line_list = line.strip().split(<span class="string">&#x27; &#x27;</span>, maxsplit=<span class="number">3</span>)  <span class="comment"># 需要使用maxsplit来指定最大的切割次数</span></span><br><span class="line">            <span class="comment"># print(f&#x27;line_list--&gt;&#123;line_list&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(line_list) != <span class="number">4</span>:  <span class="comment"># 如果切割的结果不等于4，则跳过</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> line_list[<span class="number">2</span>] <span class="keyword">not</span> <span class="keyword">in</span> relation2id:  <span class="comment"># 如果关系不在关系字典中，则跳过</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> count_dict[line_list[<span class="number">2</span>]] &gt;= <span class="number">2000</span>:  <span class="comment"># 如果当前关系类型的数量已经超过了2000，则跳过</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># 2）将关系通过relation2id字典转成具体的数值，然后放到labels列表中。</span></span><br><span class="line">            labels.append(relation2id[line_list[<span class="number">2</span>]])</span><br><span class="line">            <span class="comment"># 3）将主实体和客实体放到子列表中，然后放到entities列表中。</span></span><br><span class="line">            entities.append([line_list[<span class="number">0</span>], line_list[<span class="number">1</span>]])</span><br><span class="line">            <span class="comment"># 4）获取datas,positionE1和positionE2</span></span><br><span class="line">            sentence_str = line_list[<span class="number">3</span>]</span><br><span class="line">            <span class="comment"># 获取主实体的索引</span></span><br><span class="line">            e1_index = sentence_str.index(line_list[<span class="number">0</span>])</span><br><span class="line">            <span class="comment"># 获取客实体的索引</span></span><br><span class="line">            e2_index = sentence_str.index(line_list[<span class="number">1</span>])</span><br><span class="line">            <span class="comment"># 定义3个空列表，分别存储每个文本中的字符、主实体的相对位置编码和客实体的相对位置编码。</span></span><br><span class="line">            sentence, position1, position2 = [], [], []</span><br><span class="line">            <span class="keyword">for</span> index, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(sentence_str):</span><br><span class="line">                <span class="comment"># ①遍历原始文本，将每个字符存储到一个子列表中，遍历完成后再存到datas列表中。</span></span><br><span class="line">                sentence.append(word)</span><br><span class="line">                <span class="comment"># ②先获取主实体的索引，在遍历过程中使用原始索引-主实体的索引，获取相对于主实体的位置编码，存储到一个子列表中，遍历完成后再存到positionE1列表中。</span></span><br><span class="line">                position1.append(index - e1_index)</span><br><span class="line">                <span class="comment"># ③使用相同的方式，获取客实体的相对位置编码。</span></span><br><span class="line">                position2.append(index - e2_index)</span><br><span class="line">            <span class="comment"># ④将3个子列表分别放到datas,positionE1和positionE2列表中。</span></span><br><span class="line">            datas.append(sentence)</span><br><span class="line">            positionE1.append(position1)</span><br><span class="line">            positionE2.append(position2)</span><br><span class="line">            <span class="comment"># print(f&#x27;datas--&gt;&#123;datas&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># print(f&#x27;positionE1--&gt;&#123;positionE1&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># print(f&#x27;positionE2--&gt;&#123;positionE2&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># print(f&#x27;entities--&gt;&#123;entities&#125;&#x27;)</span></span><br><span class="line">            <span class="comment"># print(f&#x27;labels--&gt;&#123;labels&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 每处理完一个样本后，对对应的类型数量进行加一</span></span><br><span class="line">            count_dict[line_list[<span class="number">2</span>]] += <span class="number">1</span></span><br><span class="line">            <span class="comment"># break</span></span><br><span class="line">        <span class="keyword">return</span> datas, labels, positionE1, positionE2, entities</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3）文本数字化表示处理，得到 word2id, id2word</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_word_id</span>(<span class="params">file_path</span>):</span><br><span class="line">    datas, labels, positionE1, positionE2, entities = get_txt_data(file_path)</span><br><span class="line">    <span class="comment"># 初始化一个列表，用来存储所有的去重之后的字符</span></span><br><span class="line">    vocab_list = [<span class="string">&#x27;PAD&#x27;</span>, <span class="string">&#x27;UNK&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> datas:  <span class="comment"># 遍历所有的句子</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sentence:  <span class="comment"># 遍历句子中的每个字符</span></span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> vocab_list:  <span class="comment"># 如果字符不在vocab_list中，则添加到vocab_list中</span></span><br><span class="line">                vocab_list.append(word)</span><br><span class="line">    <span class="comment"># print(f&#x27;vocab_list--&gt;&#123;vocab_list&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成word2id、id2word的字典</span></span><br><span class="line">    word2id = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab_list)&#125;</span><br><span class="line">    id2word = &#123;i: word <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab_list)&#125;</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;word2id--&gt;<span class="subst">&#123;word2id&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;id2word--&gt;<span class="subst">&#123;id2word&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> word2id, id2word</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4）把句子 words 转为 id 形式，并自动补全或截断为 max_len 长度。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5）负值相对编码处理</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6）将id进行数字转换，防止为负数，而且进行句子长度的补齐或者截断</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># datas, labels, positionE1, positionE2, entities = get_txt_data(conf.train_data_path)</span></span><br><span class="line">    <span class="comment"># # print(f&#x27;labels--&gt;&#123;labels&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(Counter(labels))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    get_word_id(conf.train_data_path)</span><br></pre></td></tr></table></figure>





<h4 id="第四步-构建Dataset类与dataloader函数-1"><a href="#第四步-构建Dataset类与dataloader函数-1" class="headerlink" title="第四步: 构建Dataset类与dataloader函数"></a>第四步: 构建Dataset类与dataloader函数</h4><ul>
<li>步骤</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>构建Dataset类</span><br><span class="line"><span class="number">2.</span>构建自定义函数collate_fn()</span><br><span class="line"><span class="number">3.</span>构建get_loader_data函数，获得数据迭代器</span><br></pre></td></tr></table></figure>

<ul>
<li>代码</li>
</ul>
<p>代码位置：P04_RE&#x2F;Bilstm_Attention_RE&#x2F;utils&#x2F;data_loader.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> P04_RE.Bilstm_Attention_RE.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> P04_RE.Bilstm_Attention_RE.utils.process <span class="keyword">import</span> get_txt_data</span><br><span class="line"></span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.构建Dataset类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file_path</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyDataset, self).__init__()</span><br><span class="line">        self.datas, self.labels, self.positionE1, self.positionE2, self.entities = get_txt_data(file_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.datas)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.datas[index], self.labels[index], self.positionE1[index], self.positionE2[index], self.entities[index]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.构建自定义函数collate_fn()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch_data</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;batch_data--&gt;<span class="subst">&#123;batch_data&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.构建get_loader_data函数，获得数据迭代器</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_data_loader</span>():</span><br><span class="line">    <span class="comment"># 训练集</span></span><br><span class="line">    train_dataset = MyDataset(conf.train_data_path)</span><br><span class="line">    train_dataloader = DataLoader(train_dataset,</span><br><span class="line">                                  batch_size=conf.batch_size,</span><br><span class="line">                                  shuffle=<span class="literal">False</span>,  <span class="comment"># 在写代码的时候，需要把shuffle设置为 Fasle; 在训练时，需要把shuffle设置为 True</span></span><br><span class="line">                                  collate_fn=collate_fn,</span><br><span class="line">                                  drop_last=<span class="literal">True</span></span><br><span class="line">                                  )</span><br><span class="line">    <span class="comment"># 测试集</span></span><br><span class="line">    test_dataset = MyDataset(conf.test_data_path)</span><br><span class="line">    test_dataloader = DataLoader(test_dataset,</span><br><span class="line">                                 batch_size=conf.batch_size,</span><br><span class="line">                                 shuffle=<span class="literal">False</span>,</span><br><span class="line">                                 collate_fn=collate_fn,</span><br><span class="line">                                 drop_last=<span class="literal">True</span></span><br><span class="line">                                 )</span><br><span class="line">    <span class="keyword">return</span> train_dataloader, test_dataloader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ` <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># dt = MyDataset(conf.train_data_path)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;len(dt)--&gt;&#123;len(dt)&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># print(f&#x27;dt[0]--&gt;&#123;dt[0]&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    train_dataloader, test_dataloader = get_data_loader()</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;x--&gt;<span class="subst">&#123;x&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>





<h3 id="【掌握】BiLSTM-Attention模型搭建"><a href="#【掌握】BiLSTM-Attention模型搭建" class="headerlink" title="【掌握】BiLSTM+Attention模型搭建"></a>【掌握】BiLSTM+Attention模型搭建</h3><h4 id="第一步-编写模型类的代码-1"><a href="#第一步-编写模型类的代码-1" class="headerlink" title="第一步: 编写模型类的代码"></a>第一步: 编写模型类的代码</h4><p>（1）思路</p>
<p>（2）代码</p>
<p><code>注意：weight不能在模型定义时直接将batch_size写死，否则后期在使用时，每次传入的样本必须是相同的batch_size个。</code></p>
<p>代码位置：P04_RE&#x2F;Bilstm_Attention_RE&#x2F;model&#x2F;bilstm_atten.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="第二步-编写训练函数-1"><a href="#第二步-编写训练函数-1" class="headerlink" title="第二步: 编写训练函数"></a>第二步: 编写训练函数</h4><p>（1）基本步骤</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">训练函数基本步骤——</span></span><br><span class="line"><span class="attr">1.构建数据迭代器Dataloader(包括数据处理与构建数据源Dataset)</span></span><br><span class="line"><span class="attr">2.实例化模型</span></span><br><span class="line"><span class="attr">3.实例化损失函数对象</span></span><br><span class="line"><span class="attr">4.实例化优化器对象</span></span><br><span class="line"><span class="attr">5.定义打印日志参数</span></span><br><span class="line"><span class="attr">6.开始训练</span></span><br><span class="line"><span class="attr">6.1</span> <span class="string">实现外层大循环epoch</span></span><br><span class="line">    <span class="attr">6.2</span> <span class="string">将模型设置为训练模式</span></span><br><span class="line">    <span class="attr">6.3</span> <span class="string">内部遍历数据迭代器dataloader</span></span><br><span class="line">        <span class="attr">1）将数据送入模型得到输出结果</span></span><br><span class="line">        <span class="attr">2）计算损失</span></span><br><span class="line">        <span class="attr">3）梯度清零</span>: <span class="string">optimizer.zero_grad()</span></span><br><span class="line">        <span class="attr">4）反向传播(计算梯度)</span>: <span class="string">loss.backward()</span></span><br><span class="line">        <span class="attr">5）梯度更新(参数更新)</span>: <span class="string">optimizer.step()</span></span><br><span class="line">        <span class="attr">6）打印内部训练日志</span></span><br><span class="line">    <span class="attr">6.4</span> <span class="string">使用验证集进行模型评估【将模型设置为评估模式】</span></span><br><span class="line">    <span class="attr">6.5</span> <span class="string">保存模型: torch.save(model.state_dict(), &quot;model_path&quot;)</span></span><br><span class="line"><span class="attr">6.6</span> <span class="string">打印外部训练日志</span></span><br><span class="line"></span><br><span class="line"><span class="attr">验证函数基本步骤——</span></span><br><span class="line"><span class="attr">1.定义打印日志参数</span></span><br><span class="line"><span class="attr">2.将模型设置为评估模式</span></span><br><span class="line"><span class="attr">3.内部遍历数据迭代器dataloader</span></span><br><span class="line">  <span class="attr">3.1</span> <span class="string">将数据送入模型得到输出结果</span></span><br><span class="line">  <span class="attr">3.2</span> <span class="string">计算损失</span></span><br><span class="line">  <span class="attr">3.3</span> <span class="string">处理结果</span></span><br><span class="line">  <span class="attr">3.4</span> <span class="string">统计批次内指标</span></span><br><span class="line"><span class="attr">4.统计整体指标</span></span><br></pre></td></tr></table></figure>

<p>（2）代码</p>
<p><strong>1）课件中没有遵循标准的训练、验证流程，可以优化。</strong></p>
<p><strong>2）这次没有使用sklearn中的方法来计算指标，而是通过手动计算的，这种方法也可以熟悉一下</strong></p>
<p>代码位置：P04_RE&#x2F;Bilstm_Attention_RE&#x2F;train.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="第三步-编写模型预测函数-1"><a href="#第三步-编写模型预测函数-1" class="headerlink" title="第三步: 编写模型预测函数"></a>第三步: 编写模型预测函数</h4><p>（1）步骤</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">1.实例化模型</span></span><br><span class="line"><span class="attr">2.加载模型参数</span></span><br><span class="line"><span class="attr">3.处理数据</span></span><br><span class="line"><span class="attr">4.模型预测</span></span><br><span class="line"><span class="attr">5.结果解析</span></span><br></pre></td></tr></table></figure>

<p>（2）代码</p>
<p>代码位置：P04_RE&#x2F;Bilstm_Attention_RE&#x2F;predict.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="【掌握】Pipeline方法的优缺点"><a href="#【掌握】Pipeline方法的优缺点" class="headerlink" title="【掌握】Pipeline方法的优缺点"></a>【掌握】Pipeline方法的优缺点</h3><ul>
<li>优点：<ul>
<li>易于实现，实体模型和关系模型使用独立的数据集，不需要同时标注实体和关系的数据集.</li>
<li>两者相互独立，若关系抽取模型没训练好不会影响到实体抽取.</li>
</ul>
</li>
<li>缺点：<ul>
<li>关系和实体两者是紧密相连的，互相之间的联系没有捕捉到.</li>
<li>上游 NER 的错误会直接影响下游关系抽取，容易造成误差积累.</li>
<li>难以处理EPO问题</li>
</ul>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://liamjohnson-w.github.io">李俊泽</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://liamjohnson-w.github.io/2025/09/14/TrafficDefenceDetection/">https://liamjohnson-w.github.io/2025/09/14/TrafficDefenceDetection/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/doc/">doc</a><a class="post-meta__tags" href="/tags/AI/">AI</a></div><div class="post_share"><div class="social-share" data-image="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen132.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> Donate</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2025/07/18/Project/" title="Jason Project Demo"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Jason Project Demo</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2025/06/29/InterviewQuestions/" title="Jason Interview Note"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen121.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-29</div><div class="title">Jason Interview Note</div></div></a></div><div><a href="/2025/07/18/Project/" title="Jason Project Demo"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="title">Jason Project Demo</div></div></a></div><div><a href="/2024/05/01/2024.05.01/" title="Document"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-01</div><div class="title">Document</div></div></a></div><div><a href="/2025/06/03/2025.06.03/" title="OLLAMA"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250603132906789.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-03</div><div class="title">OLLAMA</div></div></a></div><div><a href="/2025/07/16/%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/" title="算法公式推导"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-16</div><div class="title">算法公式推导</div></div></a></div><div><a href="/2025/06/22/NLP_Base/" title="NLP自然语言处理"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/%E7%94%9F%E6%88%90%E7%8C%AB%E5%92%AA%E5%9B%BE%E7%89%87%20(2).png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-22</div><div class="title">NLP自然语言处理</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/tx.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">李俊泽</div><div class="author-info__description">机器都在学习,你有什么理由不学习?</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">235</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">58</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/weiswift/"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/weiswift" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:1265019024@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">博客为本人搭建 Github托管 仅记录学习过程 不做引流 不做排名 不打广告！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Day01"><span class="toc-number">1.</span> <span class="toc-text">Day01</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%EF%BC%9F"><span class="toc-number">1.1.</span> <span class="toc-text">1、什么是知识图谱？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E9%A1%B9%E7%9B%AE%E7%9A%84%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84%E5%9B%BE%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">1.2.</span> <span class="toc-text">2、项目的技术架构图是怎样的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E9%A1%B9%E7%9B%AE%E7%94%A8%E5%88%B0%E4%BA%86%E5%93%AA%E4%BA%9B%E5%B7%A5%E5%85%B7%EF%BC%9F"><span class="toc-number">1.3.</span> <span class="toc-text">3、项目用到了哪些工具？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%94%A8MySQL%E6%9D%A5%E5%AD%98%E5%82%A8%E4%B8%89%E5%85%83%E7%BB%84%E6%95%B0%E6%8D%AE%EF%BC%9F"><span class="toc-number">1.4.</span> <span class="toc-text">4、为什么不用MySQL来存储三元组数据？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E5%AE%9E%E4%BD%93%E5%92%8CNER%EF%BC%9F"><span class="toc-number">1.5.</span> <span class="toc-text">5、什么是实体和NER？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%9C%89%E5%93%AA%E4%BA%9B%E6%96%B9%E6%B3%95%EF%BC%9F"><span class="toc-number">1.6.</span> <span class="toc-text">6、命名实体识别有哪些方法？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81%E4%B8%BE%E4%B8%AA%E4%BE%8B%E5%AD%90%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%8B%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E8%A7%84%E5%88%99%E7%9A%84%E6%96%B9%E6%B3%95%E6%8A%BD%E5%8F%96%E5%AE%9E%E4%BD%93%EF%BC%9F"><span class="toc-number">1.7.</span> <span class="toc-text">7、举个例子描述一下如何使用规则的方法抽取实体？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day02"><span class="toc-number">2.</span> <span class="toc-text">Day02</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81LSTM%E9%9D%A2%E8%AF%95%E9%A2%98"><span class="toc-number">2.1.</span> <span class="toc-text">1、LSTM面试题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E7%BA%BF%E6%80%A7%E9%93%BE%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%EF%BC%88Linear-chain-CRF%EF%BC%89"><span class="toc-number">2.2.</span> <span class="toc-text">2、什么是线性链条件随机场（Linear-chain-CRF）?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%8BBiLSTM-CRF%E6%9E%B6%E6%9E%84%EF%BC%9F"><span class="toc-number">2.3.</span> <span class="toc-text">3、描述一下BiLSTM+CRF架构？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81CRF%E4%B8%AD%E7%9A%84%E5%8F%91%E5%B0%84%E5%88%86%E6%95%B0%E5%92%8C%E8%BD%AC%E7%A7%BB%E5%88%86%E6%95%B0%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">2.4.</span> <span class="toc-text">4、CRF中的发射分数和转移分数是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E8%AF%B4%E4%B8%80%E4%B8%8BCRF%E5%BB%BA%E6%A8%A1%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">2.5.</span> <span class="toc-text">5、说一下CRF建模的损失函数是怎样的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E5%89%8D%E5%90%91%E7%AE%97%E6%B3%95%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">2.6.</span> <span class="toc-text">6、前向算法是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81Viterbi%E8%A7%A3%E7%A0%81%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">2.7.</span> <span class="toc-text">7、Viterbi解码是什么？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day03"><span class="toc-number">3.</span> <span class="toc-text">Day03</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E5%9C%A8%E9%A1%B9%E7%9B%AE%E4%B8%AD%EF%BC%8C%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AE%E8%B7%AF%E5%BE%84%EF%BC%9F"><span class="toc-number">3.1.</span> <span class="toc-text">1、在项目中，应该如何设置路径？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E7%AE%80%E5%8D%95%E8%AF%B4%E4%B8%80%E4%B8%8B%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%9A%84%E6%9C%80%E7%BB%88%E6%A0%BC%E5%BC%8F%E8%A6%81%E6%B1%82%EF%BC%9F"><span class="toc-number">3.2.</span> <span class="toc-text">2、简单说一下数据处理的最终格式要求？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E5%9C%A8%E5%B0%86%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%88%90%E6%9C%80%E7%BB%88%E7%9A%84%E6%A0%BC%E5%BC%8F%E8%A6%81%E6%B1%82%E6%97%B6%EF%BC%8C%E4%B8%80%E8%88%AC%E5%8F%AF%E4%BB%A5%E5%9C%A8%E5%93%AA%E4%BA%9B%E5%9C%B0%E6%96%B9%E5%81%9A%E5%A4%84%E7%90%86%EF%BC%9F"><span class="toc-number">3.3.</span> <span class="toc-text">3、在将原始数据处理成最终的格式要求时，一般可以在哪些地方做处理？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E5%9C%A8%E6%9E%84%E9%80%A0%E6%95%B0%E6%8D%AE%E8%BF%AD%E4%BB%A3%E5%99%A8%EF%BC%88Dataloader%EF%BC%89%E6%97%B6%EF%BC%8C%E6%9C%89%E5%93%AA%E4%BA%9B%E6%AD%A5%E9%AA%A4%EF%BC%9F"><span class="toc-number">3.4.</span> <span class="toc-text">4、在构造数据迭代器（Dataloader）时，有哪些步骤？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E7%BB%9F%E4%B8%80%E6%A0%B7%E6%9C%AC%E9%95%BF%E5%BA%A6%E6%9C%89%E5%93%AA%E4%BA%9B%E6%96%B9%E6%B3%95%EF%BC%9F"><span class="toc-number">3.5.</span> <span class="toc-text">5、统一样本长度有哪些方法？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%8BBiLSTM-CRF%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9E%B6%E6%9E%84%EF%BC%9F"><span class="toc-number">3.6.</span> <span class="toc-text">6、描述一下BiLSTM_CRF模型的架构？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day04"><span class="toc-number">4.</span> <span class="toc-text">Day04</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">4.1.</span> <span class="toc-text">1、训练函数基本步骤是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E9%AA%8C%E8%AF%81%E5%87%BD%E6%95%B0%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">4.2.</span> <span class="toc-text">2、验证函数基本步骤是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81BiLSTM-CRF%E6%A8%A1%E5%9E%8B%E5%9C%A8%E8%AE%AD%E7%BB%83%E5%AE%8C%E5%90%8E%EF%BC%8C%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%93%AA%E4%BA%9B%E4%BC%98%E5%8C%96%E6%9D%A5%E6%94%B9%E5%96%84%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%EF%BC%9F"><span class="toc-number">4.3.</span> <span class="toc-text">3、BiLSTM_CRF模型在训练完后，可以做哪些优化来改善模型性能？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81precision%E3%80%81recall%E3%80%81f1%E3%80%81report%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">4.4.</span> <span class="toc-text">4、precision、recall、f1、report的使用方式是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">4.5.</span> <span class="toc-text">5、模型预测基本步骤是什么？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day05"><span class="toc-number">5.</span> <span class="toc-text">Day05</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%EF%BC%9F%E6%9C%AC%E8%B4%A8%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">5.1.</span> <span class="toc-text">1、什么是关系抽取？本质是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">5.2.</span> <span class="toc-text">关系抽取的常用方法有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E4%BB%BB%E5%8A%A1%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">5.3.</span> <span class="toc-text">3、关系抽取任务常见问题有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84%E6%96%B9%E6%B3%95%E5%AE%9E%E7%8E%B0%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">5.4.</span> <span class="toc-text">4、基于规则的方法实现关系抽取的优缺点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%8BBiLSTM-Attention%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9E%B6%E6%9E%84%EF%BC%9F"><span class="toc-number">5.5.</span> <span class="toc-text">5、描述一下BiLSTM+Attention模型的架构？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">5.6.</span> <span class="toc-text">6、注意力机制是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%8BBiLSTM-Attention%E6%A8%A1%E5%9E%8B%E4%B8%AD%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%98%AF%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%9A%84%EF%BC%9F"><span class="toc-number">5.7.</span> <span class="toc-text">7、描述一下BiLSTM+Attention模型中注意力机制是如何实现的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8%E3%80%81BiLSTM-Attentiom%E6%A8%A1%E5%9E%8B%E4%B8%AD%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%9A%84%E6%95%B4%E4%BD%93%E6%80%9D%E8%B7%AF%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">5.8.</span> <span class="toc-text">8、BiLSTM+Attentiom模型中数据处理的整体思路是什么？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day06"><span class="toc-number">6.</span> <span class="toc-text">Day06</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81BERT%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%89%80%E6%8E%A5%E6%94%B6%E7%9A%84%E6%9C%80%E5%A4%A7sequence%E9%95%BF%E5%BA%A6%E6%98%AF%E5%A4%9A%E5%B0%91%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%BE%E7%BD%AE%E6%9C%80%E5%A4%A7%E9%95%BF%E5%BA%A6%EF%BC%9F"><span class="toc-number">6.1.</span> <span class="toc-text">1、BERT预训练模型所接收的最大sequence长度是多少，为什么设置最大长度？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E5%AF%B9%E4%BA%8E%E9%95%BF%E6%96%87%E6%9C%AC-%E6%96%87%E6%9C%AC%E9%95%BF%E5%BA%A6%E8%B6%85%E8%BF%87512%E7%9A%84%E5%8F%A5%E5%AD%90-%E5%9C%A8%E4%BD%BF%E7%94%A8BERT%E6%97%B6-%E5%A6%82%E4%BD%95%E6%9D%A5%E6%9E%84%E9%80%A0%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%EF%BC%9F"><span class="toc-number">6.2.</span> <span class="toc-text">2、对于长文本(文本长度超过512的句子)在使用BERT时, 如何来构造训练样本？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81BiLSTM-Attention%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9E%B6%E6%9E%84%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">6.3.</span> <span class="toc-text">3、BiLSTM+Attention模型的架构是怎样的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E5%9C%A8BERT%E4%B8%AD%EF%BC%8C%E6%98%AF%E5%A6%82%E4%BD%95%E5%B0%86Token-Embedding%E3%80%81Segment-Embedding-%E5%92%8C-Position-Embedding%E7%BB%84%E5%90%88%E5%9C%A8%E4%B8%80%E8%B5%B7%E7%84%B6%E5%90%8E%E9%80%81%E5%88%B0encoder%E4%B8%AD%E7%9A%84%EF%BC%9F"><span class="toc-number">6.4.</span> <span class="toc-text">4、在BERT中，是如何将Token Embedding、Segment Embedding 和 Position Embedding组合在一起然后送到encoder中的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E4%B8%A4%E4%B8%AA%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98%E6%97%B6%EF%BC%8Cshape%E4%B8%8D%E7%AC%A6%E5%90%88%E8%A6%81%E6%B1%82%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F"><span class="toc-number">6.5.</span> <span class="toc-text">5、两个矩阵相乘时，shape不符合要求怎么办？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day07"><span class="toc-number">7.</span> <span class="toc-text">Day07</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81BiLSTM-Attention%E6%A8%A1%E5%9E%8B%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%93%AA%E4%BA%9B%E4%BC%98%E5%8C%96%E6%9D%A5%E6%94%B9%E5%96%84%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%EF%BC%9F"><span class="toc-number">7.1.</span> <span class="toc-text">1、BiLSTM+Attention模型可以做哪些优化来改善模型性能？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81Pipeline%E6%96%B9%E6%B3%95%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">7.2.</span> <span class="toc-text">2、Pipeline方法的优缺点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81Joint%E6%96%B9%E6%B3%95%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E6%9C%89%E5%93%AA%E4%B8%A4%E7%A7%8D%E7%B1%BB%E5%9E%8B%EF%BC%9F"><span class="toc-number">7.3.</span> <span class="toc-text">3、Joint方法是什么？有哪两种类型？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81Casrel%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9E%B6%E6%9E%84%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">7.4.</span> <span class="toc-text">4、Casrel模型的架构是怎样的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E8%AF%B4%E4%B8%80%E4%B8%8BCasrel%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">7.5.</span> <span class="toc-text">5、说一下Casrel模型的输入输出是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%95%B4%E4%BD%93%E6%80%9D%E8%B7%AF%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">7.6.</span> <span class="toc-text">6、数据处理整体思路是怎样的？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day08"><span class="toc-number">8.</span> <span class="toc-text">Day08</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81Casrel%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%9A%84%E6%95%B4%E4%BD%93%E6%80%9D%E8%B7%AF%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">8.1.</span> <span class="toc-text">1、Casrel模型数据处理的整体思路是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E4%BD%BF%E7%94%A8Casrel%E6%A8%A1%E5%9E%8B%E6%97%B6%EF%BC%8C%E9%81%87%E5%88%B0%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%8C%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%9A%84%EF%BC%9F"><span class="toc-number">8.2.</span> <span class="toc-text">2、使用Casrel模型时，遇到什么问题，如何解决的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81Casrel%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%93%E6%9E%84%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">8.3.</span> <span class="toc-text">3、Casrel模型的结构是怎样的？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#NER%E4%BB%BB%E5%8A%A1%E4%BB%A3%E7%A0%81"><span class="toc-number">9.</span> <span class="toc-text">NER任务代码</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#BiLSTM-CRF%E9%A1%B9%E7%9B%AE%E5%AE%8C%E6%95%B4%E5%AE%9E%E7%8E%B0"><span class="toc-number">9.1.</span> <span class="toc-text">BiLSTM+CRF项目完整实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E3%80%90%E5%AE%9E%E7%8E%B0%E3%80%91%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">9.1.1.</span> <span class="toc-text">【实现】数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5-%E6%9F%A5%E7%9C%8B%E9%A1%B9%E7%9B%AE%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">9.1.1.0.1.</span> <span class="toc-text">第一步: 查看项目数据集</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5-%E6%9E%84%E9%80%A0%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E6%95%B0%E6%8D%AE"><span class="toc-number">9.1.1.0.2.</span> <span class="toc-text">第二步: 构造序列标注数据</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5-%E7%BC%96%E5%86%99Config%E7%B1%BB%E9%A1%B9%E7%9B%AE%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%A0%81"><span class="toc-number">9.1.1.0.3.</span> <span class="toc-text">第三步: 编写Config类项目文件配置代码</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E6%AD%A5-%E6%9E%84%E5%BB%BADataset%E7%B1%BB%E4%B8%8Edataloader%E5%87%BD%E6%95%B0"><span class="toc-number">9.1.1.0.4.</span> <span class="toc-text">第四步: 构建Dataset类与dataloader函数</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E6%95%B4%E4%BD%93%E6%80%9D%E8%B7%AF"><span class="toc-number">9.1.1.0.4.1.</span> <span class="toc-text">（1）整体思路</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%9E%84%E9%80%A0-x-y-%E6%A0%B7%E6%9C%AC%E5%AF%B9%EF%BC%8C%E4%BB%A5%E5%8F%8A%E8%8E%B7%E5%8F%96vocabs"><span class="toc-number">9.1.1.0.4.2.</span> <span class="toc-text">（2）构造(x,y)样本对，以及获取vocabs</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E6%9E%84%E9%80%A0%E6%95%B0%E6%8D%AE%E8%BF%AD%E4%BB%A3%E5%99%A8"><span class="toc-number">9.1.1.0.4.3.</span> <span class="toc-text">（3）构造数据迭代器</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BiLSTM-CRF%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA"><span class="toc-number">9.2.</span> <span class="toc-text">BiLSTM+CRF模型搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5-%E7%BC%96%E5%86%99%E6%A8%A1%E5%9E%8B%E7%B1%BB%E7%9A%84%E4%BB%A3%E7%A0%81"><span class="toc-number">9.2.1.</span> <span class="toc-text">第一步: 编写模型类的代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5-%E7%BC%96%E5%86%99%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0"><span class="toc-number">9.2.2.</span> <span class="toc-text">第二步: 编写训练函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5-%E7%BC%96%E5%86%99%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E5%87%BD%E6%95%B0"><span class="toc-number">9.2.3.</span> <span class="toc-text">第三步: 编写模型预测函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E4%BB%BB%E5%8A%A1%E4%BB%A3%E7%A0%81"><span class="toc-number">10.</span> <span class="toc-text">关系抽取任务代码</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%80%90%E5%AE%9E%E7%8E%B0%E3%80%91%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96"><span class="toc-number">10.1.</span> <span class="toc-text">【实现】基于规则方式实现关系抽取</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-number">10.1.1.</span> <span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">10.1.2.</span> <span class="toc-text">代码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">10.1.3.</span> <span class="toc-text">优缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pipline%E6%96%B9%E6%B3%95%E5%AE%9E%E7%8E%B0%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96"><span class="toc-number">10.2.</span> <span class="toc-text">Pipline方法实现关系抽取</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E3%80%90%E6%8E%8C%E6%8F%A1%E3%80%91Pipeline%E6%96%B9%E6%B3%95%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-number">10.2.1.</span> <span class="toc-text">【掌握】Pipeline方法的原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E3%80%90%E6%8E%8C%E6%8F%A1%E3%80%91BiLSTM-Attention%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-number">10.2.2.</span> <span class="toc-text">【掌握】BiLSTM+Attention模型架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E3%80%90%E5%AE%9E%E7%8E%B0%E3%80%91%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E6%A6%82%E8%A7%88"><span class="toc-number">10.2.3.</span> <span class="toc-text">【实现】代码实现概览</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E3%80%90%E5%AE%9E%E7%8E%B0%E3%80%91%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-1"><span class="toc-number">10.2.4.</span> <span class="toc-text">【实现】数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5-%E6%9F%A5%E7%9C%8B%E9%A1%B9%E7%9B%AE%E6%95%B0%E6%8D%AE%E9%9B%86-1"><span class="toc-number">10.2.4.1.</span> <span class="toc-text">第一步: 查看项目数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5-%E7%BC%96%E5%86%99Config%E7%B1%BB%E9%A1%B9%E7%9B%AE%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%A0%81"><span class="toc-number">10.2.4.2.</span> <span class="toc-text">第二步: 编写Config类项目文件配置代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5-%E7%BC%96%E5%86%99%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E5%87%BD%E6%95%B0"><span class="toc-number">10.2.4.3.</span> <span class="toc-text">第三步: 编写数据处理相关函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E6%AD%A5-%E6%9E%84%E5%BB%BADataset%E7%B1%BB%E4%B8%8Edataloader%E5%87%BD%E6%95%B0-1"><span class="toc-number">10.2.4.4.</span> <span class="toc-text">第四步: 构建Dataset类与dataloader函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E3%80%90%E6%8E%8C%E6%8F%A1%E3%80%91BiLSTM-Attention%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA"><span class="toc-number">10.2.5.</span> <span class="toc-text">【掌握】BiLSTM+Attention模型搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5-%E7%BC%96%E5%86%99%E6%A8%A1%E5%9E%8B%E7%B1%BB%E7%9A%84%E4%BB%A3%E7%A0%81-1"><span class="toc-number">10.2.5.1.</span> <span class="toc-text">第一步: 编写模型类的代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5-%E7%BC%96%E5%86%99%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0-1"><span class="toc-number">10.2.5.2.</span> <span class="toc-text">第二步: 编写训练函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5-%E7%BC%96%E5%86%99%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E5%87%BD%E6%95%B0-1"><span class="toc-number">10.2.5.3.</span> <span class="toc-text">第三步: 编写模型预测函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E3%80%90%E6%8E%8C%E6%8F%A1%E3%80%91Pipeline%E6%96%B9%E6%B3%95%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">10.2.6.</span> <span class="toc-text">【掌握】Pipeline方法的优缺点</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/09/14/TrafficDefenceDetection/" title="TQ System"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen132.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="TQ System"/></a><div class="content"><a class="title" href="/2025/09/14/TrafficDefenceDetection/" title="TQ System">TQ System</a><time datetime="2025-09-13T16:00:00.000Z" title="Created 2025-09-14 00:00:00">2025-09-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/18/Project/" title="Jason Project Demo"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Jason Project Demo"/></a><div class="content"><a class="title" href="/2025/07/18/Project/" title="Jason Project Demo">Jason Project Demo</a><time datetime="2025-07-17T16:00:00.000Z" title="Created 2025-07-18 00:00:00">2025-07-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/16/%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/" title="算法公式推导"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="算法公式推导"/></a><div class="content"><a class="title" href="/2025/07/16/%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/" title="算法公式推导">算法公式推导</a><time datetime="2025-07-15T16:00:00.000Z" title="Created 2025-07-16 00:00:00">2025-07-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/29/InterviewQuestions/" title="Jason Interview Note"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen121.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Jason Interview Note"/></a><div class="content"><a class="title" href="/2025/06/29/InterviewQuestions/" title="Jason Interview Note">Jason Interview Note</a><time datetime="2025-06-28T16:00:00.000Z" title="Created 2025-06-29 00:00:00">2025-06-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/22/DK67/" title="DK67双模切换"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/%E7%94%9F%E6%88%90%E7%8C%AB%E5%92%AA%E5%9B%BE%E7%89%87.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="DK67双模切换"/></a><div class="content"><a class="title" href="/2025/06/22/DK67/" title="DK67双模切换">DK67双模切换</a><time datetime="2025-06-21T16:00:00.000Z" title="Created 2025-06-22 00:00:00">2025-06-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 李俊泽</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Welcome to 李俊泽 の <a target="_blank" rel="noopener" href="https://www.cnblogs.com/liam-sliversucks/">Blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? '' : ''

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>