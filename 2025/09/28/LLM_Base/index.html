<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLM大模型基础 | All wisdom begins with memory.</title><meta name="author" content="李俊泽"><meta name="copyright" content="李俊泽"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="大语言模型LLM一种通过学习大量的文本来预测整个序列或者下一个词的概率，从而理解和生成自然语言的模型。 大模型发展历程基于规则与概率：这类模型主要依赖于人工设计的规则和统计方法。如N-gram模型通过计算词语出现的频率来预测下一个词，但存在稀疏性问题，无法很好地处理未见过的词语组合。 神经网络语言模"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://liamjohnson-w.github.io/2025/09/28/LLM_Base/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM大模型基础',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-10-25 09:01:38'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css"> <script src="/live2d-widget/autoload.js"></script><script src="/live2d-widget/autoload.js"> </script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/tx.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">237</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">58</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-gamepad"></i><span> Games</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/mikutap/"><i class="fa-fw fa fa-music"></i><span> MikuTap 初音未来</span></a></li><li><a class="site-page child" href="/starbattle/"><i class="fa-fw fa fa-space-shuttle"></i><span> StartBattle 星际大战</span></a></li><li><a class="site-page child" href="/2048/"><i class="fa-fw fa fa-flag"></i><span> 2048 经典游戏</span></a></li><li><a class="site-page child" href="/battlecity/"><i class="fa-fw fa fa-arrow-circle-left"></i><span> BattleCity 坦克大战</span></a></li><li><a class="site-page child" href="/pacman/"><i class="fa-fw fa fa-bolt"></i><span> PacMan  吃豆人</span></a></li><li><a class="site-page child" href="/tetris/"><i class="fa-fw fa fa-arrows-alt"></i><span> Tetris 俄罗斯方块</span></a></li><li><a class="site-page child" href="/smallcat/"><i class="fa-fw fa fa-paw"></i><span> CatchCat 困住小猫</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-leaf"></i><span> Moments</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/diary/"><i class="fa-fw fas fa-bookmark"></i><span> Diary</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fa fa-hourglass-half"></i><span> Gallery</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-podcast"></i><span> More</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags标签</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About关于</span></a></li><li><a class="site-page child" href="/messageboard/"><i class="fa-fw fas fa-bookmark"></i><span> Messageboard留言板</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/cat111.png')"><nav id="nav"><span id="blog-info"><a href="/" title="All wisdom begins with memory."><span class="site-name">All wisdom begins with memory.</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-gamepad"></i><span> Games</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/mikutap/"><i class="fa-fw fa fa-music"></i><span> MikuTap 初音未来</span></a></li><li><a class="site-page child" href="/starbattle/"><i class="fa-fw fa fa-space-shuttle"></i><span> StartBattle 星际大战</span></a></li><li><a class="site-page child" href="/2048/"><i class="fa-fw fa fa-flag"></i><span> 2048 经典游戏</span></a></li><li><a class="site-page child" href="/battlecity/"><i class="fa-fw fa fa-arrow-circle-left"></i><span> BattleCity 坦克大战</span></a></li><li><a class="site-page child" href="/pacman/"><i class="fa-fw fa fa-bolt"></i><span> PacMan  吃豆人</span></a></li><li><a class="site-page child" href="/tetris/"><i class="fa-fw fa fa-arrows-alt"></i><span> Tetris 俄罗斯方块</span></a></li><li><a class="site-page child" href="/smallcat/"><i class="fa-fw fa fa-paw"></i><span> CatchCat 困住小猫</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-leaf"></i><span> Moments</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/diary/"><i class="fa-fw fas fa-bookmark"></i><span> Diary</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fa fa-hourglass-half"></i><span> Gallery</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-podcast"></i><span> More</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags标签</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About关于</span></a></li><li><a class="site-page child" href="/messageboard/"><i class="fa-fw fas fa-bookmark"></i><span> Messageboard留言板</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">LLM大模型基础</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-09-27T16:00:00.000Z" title="Created 2025-09-28 00:00:00">2025-09-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-10-25T01:01:38.283Z" title="Updated 2025-10-25 09:01:38">2025-10-25</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="LLM大模型基础"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="大语言模型LLM"><a href="#大语言模型LLM" class="headerlink" title="大语言模型LLM"></a>大语言模型LLM</h1><p>一种通过学习大量的文本来预测整个序列或者下一个词的概率，从而理解和生成自然语言的模型。</p>
<h2 id="大模型发展历程"><a href="#大模型发展历程" class="headerlink" title="大模型发展历程"></a>大模型发展历程</h2><p>基于规则与概率：这类模型<code>主要依赖于人工设计的规则和统计方法</code>。如N-gram模型通过计算词语出现的频率来预测下一个词，但存在稀疏性问题，无法很好地处理未见过的词语组合。</p>
<p>神经网络语言模型引入词向量：<code>NNLM利用神经网络，将词语映射成连续的向量（词嵌入），从而捕捉词语间的语义关系</code>。这解决了稀疏性问题，并能更好地理解上下文。然而，NNLM通常需要针对特定任务进行端到端训练。</p>
<p>预训练语言模型：预训练 + 微调<code>以Transformer架构为核心</code>，PLM（如BERT、GPT-½）通过在海量文本上进行无监督预训练来学习通用语言知识，然后针对下游任务进行微调。这极大<code>提高了模型的泛化能力和任务表现</code>。</p>
<p>大预言模型：规模化与涌现能力，LLM（如GPT-¾、PaLM）在PLM的基础上，进一步扩大模型规模（参数量、数据量），从而展现出涌现能力，可以执行多样的任务，如问答、摘要、代码生成等，甚至不需要微调（in-context learning）。</p>
<table>
<thead>
<tr>
<th align="left">阶段</th>
<th align="left">代表技术</th>
<th align="left">特点</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>基于规则与统计</strong></td>
<td align="left">N-gram模型</td>
<td align="left">简单易解释，但数据稀疏、泛化差</td>
</tr>
<tr>
<td align="left"><strong>神经网络语言模型</strong></td>
<td align="left">NNLM</td>
<td align="left">引入词向量，解决稀疏性问题</td>
</tr>
<tr>
<td align="left"><strong>预训练语言模型</strong></td>
<td align="left">BERT, GPT</td>
<td align="left">Transformer架构，预训练+微调范式</td>
</tr>
<tr>
<td align="left"><strong>大语言模型</strong></td>
<td align="left">GPT-3, LLaMA</td>
<td align="left">千亿参数，涌现能力，通用性强</td>
</tr>
</tbody></table>
<h2 id="⭐️大语言模型（LLM）核心技术架构"><a href="#⭐️大语言模型（LLM）核心技术架构" class="headerlink" title="⭐️大语言模型（LLM）核心技术架构"></a>⭐️大语言模型（LLM）核心技术架构</h2><table>
<thead>
<tr>
<th align="left">架构</th>
<th align="left">代表模型</th>
<th align="left">适用场景</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>Encoder-Only</strong></td>
<td align="left">BERT</td>
<td align="left">文本理解、分类任务</td>
</tr>
<tr>
<td align="left"><strong>Encoder-Decoder</strong></td>
<td align="left">T5</td>
<td align="left">序列到序列任务（翻译、摘要）</td>
</tr>
<tr>
<td align="left"><strong>Decoder-Only</strong></td>
<td align="left">GPT系列</td>
<td align="left">文本生成、对话任务</td>
</tr>
</tbody></table>
<h2 id="LLM训练"><a href="#LLM训练" class="headerlink" title="LLM训练"></a>LLM训练</h2><table>
<thead>
<tr>
<th align="left">阶段</th>
<th align="left">方法</th>
<th align="left">产生的模型类型</th>
<th align="left">特点</th>
</tr>
</thead>
<tbody><tr>
<td align="left">预训练</td>
<td align="left">在大规模数据集上进行无监督或自监督学习，学习通用特征</td>
<td align="left">基础模型 (Base)</td>
<td align="left">通用知识强，但不会对齐</td>
</tr>
<tr>
<td align="left">监督微调</td>
<td align="left">在特定任务数据集上进行监督学习，优化模型性能。</td>
<td align="left">指令微调模型 (SFT)</td>
<td align="left">能理解指令和任务</td>
</tr>
<tr>
<td align="left">对齐与增强</td>
<td align="left">训练奖励模型、用人类反馈做强化学习（如PPO&#x2F;DPO）让模型更符合人类偏好与安全要求</td>
<td align="left">对齐模型 (Chat&#x2F;Aligned)</td>
<td align="left">更安全、更符合人类偏好</td>
</tr>
<tr>
<td align="left">任务&#x2F;领域微调</td>
<td align="left">LoRA&#x2F;PEFT&#x2F;全参微调</td>
<td align="left">专用模型 (Domain&#x2F;Task)</td>
<td align="left">适应行业和特定任务</td>
</tr>
</tbody></table>
<h2 id="大模型权重的存储"><a href="#大模型权重的存储" class="headerlink" title="大模型权重的存储"></a>大模型权重的存储</h2><table>
<thead>
<tr>
<th align="left">格式分类 (常见扩展名)</th>
<th align="left">一句话概括</th>
<th align="left">主要优点</th>
<th align="left">最适合的场景</th>
<th align="left">关键点 &#x2F; 备注</th>
</tr>
</thead>
<tbody><tr>
<td align="left">PyTorch 格式 (.pth, .pt, .bin)</td>
<td align="left">PyTorch框架的“官方标配”，训练和研究时最常用。</td>
<td align="left">灵活，能保存任何东西，在PyTorch生态里用起来最顺手。</td>
<td align="left">模型开发与训练：自己从头开始训练模型或做学术研究。</td>
<td align="left">存在安全风险，加载不可信文件可能执行恶意代码。</td>
</tr>
<tr>
<td align="left">Safetensors 格式 (.safetensors)</td>
<td align="left">Hugging Face推出的“安全卫士”。</td>
<td align="left">绝对安全，加载速度飞快，是未来的趋势。</td>
<td align="left">分享与下载模型：从网上下载别人训练好的模型时，这是首选。</td>
<td align="left">只存储纯粹的权重数据，不包含任何可执行代码。</td>
</tr>
<tr>
<td align="left">量化格式 (.gguf, …q4_0.bin)</td>
<td align="left">模型的“减肥版”，通过降低精度给模型瘦身。</td>
<td align="left">大幅减小模型体积和内存占用，让低功耗设备也能运行。</td>
<td align="left">边缘计算：在手机、树莓派等计算资源非常有限的设备上部署模型。</td>
<td align="left">量化是一种技术，GGUF 是承载这种技术最流行的格式之一。</td>
</tr>
</tbody></table>
<h2 id="大模型精度类型"><a href="#大模型精度类型" class="headerlink" title="大模型精度类型"></a>大模型精度类型</h2><ul>
<li>高精度（如FP32）提供更细腻的数值表示，误差小，适合复杂计算。</li>
<li>低精度（如INT8、4-bit）范围受限，量化误差大，适合简单任务或资源受限场景。</li>
<li>混合精度通过高精度梯度弥补低精度权重的不足，整体精度较高。</li>
</ul>
<table>
<thead>
<tr>
<th align="left">精度类型</th>
<th align="left">描述</th>
<th align="left">优点</th>
<th align="left">缺点</th>
<th align="left">适用场景</th>
<th align="left">精度</th>
<th align="left">数值范围</th>
</tr>
</thead>
<tbody><tr>
<td align="left">FP32（单精度浮点数）</td>
<td align="left">32位浮点数，IEEE 754标准，高精度浮点表示</td>
<td align="left">高精度，广泛硬件支持，适合复杂任务</td>
<td align="left">存储和计算开销大，推理速度慢</td>
<td align="left">科学研究、复杂模型训练</td>
<td align="left">高，约7位十进制精度，误差极小</td>
<td align="left">±1.18×10⁻³⁸ 至 ±3.4×10³⁸</td>
</tr>
<tr>
<td align="left">FP16（半精度浮点数）</td>
<td align="left">16位浮点数，减少一半存储和计算需求</td>
<td align="left">推理速度快，内存占用低，精度损失较小</td>
<td align="left">可能溢出或下溢，需硬件支持</td>
<td align="left">GPU加速推理，内存受限环境</td>
<td align="left">中，约3-4位十进制精度，适合大多数任务</td>
<td align="left">±6.1×10⁻⁵ 至 ±6.55×10⁴</td>
</tr>
<tr>
<td align="left">BF16（Brain Float 16）</td>
<td align="left">16位浮点数，指数范围与FP32相同，精度稍低</td>
<td align="left">保持大动态范围，适合深度学习</td>
<td align="left">精度低于FP16，需专用硬件（如TPU）</td>
<td align="left">深度学习推理与训练，硬件优化</td>
<td align="left">中低，约2-3位十进制精度</td>
<td align="left">±1.18×10⁻³⁸ 至 ±3.4×10³⁸</td>
</tr>
</tbody></table>
<h2 id="⭐️大模型使用"><a href="#⭐️大模型使用" class="headerlink" title="⭐️大模型使用"></a>⭐️大模型使用</h2><h3 id="开源大模型使用"><a href="#开源大模型使用" class="headerlink" title="开源大模型使用"></a>开源大模型使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pip install openai</span><br><span class="line"></span><br><span class="line">Client.chat.completions.create(</span><br><span class="line">  model=os.getenv(<span class="string">&#x27;path&#x27;</span>), </span><br><span class="line">  messages=[</span><br><span class="line">    &#123;<span class="string">&#x27;role&#x27;</span>: <span class="string">&#x27;system&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: <span class="string">&#x27;你是一个小助手&#x27;</span>&#125;, &#123;<span class="string">&#x27;role&#x27;</span>: <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: <span class="string">&#x27;你是谁？&#x27;</span>&#125;])</span><br></pre></td></tr></table></figure>



<h3 id="闭源大模型使用"><a href="#闭源大模型使用" class="headerlink" title="闭源大模型使用"></a>闭源大模型使用</h3><ul>
<li>模型权重 (Model Weights): <code>加载模型本身参数所需的显存。</code><ul>
<li>参数量（B） × 单个参数大小（Bytes）</li>
</ul>
</li>
<li>KV Cache: 存储模型计算过程中的键值对缓存，<code>用于加速生成过程。这是最主要的可变开销。</code><ul>
<li>(batch_size × seq_len × num_layers × 2 × hidden_size) × Bytes_per_element</li>
</ul>
</li>
<li>激活值 (Activations): <code>模型在计算过程中产生的中间张量。</code><ul>
<li>1-2 GB</li>
</ul>
</li>
</ul>
<h2 id="⭐️语言模型的评估指标"><a href="#⭐️语言模型的评估指标" class="headerlink" title="⭐️语言模型的评估指标"></a>⭐️语言模型的评估指标</h2><h3 id="BLEU"><a href="#BLEU" class="headerlink" title="BLEU"></a>BLEU</h3><p>BLEU （双语评估替补）分数是评估一种语言翻译成另一种语言的文本质量的指标。</p>
<p>BLEU 通过计算 <strong>生成文本</strong> 与 <strong>参考文本</strong> 之间 <strong>N-gram 的精确率 (Precision)</strong> 来衡量生成文本的“正确性”和“流畅性”。</p>
<p>计算精确率：</p>
<ul>
<li><p>分别计算candidate句和reference句的N-grams模型，然后统计其匹配的个数，计算精确率。</p>
</li>
<li><p>公式：candidate和reference中匹配的 n−gram 的个数 &#x2F;candidate中n−gram 的个数</p>
</li>
</ul>
<p>计算短句惩罚 (Brevity Penalty, BP)。</p>
<ul>
<li>如果生成文本长度 <code>c</code> 大于等于参考文本长度 <code>r</code>，则 <code>BP=1</code>；否则 $BP &#x3D; exp(1 - \frac rc)$。<code>exp</code>: 自然指数函数 e^x^。本例Candidate 长度 <code>c = 6</code>，Reference 长度 <code>r = 5</code>，因为 <code>c</code> 大于 <code>r</code>，所以 <code>BP = 1</code>。</li>
</ul>
<p>计算最终 BLEU 分数:<br>$$<br>\mathrm{BLEU} &#x3D; \mathrm{BP} \cdot \exp\left( \sum_{n&#x3D;1}^{N} w_{n} \log p_{n} \right)<br>$$</p>
<h3 id="ROUGE"><a href="#ROUGE" class="headerlink" title="ROUGE"></a>ROUGE</h3><p>ROUGE指标是在机器翻译、自动摘要、问答生成等领域常见的评估指标。用于衡量信息覆盖度。ROUGE通过将模型生成的摘要或者回答与参考答案（一般是人工生成的）进行比较计算，得到对应的得分。</p>
<p><strong>核心思想</strong>：与强调“精确性”的 BLEU 不同，ROUGE 关注的是 <strong>召回率 (Recall)</strong> 。它衡量的是，一篇 <strong>“标准答案文本“（Reference） 中的要点</strong> ，有多少被你的 <strong>模型生成的文本（Candidate）给覆盖</strong> 了。</p>
<h3 id="PPL"><a href="#PPL" class="headerlink" title="PPL"></a>PPL</h3><p>困惑度（Perplexity, PPL）用来度量一个概率分布或概率模型预测样本的好坏程度。</p>
<p>PPL基本思想：一个好的语言模型，在预测下一个词时，应该给予正确答案更高的概率。困惑度衡量的就是模型在预测下一个词时的不确定性或“困惑”程度，本质上是对概率分布质量的评估。<strong>困惑度越低，说明模型对句子的预测越有信心，模型性能越好。</strong></p>
<h1 id="LLM架构"><a href="#LLM架构" class="headerlink" title="LLM架构"></a>LLM架构</h1><p>LLM大模型架构的基础为Transformer。</p>
<p>LLM分类一般分为三种：自编码模型（encoder）、自回归模型(decoder)和序列到序列模型(encoder-decoder)。</p>
<h2 id="AE自编码模型"><a href="#AE自编码模型" class="headerlink" title="AE自编码模型"></a>AE自编码模型</h2><p>AE模型，代表作<code>BERT</code>，其特点为：<code>Encoder-Only</code></p>
<p><code>基本原理：是在输入中随机MASK掉一部分单词，根据上下文预测这个词。</code></p>
<p>AE模型通常用于内容理解任务，比如<code>自然语言理解（NLU）</code>中的分类任务：情感分析、提取式问答。</p>
<h3 id="Bert构成"><a href="#Bert构成" class="headerlink" title="Bert构成"></a>Bert构成</h3><ul>
<li>Token Embeddings 是词嵌入张量， 第一个单词是CLS标志， 可以用于之后的分类任务。</li>
<li>Segment Embeddings 是句子分段嵌入张量， 是为了服务后续的两个句子为输入的预训练任务。<code>用来区别两种句子，判断两个句子是否是语义相似。</code></li>
<li>Position Embeddings 是位置编码张量， 此处注意和传统的Transformer不同， 不是三角函数计算的固定位置编码， 而是通过学习得出来的。<code>以告诉模型“第 i 个 token 在序列中的位置”。</code></li>
</ul>
<h3 id="Bert模型通识"><a href="#Bert模型通识" class="headerlink" title="Bert模型通识"></a>Bert模型通识</h3><table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">取值</th>
</tr>
</thead>
<tbody><tr>
<td align="left">transformer 层数</td>
<td align="left">12</td>
</tr>
<tr>
<td align="left">特征维度</td>
<td align="left">768</td>
</tr>
<tr>
<td align="left">transformer head 数</td>
<td align="left">12</td>
</tr>
<tr>
<td align="left">总参数量</td>
<td align="left">1.15 亿</td>
</tr>
</tbody></table>
<h3 id="AE模型总结"><a href="#AE模型总结" class="headerlink" title="AE模型总结"></a>AE模型总结</h3><p>优点：<code>BERT使用双向transformer，在语言理解相关的任务中表现很好。</code></p>
<p>缺点：</p>
<ul>
<li>输入噪声：BERT在预训练过程中使用 [MASK] 符号对输入进行处理，这些符号在下游的finetune任务中永远不会出现，这会导致 <strong>预训练-微调差异</strong> 。而AR模型不会依赖于任何被mask的输入，因此不会遇到这类问题。</li>
<li>更适合用于语言嵌入表达， 语言理解方面的任务， 不适合用于生成式的任务</li>
</ul>
<h2 id="AR自回归模型"><a href="#AR自回归模型" class="headerlink" title="AR自回归模型"></a>AR自回归模型</h2><p>AR模型，代表作GPT，其特点为：<code>Decoder-Only</code></p>
<p>基本原理：<code>从左往右学习的模型，只能利用上文或者下文的信息</code>，比如：AR模型从一系列time steps中学习，并将上一步的结果作为回归模型的输入，以预测下一个time step的值。</p>
<p><code>AR模型通常用于生成式任务(NLG)，在长文本的生成能力很强，比如摘要、翻译或抽象问答。</code></p>
<h3 id="GPT模型构成"><a href="#GPT模型构成" class="headerlink" title="GPT模型构成"></a>GPT模型构成</h3><p>GPT采用单向Transformer模型</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250928175649601.png" alt="image-20250928175649601"></p>
<h3 id="GPT模型的特点"><a href="#GPT模型的特点" class="headerlink" title="GPT模型的特点"></a>GPT模型的特点</h3><p>模型的一些关键参数为：</p>
<table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">取值</th>
</tr>
</thead>
<tbody><tr>
<td align="left">transformer 层数</td>
<td align="left">12</td>
</tr>
<tr>
<td align="left">特征维度</td>
<td align="left">768</td>
</tr>
<tr>
<td align="left">transformer head 数</td>
<td align="left">12</td>
</tr>
<tr>
<td align="left">总参数量</td>
<td align="left">1.17 亿</td>
</tr>
</tbody></table>
<h3 id="AR模型总结"><a href="#AR模型总结" class="headerlink" title="AR模型总结"></a>AR模型总结</h3><p>优点：AR模型擅长生成式NLP任务。AR模型使用注意力机制，预测下一个token，因此自然<code>适用于文本生成</code>。此外，AR模型可以简单地将训练目标设置为预测语料库中的下一个token，因此生成数据相对容易。</p>
<p>缺点：<code>AR模型只能用于前向或者后向建模，不能同时使用双向的上下文信息，不能完全捕捉token的内在联系</code>。</p>
<h2 id="Seq2Seq序列到序列"><a href="#Seq2Seq序列到序列" class="headerlink" title="Seq2Seq序列到序列"></a>Seq2Seq序列到序列</h2><p><code>encoder-decoder模型同时使用编码器和解码器。</code></p>
<p>它将每个task视作序列到序列的转换&#x2F;生成（比如，文本到文本，文本到图像或者图像到文本的多模态任务）。比如对于文本分类任务来说，编码器将文本作为输入，解码器生成文本标签。</p>
<p><code>Encoder-decoder模型通常用于需要内容理解和生成的任务，比如机器翻译。</code></p>
<h3 id="T5模型构成"><a href="#T5模型构成" class="headerlink" title="T5模型构成"></a>T5模型构成</h3><p>T5模型结构与原始的Transformer基本一致，除了做了以下几点改动：</p>
<ul>
<li>作者采用了一种简化版的Layer Normalization，去除了Layer Norm 的bias；将Layer Norm放在残差连接外面。</li>
<li>位置编码：T5使用了一种简化版的相对位置编码，即不使用传统的绝对位置编码，而是采用 相对位置偏置（Relative Position Bias）：在计算注意力分数时，模型会根据两个 token 的相对位置差，将其映射到一个离散的 bucket（距离小的精确映射，距离大的对数分桶），并为每个 bucket 学习一个可训练的偏置参数，然后将该偏置直接加到注意力分数矩阵上，从而让模型在不依赖绝对位置嵌入的情况下捕捉相对位置信息，并具备对长文本的泛化能力。</li>
</ul>
<p>T5 模型在此基础之上引入了相对位置偏置项 $b*_{ij}$，形成新的注意力公式：<br>$$<br>\text{Attention}(Q, K, V) &#x3D; \text{softmax}\left(\frac{QK^T}{\sqrt{d_*k}} + B\right)V<br>$$</p>
<h3 id="T5模型的特点"><a href="#T5模型的特点" class="headerlink" title="T5模型的特点"></a>T5模型的特点</h3><p>模型的一些关键参数为：</p>
<table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">取值</th>
</tr>
</thead>
<tbody><tr>
<td align="left">transformer 层数</td>
<td align="left">24</td>
</tr>
<tr>
<td align="left">特征维度</td>
<td align="left">768</td>
</tr>
<tr>
<td align="left">transformer head 数</td>
<td align="left">12</td>
</tr>
<tr>
<td align="left">总参数量</td>
<td align="left">2.2 亿</td>
</tr>
</tbody></table>
<h3 id="encoder-decoder模型总结"><a href="#encoder-decoder模型总结" class="headerlink" title="encoder-decoder模型总结"></a>encoder-decoder模型总结</h3><p>优点：T5模型可以处理多种NLP任务，并且可以通过微调来适应不同的应用场景，具有良好的可扩展性；相比其他语言生成模型（如GPT-2、GPT-3等），T5模型的参数数量相对较少，训练速度更快，且可以在相对较小的数据集上进行训练。</p>
<p>缺点：由于T5模型使用了大量的Transformer结构，在训练时需要大量的计算资源和时间; 模型的可解释性不足。</p>
<h1 id="提示词工程"><a href="#提示词工程" class="headerlink" title="提示词工程"></a>提示词工程</h1><h2 id="清晰的指令"><a href="#清晰的指令" class="headerlink" title="清晰的指令"></a>清晰的指令</h2><p>注意：<code>要详细的描述、让模型充当某个角色、使用分隔符标明输入的不同部分、提供例子、指定输出长度</code></p>
<ul>
<li>详细的描述：当我们进行模型的提问时，不要描述的太笼统，而是尽量多的提供重要的详细信息或上下文.<ul>
<li>不要直接说，”帮我写一封情书”；而是说：”用一些温柔的话语写一封情书，来表达我对你的仰慕和思念。最后，我要求书写字体数要不低于500个字”</li>
</ul>
</li>
<li>让模型充当某个角色：当我们使用大模型时，可以让模型充当一个角色，这样模型会更专业更明确的对你的问题进行回复.<ul>
<li>我需要你充当一个AI算法面试官的角色，要求你自主的对我进行AI面试过程中常考的面试题，你可以一次说一个问题，然后我回答完，你再出第二道题</li>
</ul>
</li>
<li>使用分隔符标明输入的不同部分：中括号、XML标签、三引号等分隔符可以帮助划分要区别对待的文本，也可以帮助模型更好的理解文本内容。常用’’’’’’把内容框起来<ul>
<li>用20个字符总结由三引号分割的文本。”””文本内容”””</li>
</ul>
</li>
<li>提供例子：本质类似于few-shot leaning。先扔给大模型举例，然后让模型按照例子来输出<ul>
<li>按照这句评论文本的格式：’””用户输入文本””‘，帮我创造新的样本</li>
</ul>
</li>
<li>指定输出长度：可以要求模型生成给定目标长度的输出。目标输出长度可以根据单词、句子、段落、要点等的计数来指定。中文效果不明显，同时你给定的长度只是个大概, 多少个字这种肯定会不精准，但是像多少段这种效果相对较好.<ul>
<li>用三个段落、30个字符概括由三引号分隔的文本。”””文本内容”””文本参考</li>
</ul>
</li>
</ul>
<h2 id="文本参考"><a href="#文本参考" class="headerlink" title="文本参考"></a>文本参考</h2><p>文本参考目的：<code>基于文本文档，辅助大模型问答，降低模型&quot;幻觉&quot;</code>（一本正经的胡说八道）问题。</p>
<ul>
<li>根据下文中三重引号引起来的文章来回答问题。如果在文章中找不到答案，请写“我找不到答案”，不要自己造答案。</li>
</ul>
<h2 id="复杂任务拆分简单子任务"><a href="#复杂任务拆分简单子任务" class="headerlink" title="复杂任务拆分简单子任务"></a>复杂任务拆分简单子任务</h2><p>类似于人工，如果你作为领导，让下属一次性完成一个非常大的事，那么出错的概率是很大的，很多大项目也是这样，你甚至无从下手。大模型也是同样的道理。<code>把复杂的任务给拆为更为简单的子任务，大模型会有更好的表现</code></p>
<p>复杂任务：写一篇关于“人工智能在医学中的应用”的讲稿。</p>
<p> ① 先生成提纲； ② 扩展每一部分内容； ③ 检查逻辑与语言流畅度； ④ 统一风格。</p>
<h2 id="给模型”思考”的时间"><a href="#给模型”思考”的时间" class="headerlink" title="给模型”思考”的时间"></a>给模型”思考”的时间</h2><p>Chain of Thought (思维链) 就是让大模型在输出答案时，不是直接给出结果，而是显式写出推理过程.先一步一步推理，再得出结论</p>
<p><code>给模型思考时间，本质为链式思考（Chain-of-Thought, CoT）。目的，让模型think step by step（一步步思考）.</code></p>
<p>比如，直接问你一道数学应用题，你肯定不能立即回答出问题，但是如果给你时间让你一步步分析并计算，那么你就很有可能把题解出来.</p>
<h2 id="借助外部工具"><a href="#借助外部工具" class="headerlink" title="借助外部工具"></a>借助外部工具</h2><p>在实际应用中，常常会通过 工具调用（Function Calling） 或 插件化扩展 来增强 ChatGPT 的能力。</p>
<ul>
<li><code>联网搜索工具</code>：模型不知道实时信息的问题。<ul>
<li>调用搜索引擎 API，获取最新新闻、论文、股市信息。</li>
</ul>
</li>
<li><code>代码执行工具</code>：需要精确计算或数据处理时，模型自身“算得不准”的问题。<ul>
<li>调用 Python 解释器运行数学计算、绘图、数据分析。</li>
</ul>
</li>
<li><code>数据库 / 知识库工具</code>：模型记忆有限，无法覆盖企业内部数据或特定领域知识。<ul>
<li>知识图谱、向量数据库（如 Milvus、FAISS）来存储和检索信息。</li>
</ul>
</li>
<li><code>外部 API 调用</code>：专业需求，比如天气查询、航班查询、地图导航、医疗工具调用。<ul>
<li>ChatGPT 通过调用天气 API，告诉用户某地实时气温。</li>
</ul>
</li>
</ul>
<h2 id="提示技术"><a href="#提示技术" class="headerlink" title="提示技术"></a>提示技术</h2><p>Zero-shot、Few-shot、Chain-of-Thought (CoT)、Prompt Chaining、ReAct、Self-Consistency、Tree of Thougths(ToT)、Reflexion。</p>
<h3 id="Zero-Shot"><a href="#Zero-Shot" class="headerlink" title="Zero-Shot"></a>Zero-Shot</h3><p>Zero-Shot 提示不提供示例，直接靠模型预训练知识完成任务。像问专家问题，他<code>凭经验</code>直接回答。</p>
<h3 id="Few-Shot"><a href="#Few-Shot" class="headerlink" title="Few-Shot"></a>Few-Shot</h3><p>虽然大型语言模型展示了惊人的Zero-Shot 能力，但它们在更复杂的任务上仍然表现不佳。</p>
<p>Few-Shot提示<code>通过 1-3 个参考示例引导模型理解模式</code>，使模型实现了更好的性能。</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">你是一个翻译专家，请将英文句子翻译成中文。</span></span><br><span class="line"></span><br><span class="line"><span class="attr">示例：</span></span><br><span class="line"><span class="attr">英文：I</span> <span class="string">like apples.</span></span><br><span class="line"><span class="attr">中文：我喜欢苹果。</span></span><br><span class="line"></span><br><span class="line"><span class="attr">现在请翻译：</span></span><br><span class="line"><span class="attr">英文：The</span> <span class="string">weather is nice today.</span></span><br><span class="line"><span class="attr">中文：</span></span><br></pre></td></tr></table></figure>

<h3 id="COT"><a href="#COT" class="headerlink" title="COT"></a>COT</h3><p>Chain-of-Thought 是一种提示技术，<code>通过展示中间推理步骤来解决复杂问题</code>。这种方法可以帮助模型更好得推理和生成答案。可以将其与少样本提示相结合，以获得更好的结果。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250929164921079.png" alt="image-20250929164921079"></p>
<h3 id="Prompt-Chaining"><a href="#Prompt-Chaining" class="headerlink" title="Prompt Chaining"></a>Prompt Chaining</h3><p>为了提高大语言模型的性能使其更可靠，一个重要的提示工程技术是<code>将任务分解为许多子任务。 确定子任务后，将子任务的提示词提供给语言模型，得到的结果作为新的提示词的一部分</code>。 这就是所谓的链式提示（prompt chaining），一个任务被分解为多个子任务，根据子任务创建一系列提示操作。</p>
<p>链式提示可以完成很复杂的任务。LLM 可能无法仅用一个非常详细的提示完成这些任务。在链式提示中，提示链对生成的回应执行转换或其他处理，直到达到期望结果。<code>除了提高性能，链式提示还有助于提高 LLM 应用的透明度，增加控制性和可靠性</code>。这意味着可以更容易地定位模型中的问题，分析并改进需要提高的不同阶段的性能。</p>
<p><strong>抽取关键信息</strong>→<strong>组织要点，转化为摘要草稿</strong>→<strong>优化摘要</strong></p>
<h3 id="ReAct"><a href="#ReAct" class="headerlink" title="ReAct"></a>ReAct</h3><p>ReAct 是一个将推理和行为与 LLMs 相结合通用的范例。ReAct 提示 LLMs 为任务生成口头推理轨迹和操作。这使得系统执行动态推理来创建、维护和调整操作计划，同时还支持与外部环境(例如，Wikipedia)的交互，以将额外信息合并到推理中。</p>
<p>简单来说，ReAct 框架赋予了模型一种“三思而后行”的能力。它将模型的响应过程分解为三个关键部分：</p>
<ul>
<li>Thought (思考): 模型首先会分析当前的任务和已有的信息，进行内在的推理和规划。它会思考“我需要做什么？”、“我缺少什么信息？”、“下一步该怎么办？”。</li>
<li>Act (行动): 根据“思考”的结果，模型会决定并执行一个具体的“行动”。这个行动通常是向外部工具（如搜索引擎、数据库、计算器，甚至是你代码中的“知识库”）发起查询，以获取完成任务所需的额外信息。</li>
<li>Observation (观察): 模型接收并“观察”执行“行动”后返回的结果。这个结果会成为下一步“思考”的新依据。</li>
</ul>
<p>这个 <strong>思考 -&gt; 行动 -&gt; 观察</strong> 的循环会一直持续，直到模型认为自己已经收集到了足够的信息，能够完美地回答用户的问题为止。</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">问题：这个月有几个法定节假日？分别是什么？</span></span><br><span class="line"></span><br><span class="line"><span class="attr">开始ReAct推理流程...</span></span><br><span class="line"></span><br><span class="line"><span class="attr">模型输出（第1步）：</span></span><br><span class="line"><span class="attr">Thought</span>: <span class="string">为了回答这个问题，我首先需要知道当前的日期来确定是哪一个月。然后，我可以查找该月有哪些法定节假日。</span></span><br><span class="line"><span class="attr">Action</span>: <span class="string">get_current_date</span></span><br><span class="line"><span class="attr">Action</span> <span class="string">Input:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">执行工具</span>: <span class="string">get_current_date | 输入: </span></span><br><span class="line"><span class="attr">Observation</span>: <span class="string">2025年9月26日</span></span><br><span class="line"></span><br><span class="line"><span class="attr">模型输出（第2步）：</span></span><br><span class="line"><span class="attr">Thought</span>: <span class="string">现在我知道了当前日期是2025年9月26日，接下来我需要查找2025年9月份有哪些法定节假日。</span></span><br><span class="line"><span class="attr">Action</span>: <span class="string">search_holidays</span></span><br><span class="line"><span class="attr">Action</span> <span class="string">Input: 2025年9月</span></span><br><span class="line"></span><br><span class="line"><span class="attr">执行工具</span>: <span class="string">search_holidays | 输入: 2025年9月</span></span><br><span class="line"><span class="attr">Observation</span>: <span class="string">2025年9月没有法定节假日。</span></span><br><span class="line"></span><br><span class="line"><span class="attr">模型输出（第3步）：</span></span><br><span class="line"><span class="attr">Thought</span>: <span class="string">根据之前的观察结果，2025年9月份没有法定节假日。</span></span><br><span class="line"><span class="attr">Action</span>: <span class="string">Final Answer</span></span><br><span class="line"><span class="attr">Action</span> <span class="string">Input: 2025年9月没有法定节假日。因此，这个月的法定节假日数量为0。</span></span><br><span class="line"></span><br><span class="line"><span class="attr">✅</span> <span class="string">任务完成！最终答案：</span></span><br><span class="line">  <span class="attr">2025年9月没有法定节假日。因此，这个月的法定节假日数量为0。</span></span><br></pre></td></tr></table></figure>



<h3 id="Reflexion"><a href="#Reflexion" class="headerlink" title="Reflexion"></a>Reflexion</h3><p>自我反思是一种 <strong>利用语言反馈来提升语言智能体表现</strong> 的方法。它的核心做法是<code>把环境给出的反馈（可能是自由文本，也可能是数值分数）转化为自然语言形式的“反思”，并作为上下文提供给智能体的下一轮推理</code>。这样，智能体就能更快、更有效地从过去的错误中总结经验，从而在复杂任务上表现得更好。</p>
<p>相对于传统的强化学习（RL）或微调需要很多样本、梯度更新、参数训练，Reflexion提供了一种轻量级、可解释的改进agent输出的方法。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250929173658441.png" alt="image-20250929173658441"></p>
<p>如上图所示，自我反思由三个不同的模型组成：</p>
<ul>
<li>参与者（Actor）：根据状态观测量生成文本和动作。参与者在环境中采取行动并接受观察结果，从而形成轨迹。链式思考（CoT） 和 ReAct 被用作参与者模型。此外，还添加了记忆组件为智能体提供额外的上下文信息。</li>
<li>评估者（Evaluator）：对参与者的输出进行评价。具体来说，它将生成的轨迹（也被称作短期记忆）作为输入并输出奖励分数。根据任务的不同，使用不同的奖励函数（决策任务使用LLM和基于规则的启发式奖励）。</li>
<li>自我反思（Self-Reflection）：生成语言强化线索来帮助参与者实现自我完善。这个角色由大语言模型承担，能够为未来的试验提供宝贵的反馈。自我反思模型利用奖励信号、当前轨迹和其持久记忆生成具体且相关的反馈，并存储在记忆组件中。智能体利用这些经验（存储在长期记忆中）来快速改进决策。</li>
</ul>
<p>总的来说，自我反思的关键步骤是 a)定义任务，b)生成轨迹，c)评估，d)执行自我反思，e)生成下一条轨迹。</p>
<p><strong>特点</strong>：</p>
<p>高质量：修正错误，提升精度。</p>
<p>低成本：不需要修改模型参数，成本低。</p>
<h3 id="Self-Consistency"><a href="#Self-Consistency" class="headerlink" title="Self-Consistency"></a>Self-Consistency</h3><p>在提示词工程里，自我一致性指的是：<code>不是只生成一个答案，而是让大模型生成多个不同的推理路径</code>。然后对这些推理路径的最终答案进行 <strong>投票&#x2F;聚合</strong>，选择出现次数最多或者最合理的答案。这样可以减少“单条思路出错”的风险，提高整体的正确率。</p>
<p><strong>特点：</strong></p>
<p>错误少：多样化推理路径，再进行投票，降低随机错误。</p>
<p>性能强：适合复杂推理任务。</p>
<p>方式一：<strong>在提示词中，直接让模型使用不同的思路生成多种答案，并让模型进行选择</strong>\</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">输入：</span></span><br><span class="line"><span class="attr">你是一个数学老师。请用3种不同的方法来推理这个问题，并分别给出最终答案。</span></span><br><span class="line"><span class="attr">最后请总结：哪个答案出现次数最多，把它作为最终的正确答案输出。</span></span><br><span class="line"><span class="attr">问题如下：</span></span><br><span class="line"><span class="attr">一个商店卖铅笔，每支2元。如果小明有20元，他最多能买多少支铅笔？</span></span><br><span class="line"></span><br><span class="line"><span class="attr">输出：</span></span><br><span class="line"><span class="attr">直接除法：20元</span> <span class="string">÷ 2元/支 = 10支</span></span><br><span class="line"><span class="attr">逐步累加：通过记录每次购买后剩余金额，共可购买10次，剩余0元</span></span><br><span class="line"><span class="attr">画图法：将20元分成10个2元的段，每段代表一支铅笔，共10支</span></span><br><span class="line"><span class="attr">结果：三种方法都得出相同答案：10支</span></span><br></pre></td></tr></table></figure>



<p>方式二：<strong>结合Prompt Chaining，先生成不同的提示词（风格、思路等不同），然后再去分别生成结果再去手动投票</strong></p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Step</span> <span class="string">1: 生成不同思路的提示词</span></span><br><span class="line"></span><br><span class="line"><span class="attr">输入：</span></span><br><span class="line"></span><br><span class="line"><span class="attr">你是一个数学老师。请用3种不同的方法来推理这个问题，只需给出推理思路，不需要解答。思路需要简洁明了，并且合理有效。输出格式为：&#123;&quot;思路一&quot;</span>:<span class="string">&quot;xxx&quot;, &quot;思路二&quot;:&quot;xxx&quot;, &quot;思路三&quot;:&quot;xxx&quot;&#125;</span></span><br><span class="line"><span class="attr">问题如下：</span></span><br><span class="line"><span class="attr">&quot;一个商店卖铅笔，每支2元。如果小明有20元，他最多能买多少支铅笔？&quot;</span></span><br><span class="line"><span class="attr">输出：</span></span><br><span class="line"></span><br><span class="line"><span class="attr">&#123;&quot;思路一&quot;</span>:<span class="string">&quot;用总钱数除以每支铅笔的单价，所得的商即为最多能购买的铅笔数量，忽略余数。&quot;,&quot;思路二&quot;:&quot;从小明的总金额开始，每次减去一支铅笔的价格，重复此过程直到金额不足购买一支，统计减法执行的次数。&quot;,&quot;思路三&quot;:&quot;列出铅笔数量与总花费的对应关系（如1支2元，2支4元……），找到总花费不超过20元的最大数量。&quot;&#125;</span></span><br><span class="line"><span class="attr">Step</span> <span class="string">2: 将每个思路拼接成一个prompt，分别调用大模型得到结果</span></span><br><span class="line"></span><br><span class="line"><span class="attr">输入：</span></span><br><span class="line"></span><br><span class="line"><span class="attr">你是一个数学老师。请用如下的思路来解决这个问题。只输出答案即可。</span></span><br><span class="line"><span class="attr">思路：</span></span><br><span class="line"><span class="attr">&quot;用总钱数除以每支铅笔的单价，所得的商即为最多能购买的铅笔数量，忽略余数。&quot;</span></span><br><span class="line"><span class="attr">问题：</span></span><br><span class="line"><span class="attr">&quot;一个商店卖铅笔，每支2元。如果小明有20元，他最多能买多少支铅笔？&quot;</span></span><br><span class="line"><span class="attr">输出：</span></span><br><span class="line"></span><br><span class="line"><span class="attr">10</span>  <span class="string">10 8</span></span><br><span class="line"><span class="attr">Step</span> <span class="string">3: 根据结果手动投票，选出最终结果</span></span><br><span class="line"></span><br><span class="line"><span class="attr">10</span></span><br><span class="line"><span class="attr">总结：示例1和示例2代表了Self-Consistency的两种常见落地方案。示例1依赖单个提示词让模型一次性生成多种思路并自我投票，优点是实现简单、成本低、速度快，但思路差异有限且投票可能不稳定；示例2通过Prompt</span> <span class="string">Chaining先生成不同提示词再独立求解并人工或程序投票，优点是多样性强、结果更稳健，但实现复杂、成本高、耗时长。</span></span><br></pre></td></tr></table></figure>



<h3 id="Tree-of-Thoughts-ToT"><a href="#Tree-of-Thoughts-ToT" class="headerlink" title="Tree of Thoughts (ToT)"></a>Tree of Thoughts (ToT)</h3><p>ToT 维护着一棵思维树，思维由连贯的语言序列表示，这个序列就是解决问题的中间步骤。使用这种方法，LLM 能够自己对严谨推理过程的中间思维进行评估。LLM 将生成及评估思维的能力与搜索算法（如广度优先搜索和深度优先搜索）相结合，在系统性探索思维的时候可以向前验证和回溯。</p>
<p>核心思想：<strong>不只依赖单一路径，而是构建“思维树”探索多个可能的中间步骤</strong>，通过大模型生成候选方案，再评估每个方案的可行性，最终找到最优解。</p>
<p><strong>特点</strong>：</p>
<p>树状探索：像搜索树一样探索不同思路，而不是单一思路</p>
<p>全局优化：避免陷入单一路径的局限，更容易得到高质量解答</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250930184352545.png" alt="image-20250930184352545"></p>
<p>要点:</p>
<p>动机：解决语言模型在问题求解中的局限性，特别是对于需要探索、策略先见性或初始决策起关键作用的任务。为克服这些挑战，引入一种新的语言模型推理框架”Tree of Thoughts”(ToT)，<code>通过提供具有连贯性的文本单元(&quot;thoughts&quot;)的探索，使语言模型能够进行有意识的决策过程，考虑多个不同的推理路径并自我评估选择以决定下一步行动。</code></p>
<p>方法：提出”Tree of Thoughts”(ToT)框架，通过维护一棵思维树，每个思维是一条连贯的语言序列，作为问题求解的中间步骤，实现语言模型的有意识推理过程。<code>通过与搜索算法(如广度优先搜索或深度优先搜索)结合，允许系统性地探索思维树并进行前瞻和回溯。</code></p>
<p>优势：<code>ToT框架显著提升了语言模型在需要复杂规划或搜索的任务中的问题求解能力</code>。在Game of 24、Creative Writing和Mini Crosswords等任务中，ToT方法的成功率明显高于传统的prompting方法，例如在Game of 24中，使用ToT方法的成功率达到74%。ToT框架提供了一种直观的方式来观察模块，从而增强了模型的可解释性。</p>
<p>Tree of Thoughts”(ToT)框架为语言模型提供了有意识的决策过程，通过探索连贯的文本单元来解决问题，<code>显著提升了语言模型在复杂任务中的问题解决能力。</code></p>
<h1 id="Prompt金融工程项目"><a href="#Prompt金融工程项目" class="headerlink" title="Prompt金融工程项目"></a>Prompt金融工程项目</h1><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>项目任务（三大业务场景）：</p>
<ul>
<li><p>金融文本分类：将金融文本分成不同类型。思路如下</p>
<ul>
<li>需要向模型解释什么叫作「文本分类任务」</li>
<li>需要让模型按照我们指定的格式输出</li>
<li>为了让模型知道什么叫做「文本分类」，我们借用 In-context Learning 的方式，先给模型展示几个正确的例子：</li>
</ul>
</li>
<li><p>金融文本信息抽取：抽取金融文本中的实体。</p>
<ul>
<li>需要向模型解释什么叫作「信息抽取任务」</li>
<li>需要让模型按照我们指定的格式（json）输出</li>
<li>为了让模型知道什么叫做「信息抽取」，我们借用 In-context Learning 的方式，先给模型展示几个正确的例子：</li>
</ul>
</li>
<li><p>金融文本匹配：判断两个金融文本是否类似。</p>
<ul>
<li>需要向模型解释什么叫作「文本匹配任务」</li>
<li>需要让模型按照我们指定的格式输出</li>
<li>为了让模型知道什么叫做「文本匹配任务」，我们借用 In-context Learning 的方式，先给模型展示几个正确的例子：</li>
</ul>
</li>
</ul>
<h2 id="LLM实现金融文本分类"><a href="#LLM实现金融文本分类" class="headerlink" title="LLM实现金融文本分类"></a>LLM实现金融文本分类</h2><h3 id="Few-Shot方式"><a href="#Few-Shot方式" class="headerlink" title="Few-Shot方式"></a>Few-Shot方式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1 导入必备的工具包</span></span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载环境变量</span></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提供所有类别以及每个类别下的样例</span></span><br><span class="line">class_examples = &#123;</span><br><span class="line">    <span class="string">&#x27;新闻报道&#x27;</span>: <span class="string">&#x27;今日，股市经历了一轮震荡，受到宏观经济数据和全球贸易紧张局势的影响。投资者密切关注美联储可能的政策调整，以适应市场的不确定性。&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;财务报告&#x27;</span>: <span class="string">&#x27;本公司年度财务报告显示，去年公司实现了稳步增长的盈利，同时资产负债表呈现强劲的状况。经济环境的稳定和管理层的有效战略执行为公司的健康发展奠定了基础。&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;公司公告&#x27;</span>: <span class="string">&#x27;本公司高兴地宣布成功完成最新一轮并购交易，收购了一家在人工智能领域领先的公司。这一战略举措将有助于扩大我们的业务领域，提高市场竞争力&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;分析师报告&#x27;</span>: <span class="string">&#x27;最新的行业分析报告指出，科技公司的创新将成为未来增长的主要推动力。云计算、人工智能和数字化转型被认为是引领行业发展的关键因素，投资者应关注这些趋势&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line">sentences = [</span><br><span class="line">    <span class="string">&quot;今日，央行发布公告宣布降低利率，以刺激经济增长。这一降息举措将影响贷款利率，并在未来几个季度内对金融市场产生影响。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;ABC公司今日发布公告称，已成功完成对XYZ公司股权的收购交易。本次交易是ABC公司在扩大业务范围、加强市场竞争力方面的重要举措。据悉，此次收购将进一步巩固ABC公司在行业中的地位，并为未来业务发展提供更广阔的发展空间。详情请见公司官方网站公告栏&quot;</span>,</span><br><span class="line">    <span class="string">&quot;公司资产负债表显示，公司偿债能力强劲，现金流充足，为未来投资和扩张提供了坚实的财务基础。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;最新的分析报告指出，可再生能源行业预计将在未来几年经历持续增长，投资者应该关注这一领域的投资机会&quot;</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 构建函数，进行prompt设计（描述清楚任务及输出格式）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_prompts</span>(<span class="params">class_examples</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">     初始化前置prompt，便于模型做 incontext learning。</span></span><br><span class="line"><span class="string">    :param class_examples:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    class_list = <span class="built_in">list</span>(class_examples.keys())</span><br><span class="line">    pre_prompt = <span class="string">f&#x27;你是一个金融专家，需要对输入的金融领域文本进行分析，将类型归类到<span class="subst">&#123;class_list&#125;</span>，对于不在这四类中的数据，输出‘不清楚类型’。以下是几个示例分类：&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> class_type, example <span class="keyword">in</span> class_examples.items():</span><br><span class="line">        pre_prompt = pre_prompt + <span class="string">f&#x27;&quot;&quot;&quot;<span class="subst">&#123;example&#125;</span>是<span class="subst">&#123;class_list&#125;</span>里的什么类别？&quot;&quot;&quot;&#x27;</span> + <span class="string">f&#x27;&quot;&quot;&quot;<span class="subst">&#123;class_type&#125;</span>。&quot;&quot;&quot;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;class_list&#x27;</span>: class_list, <span class="string">&#x27;pre_prompt&#x27;</span>: pre_prompt&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 构建推理函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model_chat</span>(<span class="params">content: <span class="built_in">str</span>, history=[]</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    调用大模型对话接口</span></span><br><span class="line"><span class="string">    :param messages: 输入内容</span></span><br><span class="line"><span class="string">    :param model: 模型名称</span></span><br><span class="line"><span class="string">    :return: 大模型输出内容 str</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    client = openai.OpenAI(</span><br><span class="line">        base_url=os.environ.get(<span class="string">&#x27;DASHSCOPE_BASE_URL&#x27;</span>, <span class="string">&#x27;&#x27;</span>),</span><br><span class="line">        api_key=os.environ.get(<span class="string">&#x27;DASHSCOPE_API_KEY&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    )</span><br><span class="line">    messages = [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: content&#125;]</span><br><span class="line">    response = client.chat.completions.create(</span><br><span class="line">        model=os.environ.get(<span class="string">&#x27;DASHSCOPE_CHAT_MODEL&#x27;</span>, <span class="string">&#x27;&#x27;</span>),</span><br><span class="line">        messages=history + messages,</span><br><span class="line">        stream=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line">    text = response.choices[<span class="number">0</span>].message.content</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 构建后处理函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5 调用推理+后处理():</span></span><br><span class="line">prompts_info = init_prompts(class_examples)</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span><br><span class="line">    content = <span class="string">f&quot;<span class="subst">&#123;prompts_info[<span class="string">&#x27;pre_prompt&#x27;</span>]&#125;</span>&quot;</span>+<span class="string">f&#x27;&quot;&quot;&quot;<span class="subst">&#123;sentence&#125;</span>是 <span class="subst">&#123;prompts_info[<span class="string">&quot;class_list&quot;</span>]&#125;</span> 里的什么类别？&quot;&quot;&quot;&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;content: &quot;</span>, content)</span><br><span class="line">    resp = model_chat(content, history=[])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;class result: &quot;</span>, resp)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="history方式"><a href="#history方式" class="headerlink" title="history方式"></a>history方式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1 导入必备的工具包</span></span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载环境变量</span></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 提供所有类别以及每个类别下的样例</span></span><br><span class="line">class_examples = &#123;</span><br><span class="line">    <span class="string">&#x27;新闻报道&#x27;</span>: <span class="string">&#x27;今日，股市经历了一轮震荡，受到宏观经济数据和全球贸易紧张局势的影响。投资者密切关注美联储可能的政策调整，以适应市场的不确定性。&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;财务报告&#x27;</span>: <span class="string">&#x27;本公司年度财务报告显示，去年公司实现了稳步增长的盈利，同时资产负债表呈现强劲的状况。经济环境的稳定和管理层的有效战略执行为公司的健康发展奠定了基础。&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;公司公告&#x27;</span>: <span class="string">&#x27;本公司高兴地宣布成功完成最新一轮并购交易，收购了一家在人工智能领域领先的公司。这一战略举措将有助于扩大我们的业务领域，提高市场竞争力&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;分析师报告&#x27;</span>: <span class="string">&#x27;最新的行业分析报告指出，科技公司的创新将成为未来增长的主要推动力。云计算、人工智能和数字化转型被认为是引领行业发展的关键因素，投资者应关注这些趋势&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line">sentences = [</span><br><span class="line">    <span class="string">&quot;今日，央行发布公告宣布降低利率，以刺激经济增长。这一降息举措将影响贷款利率，并在未来几个季度内对金融市场产生影响。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;ABC公司今日发布公告称，已成功完成对XYZ公司股权的收购交易。本次交易是ABC公司在扩大业务范围、加强市场竞争力方面的重要举措。据悉，此次收购将进一步巩固ABC公司在行业中的地位，并为未来业务发展提供更广阔的发展空间。详情请见公司官方网站公告栏&quot;</span>,</span><br><span class="line">    <span class="string">&quot;公司资产负债表显示，公司偿债能力强劲，现金流充足，为未来投资和扩张提供了坚实的财务基础。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;最新的分析报告指出，可再生能源行业预计将在未来几年经历持续增长，投资者应该关注这一领域的投资机会&quot;</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 构建函数，进行prompt设计（描述清楚任务及输出格式）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_prompts</span>(<span class="params">class_examples</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">     初始化前置prompt，便于模型做 incontext learning。</span></span><br><span class="line"><span class="string">    :param class_examples:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    class_list = <span class="built_in">list</span>(class_examples.keys())</span><br><span class="line">    pre_history = [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;content&quot;</span>: <span class="string">f&#x27;你是一个金融专家，需要对输入的金融领域文本进行分析，将类型归类到<span class="subst">&#123;class_list&#125;</span>，对于不在这四类中的数据，输出‘不清楚类型’&#x27;</span>&#125;]</span><br><span class="line">    <span class="keyword">for</span> class_type, example <span class="keyword">in</span> class_examples.items():</span><br><span class="line">        <span class="comment"># pre_history.append((f&#x27;&#123;example&#125;是&#123;class_list&#125;里的什么类别？&#x27;,class_type))</span></span><br><span class="line">        pre_history.append(&#123;<span class="string">&#x27;role&#x27;</span>: <span class="string">&quot;user&quot;</span>,</span><br><span class="line">                            <span class="string">&#x27;content&#x27;</span>: <span class="string">f&#x27;<span class="subst">&#123;example&#125;</span>是<span class="subst">&#123;class_list&#125;</span>里的什么类别？&#x27;</span></span><br><span class="line">                            &#125;)</span><br><span class="line">        pre_history.append(&#123;<span class="string">&#x27;role&#x27;</span>: <span class="string">&#x27;assistant&#x27;</span>,</span><br><span class="line">                            <span class="string">&#x27;content&#x27;</span>: class_type</span><br><span class="line">                            &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;class_list&#x27;</span>: class_list, <span class="string">&#x27;pre_history&#x27;</span>: pre_history&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 构建推理函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model_chat</span>(<span class="params">content: <span class="built_in">str</span>, history=[]</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    调用大模型对话接口</span></span><br><span class="line"><span class="string">    :param messages: 输入内容</span></span><br><span class="line"><span class="string">    :param model: 模型名称</span></span><br><span class="line"><span class="string">    :return: 大模型输出内容 str</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    client = openai.OpenAI(</span><br><span class="line">        base_url=os.environ.get(<span class="string">&#x27;DASHSCOPE_BASE_URL&#x27;</span>, <span class="string">&#x27;&#x27;</span>),</span><br><span class="line">        api_key=os.environ.get(<span class="string">&#x27;DASHSCOPE_API_KEY&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    )</span><br><span class="line">    messages = [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: content&#125;]</span><br><span class="line">    response = client.chat.completions.create(</span><br><span class="line">        model=os.environ.get(<span class="string">&#x27;DASHSCOPE_CHAT_MODEL&#x27;</span>, <span class="string">&#x27;&#x27;</span>),</span><br><span class="line">        messages=history + messages,</span><br><span class="line">        stream=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line">    text = response.choices[<span class="number">0</span>].message.content</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 构建后处理函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5 调用推理+后处理():</span></span><br><span class="line">prompts_info = init_prompts(class_examples)</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span><br><span class="line">    content = <span class="string">f&quot;“<span class="subst">&#123;sentence&#125;</span>”是 <span class="subst">&#123;prompts_info[<span class="string">&#x27;class_list&#x27;</span>]&#125;</span> 里的什么类别？&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;content : &quot;</span>, content)</span><br><span class="line">    resp = model_chat(content, history=prompts_info[<span class="string">&#x27;pre_history&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;class result : &quot;</span>, resp)</span><br></pre></td></tr></table></figure>

<h2 id="LLM实现金融文本信息抽取"><a href="#LLM实现金融文本信息抽取" class="headerlink" title="LLM实现金融文本信息抽取"></a>LLM实现金融文本信息抽取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1导入必备的工具包</span></span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载环境变量</span></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义不同实体下的具备属性</span></span><br><span class="line">schema = &#123;</span><br><span class="line">    <span class="string">&#x27;金融&#x27;</span>: [<span class="string">&#x27;日期&#x27;</span>, <span class="string">&#x27;股票名称&#x27;</span>, <span class="string">&#x27;开盘价&#x27;</span>, <span class="string">&#x27;收盘价&#x27;</span>, <span class="string">&#x27;成交量&#x27;</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">IE_PATTERN = <span class="string">&quot;&#123;&#125;\n\n提取上述句子中&#123;&#125;的实体，并按照JSON格式输出，上述句子中不存在的信息用[&#x27;原文中未提及&#x27;]来表示，多个值之间用&#x27;,&#x27;分隔。&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提供一些例子供模型参考</span></span><br><span class="line">ie_examples = &#123;</span><br><span class="line">    <span class="string">&#x27;金融&#x27;</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&#x27;content&#x27;</span>: <span class="string">&#x27;2023-01-10，股市震荡。股票古哥-D[EOOE]美股今日开盘价100美元，一度飙升至105美元，随后回落至98美元，最终以102美元收盘，成交量达到520000。&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;answers&#x27;</span>: &#123;</span><br><span class="line">                <span class="string">&#x27;日期&#x27;</span>: [<span class="string">&#x27;2023-01-10&#x27;</span>],</span><br><span class="line">                <span class="string">&#x27;股票名称&#x27;</span>: [<span class="string">&#x27;古哥-D[EOOE]美股&#x27;</span>],</span><br><span class="line">                <span class="string">&#x27;开盘价&#x27;</span>: [<span class="string">&#x27;100美元&#x27;</span>],</span><br><span class="line">                <span class="string">&#x27;收盘价&#x27;</span>: [<span class="string">&#x27;102美元&#x27;</span>],</span><br><span class="line">                <span class="string">&#x27;成交量&#x27;</span>: [<span class="string">&#x27;520000&#x27;</span>],</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sentences = [</span><br><span class="line">    <span class="string">&#x27;2023-02-15，寓意吉祥的节日，股票佰笃[BD]美股开盘价10美元，虽然经历了波动，但最终以13美元收盘，成交量微幅增加至460,000，投资者情绪较为平稳。&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;2023-04-05，市场迎来轻松氛围，股票盘古(0021)开盘价23元，尽管经历了波动，但最终以26美元收盘，成交量缩小至310,000，投资者保持观望态度。&#x27;</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 构建函数，进行prompt设计（描述清楚任务及输出格式）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_prompt</span>(<span class="params">ie_examples</span>):</span><br><span class="line">    history_list = [&#123;<span class="string">&#x27;role&#x27;</span>: <span class="string">&#x27;system&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: <span class="string">&quot;你是信息提取专家，需要需要完成信息抽取任务。我会给你一个句子，你需要提取句子中的实体，并按照JSON格式输出，如果句子中有不存在的信息用[&#x27;原文中未提及&#x27;]来表示，多个值之间用&#x27;,&#x27;分隔。&quot;</span>&#125;]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历示例，将样本和实体 添加到history_list中</span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">type</span>, example_list <span class="keyword">in</span> ie_examples.items():  <span class="comment"># 获取到金融类型</span></span><br><span class="line">        <span class="keyword">for</span> example <span class="keyword">in</span> example_list:  <span class="comment"># 遍历每个样本</span></span><br><span class="line">            sentence = example[<span class="string">&#x27;content&#x27;</span>]</span><br><span class="line">            entity_type = <span class="string">&#x27;，&#x27;</span>.join(schema[<span class="built_in">type</span>])  <span class="comment"># 获取到金融类型需要抽取的实体</span></span><br><span class="line">            history_list.append(&#123;<span class="string">&#x27;role&#x27;</span>: <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: IE_PATTERN.<span class="built_in">format</span>(sentence, entity_type)&#125;)</span><br><span class="line">            history_list.append(&#123;<span class="string">&#x27;role&#x27;</span>: <span class="string">&#x27;assistant&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: json.dumps(example[<span class="string">&#x27;answers&#x27;</span>], ensure_ascii=<span class="literal">False</span>)&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(f&#x27;history_list--&gt;&#123;history_list&#125;&#x27;)</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;history_list&#x27;</span>: history_list&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 构建推理函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model_chat</span>(<span class="params">content: <span class="built_in">str</span>, history=[]</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    调用大模型对话接口</span></span><br><span class="line"><span class="string">    :param messages: 输入内容</span></span><br><span class="line"><span class="string">    :param model: 模型名称</span></span><br><span class="line"><span class="string">    :return: 大模型输出内容 str</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    client = openai.OpenAI(</span><br><span class="line">        base_url=os.environ.get(<span class="string">&#x27;DASHSCOPE_BASE_URL&#x27;</span>, <span class="string">&#x27;&#x27;</span>),</span><br><span class="line">        api_key=os.environ.get(<span class="string">&#x27;DASHSCOPE_API_KEY&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    )</span><br><span class="line">    messages = [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: content&#125;]</span><br><span class="line">    response = client.chat.completions.create(</span><br><span class="line">        model=os.environ.get(<span class="string">&#x27;DASHSCOPE_CHAT_MODEL&#x27;</span>, <span class="string">&#x27;&#x27;</span>),</span><br><span class="line">        messages=history + messages,</span><br><span class="line">        stream=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line">    text = response.choices[<span class="number">0</span>].message.content</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 构建后处理函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clean_response</span>(<span class="params">response: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="comment"># 将模型输出进行清理, 将 ```json清理掉</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;```json&#x27;</span> <span class="keyword">in</span> response:</span><br><span class="line">        response = response.split(<span class="string">&#x27;```json&#x27;</span>)[<span class="number">1</span>].split(<span class="string">&#x27;```&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># print(&#x27;response--&gt;&#x27;, response)</span></span><br><span class="line">        <span class="comment"># 将这个json字符串转为字典</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">return</span> json.loads(response)  <span class="comment"># 防止这步转换出错，使用try except</span></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">return</span> response</span><br><span class="line">    <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5 调用推理+后处理</span></span><br><span class="line">prompt_dict = build_prompt(ie_examples)</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;sentence--&gt;<span class="subst">&#123;sentence&#125;</span>&#x27;</span>)</span><br><span class="line">    result = model_chat(sentence, prompt_dict[<span class="string">&#x27;history_list&#x27;</span>])</span><br><span class="line">    final_result = clean_response(result)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;final_result--&gt;<span class="subst">&#123;final_result&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="LLM实现金融文本匹配"><a href="#LLM实现金融文本匹配" class="headerlink" title="LLM实现金融文本匹配"></a>LLM实现金融文本匹配</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1 导入必备的工具包</span></span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载环境变量</span></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提供相似，不相似的语义匹配例子</span></span><br><span class="line">examples = &#123;</span><br><span class="line">    <span class="string">&#x27;是&#x27;</span>: [</span><br><span class="line">        (<span class="string">&#x27;公司ABC发布了季度财报，显示盈利增长。&#x27;</span>, <span class="string">&#x27;财报披露，公司ABC利润上升。&#x27;</span>),</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">&#x27;不是&#x27;</span>: [</span><br><span class="line">        (<span class="string">&#x27;黄金价格下跌，投资者抛售。&#x27;</span>, <span class="string">&#x27;外汇市场交易额创下新高。&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;央行降息，刺激经济增长。&#x27;</span>, <span class="string">&#x27;新能源技术的创新。&#x27;</span>)</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sentence_pairs = [</span><br><span class="line">    (<span class="string">&#x27;股票市场今日大涨，投资者乐观。&#x27;</span>, <span class="string">&#x27;持续上涨的市场让投资者感到满意。&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;油价大幅下跌，能源公司面临挑战。&#x27;</span>, <span class="string">&#x27;未来智能城市的建设趋势愈发明显。&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;利率上升，影响房地产市场。&#x27;</span>, <span class="string">&#x27;高利率对房地产有一定冲击。&#x27;</span>),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 构建函数，进行prompt设计（描述清楚任务及输出格式）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_prompts</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    初始化前置prompt，便于模型做 incontext learning。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    pre_history = [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;content&quot;</span>: <span class="string">&#x27;现在你需要帮助我完成文本匹配任务，当我给你两个句子时，你需要回答我这两句话语义是否相似。只需要回答是否相似，不要做多余的回答。&#x27;</span>&#125;</span><br><span class="line">                   ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key, sentence_pairs <span class="keyword">in</span> examples.items():</span><br><span class="line">        <span class="keyword">for</span> sentence_pair <span class="keyword">in</span> sentence_pairs:</span><br><span class="line">            sentence1, sentence2 = sentence_pair</span><br><span class="line">            pre_history.append(&#123;</span><br><span class="line">                <span class="string">&quot;role&quot;</span>: <span class="string">&#x27;user&#x27;</span>,</span><br><span class="line">                <span class="string">&quot;content&quot;</span>: <span class="string">f&#x27;句子一: <span class="subst">&#123;sentence1&#125;</span>\n句子二: <span class="subst">&#123;sentence2&#125;</span>\n上面两句话是相似的语义吗？&#x27;</span></span><br><span class="line">            &#125;)</span><br><span class="line">            pre_history.append(&#123;</span><br><span class="line">                <span class="string">&quot;role&quot;</span>: <span class="string">&#x27;assistant&#x27;</span>,</span><br><span class="line">                <span class="string">&quot;content&quot;</span>: key</span><br><span class="line">            &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;pre_history&#x27;</span>: pre_history&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 构建推理函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model_chat</span>(<span class="params">content: <span class="built_in">str</span>, history=[]</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    调用大模型对话接口</span></span><br><span class="line"><span class="string">    :param messages: 输入内容</span></span><br><span class="line"><span class="string">    :param model: 模型名称</span></span><br><span class="line"><span class="string">    :return: 大模型输出内容 str</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    client = openai.OpenAI(</span><br><span class="line">        base_url=os.environ.get(<span class="string">&#x27;DASHSCOPE_BASE_URL&#x27;</span>, <span class="string">&#x27;&#x27;</span>),</span><br><span class="line">        api_key=os.environ.get(<span class="string">&#x27;DASHSCOPE_API_KEY&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    )</span><br><span class="line">    messages = [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: content&#125;]</span><br><span class="line">    response = client.chat.completions.create(</span><br><span class="line">        model=os.environ.get(<span class="string">&#x27;DASHSCOPE_CHAT_MODEL&#x27;</span>, <span class="string">&#x27;&#x27;</span>),</span><br><span class="line">        messages=history + messages,</span><br><span class="line">        stream=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line">    text = response.choices[<span class="number">0</span>].message.content</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 构建后处理函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5 调用推理+后处理</span></span><br><span class="line">prompts_info = init_prompts(examples)</span><br><span class="line"><span class="keyword">for</span> sentence_pair <span class="keyword">in</span> sentence_pairs:</span><br><span class="line">    sentence1, sentence2 = sentence_pair</span><br><span class="line">    sentence_with_prompt = <span class="string">f&#x27;句子一: <span class="subst">&#123;sentence1&#125;</span>\n句子二: <span class="subst">&#123;sentence2&#125;</span>\n上面两句话是相似的语义吗？&#x27;</span></span><br><span class="line">    result=model_chat(sentence_with_prompt, history=prompts_info[<span class="string">&#x27;pre_history&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;sentence_pair--&gt;<span class="subst">&#123;sentence_pair&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;result--&gt;<span class="subst">&#123;result&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h1 id="Day01"><a href="#Day01" class="headerlink" title="Day01"></a>Day01</h1><h2 id="1、什么是RAG？有哪些作用？"><a href="#1、什么是RAG？有哪些作用？" class="headerlink" title="1、什么是RAG？有哪些作用？"></a>1、什么是RAG？有哪些作用？</h2><p><code>RAG是一种将大规模语言模型（LLM）与外部知识源的检索相结合，以改进问答能力的工程框架。</code></p>
<p>作用：<strong>缓解LLM“幻觉”问题</strong> ： LLM在生成文本时有时会“一本正经地胡说八道”，即生成听起来合理但实际上不准确或捏造的信息，这被称为“幻觉”。RAG通过提供外部事实依据，显著减少了这种幻觉现象，让LLM的输出更具 <strong>事实性</strong> 和 <strong>可靠性</strong> 。</p>
<p><strong>获取最新信息</strong> ：LLM的训练数据通常是静态的，这意味着它们无法获取到训练截止日期之后发生的事件或更新的信息。RAG允许LLM连接到实时或定期更新的外部数据源（如新闻、数据库、内部文档等），从而提供 <strong>最新、最及时</strong> 的答案。</p>
<p><strong>领域特定知识增强</strong> ：对于特定行业或企业内部的知识，LLM的通用训练数据往往不足。RAG能够将LLM与企业内部的知识库文档或特定领域的数据连接起来，使LLM能够回答高度专业化的问题，并提供 <strong>更符合上下文的答案</strong> 。</p>
<p><strong>降低模型微调成本</strong> ：传统上，为了让LLM适应特定任务或数据，需要进行昂贵的 <strong>微调（Fine-tuning）</strong>  。RAG提供了一种更<strong>经济高效</strong>的替代方案，它无需修改LLM的底层参数，只需更新外部知识库即可，大大降低了维护和更新模型的成本。</p>
<p><strong>提高答案的可解释性和溯源性</strong> ：RAG可以引用其获取信息的来源，这意味着用户可以查看LLM答案所依据的原始文档或数据，增强了答案的 <strong>透明度</strong> 和 <strong>用户信任度</strong> 。</p>
<h2 id="2、RAG标准流程是什么？"><a href="#2、RAG标准流程是什么？" class="headerlink" title="2、RAG标准流程是什么？"></a>2、RAG标准流程是什么？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20251010085034021.png" alt="image-20251010085034021"></p>
<p>RAG 标准流程由索引（Indexing）、检索（Retriever）和生成（Generation）三个核心阶段组成。</p>
<ul>
<li><p>索引阶段，通过处理多种来源多种格式的文档提取其中文本，将其切分为标准长度的文本块（chunk），并进行嵌入向量化（embedding），向量存储在向量数据库（vector database）中。</p>
<ul>
<li><p>加载文件</p>
</li>
<li><p>内容提取</p>
</li>
<li><p>文本分割 ，形成chunk</p>
</li>
<li><p>文本向量化</p>
</li>
<li><p>将向量存储到向量数据库</p>
</li>
</ul>
</li>
<li><p>检索阶段，用户输入的查询（query）被转化为向量表示，通过相似度匹配从向量数据库中检索出最相关的文本块。</p>
<ul>
<li>query向量化</li>
<li>在文本向量中匹配出与问句向量相似的top_k个</li>
</ul>
</li>
<li><p>生成阶段，检索到的相关文本与原始查询共同构成提示词（Prompt），输入大语言模型（LLM），生成精确且具备上下文关联的回答。</p>
<ul>
<li>匹配出的文本作为上下文和问题一起添加到prompt中</li>
<li>提交给LLM生成答案</li>
</ul>
</li>
</ul>
<h2 id="3、EduRAG的项目流程是怎样的？"><a href="#3、EduRAG的项目流程是怎样的？" class="headerlink" title="3、EduRAG的项目流程是怎样的？"></a>3、EduRAG的项目流程是怎样的？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20251009222655839.png" alt="image-20251009222655839"></p>
<h2 id="4、LangChain中有哪些主要组件，都是用来干什么的？"><a href="#4、LangChain中有哪些主要组件，都是用来干什么的？" class="headerlink" title="4、LangChain中有哪些主要组件，都是用来干什么的？"></a>4、LangChain中有哪些主要组件，都是用来干什么的？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20251009222805521.png" alt="image-20251009222805521"></p>
<ul>
<li>Models：模型，各种类型的模型和模型集成，比如GPT-4</li>
<li>Prompts：提示，包括提示管理、提示优化和提示序列化</li>
<li>Memory：记忆，用来保存和模型交互时的上下文状态</li>
<li>Indexes：索引，用来结构化文档，以便和模型交互</li>
<li>Chains：链，一系列对各种组件的调用</li>
<li>Agents：代理，决定模型采取哪些行动，执行并且观察流程，直到完成为止</li>
</ul>
<h2 id="5、目前LangChain支持哪三种类型的模型？"><a href="#5、目前LangChain支持哪三种类型的模型？" class="headerlink" title="5、目前LangChain支持哪三种类型的模型？"></a>5、目前LangChain支持哪三种类型的模型？</h2><ul>
<li><p>LLMs：大语言模型接收文本字符作为输入，返回的也是文本字符。</p>
<ul>
<li>可以使用OpenAI的形式进行调用；或者langchain社区提供的专用调用方式</li>
</ul>
</li>
<li><p>聊天模型：基于LLMs, 不同的是它接收聊天消(一种特定格式的数据)作为输入，返回的也是聊天消息。</p>
<ul>
<li>AIMessage、HumanMessage、SystemMessage</li>
<li>ChatMessage: Chat 消息可以接受任意角色的参数，但是在大多数时间，我们应该使用上面的三种类型</li>
</ul>
</li>
<li><p>文本嵌入模型：文本嵌入模型接收文本作为输入, 返回的是浮点数列表。</p>
<ul>
<li>每个句子或段落最终得到的是一个固定维度的一维列表</li>
</ul>
</li>
</ul>
<h2 id="6、Prompts组件的使用方式是怎样的？"><a href="#6、Prompts组件的使用方式是怎样的？" class="headerlink" title="6、Prompts组件的使用方式是怎样的？"></a>6、Prompts组件的使用方式是怎样的？</h2><ul>
<li><p>使用 PromptTemplate 或 ChatPromptTemplate 来定义一个带占位符的模板，</p>
</li>
<li><p>通过 .format() 或 .format_messages() 方法填充具体内容，得到最终的提示词</p>
</li>
<li><p>将提示词送入大模型得到预测结果</p>
</li>
</ul>
<h2 id="7、Chains组件使用方式是怎样的？"><a href="#7、Chains组件使用方式是怎样的？" class="headerlink" title="7、Chains组件使用方式是怎样的？"></a>7、Chains组件使用方式是怎样的？</h2><p>使用LLMChain将多个组件组装起来，或者使用SimpleSequentialChain将多个链组装起来，然后调用这个链即可。</p>
<p>或者使用LCEL的方式，使用|符号将不同的组件连接起来，形成一个链式结构。<code>【注意：（1）先后顺序（2）上一个组件的输出类型和下一个组件的输入类型要保持一致】</code></p>
<h2 id="8、常用的Output-Parsers有哪些？"><a href="#8、常用的Output-Parsers有哪些？" class="headerlink" title="8、常用的Output Parsers有哪些？"></a>8、常用的Output Parsers有哪些？</h2><table>
<thead>
<tr>
<th align="left">解析器名称</th>
<th align="left">核心功能</th>
<th align="left">输出的 Python 类型</th>
<th align="left">工业级应用场景</th>
</tr>
</thead>
<tbody><tr>
<td align="left">StrOutputParser</td>
<td align="left">默认解析器。将 LLM 的输出直接解析为字符串。</td>
<td align="left">str</td>
<td align="left">只需要原始回答时（如问答任务、对话场景）</td>
</tr>
<tr>
<td align="left">CommaSeparatedListParser</td>
<td align="left">将 LLM 输出的、用逗号分隔的文本解析为列表。</td>
<td align="left">list[str]</td>
<td align="left">列表&#x2F;枚举型输出</td>
</tr>
<tr>
<td align="left">JsonOutputParser</td>
<td align="left">极其常用。将 LLM 输出的 JSON 字符串解析为 Python 字典。</td>
<td align="left">dict</td>
<td align="left">结构化 JSON 输出</td>
</tr>
<tr>
<td align="left">PydanticOutputParser</td>
<td align="left">极其常用。将 LLM 输出解析为预先定义的 Pydantic 对象，提供类型安全和数据验证。</td>
<td align="left">自定义的 pydantic.BaseModel 对象</td>
<td align="left">输出需要严格结构化（JSON-like）数据时</td>
</tr>
<tr>
<td align="left">DatetimeOutputParser</td>
<td align="left">从文本中智能地解析出日期和时间信息。</td>
<td align="left">datetime.datetime</td>
<td align="left">需要时间格式时</td>
</tr>
</tbody></table>
<h1 id="Day02"><a href="#Day02" class="headerlink" title="Day02"></a>Day02</h1><h2 id="1、如何使用LangChain中Memory组件来记录上下文？"><a href="#1、如何使用LangChain中Memory组件来记录上下文？" class="headerlink" title="1、如何使用LangChain中Memory组件来记录上下文？"></a>1、如何使用LangChain中Memory组件来记录上下文？</h2><p>方法一：使用 <strong>ChatMessageHistory</strong> 手动添加上下文</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">history = ChatMessageHistory()</span><br><span class="line">history.add_user_message(<span class="string">&quot;你好&quot;</span>)  <span class="comment"># 添加用户消息</span></span><br><span class="line">history.add_ai_message(<span class="string">&quot;您好&quot;</span>)  <span class="comment"># 添加AI消息</span></span><br></pre></td></tr></table></figure>

<p>方法二：使用 <strong>ConversationChain</strong> 自动保存用户和AI的历史交互内容，作为后续回复的上下文</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conversation = ConversationChain(llm=model)</span><br><span class="line">result1 = conversation.predict(<span class="built_in">input</span>=<span class="string">&quot;小明有1只猫&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="2、LangChain中常用的文档加载器有哪些？"><a href="#2、LangChain中常用的文档加载器有哪些？" class="headerlink" title="2、LangChain中常用的文档加载器有哪些？"></a>2、LangChain中常用的文档加载器有哪些？</h2><p><a target="_blank" rel="noopener" href="https://python.langchain.com/v0.2/docs/integrations/document_loaders/">https://python.langchain.com/v0.2/docs/integrations/document_loaders/</a></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20251010205153081.png" alt="image-20251010205153081"></p>
<h2 id="3、常用的文档分割器有哪些？"><a href="#3、常用的文档分割器有哪些？" class="headerlink" title="3、常用的文档分割器有哪些？"></a>3、常用的文档分割器有哪些？</h2><ul>
<li><p>递归字符文本分割器（RecursiveCharacterTextSplitter）</p>
<ul>
<li>方法：首先尝试使用第一个分隔符（如 “\n\n”）分割文本，如果分割后的块仍然过大，则使用下一个分隔符继续分割，重复此过程，直到达到指定的 chunk_size 或用完所有分隔符。</li>
<li>优点：比简单的字符分割更能保持语义完整性。</li>
</ul>
</li>
<li><p>语义文档分割器（SemanticChunker）</p>
<ul>
<li>方法：基于语义相似性分割文本——先将文本分成语句，然后对语句进行嵌入，再对嵌入的语句计算相似度，相似度差异大的切分成块。</li>
<li>优点：能够更好地保持语义完整性。</li>
</ul>
</li>
<li><p>MarkdownHeaderTextSplitter（Markdown文档切割器）</p>
<ul>
<li>方法：按照Markdown的标题进行拆分。</li>
<li>优点：基于原始结构进行拆分，语义切分完整。</li>
</ul>
</li>
</ul>
<h2 id="4、VectorStores是什么？"><a href="#4、VectorStores是什么？" class="headerlink" title="4、VectorStores是什么？"></a>4、VectorStores是什么？</h2><p>VectorStores是一种特殊类型的数据库，它的作用是存储由嵌入创建的向量，提供相似查询等功能。</p>
<p>常用的有Faiss、Chroma、Milvus等</p>
<h2 id="5、什么是检索器？"><a href="#5、什么是检索器？" class="headerlink" title="5、什么是检索器？"></a>5、什么是检索器？</h2><p>检索器是 LangChain 中负责信息检索的模块，通常与 索引（Indexes） 模块（如向量存储、嵌入模型）结合使用。它的核心功能是：</p>
<ul>
<li>输入：接收用户查询（通常是文本）。</li>
<li>处理：根据查询从数据源中检索相关内容。</li>
<li>输出：返回一组相关文档或文本片段（通常是 Document 对象列表）。</li>
</ul>
<h2 id="6、检索器工作流程是什么？"><a href="#6、检索器工作流程是什么？" class="headerlink" title="6、检索器工作流程是什么？"></a>6、检索器工作流程是什么？</h2><p>工作流程可以分为以下步骤：</p>
<ul>
<li>查询嵌入：将用户查询通过嵌入模型（如 OpenAIEmbeddings）转为向量表示</li>
<li>相似性搜索：在向量存储中查找与查询向量最相似的文档向量。</li>
<li>文档返回：返回匹配的文档（包含内容、元数据等）。</li>
<li>后处理（可选）：对检索结果进行排序、过滤或重新排名。</li>
</ul>
<h2 id="7、常见的检索器类型有哪些？"><a href="#7、常见的检索器类型有哪些？" class="headerlink" title="7、常见的检索器类型有哪些？"></a>7、常见的检索器类型有哪些？</h2><p><code>（1）向量检索器VectorStoreRetriever</code></p>
<p>功能：基于向量数据库来完成相似文档检索</p>
<p>使用方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">retriever = vector_store.as_retriever(</span><br><span class="line">    search_type=<span class="string">&quot;similarity&quot;</span>,  <span class="comment"># 可选 &quot;similarity&quot;|&quot;mmr&quot;|&quot;similarity_score_threshold&quot;</span></span><br><span class="line">    search_kwargs=&#123;</span><br><span class="line">        <span class="string">&quot;k&quot;</span>: <span class="number">5</span>,  <span class="comment"># 返回结果数量</span></span><br><span class="line">        <span class="string">&quot;score_threshold&quot;</span>: <span class="number">0.7</span>,  <span class="comment"># 仅当search_type=&quot;similarity_score_threshold&quot;时有效,低于阈值的都丢弃。</span></span><br><span class="line">        <span class="string">&quot;filter&quot;</span>: &#123;<span class="string">&quot;source&quot;</span>: <span class="string">&quot;重要文档.pdf&quot;</span>&#125;,  <span class="comment"># 元数据过滤，只会检索满足条件的文档。</span></span><br><span class="line">        <span class="string">&quot;lambda_mult&quot;</span>: <span class="number">0.25</span>  <span class="comment"># 仅MMR搜索有效(控制多样性)：接近 0 则更强调和查询的相关性；接近 1 则更强调结果之间的差异性</span></span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20251010210821396.png" alt="image-20251010210821396"></p>
<p><code>（2）TFIDFRetriever/BM25Retriever</code></p>
<p>作用：基于词频和逆文档频率进行检索，使用的算法为TF-IDF或BM25。</p>
<p>特点：适合快速构建原型，但不支持语义搜索。</p>
<p><strong>（3）多查询检索器MultiQueryRetriever</strong></p>
<p>作用：它借助 LLM 自动生成多个语义等价的改写查询，把用户的问题扩展成多个角度，然后对每个改写进行检索，最后合并结果，以提高召回率。 </p>
<p>特点：提高召回率和精准度，适合复杂查询</p>
<p><code>（4）集成检索器EnsembleRetriever</code></p>
<p>作用：结合多种检索器（如 BM25 和向量存储），融合结果。</p>
<p>特点：提高召回率和精准度，适合复杂查询</p>
<p><strong>（5）压缩检索器ContextualCompressionRetriever</strong></p>
<p>作用：对检索结果进行压缩，提取最相关的内容</p>
<p>特点：使用语言模型对检索到的文档进行重新排序或精炼，减少无关内容，提高结果质量</p>
<p><strong>（6）自定义检索器CustomRetriever</strong></p>
<p>作用：开发者可以自定义检索逻辑，适配特定数据源或算法 </p>
<p>特点：支持任意场景</p>
<h2 id="8、什么是BM25算法？有什么特点？"><a href="#8、什么是BM25算法？有什么特点？" class="headerlink" title="8、什么是BM25算法？有什么特点？"></a>8、什么是BM25算法？有什么特点？</h2><p>BM25 是对 TF-IDF 的改进版本，在 TF-IDF 基础上做了归一化（防止长文档优势）与饱和控制（防止词频无限增长）。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20251010211721443.png" alt="image-20251010211721443"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20251012083946067.png" alt="image-20251012083946067"></p>
<h2 id="9、Agent是什么？"><a href="#9、Agent是什么？" class="headerlink" title="9、Agent是什么？"></a>9、Agent是什么？</h2><p>从大模型的角度来看，Agent其实就是基于大模型的语义理解和推理能力，让大模型拥有解决复杂问题时的任务规划能力，并调用外部工具来执行各种任务，并且能够保留“记忆”的一个智能体。</p>
<blockquote>
<p>Agent &#x3D; 大模型 + 任务规划（Planning） + 使用外部工具执行任务（Tools&amp;Action） + 记忆（Memory）</p>
</blockquote>
<h2 id="10、Milvus-是什么？"><a href="#10、Milvus-是什么？" class="headerlink" title="10、Milvus 是什么？"></a>10、Milvus 是什么？</h2><p>Milvus 是一款开源的向量数据库（2019年提出），其唯一目标是存储、索引和管理大规模嵌入向量。</p>
<h2 id="11、Milvus中核心的概念有什么？"><a href="#11、Milvus中核心的概念有什么？" class="headerlink" title="11、Milvus中核心的概念有什么？"></a>11、Milvus中核心的概念有什么？</h2><p>一个 Milvus 集群最多支持 64 个数据库。</p>
<p>在Milvus数据库中，有Collection和Field的概念，可以和关系数据库中表和字段进行对应：</p>
<table>
<thead>
<tr>
<th>Milvus</th>
<th>关系数据库</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><code>Collection</code></td>
<td>表</td>
<td>集合相当于关系数据库中的表，用于组织数据</td>
</tr>
<tr>
<td>Field</td>
<td>字段</td>
<td>字段Schema相当于表中的列</td>
</tr>
<tr>
<td><code>Entity</code></td>
<td>行</td>
<td>实体就是一条完整的数据记录，包含一个或多个字段</td>
</tr>
<tr>
<td>Index</td>
<td>索引</td>
<td>从原始数据衍生出来的重组数据结构，可以大大加快向量相似性搜索的过程</td>
</tr>
<tr>
<td>Partition</td>
<td>分区</td>
<td>在物理存储上将 Collections 数据分成多个部分，即分区</td>
</tr>
</tbody></table>
<blockquote>
<p>注意：1个collection最多支持4个向量Field</p>
</blockquote>
<h2 id="12、为什么选择-Milvus？"><a href="#12、为什么选择-Milvus？" class="headerlink" title="12、为什么选择 Milvus？"></a>12、为什么选择 Milvus？</h2><ul>
<li>高性能：性能卓越，可对海量数据集进行向量相似度检索。</li>
<li>高可用、高可靠：Milvus 支持在云上扩展，其容灾能力能够保证服务高可用。</li>
<li>混合查询：Milvus 支持在向量相似度检索过程中进行标量字段过滤，实现混合查询。</li>
<li>开发者友好：支持多语言、多工具的 Milvus 生态系统。</li>
</ul>
<h1 id="Day03"><a href="#Day03" class="headerlink" title="Day03"></a>Day03</h1><h2 id="1、Milvus中常用的索引有哪些类型？原理是什么？有什么特点？"><a href="#1、Milvus中常用的索引有哪些类型？原理是什么？有什么特点？" class="headerlink" title="1、Milvus中常用的索引有哪些类型？原理是什么？有什么特点？"></a>1、Milvus中常用的索引有哪些类型？原理是什么？有什么特点？</h2><ul>
<li>FLAT ：<ul>
<li>原理：暴力计算所有向量与查询向量的距离</li>
<li>特点：<code>精度最高，查询速度最慢</code>，适合数据量小但要求极高精度的场景</li>
</ul>
</li>
<li>IVF_FLAT<ul>
<li>原理：先聚类划分簇，再在目标簇内暴力搜索</li>
<li>特点：<code>各项能力比较均衡</code>，实现成熟，适用于中等数据规模一般要求场景</li>
</ul>
</li>
<li>IVF_SQ8<ul>
<li>原理：类似 IVF_FLAT，但向量被压缩存储（8位整数）</li>
<li>特点：相对于IVF_FLAT精度有所下降，但内存占用降低，计算速度加快，适用于<code>大规模数据下一般要求场景</code></li>
</ul>
</li>
<li>IVF_PQ<ul>
<li>原理：将向量空间划分为子空间并使用子空间簇的索引对原始向量进行量化</li>
<li>特点：<code>查询速度最快，内存占用最低</code>，适用于超大规模急速查询的场景</li>
</ul>
</li>
<li>HNSW<ul>
<li>原理：构建多层导航图进行快速近邻搜索</li>
<li>特点：<code>精度非常高，查询非常快，</code>但内存占用较高，适用于超大规模高精度查询场景。</li>
</ul>
</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20251012205348864.png" alt="image-20251012205348864"></p>
<h2 id="2、Milvus中有哪些相似度度量方法？"><a href="#2、Milvus中有哪些相似度度量方法？" class="headerlink" title="2、Milvus中有哪些相似度度量方法？"></a>2、Milvus中有哪些相似度度量方法？</h2><p>对于浮点向量，通常使用以下指标：</p>
<ul>
<li>L2（欧几里得距离）：计算向量间的直线距离，值越小越相似，常用于图像处理等领域。</li>
<li>IP（内积）：计算两个向量的点积，值越大越相似。当向量经过归一化时，其结果等价于余弦相似度，常用于NLP文本向量搜索。</li>
<li>COSINE（余弦相似度）：衡量两个向量方向的夹角余弦值，值越大（越接近1）表示方向越一致，对向量的幅度不敏感，非常适合文本和语义相似性搜索。</li>
</ul>
<p>对于稀疏向量，通常使用以下指标：</p>
<ul>
<li>IP（内积）：主要用于衡量稀疏向量的相似性。</li>
<li>BM25：一种常用于信息检索的评分函数，也支持用于稀疏向量的搜索。</li>
</ul>
<h2 id="3、Milvus如何实现过滤搜索？"><a href="#3、Milvus如何实现过滤搜索？" class="headerlink" title="3、Milvus如何实现过滤搜索？"></a>3、Milvus如何实现过滤搜索？</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 过滤搜索</span></span><br><span class="line">    res = client.search(collection_name=<span class="string">&#x27;demo_v2&#x27;</span>,</span><br><span class="line">                        data=[[<span class="number">0.19886812562848388</span>, <span class="number">0.06023560599112088</span>, <span class="number">0.6976963061752597</span>, <span class="number">0.2614474506242501</span>,</span><br><span class="line">                               <span class="number">0.838729485096104</span>]],</span><br><span class="line">                        limit=<span class="number">2</span>,  <span class="comment"># 返回的个数</span></span><br><span class="line">                        search_params=&#123;<span class="string">&quot;metric_type&quot;</span>: <span class="string">&quot;IP&quot;</span>&#125;,  <span class="comment"># 查询的参数-指定相似度度量的方式</span></span><br><span class="line">                        output_fields=[<span class="string">&quot;id&quot;</span>, <span class="string">&#x27;vector&#x27;</span>],  <span class="comment"># search_params是在查询时执行距离计算方式，如果定义索引的时候，已经制定了方式可以不写</span></span><br><span class="line">                        <span class="built_in">filter</span>=<span class="string">&quot;color like &#x27;red%&#x27;&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="4、什么叫混合检索？有哪些方式？"><a href="#4、什么叫混合检索？有哪些方式？" class="headerlink" title="4、什么叫混合检索？有哪些方式？"></a>4、什么叫混合检索？有哪些方式？</h2><p>混合检索：<strong>选择适当的重新排序策略对两组 ANN 搜索结果进行合并和重新排序</strong>。</p>
<p>方式一：加权排名</p>
<ul>
<li>原理：对不同的检索结果赋予不同的权重，然后加权求和。</li>
<li>场景：如果用户明确哪些检索结果更重要，想要赋予更高的权重时可以用这种方式。</li>
</ul>
<p>方式二：倒数排序融合</p>
<ul>
<li>原理：根据每个结果在其检索列表中的排名位置来计算分数。</li>
</ul>
<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20251012210154874-17602741152881-17602741156632.png" alt="image-20251012210154874" style="zoom:67%;" />

<ul>
<li>场景：如果不清楚哪些检索结果更重要，不进行手动赋权重的时候。</li>
</ul>
<h2 id="5、如何实现混合检索？"><a href="#5、如何实现混合检索？" class="headerlink" title="5、如何实现混合检索？"></a>5、如何实现混合检索？</h2><p><code>先使用不同的检索方式检索相似文档，然后使用hybrid_search方法进行结果融合，融合时可以使用加权排名或倒数排序融合。</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ----------------------------</span></span><br><span class="line"><span class="comment"># 多向量混合搜索（Hybrid Search）示例</span></span><br><span class="line"><span class="comment"># ----------------------------</span></span><br><span class="line"><span class="comment"># hybrid_search() 可在一次请求中同时对多个向量字段执行独立的 ANN（近似最近邻）搜索，</span></span><br><span class="line"><span class="comment"># 每个 AnnSearchRequest 代表一个字段上的查询。</span></span><br><span class="line"><span class="comment"># 搜索结果将通过 ranker（如 RRFRanker）进行融合排序。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1)检索路径1：搜索电影向量，使用 L2 距离 进行相似度评估</span></span><br><span class="line"><span class="comment"># 定义针对 filmVector 的搜索请求</span></span><br><span class="line">query_filmVector = [[<span class="number">0.8896863042430693</span>, <span class="number">0.370613100114602</span>, <span class="number">0.23779315077113428</span>,</span><br><span class="line">                     <span class="number">0.38227915951132996</span>, <span class="number">0.5997064603128835</span>]]</span><br><span class="line">dense_search_params = &#123;</span><br><span class="line">    <span class="string">&quot;data&quot;</span>: query_filmVector,</span><br><span class="line">    <span class="string">&quot;anns_field&quot;</span>: <span class="string">&quot;filmVector&quot;</span>,  <span class="comment"># 与 schema 中定义的字段名一致</span></span><br><span class="line">    <span class="string">&quot;param&quot;</span>: &#123;<span class="string">&quot;metric_type&quot;</span>: <span class="string">&quot;L2&quot;</span>, <span class="string">&quot;nprobe&quot;</span>: <span class="number">10</span>&#125;,  <span class="comment"># nprobe 控制搜索的簇数量</span></span><br><span class="line">    <span class="string">&quot;limit&quot;</span>: <span class="number">2</span></span><br><span class="line">&#125;</span><br><span class="line">request_1 = AnnSearchRequest(**dense_search_params)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Request 1--&gt;<span class="subst">&#123;request_1&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2)检索路径2：搜索导演向量，使用 COSINE 进行相似度评估</span></span><br><span class="line"><span class="comment"># 定义针对 posterVector 的搜索请求</span></span><br><span class="line">query_posterVector = [[<span class="number">0.02550758562349764</span>, <span class="number">0.006085637357292062</span>,</span><br><span class="line">                       <span class="number">0.5325251250159071</span>, <span class="number">0.7676432650114147</span>, <span class="number">0.5521074424751443</span>]]</span><br><span class="line">sparse_search_params = &#123;</span><br><span class="line">    <span class="string">&quot;data&quot;</span>: query_posterVector,</span><br><span class="line">    <span class="string">&quot;anns_field&quot;</span>: <span class="string">&quot;posterVector&quot;</span>,</span><br><span class="line">    <span class="string">&quot;param&quot;</span>: &#123;<span class="string">&quot;metric_type&quot;</span>: <span class="string">&quot;COSINE&quot;</span>&#125;,</span><br><span class="line">    <span class="string">&quot;limit&quot;</span>: <span class="number">2</span></span><br><span class="line">&#125;</span><br><span class="line">request_2 = AnnSearchRequest(**sparse_search_params)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Request 2--&gt;<span class="subst">&#123;request_2&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3)结果进行聚合</span></span><br><span class="line"><span class="comment"># 第一种融合方式：加权排名策略</span></span><br><span class="line">rerank = WeightedRanker(<span class="number">0.8</span>, <span class="number">0.3</span>)</span><br><span class="line"><span class="comment"># 使用hybrid_search来实现混合搜索</span></span><br><span class="line">final_result = client.hybrid_search(collection_name=<span class="string">&#x27;demo_v3&#x27;</span>,</span><br><span class="line">                                    reqs=[request_1, request_2],</span><br><span class="line">                                    ranker=rerank,</span><br><span class="line">                                    output_fields=[<span class="string">&quot;film_id&quot;</span>, <span class="string">&quot;filmVector&quot;</span>, <span class="string">&quot;posterVector&quot;</span>],</span><br><span class="line">                                    limit=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;final_result--&gt;<span class="subst">&#123;final_result&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种融合方式：倒数排序融合</span></span><br><span class="line">ranker = RRFRanker(k=<span class="number">100</span>)  <span class="comment"># k值默认为60，指的是平滑系数</span></span><br><span class="line"><span class="comment"># 使用hybrid_search来实现混合搜索</span></span><br><span class="line">final_result = client.hybrid_search(collection_name=<span class="string">&#x27;demo_v3&#x27;</span>,</span><br><span class="line">                                    reqs=[request_1, request_2],</span><br><span class="line">                                    ranker=ranker,</span><br><span class="line">                                    output_fields=[<span class="string">&quot;film_id&quot;</span>, <span class="string">&quot;filmVector&quot;</span>, <span class="string">&quot;posterVector&quot;</span>],</span><br><span class="line">                                    limit=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;final_result--&gt;<span class="subst">&#123;final_result&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="6、你们项目中有没有记录或管理日志？怎么做的？"><a href="#6、你们项目中有没有记录或管理日志？怎么做的？" class="headerlink" title="6、你们项目中有没有记录或管理日志？怎么做的？"></a>6、你们项目中有没有记录或管理日志？怎么做的？</h2><p>因为可以根据日志进行代码调试、程序监控、行为审计，所以我们项目记录了日志。</p>
<p>使用的logging这个模块，同时将日志打印到了控制台并记录到了文件中。</p>
<h2 id="7、如何将日志时输出到控制台和文件？"><a href="#7、如何将日志时输出到控制台和文件？" class="headerlink" title="7、如何将日志时输出到控制台和文件？"></a>7、如何将日志时输出到控制台和文件？</h2><p>创建一个控制台处理器，再创建一个文件处理器，然后将他们两个添加到日志记录器即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取日志记录器</span></span><br><span class="line">logger = logging.getLogger(<span class="string">&quot;Example4&quot;</span>)</span><br><span class="line">logger.setLevel(logging.DEBUG)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义日志格式</span></span><br><span class="line">formatter = logging.Formatter(<span class="string">&#x27;%(name)s - %(asctime)s - %(levelname)s - %(message)s&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建控制台处理器</span></span><br><span class="line">console_handler = logging.StreamHandler()</span><br><span class="line">console_handler.setFormatter(formatter)</span><br><span class="line">console_handler.setLevel(logging.WARNING)  <span class="comment"># 每个日志处理器可以单独设置日志级别，但是这个日志级别必须高于或等于处理器级别</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建文件处理器</span></span><br><span class="line">file_handler = logging.FileHandler(filename=<span class="string">&quot;C04_base_use.log&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>, mode=<span class="string">&quot;a&quot;</span>)</span><br><span class="line">file_handler.setFormatter(formatter)</span><br><span class="line">file_handler.setLevel(logging.INFO)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将处理器添加到日志记录器中</span></span><br><span class="line">logger.addHandler(console_handler)</span><br><span class="line">logger.addHandler(file_handler)</span><br></pre></td></tr></table></figure>

<h1 id="Day04"><a href="#Day04" class="headerlink" title="Day04"></a>Day04</h1><h2 id="1、rank-bm25模块如何使用？"><a href="#1、rank-bm25模块如何使用？" class="headerlink" title="1、rank_bm25模块如何使用？"></a>1、rank_bm25模块如何使用？</h2><p>将<code>文档进行分词</code>后送入BM25L或BM250KAPI或BM25Plus训练得到bm25模型，然后<code>将问题进行分词</code>，调用bm25模型的<code>get_scores</code>方法获取每个文档与问题的相似度得分，然后选取最大得分对应的文档索引即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BM25Search</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, docs</span>):</span><br><span class="line">        self.docs = docs</span><br><span class="line">        <span class="comment"># 将文档进行分词</span></span><br><span class="line">        self.tokenized_docs = [jieba.lcut(doc) <span class="keyword">for</span> doc <span class="keyword">in</span> self.docs]</span><br><span class="line">        <span class="comment"># 将分词后的文档送入BM25模型，获得BM25模型的对象</span></span><br><span class="line">        self.bm25 = BM25L(self.tokenized_docs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, query</span>):</span><br><span class="line">        <span class="comment"># 将查询语句进行分词</span></span><br><span class="line">        tokenized_query = jieba.lcut(query)</span><br><span class="line">        <span class="comment"># 获取BM25模型对象，并调用get_scores方法，传入查询语句，获取查询语句的BM25得分</span></span><br><span class="line">        scores = self.bm25.get_scores(tokenized_query)</span><br><span class="line">        <span class="comment"># print(f&#x27;scores--&gt;&#123;scores&#125;&#x27;)  # [1.09433592 0.22790195]</span></span><br><span class="line">        <span class="comment"># 获取最高分数对应的索引</span></span><br><span class="line">        best_index = scores.argmax()</span><br><span class="line">        best_score = scores[best_index]</span><br><span class="line">        best_doc = self.docs[best_index]</span><br><span class="line">        logger.info(<span class="string">f&#x27;查询语句：<span class="subst">&#123;query&#125;</span>，最匹配的文档：<span class="subst">&#123;best_doc&#125;</span>，BM25得分：<span class="subst">&#123;best_score&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> best_doc, best_score</span><br></pre></td></tr></table></figure>

<h2 id="2、rank-bm25模块中有哪几种模型？"><a href="#2、rank-bm25模块中有哪几种模型？" class="headerlink" title="2、rank_bm25模块中有哪几种模型？"></a>2、rank_bm25模块中有哪几种模型？</h2><table>
<thead>
<tr>
<th>模型</th>
<th>核心改进</th>
<th>优势</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>BM25Okapi</strong></td>
<td>标准版本</td>
<td>理论基础扎实，参数直观</td>
<td>通用文本检索任务</td>
</tr>
<tr>
<td><strong>BM25L</strong></td>
<td>对低词频项加平滑（+δ）</td>
<td>改善低频词、短文本表现</td>
<td>新闻、微博、短文本检索</td>
</tr>
<tr>
<td><strong>BM25Plus</strong></td>
<td>对整体加平滑（+Δ）</td>
<td>综合性能更稳健，长度惩罚更合理</td>
<td>通用任务、长短文混合场景</td>
</tr>
</tbody></table>
<h2 id="3、Redis是干嘛的？在项目中有什么用？"><a href="#3、Redis是干嘛的？在项目中有什么用？" class="headerlink" title="3、Redis是干嘛的？在项目中有什么用？"></a>3、Redis是干嘛的？在项目中有什么用？</h2><p>Redis（Remote Dictionary Server）是一个<code>高性能的键值对数据库</code>，常用于缓存、会话管理等场景。</p>
<p>在项目中作用：用户问题到达时，会首先在Redis中检索是否有<strong>相同问题</strong>，如果有则立即返回对应的答案，从而加快问题回复的效率。其中Redis存储的是极高频问答对（在MySQL中查询出的，与问题相似度阈值&gt;&#x3D;0.85且有答案的问题及答案）。</p>
<h2 id="4、如何实现对Redis的操作？"><a href="#4、如何实现对Redis的操作？" class="headerlink" title="4、如何实现对Redis的操作？"></a>4、如何实现对Redis的操作？</h2><p>通过redis包下的StrictRedis方法创建客户端，然后基于这个客户端实现增删改查操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.client = redis.StrictRedis(host=self.conf.REDIS_HOST,</span><br><span class="line">                                port=self.conf.REDIS_PORT,</span><br><span class="line">                                password=self.conf.REDIS_PASSWORD,</span><br><span class="line">                                db=self.conf.REDIS_DB,</span><br><span class="line">                                decode_responses=<span class="literal">True</span>)  <span class="comment"># 将响应解码为字符串</span></span><br></pre></td></tr></table></figure>

<h2 id="5、如何实现的基于MySQL的FQA系统？"><a href="#5、如何实现的基于MySQL的FQA系统？" class="headerlink" title="5、如何实现的基于MySQL的FQA系统？"></a>5、如何实现的基于MySQL的FQA系统？</h2><ol>
<li><strong>数据存储</strong>：MySQL存储FQA高频问答对数据。</li>
<li><strong>缓存管理</strong>：先基于Redis缓存，返回相同问题的答案，如果没有命中则进行MySQL问题检索；Redis中仅缓存相似度&gt;0.85且有答案的数据。</li>
<li><strong>问题检索</strong>：如果Redis中没有命中相同问题，则使用BM25计算与所有问题的相似度，将相似分数Softmax归一化。获取最相似文档的分数，并判断是否大于阈值0.85，如果是则认为是同一问题，去MySQL中查询该问题的答案；如果不是则调用RAG系统检索。</li>
<li><strong>答案返回</strong>：<ul>
<li>在MySQL中根据问题查询答案，若返回可靠答案，直接返回。</li>
<li>否则，调用RAG系统检索。</li>
</ul>
</li>
</ol>
<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20251015143818433.png" alt="image-20251015143818433"  />

<h2 id="6、MySQL在项目中起什么作用？怎么用的？"><a href="#6、MySQL在项目中起什么作用？怎么用的？" class="headerlink" title="6、MySQL在项目中起什么作用？怎么用的？"></a>6、MySQL在项目中起什么作用？怎么用的？</h2><p>MySQL用于存储高频问答对，基于缓存的问题训练BM25模型，用于检索与查询问题相似的问题。然后根据相似问题来查询对应的答案。</p>
<p>注意：</p>
<p><code>高频问答对的来源</code>——由后端工程师每天统计咨询问题的top50，去重后导入MySQL</p>
<p><code>拿到这些高频问答对之后，如何插入到MySQL</code>?——</p>
<p>遍历高频问答对中的每个问题，将这个问题进行分词后，送入到训练好的BM25模型（由历史高频问题训练得到的）中，使用BM25计算与所有历史问题的相似度，将相似分数Softmax归一化。获取最相似文档的分数，并判断是否大于阈值0.85，如果是则认为是同一问题，则不再向MySQL中进行插入；否则，则作为新问题插入到MySQL中。</p>
<h1 id="Day05"><a href="#Day05" class="headerlink" title="Day05"></a>Day05</h1><h2 id="1、详细描述一下如何使用BM25对MySQL中的高频问答对进行检索？"><a href="#1、详细描述一下如何使用BM25对MySQL中的高频问答对进行检索？" class="headerlink" title="1、详细描述一下如何使用BM25对MySQL中的高频问答对进行检索？"></a>1、详细描述一下如何使用BM25对MySQL中的高频问答对进行检索？</h2><p>首先从Redis中读取缓存的所有分词后的高频问题（如果没有则从MySQL中读取所有问题再进行分词），然后送入BM25进行训练得到BM25模型。然后当用户的问题到达时，将问题进行分词后，调用模型的get_scores方法获取用户问题与所有高频问题的相似度得分。再对这个分数进行归一化处理，获取最高分数及对应的索引。如果这个分数大于了0.85，则根据索引获取原始问题，再从MySQL中检索该问题对应的答案；如果这个分数没有大于0.85或者在MySQL中没有检索到答案，则返回None，下一步进行RAG检索。</p>
<h2 id="2、当MySQL的数据更新之后，如何更新Redis中的数据，进而更新BM25模型？"><a href="#2、当MySQL的数据更新之后，如何更新Redis中的数据，进而更新BM25模型？" class="headerlink" title="2、当MySQL的数据更新之后，如何更新Redis中的数据，进而更新BM25模型？"></a>2、当MySQL的数据更新之后，如何更新Redis中的数据，进而更新BM25模型？</h2><p>可以对Redis中 用于记录所有历史问题和所有分词问题的key ，即 “qa_original_questions” 和”qa_tokenized_questions”设置一个有效期，有效期可以设置为3小时，这样redis中的这2个key的数据会定期进行删除。当redis中这2个key的数据进行删除之后，就可以按照代码从MySQL中重新进行加载，从而实现它的更新。</p>
<h2 id="3、RAG系统的工作流程是怎样的？"><a href="#3、RAG系统的工作流程是怎样的？" class="headerlink" title="3、RAG系统的工作流程是怎样的？"></a>3、RAG系统的工作流程是怎样的？</h2><ol>
<li><strong>查询分类</strong>：<ul>
<li>系统首先判断查询类型（如“通用知识”或“专业咨询”）。</li>
<li>通用知识直接由大语言模型回答，专业咨询进入检索流程。</li>
</ul>
</li>
<li><strong>策略选择</strong>：<ul>
<li>根据查询特点选择检索策略：<ul>
<li><strong>直接检索</strong>：适用于明确查询。</li>
<li><strong>HyDE检索</strong>：适用于抽象问题，生成假设答案后检索。</li>
<li><strong>子查询检索</strong>：分解复杂查询。</li>
<li><strong>回溯检索</strong>：简化复杂问题后检索。</li>
</ul>
</li>
</ul>
</li>
<li><strong>文档检索</strong>：<ul>
<li>使用<code>vector_store.py</code>从向量数据库中检索相关文档。</li>
<li>支持稠密向量和稀疏向量的混合检索，结果经过重排序优化。</li>
</ul>
</li>
<li><strong>生成回答</strong>：<ul>
<li>将检索到的文档作为上下文，结合用户查询输入大语言模型，生成自然语言回答。</li>
<li>若大模型调用报错则联系人工。</li>
</ul>
</li>
</ol>
<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20251015192046702.png" alt="image-20251015192046702" style="zoom:67%;" />



<h2 id="4、什么是OCR？你们项目中用的哪个框架？"><a href="#4、什么是OCR？你们项目中用的哪个框架？" class="headerlink" title="4、什么是OCR？你们项目中用的哪个框架？"></a>4、什么是OCR？你们项目中用的哪个框架？</h2><p>OCR 技术，全称是光学字符识别（Optical Character Recognition），是一种将图像中的文字内容进行识别并提取的技术。</p>
<p>我们项目使用的是PaddleOCR，它是由百度飞桨（PaddlePaddle）团队基于其深度学习框架 PaddlePaddle 开发并开源的一个全流程、超轻量、高精度的 OCR 工具库。</p>
<h2 id="5、你们项目是如何来加载pdf的？"><a href="#5、你们项目是如何来加载pdf的？" class="headerlink" title="5、你们项目是如何来加载pdf的？"></a>5、你们项目是如何来加载pdf的？</h2><p>主要思路：</p>
<ul>
<li><p>继承LangChain中的BaseLoader类，实现init方法和lazy_load方法，将整个 pdf 的提取结果加上元数据信息作为一个 Document yield 出去。</p>
</li>
<li><p>内容提取的核心逻辑是：使用PyMuPDF模块中的fitz加载PDF，用来提取文字和图片元数据信息，对于图片信息使用PaddleORC（使用的rapidocr_paddle 模块下的 RapidOCR类）识别图片中的文字，最终将结果拼接到一起。</p>
</li>
</ul>
<p>主要流程：</p>
<ol>
<li>使用 <code>fitz.open()</code> 打开 PDF。</li>
<li>逐页 (<code>page</code>) 处理。</li>
<li>使用 <code>page.get_text()</code> 提取原生文本。</li>
<li>使用 <code>page.get_image_info(xrefs=True)</code> 获取页面上的图片信息。</li>
<li><strong>OCR 应用</strong>: 对获取到的图片，检查其尺寸是否超过预设阈值 <code>PDF_OCR_THRESHOLD</code>（默认为页面宽高的 60%）。仅对大于阈值的图片执行 OCR。</li>
<li>处理页面旋转 (<code>page.rotation</code>)，确保 OCR 时图像方向正确。</li>
<li>调用 <code>get_ocr()</code> 获取的 OCR 实例识别图片文字。</li>
<li>合并原生文本和 OCR 结果。</li>
</ol>
<h2 id="6、你们项目是如何来加载doc的？"><a href="#6、你们项目是如何来加载doc的？" class="headerlink" title="6、你们项目是如何来加载doc的？"></a>6、你们项目是如何来加载doc的？</h2><p>主要思路：</p>
<ul>
<li><p>继承LangChain中的BaseLoader类，实现init方法和lazy_load方法，将整个 word文档 的提取结果加上元数据信息作为一个 Document yield 出去。</p>
</li>
<li><p>内容提取的核心逻辑是：使用python-docx模块加载word文档，然后获取该文档的块信息（Paragraph或Table），接下来去处理每一个块。如果是段落，先把段落中的文字提取出来，然后段落中的图片使用PaddleORC（使用的rapidocr_paddle 模块下的 RapidOCR类）识别其中的文字；如果是表格，则直接遍历获取表格单元格中的信息。最终将结果拼接到一起。</p>
</li>
</ul>
<p>主要流程：</p>
<ol>
<li>使用 <code>docx.Document()</code> 打开 DOCX 文件。</li>
<li>定义 <code>iter_block_items</code> 辅助函数，用于统一遍历文档中的段落 (<code>Paragraph</code>) 和表格 (<code>Table</code>) 块。</li>
<li>遍历所有块：</li>
<li>如果是段落，提取 <code>block.text</code>。同时，使用 XPath (<code>.//pic:pic</code>, <code>.//a:blip/@r:embed</code>) 查找并提取段落内嵌入的图片。对提取的图片执行 OCR。</li>
<li>如果是表格，遍历所有单元格 (<code>cell</code>)，提取单元格内段落的文本。</li>
<li>合并所有提取的文本和 OCR 结果。</li>
</ol>
<h2 id="7、你们项目ppt加载器怎么做的？"><a href="#7、你们项目ppt加载器怎么做的？" class="headerlink" title="7、你们项目ppt加载器怎么做的？"></a>7、你们项目ppt加载器怎么做的？</h2><p>主要思路：</p>
<ul>
<li><p>继承LangChain中的BaseLoader类，实现init方法和lazy_load方法，将整个 ppt 的提取结果加上元数据信息作为一个 Document yield 出去。</p>
</li>
<li><p>内容提取的核心逻辑是：使用python-pptx模块加载ppt，然后逐张处理PPT。在处理PPT时，首先将PPT上的形状 (<code>shape</code>) 按视觉顺序（<code>top</code>, <code>left</code> 坐标）排序，排序完后依次去处理每个形状。在处理形状时，如果是文本框 ，则直接提取文本；如果是表格，则直接遍历获取表格单元格中的信息；如果是图片，则使用PaddleORC（使用的rapidocr_paddle 模块下的 RapidOCR类）识别其中的文字；如果是组合形状，则进行递归调用。最终将结果拼接到一起。</p>
</li>
</ul>
<p>主要流程：</p>
<ol>
<li>使用 <code>pptx.Presentation()</code> 打开演示文稿。</li>
<li>逐张幻灯片 (<code>slide</code>) 处理。</li>
<li><strong>顺序处理</strong>: 将幻灯片上的形状 (<code>shape</code>) 按视觉顺序（<code>top</code>, <code>left</code> 坐标）排序。</li>
<li>定义 extract_text 递归函数处理单个形状：<ul>
<li>提取文本框 (<code>shape.has_text_frame</code>) 的文本。</li>
<li>提取表格 (<code>shape.has_table</code>) 内所有单元格的文本。</li>
<li>如果形状是图片 (<code>shape.shape_type </code> 13<code>)，提取图片数据 (</code>shape.image.blob&#96;)，执行 OCR。</li>
<li>如果形状是组合 (<code>shape.shape_type </code> 6<code>)，递归调用 </code>extract_text&#96; 处理其包含的子形状。</li>
</ul>
</li>
<li>遍历排序后的形状，调用 <code>extract_text</code>。</li>
<li>合并所有提取的文本和 OCR 结果。</li>
</ol>
<h2 id="8、你们项目图片加载器怎么做的？"><a href="#8、你们项目图片加载器怎么做的？" class="headerlink" title="8、你们项目图片加载器怎么做的？"></a>8、你们项目图片加载器怎么做的？</h2><p>主要思路：</p>
<ul>
<li>继承LangChain中的BaseLoader类，实现init方法和lazy_load方法，将整个图片的提取结果加上元数据信息作为一个 Document yield 出去。</li>
<li>内容提取的核心逻辑是：使用PaddleORC（使用的rapidocr_paddle 模块下的 RapidOCR类）识别其中的文字。</li>
</ul>
<p>主要流程：</p>
<ol>
<li>接收图像文件路径 <code>img_path</code>。</li>
<li>调用 <code>get_ocr()</code> 获取 OCR 实例。</li>
<li>直接对图像文件执行 OCR。</li>
<li>将 OCR 结果（所有识别出的文本行）合并成一个字符串。</li>
</ol>
<h1 id="Day06"><a href="#Day06" class="headerlink" title="Day06"></a>Day06</h1><h2 id="1、你们项目用到了哪些文本切分器？怎么切分的？"><a href="#1、你们项目用到了哪些文本切分器？怎么切分的？" class="headerlink" title="1、你们项目用到了哪些文本切分器？怎么切分的？"></a>1、你们项目用到了哪些文本切分器？怎么切分的？</h2><ul>
<li>文本切分器的选择：根据文档的类型进行选择——如果是markdown，则使用MarkdownTextSplitter进行切割；如果是一大段文本，没有明确的段落标识，比如从网上爬取到的信息，则使用AliTextSplitter；其他使用ChineseRecursiveTextSplitter进行切割。</li>
<li>文本切块的方式：先切分了父块，然后在每个父块里边切分了子块。在子块中保存了元数据信息，包括父块的id,路径，<code>父块的内容</code>！最后返回的是所有的子块，需要将子块进行embedding后，存入Milvus中。</li>
</ul>
<h2 id="2、为什么要进行父块和子块的区分？"><a href="#2、为什么要进行父块和子块的区分？" class="headerlink" title="2、为什么要进行父块和子块的区分？"></a>2、为什么要进行父块和子块的区分？</h2><ul>
<li>如果块特别大，这个时候块中会包括大量跟问题无关的信息，在检索时，没有办法精准找到我们想要的数据。如果将文档切分成了一个一个的小块，可以提高检索时的精准性。但是有一个新的问题，因为块比较小，所以块的上下文信息是比较缺失的，在回答问题时，可能语义不完整。</li>
<li>所以解决方式是：大块切分和小块切分进行了融合，即使用父块和子块组合切分的方式。具体来说，先切分父块，然后在每个父块里边切分了子块，然后在子块中保存父块的内容。最终存储到向量数据时，是将子块进行embedding后进行存储。在检索的时候，就是用子块进行检索，此时可以实现精准检索。当检索到子块后，会将该子块元数据信息中的父块内容进行返回，用于问答，从而解决了上下文缺失问题！</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20251018095618518.png" alt="image-20251018095618518"></p>
<h2 id="3、为什么自定义中文递归文本切分器？怎么做的？"><a href="#3、为什么自定义中文递归文本切分器？怎么做的？" class="headerlink" title="3、为什么自定义中文递归文本切分器？怎么做的？"></a>3、为什么自定义中文递归文本切分器？怎么做的？</h2><p>原因：将长文本按照预设的中文分隔符递归地切分成指定大小的块。</p>
<p>主要实现逻辑：继承langchain.text_splitter.RecursiveCharacterTextSplitter，对分割符列表进行了修改，修改成包括常见的中文标点和换行符，如 <code>[&quot;\n\n&quot;, &quot;\n&quot;, &quot;。|！|？&quot;, &quot;\.\s|\!\s|\?\s&quot;, &quot;；|;\s&quot;, &quot;，|,\s&quot;]</code>。这有助于在切分时尽量保持中文句子的完整性。</p>
<h2 id="4、为什么自定义基于模型的语义切分器？怎么做的？"><a href="#4、为什么自定义基于模型的语义切分器？怎么做的？" class="headerlink" title="4、为什么自定义基于模型的语义切分器？怎么做的？"></a>4、为什么自定义基于模型的语义切分器？怎么做的？</h2><p>原因：通过达摩院开源的文档语义分割模型实现对文本的切分，<code>准确度高并且效率高</code></p>
<p>主要实现逻辑：继承langchain.text_splitter.CharacterTextSplitter，对split_text方法进行了修改。修改的方式就是利用预训练的文档语义分割模型对文本进行切分。具体的切分方式调用 <code>modelscope.pipeline</code> 加载指定的文档分割模型（nlp_bert_document-segmentation_chinese-base，达摩院开源的文档语义分割模型）模型，将输入文本传递给模型 pipeline 进行处理，最后解析模型输出，得到分块的结果。</p>
<p><strong>为什么选用这个模型？</strong></p>
<p>因为 nlp_bert_document-segmentation_chinese-base 是 达摩院开源的文档语义分割模型，通过自适应滑动窗口的序列模型，对输入文本进行语义层面的段落分割。它先将整个文本作为序列输入，然后使用使用 BERT 基础模型预测每个 token 是否是段落边界，通过自适应滑动窗口动态调整处理窗口大小，提高准确性和效率。</p>
<h2 id="5、项目中如何做的Embedding？"><a href="#5、项目中如何做的Embedding？" class="headerlink" title="5、项目中如何做的Embedding？"></a>5、项目中如何做的Embedding？</h2><p>使用milvus_model模块下的BGEM3EmbeddingFunction类加载<code>bge-m3模型</code>，对文本进行嵌入，同时生成<code>用于语义相似度计算的稠密向量</code>和<code>用于关键词匹配的稀疏向量</code>，进而做混合检索，提高检索的准确率和召回率。</p>
<p>使用方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> milvus_model.hybrid <span class="keyword">import</span> BGEM3EmbeddingFunction</span><br><span class="line"></span><br><span class="line">ef = BGEM3EmbeddingFunction(model_name_or_path=<span class="string">&#x27;./bge-m3&#x27;</span>, use_fp16=<span class="literal">True</span>, device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line"></span><br><span class="line">texts = [<span class="string">&quot;什么是人工智能？&quot;</span>, <span class="string">&quot;iPhone 价格是多少？&quot;</span>]</span><br><span class="line">embeddings = ef(texts)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(embeddings.keys())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;ef.dim[&#x27;dense&#x27;]--&gt;<span class="subst">&#123;ef.dim[<span class="string">&#x27;dense&#x27;</span>]&#125;</span>&quot;</span>)  <span class="comment"># 1024</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;ef.dim[&#x27;sparse&#x27;]--&gt;<span class="subst">&#123;ef.dim[<span class="string">&#x27;sparse&#x27;</span>]&#125;</span>&quot;</span>)  <span class="comment"># 250002</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取稠密向量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;第一个文本的稠密向量--&gt;<span class="subst">&#123;embeddings[<span class="string">&#x27;dense&#x27;</span>][<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 获取Milvus格式的稀疏向量</span></span><br><span class="line">sparse = embeddings[<span class="string">&#x27;sparse&#x27;</span>]._getrow(<span class="number">0</span>)</span><br><span class="line">sparse_dict = &#123;index:value <span class="keyword">for</span> index, value <span class="keyword">in</span> <span class="built_in">zip</span>(sparse.indices, sparse.data)&#125;</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;第一个文本的稀疏向量--&gt;<span class="subst">&#123;sparse_dict&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20251018095656008.png" alt="image-20251018095656008"></p>
<h2 id="6、稠密向量和稀疏向量是如何存储和构建索引的？"><a href="#6、稠密向量和稀疏向量是如何存储和构建索引的？" class="headerlink" title="6、稠密向量和稀疏向量是如何存储和构建索引的？"></a>6、稠密向量和稀疏向量是如何存储和构建索引的？</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加稠密向量字段，FLOAT_VECTOR 类型，维度由嵌入函数指定</span></span><br><span class="line">schema.add_field(field_name=<span class="string">&quot;dense_vector&quot;</span>, datatype=DataType.FLOAT_VECTOR, dim=self.dense_dim)</span><br><span class="line"><span class="comment"># 添加稀疏向量字段，SPARSE_FLOAT_VECTOR 类型，这个类型是专门用来用于存储稀疏向量数据的</span></span><br><span class="line">schema.add_field(field_name=<span class="string">&quot;sparse_vector&quot;</span>, datatype=DataType.SPARSE_FLOAT_VECTOR)        </span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建索引参数对象</span></span><br><span class="line">index_params = self.client.prepare_index_params()</span><br><span class="line"><span class="comment"># 为稠密向量字段创建索引</span></span><br><span class="line">index_params.add_index(field_name=<span class="string">&quot;dense_vector&quot;</span>,</span><br><span class="line">                       index_name=<span class="string">&quot;dense_index&quot;</span>,</span><br><span class="line">                       index_type=<span class="string">&quot;IVF_SQ8&quot;</span>,</span><br><span class="line">                       metric_type=<span class="string">&quot;IP&quot;</span>,</span><br><span class="line">                       <span class="comment"># 设置簇的个数：设置簇的个数越多，每个簇中向量数越少，查询的速度变化越快，但是查询的准确度越低</span></span><br><span class="line">                       <span class="comment"># 一般来说设的值是 根号下文档数</span></span><br><span class="line">                       params=&#123;<span class="string">&quot;nlist&quot;</span>: <span class="number">10</span>&#125;)</span><br><span class="line"><span class="comment"># 为稀疏向量字段创建索引</span></span><br><span class="line">index_params.add_index(field_name=<span class="string">&quot;sparse_vector&quot;</span>,</span><br><span class="line">                       index_name=<span class="string">&quot;sparse_index&quot;</span>,</span><br><span class="line">                       <span class="comment"># 设置索引类型为稀疏倒排索引</span></span><br><span class="line">                       index_type=<span class="string">&quot;SPARSE_INVERTED_INDEX&quot;</span>,</span><br><span class="line">                       metric_type=<span class="string">&quot;IP&quot;</span></span><br><span class="line">                       )</span><br></pre></td></tr></table></figure>

<h2 id="7、重排序模型是干什么的？为什么需要重排序？"><a href="#7、重排序模型是干什么的？为什么需要重排序？" class="headerlink" title="7、重排序模型是干什么的？为什么需要重排序？"></a>7、重排序模型是干什么的？为什么需要重排序？</h2><p>重排序模型是检索增强生成（RAG）流程中的关键组件，负责对初步检索的大量候选文档进行精确排序，从而提高下游任务的质量。</p>
<p><strong>需要重排序的原因：</strong></p>
<p>（1）使用不同的检索方式检索到的候选文档过多，会超过大模型的输入限制，需要进行重排后，选取最相关topn文档送到大模型中进行内容生成</p>
<p>（2）有论文实验证明，在将相似文档输入给大模型时，如果按相关性由高到低的顺序输入，效果会优于其他顺序，所以需要将模型进行重排序后再输入给大模型。</p>
<h2 id="8、重排序模型用的是什么模型？为什么？"><a href="#8、重排序模型用的是什么模型？为什么？" class="headerlink" title="8、重排序模型用的是什么模型？为什么？"></a>8、重排序模型用的是什么模型？为什么？</h2><p>用的BGE-Reranker系列中的bge-reranker-large，它采用交叉编码器(cross-encoder)架构，能够直接计算查询和文档之间的相关性得分，具有强大的语义理解能力，在重排序任务中表现优异。</p>
<p>使用方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> CrossEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">reranker = CrossEncoder(<span class="string">&#x27;./bge-reranker-large&#x27;</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line"></span><br><span class="line">queries = [<span class="string">&quot;什么是机器学习？&quot;</span>]</span><br><span class="line">documents = [</span><br><span class="line">    <span class="string">&quot;机器学习是一种让计算机从数据中自动学习规律的方法。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;深度学习是机器学习的一个分支。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;今天的天气很好。&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入格式必须是二维列表：[[query, document]]</span></span><br><span class="line">pairs = [[queries[<span class="number">0</span>], doc] <span class="keyword">for</span> doc <span class="keyword">in</span> documents]</span><br><span class="line">scores = reranker.predict(pairs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;scores--&gt;<span class="subst">&#123;scores&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出排序结果</span></span><br><span class="line"><span class="keyword">for</span> doc, score <span class="keyword">in</span> <span class="built_in">sorted</span>(<span class="built_in">zip</span>(documents, scores), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;score:<span class="number">.4</span>f&#125;</span> - <span class="subst">&#123;doc&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scores--&gt;[9.9982554e-01 3.1844237e-01 7.6328361e-05]</span><br><span class="line">0.9998 - 机器学习是一种让计算机从数据中自动学习规律的方法。</span><br><span class="line">0.3184 - 深度学习是机器学习的一个分支。</span><br><span class="line">0.0001 - 今天的天气很好。</span><br></pre></td></tr></table></figure>



<h1 id="Day07"><a href="#Day07" class="headerlink" title="Day07"></a>Day07</h1><h2 id="1、项目中如何实现的混合检索和重排序？"><a href="#1、项目中如何实现的混合检索和重排序？" class="headerlink" title="1、项目中如何实现的混合检索和重排序？"></a>1、项目中如何实现的混合检索和重排序？</h2><ul>
<li>首先使用BGE-M3将问题编码成稠密向量和稀疏向量。</li>
<li>然后为稠密向量和稀疏向量分别创建检索请求，其中稠密向量使用的相似度度量方式为IP，nprobe设置为50【子块总数量为50w, nlist为700，nprobe为50】。稀疏向量使用的相似度度量方式也是IP。在检索时，将学科类型作为过滤条件。</li>
<li>使用client.hybrid_search()做结果融合，混合检索方式为加权排名，稠密向量和稀疏向量的权重分别是1和0.7</li>
<li>根据混合检索得到的子块的元数据信息，获取对应的父块内容。对父块内容进行去重后，使用CrossEncoder加载的bge-reranker-large进行重排序。最后返回topk个重排序后的父块数据。</li>
</ul>
<p><strong>示例代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  定义方法，实现混合检索和重排序</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hybrid_search_with_rerank</span>(<span class="params">self, query, top_k=conf.RETRIEVAL_K, source_filter=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param query: 查询的问题</span></span><br><span class="line"><span class="string">    :param top_k: 检索返回的topk个文档</span></span><br><span class="line"><span class="string">    :param source_filter: 进行查询过滤的条件，这里指的是学科</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># todo: 1.对查询进行embedding</span></span><br><span class="line">    query_embeddings = self.embedding_function([query])</span><br><span class="line">    <span class="comment"># 获取稠密向量</span></span><br><span class="line">    dense_vector = query_embeddings[<span class="string">&quot;dense&quot;</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># print(f&#x27;dense_vector--&gt;&#123;dense_vector&#125;&#x27;)  # 维度是1024</span></span><br><span class="line">    <span class="comment"># 获取稀疏向量</span></span><br><span class="line">    sparse_vector = query_embeddings[<span class="string">&quot;sparse&quot;</span>]._getrow(<span class="number">0</span>)</span><br><span class="line">    sparse_dict = &#123;index: value <span class="keyword">for</span> index, value <span class="keyword">in</span> <span class="built_in">zip</span>(sparse_vector.indices, sparse_vector.data)&#125;</span><br><span class="line">    <span class="comment"># print(f&#x27;sparse_dict--&gt;&#123;sparse_dict&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># todo: 2.构建稠密向量和稀疏向量的检索请求</span></span><br><span class="line">    filter_expression = <span class="string">f&quot;source == &#x27;<span class="subst">&#123;source_filter&#125;</span>&#x27;&quot;</span> <span class="keyword">if</span> source_filter <span class="keyword">else</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 创建稠密向量的搜索请求</span></span><br><span class="line">    dense_search_params = &#123;</span><br><span class="line">        <span class="string">&quot;data&quot;</span>: [dense_vector],</span><br><span class="line">        <span class="string">&quot;anns_field&quot;</span>: <span class="string">&quot;dense_vector&quot;</span>,  <span class="comment"># 要查询的字段</span></span><br><span class="line">        <span class="comment"># nprobe 控制搜索的簇数量，nprobe 值越小，搜索速度越快，但是搜索的准确度越低，通常是nlist的1%-10%</span></span><br><span class="line">        <span class="string">&quot;param&quot;</span>: &#123;<span class="string">&quot;metric_type&quot;</span>: <span class="string">&quot;IP&quot;</span>, <span class="string">&quot;nprobe&quot;</span>: <span class="number">2</span>&#125;,</span><br><span class="line">        <span class="string">&quot;limit&quot;</span>: top_k,</span><br><span class="line">        <span class="string">&quot;expr&quot;</span>: filter_expression</span><br><span class="line">    &#125;</span><br><span class="line">    request_1 = AnnSearchRequest(**dense_search_params)</span><br><span class="line">    <span class="comment"># 创建稀疏向量的搜索请求</span></span><br><span class="line">    request_2 = AnnSearchRequest(</span><br><span class="line">        data=[sparse_dict],</span><br><span class="line">        anns_field=<span class="string">&quot;sparse_vector&quot;</span>,</span><br><span class="line">        param=&#123;<span class="string">&quot;metric_type&quot;</span>: <span class="string">&quot;IP&quot;</span>&#125;,</span><br><span class="line">        limit=top_k,</span><br><span class="line">        expr=filter_expression</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># todo: 3.使用 WeightedRanker 融合结果</span></span><br><span class="line">    <span class="comment"># 创建 WeightedRanker 对象，设置权重为稠密向量1.0和稀疏向量0.7</span></span><br><span class="line">    ranker = WeightedRanker(<span class="number">1.0</span>, <span class="number">0.7</span>)</span><br><span class="line">    <span class="comment"># 进行混合检索。因为检索结果是一个列表，所以需要取第一个元素</span></span><br><span class="line">    results = self.client.hybrid_search(collection_name=self.collection_name,</span><br><span class="line">                                        reqs=[request_1, request_2],</span><br><span class="line">                                        ranker=ranker,</span><br><span class="line">                                        limit=top_k,</span><br><span class="line">                                        output_fields=[<span class="string">&quot;text&quot;</span>, <span class="string">&quot;parent_id&quot;</span>, <span class="string">&quot;parent_content&quot;</span>, <span class="string">&quot;source&quot;</span>, <span class="string">&quot;timestamp&quot;</span>])[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># print(f&#x27;results--&gt;&#123;type(results), results&#125;&#x27;)</span></span><br><span class="line">    <span class="comment"># 对结果进行处理，封装成 Document 对象</span></span><br><span class="line">    sub_chunks = [self._doc_from_hit(hit[<span class="string">&#x27;entity&#x27;</span>]) <span class="keyword">for</span> hit <span class="keyword">in</span> results]</span><br><span class="line">    <span class="comment"># print(f&#x27;sub_chunks--&gt;&#123;len(sub_chunks), sub_chunks&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># todo: 4.对去重后的父块进行重排序</span></span><br><span class="line">    <span class="comment"># 从子块中去重父块内容，并进行去重</span></span><br><span class="line">    parent_docs = self._get_unique_parent_docs(sub_chunks)</span><br><span class="line">    <span class="comment"># print(f&#x27;parent_docs--&gt;&#123;len(parent_docs), parent_docs&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对父块进行重排序</span></span><br><span class="line">    <span class="keyword">if</span> parent_docs:</span><br><span class="line">        <span class="comment"># 数据对为 查询和文档</span></span><br><span class="line">        data_pairs = [(query, doc.page_content) <span class="keyword">for</span> doc <span class="keyword">in</span> parent_docs]</span><br><span class="line">        scores = self.reranker.predict(data_pairs)</span><br><span class="line">        <span class="comment"># print(f&#x27;scores--&gt;&#123;scores&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># 根据得分，从高到低去排序文档</span></span><br><span class="line">        ranked_parent_docs = [doc <span class="keyword">for</span> score, doc <span class="keyword">in</span> <span class="built_in">sorted</span>(<span class="built_in">zip</span>(scores, parent_docs), reverse=<span class="literal">True</span>)]</span><br><span class="line">        <span class="comment"># print(f&#x27;ranked_parent_docs--&gt;&#123;ranked_parent_docs&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最终返回排序后的父块文档，返回前 conf.CANDIDATE_M 个</span></span><br><span class="line">        self.logger.info(<span class="string">f&quot;Milvus中已找到 <span class="subst">&#123;<span class="built_in">len</span>(ranked_parent_docs)&#125;</span> 个去重父块，最终返回 <span class="subst">&#123;conf.CANDIDATE_M&#125;</span> 个父块&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> ranked_parent_docs[:conf.CANDIDATE_M]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.logger.warning(<span class="string">f&quot;在Milvus中没有找到去重父块&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> []</span><br></pre></td></tr></table></figure>

<h2 id="2、如果在文档检索时，发现有效文档的召回率较低，怎么处理？"><a href="#2、如果在文档检索时，发现有效文档的召回率较低，怎么处理？" class="headerlink" title="2、如果在文档检索时，发现有效文档的召回率较低，怎么处理？"></a>2、如果在文档检索时，发现有效文档的召回率较低，怎么处理？</h2><p>（1）可以优先修改在检索时nprobe的个数，nprobe的个数越大，检索的簇越多，检索的精度越高。</p>
<p>（2）可以修改召回的数量，召回的数量越大，有效文档的召回率越高。</p>
<p>（3）修改或者调整重排序模型【有条件可以微调这个重排序模型】，使有效文档的排名更靠前，从而避免被截取掉。</p>
<p>（4）可以尝试在构建索引时，调低nlist的个数。因为nlist越小，单个簇中的向量数越多，在检索时暴力搜索的向量越多，检索的精度越高。</p>
<p>（5）尝试适当调整索引类型，如果使用的是IVF_PQ，可以调整为更高精度的IVF_SQ8或HNSW。如果数据量少，还可以尝试IVF_FLAT或FLAT。</p>
<h2 id="3、查询分类是干嘛的，怎么做的？"><a href="#3、查询分类是干嘛的，怎么做的？" class="headerlink" title="3、查询分类是干嘛的，怎么做的？"></a>3、查询分类是干嘛的，怎么做的？</h2><p>查询分类负责将用户查询分为“通用知识”和“专业咨询”两类，以决定查询路由是直接由大模型生成（通用知识），还是进行RAG检索（专业咨询）。</p>
<p>做法：使用BertForSequenceClassification加载并训练bert_base_chinese模型，得到最终的模型。</p>
<h2 id="4、使用过Trainer训练模型吗？如何使用？"><a href="#4、使用过Trainer训练模型吗？如何使用？" class="headerlink" title="4、使用过Trainer训练模型吗？如何使用？"></a>4、使用过Trainer训练模型吗？如何使用？</h2><p>使用过。</p>
<p><strong>需要先配置TrainingArguments参数，然后实例化Trainer对象，将模型、训练参数、训练数据集、验证数据集以及评估函数进行传入，调用Trainer对象的train方法进行训练。</strong></p>
<p>其中TrainingArguments常用的配置参数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5）模型训练-设置训练参数</span></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">&quot;./bert_results&quot;</span>,  <span class="comment"># 检查点的保存路径</span></span><br><span class="line">    num_train_epochs=<span class="number">3</span>,  <span class="comment"># 训练轮数</span></span><br><span class="line">    per_device_train_batch_size=<span class="number">8</span>,  <span class="comment"># 训练批次大小</span></span><br><span class="line">    per_device_eval_batch_size=<span class="number">8</span>,  <span class="comment"># 评估批次大小</span></span><br><span class="line">    warmup_steps=<span class="number">50</span>,  <span class="comment"># 预热步数</span></span><br><span class="line">    weight_decay=<span class="number">0.01</span>,  <span class="comment"># 使用adamW的权重衰减系数</span></span><br><span class="line">    logging_dir=<span class="string">&quot;./bert_logs&quot;</span>,  <span class="comment"># 日志保存路径：如果想生成这个文件夹，需要安装tensorboard</span></span><br><span class="line">    logging_steps=<span class="number">10</span>,  <span class="comment"># 每10个批次打印一次日志</span></span><br><span class="line">    evaluation_strategy=<span class="string">&quot;epoch&quot;</span>,  <span class="comment"># 评估策略，这里选择每个epoch评估一次</span></span><br><span class="line">    save_strategy=<span class="string">&quot;epoch&quot;</span>,  <span class="comment"># 保存策略，这里选择每个epoch保存一次</span></span><br><span class="line">    load_best_model_at_end=<span class="literal">True</span>,  <span class="comment"># 在训练结束时加载最佳模型</span></span><br><span class="line">    save_total_limit=<span class="number">1</span>,  <span class="comment"># 只保存1个检查点，注意这个检查点不一定是最优的模型</span></span><br><span class="line">    metric_for_best_model=<span class="string">&quot;eval_loss&quot;</span>,  <span class="comment"># 评估最优模型的指标 eval_loss是自带的，如果使用自定义的指标，需要在compute_metrics方法中进行定义</span></span><br><span class="line">    greater_is_better=<span class="literal">False</span>,  <span class="comment"># 明确指定loss指标越小越好</span></span><br><span class="line">    fp16=<span class="literal">False</span>,  <span class="comment"># 使用混合精度训练（GPU 支持时）</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 模型训练-创建Trainer对象</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=self.model,  <span class="comment"># 模型</span></span><br><span class="line">    args=training_args,  <span class="comment"># 训练参数</span></span><br><span class="line">    train_dataset=train_dataset,  <span class="comment"># 训练数据集</span></span><br><span class="line">    eval_dataset=val_dataset,  <span class="comment"># 验证数据集</span></span><br><span class="line">    compute_metrics=self.compute_metrics,  <span class="comment"># 自定义的评估指标</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 模型训练-开始训练</span></span><br><span class="line">self.logger.info(<span class="string">&quot;开始训练模型...&quot;</span>)</span><br><span class="line">trainer.train()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6）保存模型</span></span><br><span class="line">self.save_model()</span><br></pre></td></tr></table></figure>

<h1 id="Day08"><a href="#Day08" class="headerlink" title="Day08"></a>Day08</h1><h2 id="1、详细叙述RAG系统工作流程是如何来实现的？"><a href="#1、详细叙述RAG系统工作流程是如何来实现的？" class="headerlink" title="1、详细叙述RAG系统工作流程是如何来实现的？"></a>1、详细叙述RAG系统工作流程是如何来实现的？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20251021032402042.png" alt="image-20251021032402042"></p>
<h3 id="如何实现的查询改写策略选择？"><a href="#如何实现的查询改写策略选择？" class="headerlink" title="如何实现的查询改写策略选择？"></a>如何实现的查询改写策略选择？</h3><p>在我们项目中，主要是通过写提示词的方式，告诉大模型每种策略的含义、适用的场景，并通过示例进行说明。然后让大模型根据输入的问题输出对应的策略。</p>
<p>后期准备构建一些数据集，然后微调大模型，从而实现更加精准的策略选择。</p>
<h3 id="对于子查询检索策略具体怎么实现的？"><a href="#对于子查询检索策略具体怎么实现的？" class="headerlink" title="对于子查询检索策略具体怎么实现的？"></a>对于子查询检索策略具体怎么实现的？</h3><p>首先将原始问题交给大模型，让大模型基于子查询的方式将原始问题拆分成几个子问题。然后对于每个子问题分别对Milvus向量数据库进行混合检索和重排序，得到每个子问题对应的排序后的去重的父块内容。然后再根据父块的ID进行去重，得到最终需要的父块内容。最后将父块文档内容进行拼接，再使用RAG的提示词模版和问题进行拼接，一起送到大模型中得到答案。</p>
<h2 id="2、查询改写使用了哪些策略？有什么区别？"><a href="#2、查询改写使用了哪些策略？有什么区别？" class="headerlink" title="2、查询改写使用了哪些策略？有什么区别？"></a>2、查询改写使用了哪些策略？有什么区别？</h2><ul>
<li>直接检索<ul>
<li>描述：对用户查询直接进行检索，不进行任何增强处理。</li>
<li>适用场景：适用于查询意图明确，可以直接从知识库中进行检索的问题。</li>
</ul>
</li>
<li>假设问题检索（HyDE）<ul>
<li>描述：使用 LLM 生成一个假设的答案，然后基于假设答案进行检索。</li>
<li>适用场景：适用于查询较为抽象，直接检索效果不佳的问题，可以先生成一个近似的答案，再去检索这个答案相似的文档，效果更好。</li>
</ul>
</li>
<li>子查询检索<ul>
<li>描述：将复杂的用户查询拆分为多个简单的子查询，分别检索并合并结果。</li>
<li>适用场景：适用于查询涉及多个实体或方面，需要将原始问题拆分成几个子问题，分别检索不同子问题的相似问题，然后进行汇总。</li>
</ul>
</li>
<li>回溯问题检索<ul>
<li>描述：将复杂的用户查询转化为更基础、更易于检索的问题，然后进行检索。</li>
<li>适用场景：适用于查询较为复杂，需要简化后才能有效检索的问题。</li>
</ul>
</li>
<li>历史会话改写检索<ul>
<li>描述：结合前后文信息，对查询的问题进行改写，将缺失的信息进行补齐。</li>
<li>适用场景：在对话中，前后文是相互关联的。如果仅凭当前的query进行检索，可能会导致召回精度大幅下降，因为query中往往缺少重要的上下文信息。</li>
</ul>
</li>
</ul>
<h1 id="Day09"><a href="#Day09" class="headerlink" title="Day09"></a>Day09</h1><h2 id="1、RAG系统评估的数据集是如何构造的？"><a href="#1、RAG系统评估的数据集是如何构造的？" class="headerlink" title="1、RAG系统评估的数据集是如何构造的？"></a>1、RAG系统评估的数据集是如何构造的？</h2><p>流程如下：</p>
<ul>
<li>使用大模型生成测试样本，具体来说是<strong>使用Milvus中存储好的分块文档（子块内容），基于这些文档逆向生成问题和答案</strong> ，这些数据构成了评估系统的基础。</li>
<li>使用大模型对测试样本进行质量审核，确保数据具备清晰性和可检索性。</li>
<li>使用RAG系统对测试样本进行处理，记录检索文档和最终答案，连同问题和真实答案，构造评估数据集，用于评估系统。</li>
<li>使用大模型或评估框架对RAG系统性能进行量化评估。</li>
</ul>
<p>评估数据集格式：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">[</span></span><br><span class="line">  <span class="attr">&#123;</span></span><br><span class="line">    <span class="attr">&quot;question&quot;</span>: <span class="string">&quot;人工智能就业课的课程版本是什么？&quot;,</span></span><br><span class="line">    <span class="attr">&quot;context&quot;</span>: <span class="string">[&quot;人工智能学科全新升级——人工智能开发V6.0课程。&quot;],</span></span><br><span class="line">    <span class="attr">&quot;answer&quot;</span>: <span class="string">&quot;人工智能就业课的课程版本是V6.0。&quot;,</span></span><br><span class="line">    <span class="attr">&quot;ground_truth&quot;</span>: <span class="string">&quot;V6.0&quot;</span></span><br><span class="line">  <span class="attr">&#125;,</span></span><br><span class="line">  <span class="attr">&#123;</span></span><br><span class="line">    <span class="attr">&quot;question&quot;</span>: <span class="string">&quot;课程的一句话概括是什么？&quot;,</span></span><br><span class="line">    <span class="attr">&quot;context&quot;</span>: <span class="string">[&quot;解锁「大模型」 新技能成就「高薪AI」人才&quot;],</span></span><br><span class="line">    <span class="attr">&quot;answer&quot;</span>: <span class="string">&quot;课程的一句话概括是：解锁「大模型」新技能成就「高薪AI」人才。&quot;,</span></span><br><span class="line">    <span class="attr">&quot;ground_truth&quot;</span>: <span class="string">&quot;解锁「大模型」新技能成就「高薪AI」人才。&quot;</span></span><br><span class="line">  <span class="attr">&#125;</span></span><br><span class="line"><span class="attr">]</span></span><br></pre></td></tr></table></figure>

<h2 id="2、RAGAS评估框架中有哪些指标？什么作用？"><a href="#2、RAGAS评估框架中有哪些指标？什么作用？" class="headerlink" title="2、RAGAS评估框架中有哪些指标？什么作用？"></a>2、RAGAS评估框架中有哪些指标？什么作用？</h2><table>
<thead>
<tr>
<th>指标</th>
<th>阶段</th>
<th>作用</th>
<th>谁和谁</th>
<th>通俗理解</th>
</tr>
</thead>
<tbody><tr>
<td>上下文相关性</td>
<td>检索</td>
<td>衡量检索到的文档是否全和原始问题有关系</td>
<td>文档与问题</td>
<td>开卷资料带没带对</td>
</tr>
<tr>
<td>上下文召回率</td>
<td>检索</td>
<td>衡量真实答案能否全从检索到的文档中推出来</td>
<td>真实答案与文档</td>
<td>开卷资料带没带全</td>
</tr>
<tr>
<td>忠实度</td>
<td>生成</td>
<td>衡量模型生成的答案能否全从检索到的文档中推出来</td>
<td>模型答案与文档</td>
<td>是不是根据开卷资料里回答的</td>
</tr>
<tr>
<td>答案相关性</td>
<td>生成</td>
<td>衡量模型生成的答案是否和原始问题有关系</td>
<td>模型答案与问题</td>
<td>有没有跑题（是否抄对资料）</td>
</tr>
</tbody></table>
<p><strong>具体计算方式：</strong></p>
<ul>
<li>上下文相关性：将文档和问题交给LLM，由LLM统计与问题相关的句子数量，然后除以总的句子数量。</li>
<li>上下文召回率：将真实答案拆分成几个事实点，然后由LLM判断每个事实点是否能由文档推出，召回率等于能推出的事实数&#x2F;总的事实数。</li>
<li>忠实度：从模型答案生成几个陈述，然后由LLM判断每个陈述是否能由文档推出，忠实度等于能推出的陈述数&#x2F;总的陈述数。</li>
<li>答案相关性：使用LLM对模型答案生成几个问题，然后使用embedding模型获取所有问题的嵌入，然后计算原始问题与生成问题的平均余弦相似度作为答案相关性。</li>
</ul>
<h2 id="3、你们企业中RAGAS的指标是多少？"><a href="#3、你们企业中RAGAS的指标是多少？" class="headerlink" title="3、你们企业中RAGAS的指标是多少？"></a>3、你们企业中RAGAS的指标是多少？</h2><p><strong>一般企业中这几个指标的范围</strong>：</p>
<p>context_recall：86%左右</p>
<p>context_precision：78%左右</p>
<p>faithfulness：91%左右</p>
<p>answer_relevancy：92%左右</p>
<p>注意：</p>
<ul>
<li>context_precision和context_recall是负相关的。一般会牺牲context_precision来换取高的context_recall。</li>
<li>answer_relevancy是答案相关性，不等同于答案正确率，答案正确率由context_recall、faithfulness、answer_relevancy共同决定。答案正确率大概在75%。</li>
<li>faithfulness和answer_relevancy也是有负相关性的，只是没有那么强烈。在选择时，如果上下文检索的特别好，就可以提高faithfulness；如果上下文检索的不好，就提高answer_relevancy。</li>
</ul>
<h2 id="4、叙述一下如何将基于MySQL的FQA系统和基于Milvus的RAG系统进行整合？"><a href="#4、叙述一下如何将基于MySQL的FQA系统和基于Milvus的RAG系统进行整合？" class="headerlink" title="4、叙述一下如何将基于MySQL的FQA系统和基于Milvus的RAG系统进行整合？"></a>4、叙述一下如何将基于MySQL的FQA系统和基于Milvus的RAG系统进行整合？</h2><p>分别实例化BM25Search对象和RAGSystem对象，调用BM25Search对象的search方法进行缓存查询，如果拿到了缓存答案则直接进行返回；如果没有拿到缓存答案，则调用RAGSystem对象的generate_answer方法进行RAG检索；如果用户的问题有问题，则返回默认答案。</p>
<h2 id="5、说一下项目是如何来做会话管理的？"><a href="#5、说一下项目是如何来做会话管理的？" class="headerlink" title="5、说一下项目是如何来做会话管理的？"></a>5、说一下项目是如何来做会话管理的？</h2><p>建库建表：项目使用MySQL来存储每个用户的会话信息，将用户的ID、SessionID、问题、答案与时间等信息存储到MySQL中。</p>
<p>数据更新：每次当问答系统生成答案时，将问题和对应的答案更新到MySQL中。</p>
<p>使用：在使用大模型生成答案时，会从MySQL中获取最近5轮对话，将其拼接到提示词中，然后送入大模型。</p>
<h2 id="6、如何让大模型进行流式输出？在使用流式输出的时候需要注意什么？"><a href="#6、如何让大模型进行流式输出？在使用流式输出的时候需要注意什么？" class="headerlink" title="6、如何让大模型进行流式输出？在使用流式输出的时候需要注意什么？"></a>6、如何让大模型进行流式输出？在使用流式输出的时候需要注意什么？</h2><p>在调用大模型时，将stream参数设置成true，即可进行流式输出。</p>
<p>需要注意的点是：流式输出的结果是一个迭代器，需要使用遍历的方式进行获取，如果需要将结果进行返回，则使用yield进行返回。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">call_dashscope_stream</span>(<span class="params">self, prompt</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.client:  <span class="comment"># 检查客户端是否可用</span></span><br><span class="line">        logger.error(<span class="string">&quot;LLM 客户端未初始化，无法调用 call_dashscope&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;错误: LLM客户端不可用&quot;</span></span><br><span class="line">    <span class="comment"># 调用 DashScope API</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 创建聊天完成请求</span></span><br><span class="line">        completion = self.client.chat.completions.create(</span><br><span class="line">            model=conf.LLM_MODEL,</span><br><span class="line">            messages=[</span><br><span class="line">                &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;你是一个有用的助手。&quot;</span>&#125;,</span><br><span class="line">                &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt&#125;,</span><br><span class="line">            ],</span><br><span class="line">            temperature=<span class="number">0.1</span>,</span><br><span class="line">            stream=<span class="literal">True</span>,  <span class="comment"># 启用流式输出</span></span><br><span class="line">            timeout=<span class="number">30</span>  <span class="comment"># 设置超时时间为30秒</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 遍历流式输出的每个chunk</span></span><br><span class="line">        <span class="keyword">for</span> chunk <span class="keyword">in</span> completion:</span><br><span class="line">            <span class="keyword">if</span> chunk.choices <span class="keyword">and</span> chunk.choices[<span class="number">0</span>].delta.content:</span><br><span class="line">                <span class="comment"># 使用yield语句返回每个chunk的content部分</span></span><br><span class="line">                <span class="keyword">yield</span> chunk.choices[<span class="number">0</span>].delta.content</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="comment"># 记录 API 调用失败</span></span><br><span class="line">        self.logger.error(<span class="string">f&quot;DashScope API 调用失败: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">yield</span> <span class="string">f&#x27;错误：LLM API 调用失败 <span class="subst">&#123;e&#125;</span>&#x27;</span></span><br></pre></td></tr></table></figure>



<h2 id="7、FastAPI和Flask的联系和区别是什么？"><a href="#7、FastAPI和Flask的联系和区别是什么？" class="headerlink" title="7、FastAPI和Flask的联系和区别是什么？"></a>7、FastAPI和Flask的联系和区别是什么？</h2><ul>
<li><p>联系：</p>
<p>它们都是 Python Web 框架，用来创建 API 或 Web 应用。都能接收 HTTP 请求，根据 URL 路由到相应的处理函数，然后返回 HTTP 响应。</p>
</li>
<li><p>区别：</p>
<p>Flask的核心机制是同步阻塞 ，并发能力较低，适合CPU 密集型任务 (需要持续计算)。</p>
<p>而FastAPI 的核心机制是异步非阻塞，并发能力高，适合I&#x2F;O 密集型任务 (大量等待时间，如网络、磁盘、LLM)。</p>
</li>
</ul>
<h2 id="8、在利用大模型部署时怎么选择？"><a href="#8、在利用大模型部署时怎么选择？" class="headerlink" title="8、在利用大模型部署时怎么选择？"></a>8、在利用大模型部署时怎么选择？</h2><p>强烈推荐使用 FastAPI (异步)</p>
<p>原因：LLM 调用是典型的需要大量耗时的操作；异步能极大提高并发处理能力；FastAPI 基于 Pydantic 进行自动的数据校验和文档生成，方便处理数据。</p>
<h1 id="Agent-Day01"><a href="#Agent-Day01" class="headerlink" title="Agent-Day01"></a>Agent-Day01</h1><h2 id="1、什么是Function-Call？"><a href="#1、什么是Function-Call？" class="headerlink" title="1、什么是Function Call？"></a>1、什么是Function Call？</h2><p>概念：大模型基于具体任务，智能决策何时需要调用某个函数，同时返回符合函数参数的 JSON对象。</p>
<p>能力获得的方式：基于训练来得到的，所以并不是所有大模型都具有Function Call能力。</p>
<p>优势：信息实时性、数据局限性、功能扩展性。</p>
<h2 id="2、Function-Call-工作原理是什么？"><a href="#2、Function-Call-工作原理是什么？" class="headerlink" title="2、Function Call 工作原理是什么？"></a>2、Function Call 工作原理是什么？</h2><p>主要步骤：</p>
<ul>
<li>用户（客户端）发送请求和提示词，聊天服务器（Chat Server）将该提示词以及当前可调用的函数列表一并发送给大模型。</li>
<li>大模型根据提示词的内容和上下文，判断应生成普通文本回复，还是以函数调用的格式进行响应。</li>
<li>如果模型决定调用函数，它会返回一个包含函数名称和参数的结构化调用指令；聊天服务器接收到该指令后，执行对应的函数，并将函数的实际执行结果返回给大模型。</li>
<li>模型再根据函数返回的数据，将其整合并生成一段自然、连贯的文本作为最终回复，返回给用户。</li>
</ul>
<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20251025082636706.png" alt="image-20251025082636706" style="zoom:67%;" />

<h2 id="3、Function-Call的使用方式"><a href="#3、Function-Call的使用方式" class="headerlink" title="3、Function Call的使用方式"></a>3、Function Call的使用方式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># todo: 第一步：定义工具函数</span></span><br><span class="line"><span class="meta">@tool</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">a: <span class="built_in">int</span>, b: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将数字a与数字b相加</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        a: 第一个数字</span></span><br><span class="line"><span class="string">        b: 第二个数字</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> a + b</span><br><span class="line"></span><br><span class="line"><span class="meta">@tool</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multiply</span>(<span class="params">a: <span class="built_in">int</span>, b: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将数字a与数字b相乘</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        a: 第一个数字</span></span><br><span class="line"><span class="string">        b: 第二个数字</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> a * b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 JSON 格式的工具 schema</span></span><br><span class="line">tools = [add, multiply]</span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 第二步：初始化模型</span></span><br><span class="line">llm = ChatOpenAI(base_url=conf.base_url,</span><br><span class="line">                 model=conf.model_name,</span><br><span class="line">                 api_key=conf.api_key,</span><br><span class="line">                 temperature=<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># 绑定工具，允许模型自动选择工具</span></span><br><span class="line">llm_with_tools = llm.bind_tools(tools, tool_choice=<span class="string">&quot;auto&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># todo: 第三步：调用回复</span></span><br><span class="line">query = <span class="string">&quot;2+1等于多少？&quot;</span></span><br><span class="line"><span class="comment"># 使用列表的方式来存储对话信息</span></span><br><span class="line">messages = [HumanMessage(query)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># todo: 第一次调用</span></span><br><span class="line">    ai_msg = llm_with_tools.invoke(messages)</span><br><span class="line">    messages.append(ai_msg)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\n第一轮调用后结果：\n<span class="subst">&#123;messages&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># todo: 处理工具调用</span></span><br><span class="line">    <span class="comment"># 判断消息中是否有tool_calls，以判断工具是否被调用</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(ai_msg, <span class="string">&quot;tool_calls&quot;</span>) <span class="keyword">and</span> ai_msg.tool_calls:</span><br><span class="line">        <span class="keyword">for</span> tool_call <span class="keyword">in</span> ai_msg.tool_calls:</span><br><span class="line">            <span class="comment"># 基于工具名称获取对应的函数</span></span><br><span class="line">            func = &#123;<span class="string">&quot;add&quot;</span>: add, <span class="string">&quot;multiply&quot;</span>: multiply&#125;[tool_call[<span class="string">&quot;name&quot;</span>].lower()]</span><br><span class="line">            <span class="comment"># 解析参数，并调用对应的函数【因为原始的函数使用了注解，现在就是一个被langchain封装后的方法，调用时需要使用invoke进行调用】</span></span><br><span class="line">            tool_result = func.invoke(tool_call[<span class="string">&quot;args&quot;</span>])</span><br><span class="line">            <span class="comment"># 将工具调用结果添加到messages中，用于下一次调用</span></span><br><span class="line">            messages.append(ToolMessage(content=tool_result, tool_call_id=tool_call[<span class="string">&quot;id&quot;</span>]))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\n工具调用结果添加到messages后：\n<span class="subst">&#123;messages&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># todo: 第二次调用，将工具结果传回模型以生成最终回答</span></span><br><span class="line">        final_response = llm_with_tools.invoke(messages)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\n最终模型响应：\n<span class="subst">&#123;final_response.content&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;模型未生成工具调用，直接返回文本:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(ai_msg.content)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;调用失败：<span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="4、什么是MCP协议？"><a href="#4、什么是MCP协议？" class="headerlink" title="4、什么是MCP协议？"></a>4、什么是MCP协议？</h2><p>MCP（Model Context Protocol，模型上下文协议）是由 Anthropic 在2024年1月提出的一套开放协议，<strong>旨在实现大型语言模型（LLM）与外部数据源和工具的无缝集成，用来在大模型和数据源之间建立安全双向的链接</strong>。也就是说，将这些外部数据源和工具进行统一管理，以方便大模型进行统一调用。</p>
<h2 id="5、MCP-工具调用流程"><a href="#5、MCP-工具调用流程" class="headerlink" title="5、MCP 工具调用流程"></a>5、MCP 工具调用流程</h2><ul>
<li><p>客户端连接多个 MCP Server，获取并缓存工具清单。</p>
</li>
<li><p>LLM 通过这些清单“知道”有哪些可用工具。</p>
</li>
<li><p>LLM 根据用户请求决定调用哪个工具，并发出调用指令。</p>
</li>
<li><p>MCP Client 把指令转发给对应 Server，Server 执行工具逻辑。</p>
</li>
<li><p>结果返回给 Client，再传回 LLM，生成最终回答。</p>
</li>
</ul>
<h2 id="6、MCP的通信传输方式有哪些？"><a href="#6、MCP的通信传输方式有哪些？" class="headerlink" title="6、MCP的通信传输方式有哪些？"></a>6、MCP的通信传输方式有哪些？</h2><p>Stdio：本地进程间通信，部署简单、延迟低，适合私有化或本地化场景；缺点是不能跨网络使用。</p>
<p>SSE（Server-Sent Events）：通过 HTTP 长连接单向推送服务器事件，适合实时通知；但需要维护长连接，对横向扩展和断连重连有额外设计成本。</p>
<p>Streamable HTTP：基于单个 HTTP 端点同时支持请求—响应（双向）与流式传输，集成云&#x2F;负载均衡器更方便，灵活性和工程友好度使其成为当前主流选择。</p>
<h2 id="7、服务端和客户端使用示例"><a href="#7、服务端和客户端使用示例" class="headerlink" title="7、服务端和客户端使用示例"></a>7、服务端和客户端使用示例</h2><p>服务端：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mcp.server.fastmcp <span class="keyword">import</span> FastMCP</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 MCP 实例，指定服务名称、日志级别、主机和端口</span></span><br><span class="line">mcp = FastMCP(<span class="string">&quot;sdg&quot;</span>, log_level=<span class="string">&quot;ERROR&quot;</span>, host=<span class="string">&quot;127.0.0.1&quot;</span>, port=<span class="number">8001</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@mcp.tool(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="meta">    name=<span class="string">&quot;query_high_frequency_question&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">    description=<span class="string">&quot;从知识库中检索常见问题解答（FAQ）,返回包含问题和答案的结构化JSON数据。&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta"></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">query_high_frequency_question</span>() -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;高频问题是: 恐龙是怎么灭绝的？&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    mcp.run(transport=<span class="string">&quot;streamable-http&quot;</span>)  <span class="comment"># 使用 streamable-http 传输方式</span></span><br></pre></td></tr></table></figure>

<p>客户端：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mcp.client.streamable_http <span class="keyword">import</span> streamablehttp_client</span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> create_tool_calling_agent, AgentExecutor</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_mcp_adapters.tools <span class="keyword">import</span> load_mcp_tools</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> mcp <span class="keyword">import</span> ClientSession</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">from</span> agent_learn.config <span class="keyword">import</span> Config</span><br><span class="line">conf = Config()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">llm = ChatOpenAI(base_url=conf.base_url,</span><br><span class="line">                 api_key=conf.api_key,</span><br><span class="line">                 model=conf.model_name,</span><br><span class="line">                 temperature=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># MCP Server URL</span></span><br><span class="line">server_url = <span class="string">&quot;http://localhost:8001/mcp&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个异步函数，来实现客户端的创建及使用</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="keyword">global</span> mcp_client</span><br><span class="line">    <span class="comment"># 启动 MCP server，并通过标准输入输出建立异步连接。</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> streamablehttp_client(url=server_url) <span class="keyword">as</span> (read, write, _):</span><br><span class="line">        <span class="comment"># 使用读写流创建MCP会话对象</span></span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> ClientSession(read, write) <span class="keyword">as</span> session:</span><br><span class="line">            <span class="comment"># 初始化session</span></span><br><span class="line">            <span class="keyword">await</span> session.initialize()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 从 session 自动获取 MCP server 提供的工具列表。</span></span><br><span class="line">            tools = <span class="keyword">await</span> load_mcp_tools(session)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 创建prompt模板</span></span><br><span class="line">            <span class="comment"># agent_scratchpad 这个参数是agent在进行推理的时候，自动生成的，用于记录agent的推理过程。</span></span><br><span class="line">            prompt_template = ChatPromptTemplate.from_messages([</span><br><span class="line">                (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是一个乐于助人的助手，能够调用工具回答用户问题。&quot;</span>),</span><br><span class="line">                (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;input&#125;&quot;</span>),</span><br><span class="line">                (<span class="string">&quot;placeholder&quot;</span>, <span class="string">&quot;&#123;agent_scratchpad&#125;&quot;</span>),</span><br><span class="line">            ])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 创建agent对象</span></span><br><span class="line">            agent = create_tool_calling_agent(llm, tools, prompt_template)</span><br><span class="line">            <span class="comment"># 使用agent对象创建agent执行器</span></span><br><span class="line">            agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># agent调用</span></span><br><span class="line">            query = <span class="string">&quot;北京天气如何？&quot;</span></span><br><span class="line">            response = <span class="keyword">await</span> agent_executor.ainvoke(&#123;<span class="string">&quot;input&quot;</span>: query&#125;)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;response--&gt;<span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="comment"># 终止进程</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动运行</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    asyncio.run(main())</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://liamjohnson-w.github.io">李俊泽</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://liamjohnson-w.github.io/2025/09/28/LLM_Base/">https://liamjohnson-w.github.io/2025/09/28/LLM_Base/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/AI/">AI</a></div><div class="post_share"><div class="social-share" data-image="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen112.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> Donate</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/10/21/RAG/" title="RAG"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen3.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">RAG</div></div></a></div><div class="next-post pull-right"><a href="/2025/09/14/TrafficDefenceDetection/" title="TQ System"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen132.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">TQ System</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2025/06/03/2025.06.03/" title="OLLAMA"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250603132906789.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-03</div><div class="title">OLLAMA</div></div></a></div><div><a href="/2025/06/22/NLP_Base/" title="NLP自然语言处理"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/%E7%94%9F%E6%88%90%E7%8C%AB%E5%92%AA%E5%9B%BE%E7%89%87%20(2).png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-22</div><div class="title">NLP自然语言处理</div></div></a></div><div><a href="/2025/07/18/Project/" title="Jason Project Demo"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="title">Jason Project Demo</div></div></a></div><div><a href="/2025/07/16/%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/" title="算法公式推导"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-16</div><div class="title">算法公式推导</div></div></a></div><div><a href="/2025/06/20/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_AI/" title="数据结构"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/cat1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-20</div><div class="title">数据结构</div></div></a></div><div><a href="/2025/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" title="机器学习"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1d5d1a46-65f8-4544-8195-231e2c2da969.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-04</div><div class="title">机器学习</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/tx.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">李俊泽</div><div class="author-info__description">机器都在学习,你有什么理由不学习?</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">237</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">58</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/weiswift/"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/weiswift" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:1265019024@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">博客为本人搭建 Github托管 仅记录学习过程 不做引流 不做排名 不打广告！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8BLLM"><span class="toc-number">1.</span> <span class="toc-text">大语言模型LLM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B"><span class="toc-number">1.1.</span> <span class="toc-text">大模型发展历程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%AD%90%EF%B8%8F%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84"><span class="toc-number">1.2.</span> <span class="toc-text">⭐️大语言模型（LLM）核心技术架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LLM%E8%AE%AD%E7%BB%83"><span class="toc-number">1.3.</span> <span class="toc-text">LLM训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E7%9A%84%E5%AD%98%E5%82%A8"><span class="toc-number">1.4.</span> <span class="toc-text">大模型权重的存储</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B2%BE%E5%BA%A6%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.5.</span> <span class="toc-text">大模型精度类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%AD%90%EF%B8%8F%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8"><span class="toc-number">1.6.</span> <span class="toc-text">⭐️大模型使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%80%E6%BA%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8"><span class="toc-number">1.6.1.</span> <span class="toc-text">开源大模型使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AD%E6%BA%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8"><span class="toc-number">1.6.2.</span> <span class="toc-text">闭源大模型使用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%AD%90%EF%B8%8F%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-number">1.7.</span> <span class="toc-text">⭐️语言模型的评估指标</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#BLEU"><span class="toc-number">1.7.1.</span> <span class="toc-text">BLEU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ROUGE"><span class="toc-number">1.7.2.</span> <span class="toc-text">ROUGE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PPL"><span class="toc-number">1.7.3.</span> <span class="toc-text">PPL</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#LLM%E6%9E%B6%E6%9E%84"><span class="toc-number">2.</span> <span class="toc-text">LLM架构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#AE%E8%87%AA%E7%BC%96%E7%A0%81%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.</span> <span class="toc-text">AE自编码模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Bert%E6%9E%84%E6%88%90"><span class="toc-number">2.1.1.</span> <span class="toc-text">Bert构成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bert%E6%A8%A1%E5%9E%8B%E9%80%9A%E8%AF%86"><span class="toc-number">2.1.2.</span> <span class="toc-text">Bert模型通识</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AE%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93"><span class="toc-number">2.1.3.</span> <span class="toc-text">AE模型总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AR%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.2.</span> <span class="toc-text">AR自回归模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT%E6%A8%A1%E5%9E%8B%E6%9E%84%E6%88%90"><span class="toc-number">2.2.1.</span> <span class="toc-text">GPT模型构成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E7%82%B9"><span class="toc-number">2.2.2.</span> <span class="toc-text">GPT模型的特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AR%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93"><span class="toc-number">2.2.3.</span> <span class="toc-text">AR模型总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Seq2Seq%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97"><span class="toc-number">2.3.</span> <span class="toc-text">Seq2Seq序列到序列</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#T5%E6%A8%A1%E5%9E%8B%E6%9E%84%E6%88%90"><span class="toc-number">2.3.1.</span> <span class="toc-text">T5模型构成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#T5%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E7%82%B9"><span class="toc-number">2.3.2.</span> <span class="toc-text">T5模型的特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#encoder-decoder%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93"><span class="toc-number">2.3.3.</span> <span class="toc-text">encoder-decoder模型总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B"><span class="toc-number">3.</span> <span class="toc-text">提示词工程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B8%85%E6%99%B0%E7%9A%84%E6%8C%87%E4%BB%A4"><span class="toc-number">3.1.</span> <span class="toc-text">清晰的指令</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E5%8F%82%E8%80%83"><span class="toc-number">3.2.</span> <span class="toc-text">文本参考</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%8D%E6%9D%82%E4%BB%BB%E5%8A%A1%E6%8B%86%E5%88%86%E7%AE%80%E5%8D%95%E5%AD%90%E4%BB%BB%E5%8A%A1"><span class="toc-number">3.3.</span> <span class="toc-text">复杂任务拆分简单子任务</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%99%E6%A8%A1%E5%9E%8B%E2%80%9D%E6%80%9D%E8%80%83%E2%80%9D%E7%9A%84%E6%97%B6%E9%97%B4"><span class="toc-number">3.4.</span> <span class="toc-text">给模型”思考”的时间</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%80%9F%E5%8A%A9%E5%A4%96%E9%83%A8%E5%B7%A5%E5%85%B7"><span class="toc-number">3.5.</span> <span class="toc-text">借助外部工具</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8F%90%E7%A4%BA%E6%8A%80%E6%9C%AF"><span class="toc-number">3.6.</span> <span class="toc-text">提示技术</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Zero-Shot"><span class="toc-number">3.6.1.</span> <span class="toc-text">Zero-Shot</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Few-Shot"><span class="toc-number">3.6.2.</span> <span class="toc-text">Few-Shot</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#COT"><span class="toc-number">3.6.3.</span> <span class="toc-text">COT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Prompt-Chaining"><span class="toc-number">3.6.4.</span> <span class="toc-text">Prompt Chaining</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReAct"><span class="toc-number">3.6.5.</span> <span class="toc-text">ReAct</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reflexion"><span class="toc-number">3.6.6.</span> <span class="toc-text">Reflexion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Self-Consistency"><span class="toc-number">3.6.7.</span> <span class="toc-text">Self-Consistency</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tree-of-Thoughts-ToT"><span class="toc-number">3.6.8.</span> <span class="toc-text">Tree of Thoughts (ToT)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Prompt%E9%87%91%E8%9E%8D%E5%B7%A5%E7%A8%8B%E9%A1%B9%E7%9B%AE"><span class="toc-number">4.</span> <span class="toc-text">Prompt金融工程项目</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9C%80%E6%B1%82"><span class="toc-number">4.1.</span> <span class="toc-text">需求</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LLM%E5%AE%9E%E7%8E%B0%E9%87%91%E8%9E%8D%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB"><span class="toc-number">4.2.</span> <span class="toc-text">LLM实现金融文本分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Few-Shot%E6%96%B9%E5%BC%8F"><span class="toc-number">4.2.1.</span> <span class="toc-text">Few-Shot方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#history%E6%96%B9%E5%BC%8F"><span class="toc-number">4.2.2.</span> <span class="toc-text">history方式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LLM%E5%AE%9E%E7%8E%B0%E9%87%91%E8%9E%8D%E6%96%87%E6%9C%AC%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96"><span class="toc-number">4.3.</span> <span class="toc-text">LLM实现金融文本信息抽取</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LLM%E5%AE%9E%E7%8E%B0%E9%87%91%E8%9E%8D%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D"><span class="toc-number">4.4.</span> <span class="toc-text">LLM实现金融文本匹配</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day01"><span class="toc-number">5.</span> <span class="toc-text">Day01</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AFRAG%EF%BC%9F%E6%9C%89%E5%93%AA%E4%BA%9B%E4%BD%9C%E7%94%A8%EF%BC%9F"><span class="toc-number">5.1.</span> <span class="toc-text">1、什么是RAG？有哪些作用？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81RAG%E6%A0%87%E5%87%86%E6%B5%81%E7%A8%8B%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">5.2.</span> <span class="toc-text">2、RAG标准流程是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81EduRAG%E7%9A%84%E9%A1%B9%E7%9B%AE%E6%B5%81%E7%A8%8B%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">5.3.</span> <span class="toc-text">3、EduRAG的项目流程是怎样的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81LangChain%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E4%B8%BB%E8%A6%81%E7%BB%84%E4%BB%B6%EF%BC%8C%E9%83%BD%E6%98%AF%E7%94%A8%E6%9D%A5%E5%B9%B2%E4%BB%80%E4%B9%88%E7%9A%84%EF%BC%9F"><span class="toc-number">5.4.</span> <span class="toc-text">4、LangChain中有哪些主要组件，都是用来干什么的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E7%9B%AE%E5%89%8DLangChain%E6%94%AF%E6%8C%81%E5%93%AA%E4%B8%89%E7%A7%8D%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="toc-number">5.5.</span> <span class="toc-text">5、目前LangChain支持哪三种类型的模型？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81Prompts%E7%BB%84%E4%BB%B6%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">5.6.</span> <span class="toc-text">6、Prompts组件的使用方式是怎样的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81Chains%E7%BB%84%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">5.7.</span> <span class="toc-text">7、Chains组件使用方式是怎样的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8%E3%80%81%E5%B8%B8%E7%94%A8%E7%9A%84Output-Parsers%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">5.8.</span> <span class="toc-text">8、常用的Output Parsers有哪些？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day02"><span class="toc-number">6.</span> <span class="toc-text">Day02</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8LangChain%E4%B8%ADMemory%E7%BB%84%E4%BB%B6%E6%9D%A5%E8%AE%B0%E5%BD%95%E4%B8%8A%E4%B8%8B%E6%96%87%EF%BC%9F"><span class="toc-number">6.1.</span> <span class="toc-text">1、如何使用LangChain中Memory组件来记录上下文？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81LangChain%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%96%87%E6%A1%A3%E5%8A%A0%E8%BD%BD%E5%99%A8%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">6.2.</span> <span class="toc-text">2、LangChain中常用的文档加载器有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E5%B8%B8%E7%94%A8%E7%9A%84%E6%96%87%E6%A1%A3%E5%88%86%E5%89%B2%E5%99%A8%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">6.3.</span> <span class="toc-text">3、常用的文档分割器有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81VectorStores%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">6.4.</span> <span class="toc-text">4、VectorStores是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E6%A3%80%E7%B4%A2%E5%99%A8%EF%BC%9F"><span class="toc-number">6.5.</span> <span class="toc-text">5、什么是检索器？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E6%A3%80%E7%B4%A2%E5%99%A8%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">6.6.</span> <span class="toc-text">6、检索器工作流程是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81%E5%B8%B8%E8%A7%81%E7%9A%84%E6%A3%80%E7%B4%A2%E5%99%A8%E7%B1%BB%E5%9E%8B%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">6.7.</span> <span class="toc-text">7、常见的检索器类型有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AFBM25%E7%AE%97%E6%B3%95%EF%BC%9F%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%EF%BC%9F"><span class="toc-number">6.8.</span> <span class="toc-text">8、什么是BM25算法？有什么特点？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9%E3%80%81Agent%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">6.9.</span> <span class="toc-text">9、Agent是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10%E3%80%81Milvus-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">6.10.</span> <span class="toc-text">10、Milvus 是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11%E3%80%81Milvus%E4%B8%AD%E6%A0%B8%E5%BF%83%E7%9A%84%E6%A6%82%E5%BF%B5%E6%9C%89%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">6.11.</span> <span class="toc-text">11、Milvus中核心的概念有什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E9%80%89%E6%8B%A9-Milvus%EF%BC%9F"><span class="toc-number">6.12.</span> <span class="toc-text">12、为什么选择 Milvus？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day03"><span class="toc-number">7.</span> <span class="toc-text">Day03</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81Milvus%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E7%B4%A2%E5%BC%95%E6%9C%89%E5%93%AA%E4%BA%9B%E7%B1%BB%E5%9E%8B%EF%BC%9F%E5%8E%9F%E7%90%86%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%EF%BC%9F"><span class="toc-number">7.1.</span> <span class="toc-text">1、Milvus中常用的索引有哪些类型？原理是什么？有什么特点？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81Milvus%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%BA%A6%E9%87%8F%E6%96%B9%E6%B3%95%EF%BC%9F"><span class="toc-number">7.2.</span> <span class="toc-text">2、Milvus中有哪些相似度度量方法？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81Milvus%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E8%BF%87%E6%BB%A4%E6%90%9C%E7%B4%A2%EF%BC%9F"><span class="toc-number">7.3.</span> <span class="toc-text">3、Milvus如何实现过滤搜索？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E4%BB%80%E4%B9%88%E5%8F%AB%E6%B7%B7%E5%90%88%E6%A3%80%E7%B4%A2%EF%BC%9F%E6%9C%89%E5%93%AA%E4%BA%9B%E6%96%B9%E5%BC%8F%EF%BC%9F"><span class="toc-number">7.4.</span> <span class="toc-text">4、什么叫混合检索？有哪些方式？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%B7%B7%E5%90%88%E6%A3%80%E7%B4%A2%EF%BC%9F"><span class="toc-number">7.5.</span> <span class="toc-text">5、如何实现混合检索？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E4%BD%A0%E4%BB%AC%E9%A1%B9%E7%9B%AE%E4%B8%AD%E6%9C%89%E6%B2%A1%E6%9C%89%E8%AE%B0%E5%BD%95%E6%88%96%E7%AE%A1%E7%90%86%E6%97%A5%E5%BF%97%EF%BC%9F%E6%80%8E%E4%B9%88%E5%81%9A%E7%9A%84%EF%BC%9F"><span class="toc-number">7.6.</span> <span class="toc-text">6、你们项目中有没有记录或管理日志？怎么做的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81%E5%A6%82%E4%BD%95%E5%B0%86%E6%97%A5%E5%BF%97%E6%97%B6%E8%BE%93%E5%87%BA%E5%88%B0%E6%8E%A7%E5%88%B6%E5%8F%B0%E5%92%8C%E6%96%87%E4%BB%B6%EF%BC%9F"><span class="toc-number">7.7.</span> <span class="toc-text">7、如何将日志时输出到控制台和文件？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day04"><span class="toc-number">8.</span> <span class="toc-text">Day04</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81rank-bm25%E6%A8%A1%E5%9D%97%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%EF%BC%9F"><span class="toc-number">8.1.</span> <span class="toc-text">1、rank_bm25模块如何使用？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81rank-bm25%E6%A8%A1%E5%9D%97%E4%B8%AD%E6%9C%89%E5%93%AA%E5%87%A0%E7%A7%8D%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="toc-number">8.2.</span> <span class="toc-text">2、rank_bm25模块中有哪几种模型？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81Redis%E6%98%AF%E5%B9%B2%E5%98%9B%E7%9A%84%EF%BC%9F%E5%9C%A8%E9%A1%B9%E7%9B%AE%E4%B8%AD%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8%EF%BC%9F"><span class="toc-number">8.3.</span> <span class="toc-text">3、Redis是干嘛的？在项目中有什么用？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%AF%B9Redis%E7%9A%84%E6%93%8D%E4%BD%9C%EF%BC%9F"><span class="toc-number">8.4.</span> <span class="toc-text">4、如何实现对Redis的操作？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%9A%84%E5%9F%BA%E4%BA%8EMySQL%E7%9A%84FQA%E7%B3%BB%E7%BB%9F%EF%BC%9F"><span class="toc-number">8.5.</span> <span class="toc-text">5、如何实现的基于MySQL的FQA系统？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81MySQL%E5%9C%A8%E9%A1%B9%E7%9B%AE%E4%B8%AD%E8%B5%B7%E4%BB%80%E4%B9%88%E4%BD%9C%E7%94%A8%EF%BC%9F%E6%80%8E%E4%B9%88%E7%94%A8%E7%9A%84%EF%BC%9F"><span class="toc-number">8.6.</span> <span class="toc-text">6、MySQL在项目中起什么作用？怎么用的？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day05"><span class="toc-number">9.</span> <span class="toc-text">Day05</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E8%AF%A6%E7%BB%86%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%8B%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8BM25%E5%AF%B9MySQL%E4%B8%AD%E7%9A%84%E9%AB%98%E9%A2%91%E9%97%AE%E7%AD%94%E5%AF%B9%E8%BF%9B%E8%A1%8C%E6%A3%80%E7%B4%A2%EF%BC%9F"><span class="toc-number">9.1.</span> <span class="toc-text">1、详细描述一下如何使用BM25对MySQL中的高频问答对进行检索？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E5%BD%93MySQL%E7%9A%84%E6%95%B0%E6%8D%AE%E6%9B%B4%E6%96%B0%E4%B9%8B%E5%90%8E%EF%BC%8C%E5%A6%82%E4%BD%95%E6%9B%B4%E6%96%B0Redis%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%8C%E8%BF%9B%E8%80%8C%E6%9B%B4%E6%96%B0BM25%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="toc-number">9.2.</span> <span class="toc-text">2、当MySQL的数据更新之后，如何更新Redis中的数据，进而更新BM25模型？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81RAG%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">9.3.</span> <span class="toc-text">3、RAG系统的工作流程是怎样的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AFOCR%EF%BC%9F%E4%BD%A0%E4%BB%AC%E9%A1%B9%E7%9B%AE%E4%B8%AD%E7%94%A8%E7%9A%84%E5%93%AA%E4%B8%AA%E6%A1%86%E6%9E%B6%EF%BC%9F"><span class="toc-number">9.4.</span> <span class="toc-text">4、什么是OCR？你们项目中用的哪个框架？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E4%BD%A0%E4%BB%AC%E9%A1%B9%E7%9B%AE%E6%98%AF%E5%A6%82%E4%BD%95%E6%9D%A5%E5%8A%A0%E8%BD%BDpdf%E7%9A%84%EF%BC%9F"><span class="toc-number">9.5.</span> <span class="toc-text">5、你们项目是如何来加载pdf的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E4%BD%A0%E4%BB%AC%E9%A1%B9%E7%9B%AE%E6%98%AF%E5%A6%82%E4%BD%95%E6%9D%A5%E5%8A%A0%E8%BD%BDdoc%E7%9A%84%EF%BC%9F"><span class="toc-number">9.6.</span> <span class="toc-text">6、你们项目是如何来加载doc的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81%E4%BD%A0%E4%BB%AC%E9%A1%B9%E7%9B%AEppt%E5%8A%A0%E8%BD%BD%E5%99%A8%E6%80%8E%E4%B9%88%E5%81%9A%E7%9A%84%EF%BC%9F"><span class="toc-number">9.7.</span> <span class="toc-text">7、你们项目ppt加载器怎么做的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8%E3%80%81%E4%BD%A0%E4%BB%AC%E9%A1%B9%E7%9B%AE%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E5%99%A8%E6%80%8E%E4%B9%88%E5%81%9A%E7%9A%84%EF%BC%9F"><span class="toc-number">9.8.</span> <span class="toc-text">8、你们项目图片加载器怎么做的？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day06"><span class="toc-number">10.</span> <span class="toc-text">Day06</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E4%BD%A0%E4%BB%AC%E9%A1%B9%E7%9B%AE%E7%94%A8%E5%88%B0%E4%BA%86%E5%93%AA%E4%BA%9B%E6%96%87%E6%9C%AC%E5%88%87%E5%88%86%E5%99%A8%EF%BC%9F%E6%80%8E%E4%B9%88%E5%88%87%E5%88%86%E7%9A%84%EF%BC%9F"><span class="toc-number">10.1.</span> <span class="toc-text">1、你们项目用到了哪些文本切分器？怎么切分的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%BF%9B%E8%A1%8C%E7%88%B6%E5%9D%97%E5%92%8C%E5%AD%90%E5%9D%97%E7%9A%84%E5%8C%BA%E5%88%86%EF%BC%9F"><span class="toc-number">10.2.</span> <span class="toc-text">2、为什么要进行父块和子块的区分？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E8%87%AA%E5%AE%9A%E4%B9%89%E4%B8%AD%E6%96%87%E9%80%92%E5%BD%92%E6%96%87%E6%9C%AC%E5%88%87%E5%88%86%E5%99%A8%EF%BC%9F%E6%80%8E%E4%B9%88%E5%81%9A%E7%9A%84%EF%BC%9F"><span class="toc-number">10.3.</span> <span class="toc-text">3、为什么自定义中文递归文本切分器？怎么做的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E8%87%AA%E5%AE%9A%E4%B9%89%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%AD%E4%B9%89%E5%88%87%E5%88%86%E5%99%A8%EF%BC%9F%E6%80%8E%E4%B9%88%E5%81%9A%E7%9A%84%EF%BC%9F"><span class="toc-number">10.4.</span> <span class="toc-text">4、为什么自定义基于模型的语义切分器？怎么做的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E9%A1%B9%E7%9B%AE%E4%B8%AD%E5%A6%82%E4%BD%95%E5%81%9A%E7%9A%84Embedding%EF%BC%9F"><span class="toc-number">10.5.</span> <span class="toc-text">5、项目中如何做的Embedding？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E7%A8%A0%E5%AF%86%E5%90%91%E9%87%8F%E5%92%8C%E7%A8%80%E7%96%8F%E5%90%91%E9%87%8F%E6%98%AF%E5%A6%82%E4%BD%95%E5%AD%98%E5%82%A8%E5%92%8C%E6%9E%84%E5%BB%BA%E7%B4%A2%E5%BC%95%E7%9A%84%EF%BC%9F"><span class="toc-number">10.6.</span> <span class="toc-text">6、稠密向量和稀疏向量是如何存储和构建索引的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81%E9%87%8D%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%B9%B2%E4%BB%80%E4%B9%88%E7%9A%84%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E9%87%8D%E6%8E%92%E5%BA%8F%EF%BC%9F"><span class="toc-number">10.7.</span> <span class="toc-text">7、重排序模型是干什么的？为什么需要重排序？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8%E3%80%81%E9%87%8D%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B%E7%94%A8%E7%9A%84%E6%98%AF%E4%BB%80%E4%B9%88%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">10.8.</span> <span class="toc-text">8、重排序模型用的是什么模型？为什么？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day07"><span class="toc-number">11.</span> <span class="toc-text">Day07</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E9%A1%B9%E7%9B%AE%E4%B8%AD%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%9A%84%E6%B7%B7%E5%90%88%E6%A3%80%E7%B4%A2%E5%92%8C%E9%87%8D%E6%8E%92%E5%BA%8F%EF%BC%9F"><span class="toc-number">11.1.</span> <span class="toc-text">1、项目中如何实现的混合检索和重排序？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E5%A6%82%E6%9E%9C%E5%9C%A8%E6%96%87%E6%A1%A3%E6%A3%80%E7%B4%A2%E6%97%B6%EF%BC%8C%E5%8F%91%E7%8E%B0%E6%9C%89%E6%95%88%E6%96%87%E6%A1%A3%E7%9A%84%E5%8F%AC%E5%9B%9E%E7%8E%87%E8%BE%83%E4%BD%8E%EF%BC%8C%E6%80%8E%E4%B9%88%E5%A4%84%E7%90%86%EF%BC%9F"><span class="toc-number">11.2.</span> <span class="toc-text">2、如果在文档检索时，发现有效文档的召回率较低，怎么处理？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E6%9F%A5%E8%AF%A2%E5%88%86%E7%B1%BB%E6%98%AF%E5%B9%B2%E5%98%9B%E7%9A%84%EF%BC%8C%E6%80%8E%E4%B9%88%E5%81%9A%E7%9A%84%EF%BC%9F"><span class="toc-number">11.3.</span> <span class="toc-text">3、查询分类是干嘛的，怎么做的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E4%BD%BF%E7%94%A8%E8%BF%87Trainer%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%90%97%EF%BC%9F%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%EF%BC%9F"><span class="toc-number">11.4.</span> <span class="toc-text">4、使用过Trainer训练模型吗？如何使用？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day08"><span class="toc-number">12.</span> <span class="toc-text">Day08</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E8%AF%A6%E7%BB%86%E5%8F%99%E8%BF%B0RAG%E7%B3%BB%E7%BB%9F%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E6%98%AF%E5%A6%82%E4%BD%95%E6%9D%A5%E5%AE%9E%E7%8E%B0%E7%9A%84%EF%BC%9F"><span class="toc-number">12.1.</span> <span class="toc-text">1、详细叙述RAG系统工作流程是如何来实现的？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%9A%84%E6%9F%A5%E8%AF%A2%E6%94%B9%E5%86%99%E7%AD%96%E7%95%A5%E9%80%89%E6%8B%A9%EF%BC%9F"><span class="toc-number">12.1.1.</span> <span class="toc-text">如何实现的查询改写策略选择？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8E%E5%AD%90%E6%9F%A5%E8%AF%A2%E6%A3%80%E7%B4%A2%E7%AD%96%E7%95%A5%E5%85%B7%E4%BD%93%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84%EF%BC%9F"><span class="toc-number">12.1.2.</span> <span class="toc-text">对于子查询检索策略具体怎么实现的？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E6%9F%A5%E8%AF%A2%E6%94%B9%E5%86%99%E4%BD%BF%E7%94%A8%E4%BA%86%E5%93%AA%E4%BA%9B%E7%AD%96%E7%95%A5%EF%BC%9F%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">12.2.</span> <span class="toc-text">2、查询改写使用了哪些策略？有什么区别？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Day09"><span class="toc-number">13.</span> <span class="toc-text">Day09</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81RAG%E7%B3%BB%E7%BB%9F%E8%AF%84%E4%BC%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E6%98%AF%E5%A6%82%E4%BD%95%E6%9E%84%E9%80%A0%E7%9A%84%EF%BC%9F"><span class="toc-number">13.1.</span> <span class="toc-text">1、RAG系统评估的数据集是如何构造的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81RAGAS%E8%AF%84%E4%BC%B0%E6%A1%86%E6%9E%B6%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%8C%87%E6%A0%87%EF%BC%9F%E4%BB%80%E4%B9%88%E4%BD%9C%E7%94%A8%EF%BC%9F"><span class="toc-number">13.2.</span> <span class="toc-text">2、RAGAS评估框架中有哪些指标？什么作用？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E4%BD%A0%E4%BB%AC%E4%BC%81%E4%B8%9A%E4%B8%ADRAGAS%E7%9A%84%E6%8C%87%E6%A0%87%E6%98%AF%E5%A4%9A%E5%B0%91%EF%BC%9F"><span class="toc-number">13.3.</span> <span class="toc-text">3、你们企业中RAGAS的指标是多少？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E5%8F%99%E8%BF%B0%E4%B8%80%E4%B8%8B%E5%A6%82%E4%BD%95%E5%B0%86%E5%9F%BA%E4%BA%8EMySQL%E7%9A%84FQA%E7%B3%BB%E7%BB%9F%E5%92%8C%E5%9F%BA%E4%BA%8EMilvus%E7%9A%84RAG%E7%B3%BB%E7%BB%9F%E8%BF%9B%E8%A1%8C%E6%95%B4%E5%90%88%EF%BC%9F"><span class="toc-number">13.4.</span> <span class="toc-text">4、叙述一下如何将基于MySQL的FQA系统和基于Milvus的RAG系统进行整合？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E8%AF%B4%E4%B8%80%E4%B8%8B%E9%A1%B9%E7%9B%AE%E6%98%AF%E5%A6%82%E4%BD%95%E6%9D%A5%E5%81%9A%E4%BC%9A%E8%AF%9D%E7%AE%A1%E7%90%86%E7%9A%84%EF%BC%9F"><span class="toc-number">13.5.</span> <span class="toc-text">5、说一下项目是如何来做会话管理的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E5%A6%82%E4%BD%95%E8%AE%A9%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E6%B5%81%E5%BC%8F%E8%BE%93%E5%87%BA%EF%BC%9F%E5%9C%A8%E4%BD%BF%E7%94%A8%E6%B5%81%E5%BC%8F%E8%BE%93%E5%87%BA%E7%9A%84%E6%97%B6%E5%80%99%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">13.6.</span> <span class="toc-text">6、如何让大模型进行流式输出？在使用流式输出的时候需要注意什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81FastAPI%E5%92%8CFlask%E7%9A%84%E8%81%94%E7%B3%BB%E5%92%8C%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">13.7.</span> <span class="toc-text">7、FastAPI和Flask的联系和区别是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8%E3%80%81%E5%9C%A8%E5%88%A9%E7%94%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E6%97%B6%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9%EF%BC%9F"><span class="toc-number">13.8.</span> <span class="toc-text">8、在利用大模型部署时怎么选择？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Agent-Day01"><span class="toc-number">14.</span> <span class="toc-text">Agent-Day01</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AFFunction-Call%EF%BC%9F"><span class="toc-number">14.1.</span> <span class="toc-text">1、什么是Function Call？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81Function-Call-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">14.2.</span> <span class="toc-text">2、Function Call 工作原理是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81Function-Call%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F"><span class="toc-number">14.3.</span> <span class="toc-text">3、Function Call的使用方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AFMCP%E5%8D%8F%E8%AE%AE%EF%BC%9F"><span class="toc-number">14.4.</span> <span class="toc-text">4、什么是MCP协议？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81MCP-%E5%B7%A5%E5%85%B7%E8%B0%83%E7%94%A8%E6%B5%81%E7%A8%8B"><span class="toc-number">14.5.</span> <span class="toc-text">5、MCP 工具调用流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81MCP%E7%9A%84%E9%80%9A%E4%BF%A1%E4%BC%A0%E8%BE%93%E6%96%B9%E5%BC%8F%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">14.6.</span> <span class="toc-text">6、MCP的通信传输方式有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%92%8C%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="toc-number">14.7.</span> <span class="toc-text">7、服务端和客户端使用示例</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/10/21/RAG/" title="RAG"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="RAG"/></a><div class="content"><a class="title" href="/2025/10/21/RAG/" title="RAG">RAG</a><time datetime="2025-10-20T16:00:00.000Z" title="Created 2025-10-21 00:00:00">2025-10-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/28/LLM_Base/" title="LLM大模型基础"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen112.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM大模型基础"/></a><div class="content"><a class="title" href="/2025/09/28/LLM_Base/" title="LLM大模型基础">LLM大模型基础</a><time datetime="2025-09-27T16:00:00.000Z" title="Created 2025-09-28 00:00:00">2025-09-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/14/TrafficDefenceDetection/" title="TQ System"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen132.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="TQ System"/></a><div class="content"><a class="title" href="/2025/09/14/TrafficDefenceDetection/" title="TQ System">TQ System</a><time datetime="2025-09-13T16:00:00.000Z" title="Created 2025-09-14 00:00:00">2025-09-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/18/Project/" title="Jason Project Demo"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Jason Project Demo"/></a><div class="content"><a class="title" href="/2025/07/18/Project/" title="Jason Project Demo">Jason Project Demo</a><time datetime="2025-07-17T16:00:00.000Z" title="Created 2025-07-18 00:00:00">2025-07-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/16/%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/" title="算法公式推导"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="算法公式推导"/></a><div class="content"><a class="title" href="/2025/07/16/%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/" title="算法公式推导">算法公式推导</a><time datetime="2025-07-15T16:00:00.000Z" title="Created 2025-07-16 00:00:00">2025-07-16</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 李俊泽</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Welcome to 李俊泽 の <a target="_blank" rel="noopener" href="https://www.cnblogs.com/liam-sliversucks/">Blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? '' : ''

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>