<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>机器学习 | All wisdom begins with memory.</title><meta name="author" content="李俊泽"><meta name="copyright" content="李俊泽"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="AI人工智能 讲到AI人工智能首先得从图灵测试开始说起： 图灵测试就是：测试者与被测试者（一个人和一台机器）隔开的情况下，遍过一些装置（如键盘）向被测试者随意提问。多次测试（一般为5min之内），如果有超过30%的测试者不能确定被测试者是人还是机器，那么这台机器就通过了测试，并被认为具有人类智能。"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://liamjohnson-w.github.io/2025/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-07-14 20:38:47'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css"> <script src="/live2d-widget/autoload.js"></script><script src="/live2d-widget/autoload.js"> </script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/tx.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">240</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">58</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-gamepad"></i><span> Games</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/mikutap/"><i class="fa-fw fa fa-music"></i><span> MikuTap 初音未来</span></a></li><li><a class="site-page child" href="/starbattle/"><i class="fa-fw fa fa-space-shuttle"></i><span> StartBattle 星际大战</span></a></li><li><a class="site-page child" href="/2048/"><i class="fa-fw fa fa-flag"></i><span> 2048 经典游戏</span></a></li><li><a class="site-page child" href="/battlecity/"><i class="fa-fw fa fa-arrow-circle-left"></i><span> BattleCity 坦克大战</span></a></li><li><a class="site-page child" href="/pacman/"><i class="fa-fw fa fa-bolt"></i><span> PacMan  吃豆人</span></a></li><li><a class="site-page child" href="/tetris/"><i class="fa-fw fa fa-arrows-alt"></i><span> Tetris 俄罗斯方块</span></a></li><li><a class="site-page child" href="/smallcat/"><i class="fa-fw fa fa-paw"></i><span> CatchCat 困住小猫</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-leaf"></i><span> Moments</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/diary/"><i class="fa-fw fas fa-bookmark"></i><span> Diary</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fa fa-hourglass-half"></i><span> Gallery</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-podcast"></i><span> More</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags标签</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About关于</span></a></li><li><a class="site-page child" href="/messageboard/"><i class="fa-fw fas fa-bookmark"></i><span> Messageboard留言板</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s1.ax1x.com/2023/04/18/p9i6u5D.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="All wisdom begins with memory."><span class="site-name">All wisdom begins with memory.</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-gamepad"></i><span> Games</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/mikutap/"><i class="fa-fw fa fa-music"></i><span> MikuTap 初音未来</span></a></li><li><a class="site-page child" href="/starbattle/"><i class="fa-fw fa fa-space-shuttle"></i><span> StartBattle 星际大战</span></a></li><li><a class="site-page child" href="/2048/"><i class="fa-fw fa fa-flag"></i><span> 2048 经典游戏</span></a></li><li><a class="site-page child" href="/battlecity/"><i class="fa-fw fa fa-arrow-circle-left"></i><span> BattleCity 坦克大战</span></a></li><li><a class="site-page child" href="/pacman/"><i class="fa-fw fa fa-bolt"></i><span> PacMan  吃豆人</span></a></li><li><a class="site-page child" href="/tetris/"><i class="fa-fw fa fa-arrows-alt"></i><span> Tetris 俄罗斯方块</span></a></li><li><a class="site-page child" href="/smallcat/"><i class="fa-fw fa fa-paw"></i><span> CatchCat 困住小猫</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-leaf"></i><span> Moments</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/diary/"><i class="fa-fw fas fa-bookmark"></i><span> Diary</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fa fa-hourglass-half"></i><span> Gallery</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-podcast"></i><span> More</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags标签</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About关于</span></a></li><li><a class="site-page child" href="/messageboard/"><i class="fa-fw fas fa-bookmark"></i><span> Messageboard留言板</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">机器学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-06-03T16:00:00.000Z" title="Created 2025-06-04 00:00:00">2025-06-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-07-14T12:38:47.324Z" title="Updated 2025-07-14 20:38:47">2025-07-14</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="机器学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="AI人工智能"><a href="#AI人工智能" class="headerlink" title="AI人工智能"></a>AI人工智能</h1><blockquote>
<p>讲到AI人工智能首先得从<code>图灵测试</code>开始说起：</p>
<p>图灵测试就是：测试者与被测试者（一个人和一台机器）隔开的情况下，遍过一些装置（如键盘）向被测试者随意提问。<br>多次测试（一般为5min之内），如果有超过30%的测试者不能确定被测试者是人还是机器，那么这台机器就通过了测试，并被认为具有人类智能。</p>
<p><code>当有人骂你是人机的时候，你不要骂过去，你要说你还没通过图灵测试（这样别人就听不懂了doge）</code>。</p>
</blockquote>
<h2 id="人工智能的分类"><a href="#人工智能的分类" class="headerlink" title="人工智能的分类"></a>人工智能的分类</h2><p>通讯、感知与行动是现代人工智能的三个关键能力</p>
<p>与此对应的三个技术领域分别是</p>
<ul>
<li><code>计算机视觉（CV）</code><ul>
<li>计算机视觉（CV）是指机器感知环境的能力。这一技术类别中的经典任务有图像形成、图像处理、图像提取和图像的三维推理。物体检测和人脸识别是其比较成功的研究领域。</li>
</ul>
</li>
<li><code>自然语言处理（NLP）</code>:在NLP领域中，将覆盖文本挖掘&#x2F;分类、机器翻译和语音识别<ul>
<li>语音识别：是指识别语音（说出的语言）并将其转换成对应文本的技术。相反的任务（文本转语音&#x2F;TTS）也是这一领域内一个类似的研究主题。</li>
<li>文本挖掘：主要是指文本分类，该技术可用于理解、组织和分类结构化或非结构化文本文档。其涵盖的主要任务有句法分析、情绪分析和垃圾信息检测。</li>
<li>机器翻译（MT）：是利用机器的力量自动将一种自然语言（源语言）的文本翻译成另一种语言（目标语言）。</li>
</ul>
</li>
<li><code>机器人</code><ul>
<li>机器人学（Robotics）研究的是机器人的设计、制造、运作和应用，以及控制它们的计算机系统、传感反馈和信息处理。</li>
<li>机器人可以分成两大类：固定机器人和移动机器人。<ul>
<li>固定机器人通常被用于工业生产（比如用于装配线）。</li>
<li>常见的移动机器人应用有货运机器人、空中机器人和自动载具。机器人需要不同部件和系统的协作才能实现最优的作业。其中在硬件上包含传感器、反应器和控制器；另外还有能够实现感知能力的软件，比如定位、地图测绘和目标识别。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a>机器学习基础</h1><h2 id="机器学习的概念"><a href="#机器学习的概念" class="headerlink" title="机器学习的概念"></a>机器学习的概念</h2><p><code>机器学习是从数据中自动分析获得模型，并利用模型对未知数据进行预测</code></p>
<p>如下图所示：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250605141520596.png" alt="image-20250605141520596"></p>
<h2 id="机器学习工作流程⭐️"><a href="#机器学习工作流程⭐️" class="headerlink" title="机器学习工作流程⭐️"></a>机器学习工作流程⭐️</h2><ol>
<li><p>获取数据</p>
<ul>
<li>区分样本和特征</li>
<li>根据数据有无特征值和目标值区分第四步机器学习选择的算法：<ul>
<li>监督学习、无监督学习、强化学习、半监督学习算法</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>学习类型</th>
<th>In</th>
<th>Out</th>
<th>目的</th>
<th>案例</th>
</tr>
</thead>
<tbody><tr>
<td>监督学习（Supervised Learning）</td>
<td>有标签</td>
<td>有反馈</td>
<td>预测结果</td>
<td>猫狗分类、房价预测</td>
</tr>
<tr>
<td>无监督学习（Unsupervised Learning）</td>
<td>无标签</td>
<td>无反馈</td>
<td>发现潜在结构</td>
<td>“物以类聚，人以群分”</td>
</tr>
<tr>
<td>半监督学习（Semi-Supervised Learning）</td>
<td>部分有标签，部分无标签</td>
<td>有反馈</td>
<td>降低数据标记难度</td>
<td></td>
</tr>
<tr>
<td>强化学习（Reinforcement Learning）</td>
<td>决策流程及激励系统</td>
<td>一系列行动</td>
<td>长期利益最大化</td>
<td>学下棋</td>
</tr>
</tbody></table>
</li>
<li><p>数据基本处理</p>
</li>
<li><p>特征工程</p>
<ul>
<li>特征提取</li>
<li>特征预处理</li>
<li>特征降维</li>
</ul>
</li>
<li><p>机器学习（模型训练）</p>
<ul>
<li>选择合适的算法对模型进行训练</li>
</ul>
</li>
<li><p>模型评估</p>
<ul>
<li>结果达到要求，上线服务</li>
<li>没有达到要求，重新上面步骤</li>
</ul>
</li>
</ol>
<h3 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250605142251204.png" alt="image-20250605142251204"></p>
<p>对于数据集:</p>
<ul>
<li>一行数据我们称为一个<code>样本</code></li>
<li>一列数据我们成为一个<code>特征</code></li>
<li>有些数据有目标值（标签值），有些数据没有目标值（如上表中，电影类型就是这个数据集的目标值）</li>
</ul>
<p>数据分割：</p>
<ul>
<li>数据类型一：特征值+目标值（目标值是连续的和离散的）</li>
<li>数据类型二：只有特征值，没有目标值</li>
</ul>
<p>针对上述的数据可以分类为<code>有监督学习</code>和<code>无监督学习</code>，可以看看之前的<a href="https://liamjohnson-w.github.io/2023/08/07/2023.08.07/">Note</a>：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/09.png" alt="09"></p>
<p>监督学习典型模型：Linear regression、Logistic regression、SVM、Neural Network等</p>
<p><strong>监督学习</strong>：监督学习指的是人们给机器标记好的数据，有特征值和目标值，比如：一大堆照片，人工先标记出哪些是猫的照片，哪些是狗的照片；训练模型；让模型判断其余照片是什么动物</p>
<p>监督学习一般有两个问题（分类问题、回归问题）</p>
<ul>
<li>分类问题：</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/06.png" alt="img"></p>
<p>​		垃圾邮件识别就是一个分类问题，使用相应的机器学习算法判定邮件属于垃圾邮件还是非垃圾邮件。</p>
<ul>
<li>回归问题：<ul>
<li>数据中会给出大量的自变量和相应的连续因变量（对应输出结果），通过尝试寻找自变量和因变量的关系，就能够预测输出变量。</li>
<li>如房价的预测，根据房屋的面积以及房价的结果来进行后续的预测</li>
<li>还有股票基金的涨跌预测</li>
</ul>
</li>
</ul>
<p><strong>无监督学习</strong>：非监督学习(unsupervised learning)指的是人们给机器一大堆没有分类标记的数据，让机器可以对数据分类、检测异常等。相应的无监督学习只有一个聚类问题</p>
<ul>
<li>聚类问题：聚类是一种探索性数据分析技术，在没有任何相关先验信息的情况下（相当于不清楚数据的信息），它可以帮助我们将数据划分为有意义的小的组别（也叫簇cluster）。其中每个簇内部成员之间有一定的相似度，簇之间有较大的不同。这也正是聚类作为无监督学习的原因。</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/08.png" alt="img"></p>
<ul>
<li><p>聚类的应用场景</p>
<ul>
<li>在对业务不是很熟悉的情况下, 使用聚类可以帮助打开思路</li>
<li>使用聚类算法做聚类分析 是分析过程的开头, 得到聚类结果之后, 再详细的分析每个类别各自的特点</li>
</ul>
</li>
</ul>
<p><strong>强化学习</strong>：实质是make decisions 问题，即自动进行决策，并且可以做连续决策。</p>
<ul>
<li><p>举例：小孩想要走路，但在这之前，他需要先站起来，站起来之后还要保持平衡，接下来还要先迈出一条腿，是左腿还是右腿，迈出一步后还要迈出下一步。</p>
</li>
<li><p>小孩就是 agent，他试图通过采取行动（即行走）来操纵环境（行走的表面），并且从一个状态转变到另一个状态（即他走的每一步），当他</p>
</li>
<li><p>完成任务的子任务（即走了几步）时，孩子得到奖励（给巧克力吃），并且当他不能走路时，就不会给巧克力。</p>
</li>
</ul>
<p><strong>半监督学习</strong>：训练集同时包含有标记样本数据和未标记样本数据。</p>
<p>机器学习一般的数据集会划分为两个部分：</p>
<ul>
<li>训练数据：用于训练，构建模型</li>
<li>测试数据：在模型检验时使用，用于评估模型是否有效</li>
</ul>
<p>划分比例：</p>
<ul>
<li>训练集：70% 80% 75%</li>
<li>测试集：30% 20% 25%</li>
</ul>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><blockquote>
<p>即对数据进行缺失值、去除异常值等处理</p>
</blockquote>
<h3 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h3><ul>
<li><p>特征提取：将任意数据（如文本或图像）转换为可用于机器学习的数字特征</p>
<ul>
<li>例如将语言、图像等自然语言转为计算机可以识别的字符（base64编码or二进制编码</li>
</ul>
</li>
<li><p>特征预处理：通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程；特征的<strong>单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级</strong>，<strong>容易影响（支配）目标结果</strong>，使得一些模型（算法）无法学习到其它的特征。</p>
</li>
</ul>
<p>归一化：</p>
<p>最值归一化（Min-Max Scaling）：将数据线性映射到[0, 1]区间<br>$$<br>X_{\text{norm}} &#x3D; \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}<br>$$</p>
<p>确定归一化区间（mx,mi），如果不指定默认（0,1）<br>$$<br>X’’ &#x3D; X’ \times (mx - mi) + mi<br>$$</p>
<p>归一化可以<code>提升模型收敛速度</code></p>
<p>API：<code>sklearn.preprocessing.MinMaxScaler</code></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250614174901792.png" alt="image-20250614174901792"></p>
<p>均值方差归一化（Z-Score标准化）：数据分布转为均值为0、标准差为1的正态分布。<br>$$<br>X_{\text{std}} &#x3D; \frac{X - \mu}{\sigma}<br>$$</p>
<p>$μ$为均值，$σ$为标准差</p>
<p>API：<code>sklearn.preprocessing.StandardScaler()</code></p>
<table>
<thead>
<tr>
<th align="center"><strong>维度</strong></th>
<th align="center"><strong>均值方差归一化（标准化）</strong></th>
<th align="center"><strong>Min-Max归一化</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>目标</strong></td>
<td align="center">数据服从标准正态分布（均值为0，标准差为1）</td>
<td align="center">数据缩放到固定区间（如[0,1]）</td>
</tr>
<tr>
<td align="center"><strong>公式</strong></td>
<td align="center">$X&#x3D;\frac{X−μ}{σ}$</td>
<td align="center">$X &#x3D; \frac{X-X_{min}}{X_{max}-X_{min}}$</td>
</tr>
<tr>
<td align="center"><strong>异常值影响</strong></td>
<td align="center">低（依赖整体统计量）</td>
<td align="center">高（依赖极值，异常值会压缩正常数据范围）</td>
</tr>
<tr>
<td align="center"><strong>输出范围</strong></td>
<td align="center">(−∞,+∞)</td>
<td align="center">[0,1] 或 [−1,1]</td>
</tr>
<tr>
<td align="center"><strong>适用场景</strong></td>
<td align="center">数据分布近似正态、含噪声或异常值</td>
<td align="center">边界明确、分布均匀的数据</td>
</tr>
</tbody></table>
<ul>
<li>特征降维（特征缩放）：指在某些限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量的过程</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250605160141881.png" alt="image-20250605160141881"></p>
<h3 id="机器学习（模型训练）"><a href="#机器学习（模型训练）" class="headerlink" title="机器学习（模型训练）"></a>机器学习（模型训练）</h3><ul>
<li>选择合适的算法对模型进行训练</li>
</ul>
<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><blockquote>
<p>分为回归模型评估以及分类模型评估。</p>
<p>其中分类模型评估以混淆矩阵为基础，而回归模型评估中的均方误差比较重要。</p>
</blockquote>
<ul>
<li><p>分类模型评估</p>
<ol>
<li>混淆矩阵（Confusion Matrix）</li>
</ol>
<p>基础工具，用于计算后续指标。二分类混淆矩阵结构如下：</p>
<table>
<thead>
<tr>
<th></th>
<th><strong>预测为正例</strong></th>
<th><strong>预测为负例</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>实际为正例</strong></td>
<td>TP（真正例）</td>
<td>FN（假反例）</td>
</tr>
<tr>
<td><strong>实际为负例</strong></td>
<td>FP（假正例）</td>
<td>TN（真反例）</td>
</tr>
</tbody></table>
<ul>
<li><strong>TP</strong>：实际为正，预测为正</li>
<li><strong>FN</strong>：实际为正，预测为负（漏报）</li>
<li><strong>FP</strong>：实际为负，预测为正（误报）</li>
<li><strong>TN</strong>：实际为负，预测为负</li>
</ul>
<ol start="2">
<li>准确率（Accuracy）</li>
</ol>
<p>预测正确的样本占总样本比例：</p>
<p>$$<br>\text{Accuracy} &#x3D; \frac{TP + TN}{TP + TN + FP + FN}<br>$$</p>
<ul>
<li><strong>适用场景</strong>：类别平衡的数据（如正负样本比例接近）</li>
</ul>
<ol start="3">
<li>精确率（Precision）</li>
</ol>
<p>预测为正的样本中实际为正的比例：</p>
<p>$$<br>\text{Precision} &#x3D; \frac{TP}{TP + FP}<br>$$</p>
<ul>
<li><strong>侧重</strong>：减少误报（如垃圾邮件检测中避免将正常邮件判为垃圾）</li>
</ul>
<ol start="4">
<li>召回率（Recall）</li>
</ol>
<p>实际为正的样本中被正确预测的比例：</p>
<p>$$<br>\text{Recall} &#x3D; \frac{TP}{TP + FN}<br>$$</p>
<ul>
<li><strong>侧重</strong>：减少漏报（如疾病诊断中避免漏掉患者）</li>
</ul>
<ol start="5">
<li>F1-Score</li>
</ol>
<p>精确率与召回率的调和平均，平衡二者：</p>
<p>$$<br>F1 &#x3D; 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}<br>$$</p>
<ul>
<li><strong>适用场景</strong>：类别不平衡或需权衡误报&#x2F;漏报代价时</li>
</ul>
<ol start="6">
<li><p>ROC曲线与AUC值</p>
<ul>
<li><p>ROC曲线：以假阳率（FPR）为横轴、真阳率（TPR）为纵轴绘制，反映不同阈值下的性能。</p>
<p>$$<br>\text{TPR} &#x3D; \frac{TP}{TP + FN}, \quad \text{FPR} &#x3D; \frac{FP}{FP + TN}<br>$$</p>
</li>
<li><p>AUC：ROC曲线下面积，衡量模型区分正负样本的能力：</p>
<ul>
<li>AUC&#x3D;0.5：随机猜测</li>
<li>AUC&gt;0.8：模型效果较好</li>
<li>AUC&#x3D;1：完美分类器</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li><p>回归模型评估</p>
<ul>
<li><p>回归模型评估关注预测值与真实值的偏差，核心指标如下：</p>
<h4 id="1-平均绝对误差（MAE）"><a href="#1-平均绝对误差（MAE）" class="headerlink" title="1. 平均绝对误差（MAE）"></a>1. 平均绝对误差（MAE）</h4><p>预测值与真实值绝对误差的平均值：</p>
<p>$$<br>\text{MAE} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} |y_i - \hat{y}_i|<br>$$</p>
<ul>
<li><strong>特点</strong>：对异常值不敏感，单位与原始数据一致（如房价预测误差单位为”万元”）</li>
</ul>
<h4 id="2-均方误差（MSE）"><a href="#2-均方误差（MSE）" class="headerlink" title="2. 均方误差（MSE）"></a>2. 均方误差（MSE）</h4><p>预测值与真实值误差平方的平均值：</p>
<p>$$<br>\text{MSE} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} (y_i - \hat{y}_i)^2<br>$$</p>
<ul>
<li><strong>特点</strong>：放大异常值影响（平方效应），常用于梯度下降优化</li>
</ul>
<h4 id="3-均方根误差（RMSE）"><a href="#3-均方根误差（RMSE）" class="headerlink" title="3. 均方根误差（RMSE）"></a>3. 均方根误差（RMSE）</h4><p>MSE的平方根，恢复量纲一致性：</p>
<p>$$<br>\text{RMSE} &#x3D; \sqrt{\text{MSE}}<br>$$</p>
<ul>
<li><strong>解释</strong>：RMSE&#x3D;10表示平均预测误差约为10个单位（如温度预测误差±10℃）</li>
</ul>
<h4 id="4-可决系数（R²）"><a href="#4-可决系数（R²）" class="headerlink" title="4. 可决系数（R²）"></a>4. 可决系数（R²）</h4><p>模型解释的目标变量方差比例：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250627160407264.png" alt="image-20250627160407264"></p>
<ul>
<li><strong>范围</strong>：[0, 1]，越接近1说明模型拟合越好</li>
<li><strong>意义</strong>：若R²&#x3D;0.8，表示模型能解释80%的数据变异</li>
</ul>
</li>
</ul>
</li>
<li><p>拟合</p>
<ul>
<li>过拟合：机器已经基本能区别天鹅和其他动物了。但很不巧已有的天鹅图片全是白天鹅的，于是机器判断黑的天鹅不是天鹅。</li>
<li>欠拟合：模型学习的太过粗糙，连训练集中的样本数据特征关系都没有学出来。比如判断鹦鹉、鸭子也是天鹅。</li>
</ul>
</li>
</ul>
<h1 id="Sklearn与特征工程⭐️"><a href="#Sklearn与特征工程⭐️" class="headerlink" title="Sklearn与特征工程⭐️"></a>Sklearn与特征工程⭐️</h1><blockquote>
<p><code>scikit-learn</code>（简称 <strong>sklearn</strong>）基于 NumPy、SciPy 和 Matplotlib 构建，提供了简单高效的算法工具，适合从数据预处理到模型训练的全流程。</p>
</blockquote>
<h2 id="sklearn-的核心组成"><a href="#sklearn-的核心组成" class="headerlink" title="sklearn 的核心组成"></a><strong>sklearn 的核心组成</strong></h2><blockquote>
<p>sklearn的核心部分主要有</p>
<ul>
<li>数据预处理（Preprocessing）<ul>
<li><code>StandardScaler</code> &#x2F; <code>MinMaxScaler</code>：数据标准化&#x2F;归一化。</li>
<li><code>OneHotEncoder</code>：分类变量独热编码。</li>
<li><code>train_test_split</code>：划分训练集和测试集。</li>
<li><code>SimpleImputer</code>：处理缺失值。</li>
</ul>
</li>
<li>监督学习算法（Supervised Learning）<ul>
<li>分类（Classification）：<ul>
<li><code>LogisticRegression</code>（逻辑回归）</li>
<li><code>SVM</code>（支持向量机）</li>
<li><code>RandomForestClassifier</code>（随机森林）</li>
<li><code>KNeighborsClassifier</code>（K近邻）</li>
</ul>
</li>
<li>回归（Regression）：<ul>
<li><code>LinearRegression</code>（线性回归）</li>
<li><code>DecisionTreeRegressor</code>（决策树回归）</li>
<li><code>GradientBoostingRegressor</code>（梯度提升回归）</li>
</ul>
</li>
</ul>
</li>
<li>无监督学习算法（Unsupervised Learning）<ul>
<li>聚类（Clustering）：<ul>
<li><code>KMeans</code>（K均值聚类）</li>
<li><code>DBSCAN</code>（基于密度的聚类）</li>
</ul>
</li>
<li>降维（Dimensionality Reduction）：<ul>
<li><code>PCA</code>（主成分分析）</li>
<li><code>t-SNE</code>（数据可视化降维）</li>
</ul>
</li>
</ul>
</li>
<li>模型评估与选择（Model Evaluation）<ul>
<li>指标计算：<ul>
<li><code>accuracy_score</code>（分类准确率）</li>
<li><code>mean_squared_error</code>（回归均方误差）</li>
<li><code>confusion_matrix</code>（混淆矩阵）</li>
</ul>
</li>
<li>交叉验证：<ul>
<li><code>cross_val_score</code>（K折交叉验证）</li>
</ul>
</li>
</ul>
</li>
<li>特征工程（Feature Engineering）<ul>
<li><code>SelectKBest</code>：基于统计检验选择特征。</li>
<li><code>RFE</code>（递归特征消除）：自动筛选重要特征。</li>
</ul>
</li>
<li>流水线（Pipeline）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line">pipeline = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;scaler&#x27;</span>, StandardScaler()),</span><br><span class="line">    (<span class="string">&#x27;classifier&#x27;</span>, RandomForestClassifier())</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

</blockquote>
<table>
<thead>
<tr>
<th>库</th>
<th>特点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>sklearn</strong></td>
<td>传统机器学习算法（非深度学习）</td>
<td>中小规模结构化数据</td>
</tr>
<tr>
<td><strong>TensorFlow&#x2F;PyTorch</strong></td>
<td>深度学习框架</td>
<td>图像、文本等复杂数据</td>
</tr>
<tr>
<td><strong>XGBoost</strong></td>
<td>高性能梯度提升树</td>
<td>表格数据竞赛&#x2F;高精度需求</td>
</tr>
</tbody></table>
<p>Iris 鸢尾花数据集内包含 3 种类别，分别为</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250614210851372.png" alt="image-20250614210851372"></p>
<ul>
<li>山鸢尾（Iris-setosa）</li>
<li>变色鸢尾（Iris-versicolor）</li>
<li>维吉尼亚鸢尾（Iris-virginica）<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250606102258377.png" alt="image-20250606102258377"></li>
</ul>
<h2 id="sklearn随机森林-鸢尾花案例的典型代码流程"><a href="#sklearn随机森林-鸢尾花案例的典型代码流程" class="headerlink" title="sklearn随机森林-鸢尾花案例的典型代码流程"></a><strong>sklearn随机森林-鸢尾花案例的典型代码流程</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 加载数据</span></span><br><span class="line">data = load_iris()</span><br><span class="line">X, y = data.data, data.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 划分数据集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 训练模型</span></span><br><span class="line">model = RandomForestClassifier()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 预测与评估</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;准确率:&quot;</span>, accuracy_score(y_test, y_pred))</span><br></pre></td></tr></table></figure>

<h3 id="sklear典型流程-带注释详解"><a href="#sklear典型流程-带注释详解" class="headerlink" title="sklear典型流程(带注释详解)"></a>sklear典型流程(带注释详解)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1, 加载数据</span></span><br><span class="line">data = load_iris()</span><br><span class="line">x, y = data.data, data.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据结构</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;特征矩阵 x 的形状:&quot;</span>, x.shape)  <span class="comment"># 输出: (150, 4) → 150个样本，每个样本4个特征</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;标签数组 y 的形状:&quot;</span>, y.shape)  <span class="comment"># 输出: (150,)  → 150个标签</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;前5个样本的特征:\n&quot;</span>, x[:<span class="number">5</span>])</span><br><span class="line"><span class="comment">#  [[5.1 3.5 1.4 0.2]</span></span><br><span class="line"><span class="comment">#   [4.9 3.  1.4 0.2]</span></span><br><span class="line"><span class="comment">#   [4.7 3.2 1.3 0.2]</span></span><br><span class="line"><span class="comment">#   [4.6 3.1 1.5 0.2]</span></span><br><span class="line"><span class="comment">#   [5.  3.6 1.4 0.2]]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;前5个样本的标签:&quot;</span>, y[:<span class="number">5</span>])</span><br><span class="line"><span class="comment"># 前5个样本的标签: [0 0 0 0 0]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2，划分数据集</span></span><br><span class="line"><span class="comment"># 划分训练集和测试集。表示将数据的 20% 划分为测试集，剩余的 80% 自动作为训练集。也可指定绝对数量（如 test_size=200 表示 200 个样本作为测试集）。</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">其余参数：</span></span><br><span class="line"><span class="string">random_state：控制随机划分的种子（如 random_state=42 确保每次运行结果一致）。</span></span><br><span class="line"><span class="string">shuffle：是否打乱数据后再划分（默认为 True）。</span></span><br><span class="line"><span class="string">stratify：按标签分层划分（确保训练集和测试集的类别比例一致，适用于分类任务）。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># x：特征矩阵（输入数据），形状通常为 [样本数, 特征数]（如 (1000, 5) 表示 1000 个样本，每个样本 5 个特征）。</span></span><br><span class="line"><span class="comment"># y：标签数组（输出数据），形状为 [样本数,]（如 (1000,) 表示 1000 个样本对应的类别或回归值）。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># x_train：训练集特征矩阵（80% 的原始 x）。</span></span><br><span class="line"><span class="comment"># x_test：测试集特征矩阵（20% 的原始 x）。</span></span><br><span class="line"><span class="comment"># y_train：训练集标签（与 x_train 对应）。</span></span><br><span class="line"><span class="comment"># y_test：测试集标签（与 x_test 对应）。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3，训练模型</span></span><br><span class="line"><span class="comment"># 创建随机森林模型对象</span></span><br><span class="line">model = RandomForestClassifier()</span><br><span class="line"><span class="comment"># 用训练数据（x_train 和 y_train）训练模型，也就是训练集特征矩阵以及训练集标签进行有监督学习训练</span></span><br><span class="line">model.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4，预测与评估</span></span><br><span class="line"><span class="comment"># 让训练好的模型处理新数据（如预测新样本的类别）</span></span><br><span class="line">y_pred = model.predict(x_test)</span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br><span class="line"><span class="comment"># [0 2 2 1 0 0 1 0 1 0 2 0 1 1 2 1 2 2 2 1 0 1 1 1 0 1 1 1 1 0]</span></span><br></pre></td></tr></table></figure>

<p>注意事项：</p>
<ul>
<li>如果数据未标准化或存在缺失值，可能需要在 <code>fit</code> 之前进行预处理（如使用 <code>StandardScaler</code>）。</li>
<li>训练后可通过 <code>model.score(x_test, y_test)</code> 快速评估模型在测试集上的准确率。</li>
</ul>
<h2 id="超参数选择与调优"><a href="#超参数选择与调优" class="headerlink" title="超参数选择与调优"></a>超参数选择与调优</h2><blockquote>
<p>超参数：模型当中有一些人为指定的参数，称为超参数；超参数的选择不同会对模型预测的准确率产生影响。</p>
<p>比如：KNeighborsClassifier(n_neighbors&#x3D;5)中n_neighbors可以称为超参数。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250707205749031.png" alt="image-20250707205749031"></p>
<h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><blockquote>
<p>交叉验证是一种数据集的分割方法，将训练集划分为 n 份，其中一份做验证集、其他n-1份做训练集</p>
<p>交叉验证法原理：将数据集划分为 cv&#x3D;10 份：</p>
<ol>
<li>第一次：把第一份数据做验证集，其他数据做训练</li>
<li>第二次：把第二份数据做验证集，其他数据做训练</li>
<li>…. 以此类推，总共训练10次，评估10次。</li>
<li>使用训练集+验证集多次评估模型，取平均值做交叉验证为模型得分</li>
<li>若k&#x3D;5模型得分最好，再使用全部训练集(训练集+验证集) 对k&#x3D;5模型再训练一边，再使用测试集对k&#x3D;5模型做评估</li>
</ol>
</blockquote>
<h3 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h3><blockquote>
<p>简单来说就是寻找最优超参</p>
</blockquote>
<h3 id="交叉验证-网格搜索（模型选择和调优）"><a href="#交叉验证-网格搜索（模型选择和调优）" class="headerlink" title="交叉验证+网格搜索（模型选择和调优）"></a>交叉验证+网格搜索（模型选择和调优）</h3><ul>
<li>交叉验证解决模型的数据输入问题(数据集划分)得到更可靠的模型</li>
<li>网格搜索解决超参数的组合</li>
<li>两个组合再一起形成一个模型参数调优的解决方案</li>
</ul>
<h2 id="鸢尾花案例-数据预处理-K值调优"><a href="#鸢尾花案例-数据预处理-K值调优" class="headerlink" title="鸢尾花案例(数据预处理+K值调优)"></a>鸢尾花案例(数据预处理+K值调优)</h2><blockquote>
<p><code>fit_transform(训练+转化)，适用于训练集</code></p>
<p><code>transform(转化)，适用于测试集</code></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">iris_data = load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里只是加载样本和标签</span></span><br><span class="line">x, y = iris_data.data, iris_data.target</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test =train_test_split(x, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行特征工程</span></span><br><span class="line">transfer = StandardScaler()</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit_transform(训练+转化)，适用于训练集 transform(转化)，适用于测试集</span></span><br><span class="line">x_train = transfer.fit_transform(x_train)</span><br><span class="line">x_test = transfer.transform(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行训练</span></span><br><span class="line">es_model = KNeighborsClassifier()</span><br><span class="line">param_dict = &#123;<span class="string">&#x27;n_neighbors&#x27;</span>: [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>)]&#125;</span><br><span class="line">es = GridSearchCV(es_model, param_dict, cv=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在获取最优评分之前 需要先训练</span></span><br><span class="line"><span class="comment"># 4-4:打印最优的超参组合</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;最优评分<span class="subst">&#123;es.best_score_&#125;</span>&quot;</span>)  <span class="comment"># 最优组合平均分</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;最优超参组合:<span class="subst">&#123;es.best_params_&#125;</span>&quot;</span>)  <span class="comment"># 最终超参 （仅供参考）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;最优的估计器对象:<span class="subst">&#123;es.best_estimator_&#125;</span>&quot;</span>)  <span class="comment"># 最优组合模型的对象</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;具体的交叉验证结果<span class="subst">&#123;es.cv_results_&#125;</span>&quot;</span>)  <span class="comment"># 所有组合的 评分结果（过程）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 再次评估</span></span><br><span class="line"><span class="comment"># 获取最优超参模型对象</span></span><br><span class="line">es = KNeighborsClassifier(n_neighbors=<span class="number">6</span>)</span><br><span class="line">es.fit(x_train, y_train)</span><br><span class="line">y_pre = es.predict(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;准确率:<span class="subst">&#123;accuracy_score(y_test, y_pre)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>



<h2 id="数字（图像）识别案例"><a href="#数字（图像）识别案例" class="headerlink" title="数字（图像）识别案例"></a>数字（图像）识别案例</h2><blockquote>
<p>严格来讲不是CV，而是一个基于KNN算法的多分类小案例。</p>
<ul>
<li>忽略告警：<ul>
<li>import warnings</li>
<li>warnings.filterwarnings(“ignore”,module&#x3D;”sklearn”)</li>
</ul>
</li>
</ul>
<p>参考链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_51385258/article/details/143445040">KNN算法实现手写数字识别</a></p>
</blockquote>
<p>自己做的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图像识别案例</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler, MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> joblib  <span class="comment"># 用于保存模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一步，从csv文件中还原出图像</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;/Users/lifuyao/PycharmProjects/pythonProject/Files/手写数字识别.csv&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">x = df.iloc[:, <span class="number">1</span>:]</span><br><span class="line">y = df.iloc[:, :<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_pics</span>(<span class="params">idx</span>):</span><br><span class="line">    <span class="keyword">if</span> idx &lt; <span class="number">0</span> <span class="keyword">or</span> idx &gt; <span class="built_in">len</span>(df) - <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment"># 使用matplotlib 还原图像</span></span><br><span class="line">    pic_idx = x.iloc[idx]</span><br><span class="line">    pic_idx = pic_idx.values.reshape(<span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    plt.imshow(pic_idx, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)  <span class="comment"># 不显示坐标轴</span></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>():</span><br><span class="line">    df = pd.read_csv(<span class="string">&#x27;/Users/lifuyao/PycharmProjects/pythonProject/Files/手写数字识别.csv&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    x = df.iloc[:, <span class="number">1</span>:]</span><br><span class="line">    <span class="comment"># y = df.iloc[:, :1] # 虽然选取0列，但是这个会返回一个DataFrame</span></span><br><span class="line">    y = df.iloc[:, <span class="number">0</span>]  <span class="comment"># 返回Series</span></span><br><span class="line">    <span class="comment"># 数据归一化（AI 说这里应该放在切分train_test_split后，这样会导致准确率虚高）</span></span><br><span class="line">    x = x / <span class="number">255</span></span><br><span class="line">    <span class="built_in">print</span>(x.shape)</span><br><span class="line">    <span class="built_in">print</span>(Counter(y))</span><br><span class="line">    x_train,x_test,  y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">12</span>)</span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    es_model = KNeighborsClassifier(n_neighbors=<span class="number">5</span>)</span><br><span class="line">    es_model.fit(x_train, y_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;准确率为：&quot;</span>, es_model.score(x_test, y_test))</span><br><span class="line">    <span class="comment"># 保存模型</span></span><br><span class="line">    joblib.dump(es_model, <span class="string">&#x27;./knn.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">use_model</span>():</span><br><span class="line">    <span class="comment"># 加载图片()中的数据并用模型识别</span></span><br><span class="line">    img = plt.imread(<span class="string">&#x27;/Users/lifuyao/PycharmProjects/pythonProject/Files/demo.png&#x27;</span>)</span><br><span class="line">    <span class="comment"># print(img)   #这里好像进行了缩放 不能使用plt.show()展示  0.7137255  0.8901961 </span></span><br><span class="line">    <span class="comment"># 加载模型</span></span><br><span class="line">    es_model = joblib.load(<span class="string">&#x27;./knn.pth&#x27;</span>)</span><br><span class="line">    <span class="comment"># 这里-1代表全部取，相当于784</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;识别到图片中的数字为：&quot;</span>, es_model.predict(img.reshape(<span class="number">1</span>, -<span class="number">1</span>)))</span><br><span class="line">    plt.imshow(img, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># get_pics(5)</span></span><br><span class="line">    <span class="comment"># train_model()</span></span><br><span class="line">    use_model()</span><br></pre></td></tr></table></figure>



<p>补充代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">案例：演示 KNN算法 识别图片 ，即：手写数字识别哪里</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">介绍:</span></span><br><span class="line"><span class="string">    每个图片都是有28*28像素组成，即我们csv文件每一行有784个像素点，表示图片（每个像素点）的颜色（0~255）</span></span><br><span class="line"><span class="string">    最终构成图像</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="comment">#拓展：忽略警告信息</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>,module=<span class="string">&quot;sklearn&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义一个函数 ，接收用户传入的索引，展示该索引对应的图片</span></span><br><span class="line"><span class="comment">#记住：excel表格中的行号从1开始，索引从0开始，找到对应的正确的数字，应该行号-1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_digit</span>(<span class="params">idx</span>):</span><br><span class="line">    <span class="comment">#1、读取数据集，获取源数据</span></span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;../data/手写数字识别.csv&quot;</span>)</span><br><span class="line">    <span class="comment"># print(df)  #[42000 行 x 785 列]</span></span><br><span class="line">    <span class="comment">#2、判断传入的索引是否越界：</span></span><br><span class="line">    <span class="comment">#根据excel表格来看，索引的最大值是4w2000 不是4w2001 因为第一个标签是lable</span></span><br><span class="line">    <span class="keyword">if</span> idx&lt;<span class="number">0</span> <span class="keyword">or</span> idx&gt;<span class="built_in">len</span>(df)-<span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;索引越界&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment">#iloc 根据索引来获取</span></span><br><span class="line">    <span class="comment"># 格式:df.iloc[行号，列索引]</span></span><br><span class="line">    <span class="comment"># print(df.iloc[0:5, 0:2])</span></span><br><span class="line">    <span class="comment">#3、走到这里，说明索引没有越界，就是可以正常获取数据</span></span><br><span class="line">    x=df.iloc[:,<span class="number">1</span>:]  <span class="comment">#1:获取到所有的像素点</span></span><br><span class="line">    y=df.iloc[:,<span class="number">0</span>]  <span class="comment">#获取到所有的手写数字，excel表格中标签</span></span><br><span class="line">    <span class="comment"># print(f&quot;像素的形状:&#123;x.shape&#125;&quot;)</span></span><br><span class="line">    <span class="comment"># 4、根据传入的索引值，获取该行的数据 （excel表的第一行是lable pixel0...所以这行要去掉，然后索引数字要从0开始）</span></span><br><span class="line">    <span class="comment">#如果我想取表格中5（label）个值，（excel显示10行 ，实际传入的是8）</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;您传入的索引，对应的数字是:<span class="subst">&#123;y[idx]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#5、绘制图片(784,) 转换成（28,28）</span></span><br><span class="line">    <span class="comment"># print(x.iloc[idx].values)   784个像素点</span></span><br><span class="line">    x=x.iloc[idx].values.reshape(<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">    <span class="comment"># print(x)</span></span><br><span class="line">    <span class="comment">#6、绘制具体的灰度图</span></span><br><span class="line">    plt.imshow(x,cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)  <span class="comment">#不显示坐标轴</span></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;查看所有标签的分布情况:<span class="subst">&#123;Counter(y)&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment">#&#123;1: 4684, 7: 4401, 3: 4351, 9: 4188, 2: 4177, 6: 4137, 0: 4132, 4: 4072, 8: 4063, 5: 3795&#125;</span></span><br><span class="line">    <span class="comment"># 1: 4684   表示里面有4684种形态的1  笔迹</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="comment"># 核心需求 ：给一个图片，解析像素点，</span></span><br><span class="line"><span class="comment"># 核心代码</span></span><br><span class="line"><span class="comment"># 1：读取csv 获取数据</span></span><br><span class="line"><span class="comment"># 2: 数据预处理</span></span><br><span class="line"><span class="comment"># 3：特征工程</span></span><br><span class="line"><span class="comment"># 4；模型训练</span></span><br><span class="line"><span class="comment"># 5：模型评估</span></span><br><span class="line"><span class="comment"># 6：模型保存</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、定义函数 ，训练模型，并且保存好训练好的模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>():</span><br><span class="line">    <span class="comment"># 1：读取csv 获取数据</span></span><br><span class="line">    df=pd.read_csv(<span class="string">&quot;../data/手写数字识别.csv&quot;</span>)</span><br><span class="line">    <span class="comment"># 2: 数据预处理</span></span><br><span class="line">    <span class="comment"># 2-1 拆分特征列</span></span><br><span class="line">    x=df.iloc[:,<span class="number">1</span>:]</span><br><span class="line">    <span class="comment"># 2-2 拆分标签列</span></span><br><span class="line">    y=df.iloc[:,<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 3：特征工程</span></span><br><span class="line">    <span class="comment">#   x是像素点的值  [0,255]</span></span><br><span class="line">    <span class="comment">#   y是标签值  [0,9]</span></span><br><span class="line">    x=x/<span class="number">255</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4；模型训练</span></span><br><span class="line">    <span class="comment">#  参5： 参考y之进行抽取 ，保持标签比例的均衡</span></span><br><span class="line">    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=<span class="number">0.2</span>,random_state=<span class="number">21</span>,stratify=y)</span><br><span class="line">    es=KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line">    es.fit(x_train,y_train)</span><br><span class="line">    <span class="comment"># 5：模型评估</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;准确率<span class="subst">&#123;accuracy_score(y_test,es.predict(x_test))&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment"># 6：模型保存</span></span><br><span class="line">    joblib.dump(es,<span class="string">&quot;../model/手写数字识别.pkl&quot;</span>)  <span class="comment"># pkl 文件 pickle文件 ，python独有文件类型</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型保存成功&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">use_model</span>():</span><br><span class="line">    <span class="comment">#1、读取图片，</span></span><br><span class="line">    img=plt.imread(<span class="string">&quot;../data/demo_0.png&quot;</span>)  <span class="comment"># 28*28像素</span></span><br><span class="line">    <span class="comment"># print(img)   # [ [像素1,像素2,...像素28],[像素1,像素2,...像素28],.... ]  28*28</span></span><br><span class="line">    <span class="comment">#2、读取模型，加载模型对象</span></span><br><span class="line">    es=joblib.load(<span class="string">&quot;../model/手写数字识别.pkl&quot;</span>)</span><br><span class="line">    <span class="comment"># print(knn)</span></span><br><span class="line">    <span class="comment">#3、模型预测</span></span><br><span class="line">    <span class="comment">#写法一：</span></span><br><span class="line">    y_predict=es.predict(img.reshape(<span class="number">1</span>,<span class="number">784</span>))</span><br><span class="line">    <span class="comment">#写法二:</span></span><br><span class="line">    <span class="comment"># y_predict = es.predict(img.reshape(1, -1))</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;预测结果为<span class="subst">&#123;y_predict&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># show_digit(6)</span></span><br><span class="line">    <span class="comment"># train_model()</span></span><br><span class="line">    use_model()</span><br></pre></td></tr></table></figure>





<h1 id="KNN算法"><a href="#KNN算法" class="headerlink" title="KNN算法"></a>KNN算法</h1><p>K Nearest Neighbor算法又叫KNN算法，这个算法是机器学习里面一个比较经典的算法。</p>
<h2 id="KNN算法流程"><a href="#KNN算法流程" class="headerlink" title="KNN算法流程"></a>KNN算法流程</h2><blockquote>
<p>KNN既可以处理分类问题，又可以处理回归问题，不同的问题最后一步的处理方式不同。</p>
<p>对于分类问题：取前n个标签个数最多的，作为最终结果。</p>
<p>对于回归问题：取前n个标签的均值。</p>
</blockquote>
<ul>
<li>计算测试样本和训练样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；</li>
<li>对上面所有的距离值进行排序；</li>
<li>选前 k 个最小距离的样本；</li>
<li><code>根据这 k 个样本的标签进行投票，得到最后的分类类别</code>；</li>
</ul>
<h3 id="K值大小对于训练的影响"><a href="#K值大小对于训练的影响" class="headerlink" title="K值大小对于训练的影响"></a>K值大小对于训练的影响</h3><ul>
<li>K值过小：<ul>
<li>容易受到异常点的影响</li>
<li>容易过拟合</li>
</ul>
</li>
<li>k值过大：<ul>
<li>受到样本均衡的问题</li>
<li>容易欠拟合</li>
</ul>
</li>
</ul>
<h2 id="欧式距离"><a href="#欧式距离" class="headerlink" title="欧式距离"></a>欧式距离</h2><blockquote>
<p>其实就是两点之间的距离计算方式：</p>
</blockquote>
<h3 id="二维空间"><a href="#二维空间" class="headerlink" title="二维空间"></a>二维空间</h3><p>点 $a(x_{1},y_{1})$ 与 $b(x_{2},y_{2})$ 间的距离：<br>$$<br>d_{12} &#x3D; \sqrt{(x_{1}-x_{2})^{2} + (y_{1}-y_{2})^{2}}<br>$$</p>
<h3 id="三维空间"><a href="#三维空间" class="headerlink" title="三维空间"></a>三维空间</h3><p>点 $a(x_{1},y_{1},z_{1})$ 与 $b(x_{2},y_{2},z_{2})$ 间的距离：<br>$$<br> d_{12} &#x3D; \sqrt{(x_{1}-x_{2})^{2} + (y_{1}-y_{2})^{2} + (z_{1}-z_{2})^{2}}<br>$$</p>
<h3 id="n维空间"><a href="#n维空间" class="headerlink" title="n维空间"></a>n维空间</h3><p>点 $a(x_{11},x_{12},\dots,x_{1n})$ 与 $b(x_{21},x_{22},\dots,x_{2n})$ 间的距离：<br>$$<br> d_{12} &#x3D; \sqrt{\sum_{k&#x3D;1}^{n}(x_{1k} - x_{2k})^{2}}<br>$$</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250606110103367.png" alt="image-20250606110103367"></p>
<p>计算唐探与各个电影的欧式距离，取前5个，统计各个电影类型出现的频率，根据频率判断唐探的目标值为喜剧片</p>
<h2 id="sklearn-KNN算法"><a href="#sklearn-KNN算法" class="headerlink" title="sklearn-KNN算法"></a>sklearn-KNN算法</h2><h3 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier, KNeighborsRegressor</span><br><span class="line"></span><br><span class="line">es = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line">x_train = [[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>], [<span class="number">4</span>]]</span><br><span class="line"><span class="comment"># 注意这里是分类问题，如果属于二分类 只有0/1</span></span><br><span class="line">y_train = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果k近邻距离（2个样本）举例相等，则会选择标签值较小的值。剃刀</span></span><br><span class="line">x_test = [[<span class="number">5</span>]]</span><br><span class="line"></span><br><span class="line">es.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(es.predict(x_test))</span><br></pre></td></tr></table></figure>



<h3 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor</span><br><span class="line"></span><br><span class="line">es = KNeighborsRegressor(n_neighbors=<span class="number">4</span>)</span><br><span class="line">x_train = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">3</span>, <span class="number">10</span>, <span class="number">10</span>], [<span class="number">4</span>, <span class="number">11</span>, <span class="number">12</span>]]</span><br><span class="line">y_train = [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>]</span><br><span class="line"></span><br><span class="line">x_test = [[<span class="number">3</span>, <span class="number">11</span>, <span class="number">10</span>]]</span><br><span class="line"></span><br><span class="line">es.fit(x_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(es.predict(x_test))</span><br></pre></td></tr></table></figure>



<h2 id="各种距离计算方式"><a href="#各种距离计算方式" class="headerlink" title="各种距离计算方式"></a>各种距离计算方式</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250606114135017.png" alt="image-20250606114135017"></p>
<h3 id="欧式距离-Euclidean-Distance"><a href="#欧式距离-Euclidean-Distance" class="headerlink" title="欧式距离(Euclidean Distance)"></a>欧式距离(Euclidean Distance)</h3><p><a href="https://liamjohnson-w.github.io/2025/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/#%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB">这里</a></p>
<h3 id="曼哈顿距离-Manhattan-Distance"><a href="#曼哈顿距离-Manhattan-Distance" class="headerlink" title="曼哈顿距离(Manhattan Distance)"></a>曼哈顿距离(Manhattan Distance)</h3><p>二维空间</p>
<p>点 $a(x_{1},y_{1})$ 与 $b(x_{2},y_{2})$ 间的距离：<br>$$<br> d_{12} &#x3D; |x_{1} - x_{2}| + |y_{1} - y_{2}|<br>$$</p>
<p>n维空间</p>
<p>点 $a(x_{11},x_{12},\ldots,x_{1n})$ 与 $b(x_{21},x_{22},\ldots,x_{2n})$ 间的距离：<br>$$<br> d_{12} &#x3D; \sum_{k&#x3D;1}^{n} |x_{1k} - x_{2k}|<br>$$</p>
<h3 id="切比雪夫距离-Chebyshev-Distance"><a href="#切比雪夫距离-Chebyshev-Distance" class="headerlink" title="切比雪夫距离 (Chebyshev Distance)"></a>切比雪夫距离 (Chebyshev Distance)</h3><p>二维空间</p>
<p>点 $a(x_{1},y_{1})$ 与 $b(x_{2},y_{2})$ 间的距离：<br>$$<br> d_{12} &#x3D; \max\left(|x_{1}-x_{2}|,\ |y_{1}-y_{2}|\right)<br>$$</p>
<p>n维空间</p>
<p>点 $a(x_{11},x_{12},\ldots,x_{1n})$ 与 $b(x_{21},x_{22},\ldots,x_{2n})$ 间的距离：<br>$$<br> d_{12} &#x3D; \max_{1 \leq i \leq n} \left( |x_{1i} - x_{2i}| \right)<br>$$</p>
<h3 id="标准化欧氏距离-Standardized-EuclideanDistance"><a href="#标准化欧氏距离-Standardized-EuclideanDistance" class="headerlink" title="标准化欧氏距离 (Standardized EuclideanDistance)"></a>标准化欧氏距离 (Standardized EuclideanDistance)</h3><p>既然数据各维分量的分布不一样，那先将各个分量都”标准化”到均值、方差相等，$$S_k$$表示各个维度的标准差，如果将方差的倒数看成一个权重，也可称之为加权欧氏距离Weiahted Euclidean distance)<br>$$<br> d_{12} &#x3D; \sqrt{\sum_{k&#x3D;1}^{n} \left( \frac{x_{1k} - x_{2k}}{s_{k}} \right)^2 }<br>$$</p>
<h3 id="余弦距离-Cosine-Distance"><a href="#余弦距离-Cosine-Distance" class="headerlink" title="余弦距离(Cosine Distance)"></a>余弦距离(Cosine Distance)</h3><p>几何中，夹角余弦可用来衡量两个向量方向的差异;机器学习中，借用这一概念来衡量样本向量之间的差异。</p>
<p>二维空间向量</p>
<p>向量 $\vec{a}(x_1,y_1)$ 与 $\vec{b}(x_2,y_2)$ 的夹角余弦：<br>$$<br> \cos \theta &#x3D; \frac{x_{1}x_{2} + y_{1}y_{2}}{\sqrt{x_{1}^{2} + y_{1}^{2}} \sqrt{x_{2}^{2} + y_{2}^{2}}}<br>$$</p>
<p>n维空间向量</p>
<p>对于 $n$ 维样本点：$a(x_{11},x_{12},\ldots,x_{1n})$、$b(x_{21},x_{22},\ldots,x_{2n})$</p>
<p>夹角余弦存在两种表达形式：</p>
<p>点积与模长形式：<br>$$<br> \cos (\theta) &#x3D; \frac{a \cdot b}{|a| \ |b|}<br>$$</p>
<p>展开形式：<br>$$<br> \cos (\theta) &#x3D; \frac{\sum\limits_{k&#x3D;1}^{n} x_{1k} x_{2k}}{\sqrt{\sum\limits_{k&#x3D;1}^{n} x_{1k}^{2}} \sqrt{\sum\limits_{k&#x3D;1}^{n} x_{2k}^{2}}}<br>$$</p>
<h3 id="汉明距离-Hamming-Distance"><a href="#汉明距离-Hamming-Distance" class="headerlink" title="汉明距离(Hamming Distance)"></a>汉明距离(Hamming Distance)</h3><p>汉明距离（Hamming Distance）是用于衡量两个<strong>等长字符串</strong>在相同位置上不同字符的个数</p>
<p>对于两个长度为 $n$ 的字符串 $S$ 和 $T$（或二进制向量），汉明距离 $D_H$ 定义为：<br>$$<br>D_H(S, T) &#x3D; \sum_{i&#x3D;1}^{n} \mathbb{I}(S_i \neq T_i)<br>$$</p>
<p>其中：</p>
<ul>
<li>$S_i$ 和 $T_i$ 分别表示字符串 $S$ 和 $T$ 的第 $i$ 个字符（或二进制位）</li>
<li>$\mathbb{I}(x)$ 是指示函数（若 $x$ 为真则返回 1，否则返回 0）</li>
</ul>
<h3 id="杰卡德距离-Jaccard-Distance"><a href="#杰卡德距离-Jaccard-Distance" class="headerlink" title="杰卡德距离(Jaccard Distance)"></a>杰卡德距离(Jaccard Distance)</h3><p>杰卡德相似系数(Jaccard slmilarity coeficient): 两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示:<br>$$<br>J(A,B) &#x3D; \frac{|A \cap B|}{|A \cup B|}<br>$$</p>
<h3 id="闵可夫斯基距离-Minkowski-Distance"><a href="#闵可夫斯基距离-Minkowski-Distance" class="headerlink" title="闵可夫斯基距离(Minkowski Distance)"></a>闵可夫斯基距离(Minkowski Distance)</h3><p>也叫闵式距离：<br>$$<br>d_{12} &#x3D; \sqrt[p]{\sum_{k&#x3D;1}^{n} \left| x_{1k} - x_{2k} \right|^{p}}<br>$$<br>当$p&#x3D;1$时，为曼哈顿距离</p>
<p>当$p&#x3D;2$时，为欧氏距离</p>
<h2 id="KD树"><a href="#KD树" class="headerlink" title="KD树"></a>KD树</h2><blockquote>
<p>根据<strong>KNN</strong>每次需要预测一个点时，我们都需要计算训练数据集里每个点到这个点的距离，然后选出距离最近的k个点进行投票。<strong>当数据集很大时，这个计算成本非常高</strong>。</p>
<p><strong>kd树</strong>：为了避免每次都重新计算一遍距离，算法会把距离信息保存在一棵树里，这样在计算之前从树里查询距离信息，尽量避免重新计算。其基本原理是，<strong>如果A和B距离很远，B和C距离很近，那么A和C的距离也很远</strong>。有了这个信息，就可以在合适的时候跳过距离远的点。</p>
</blockquote>
<h3 id="KD树的构建（二维平面）"><a href="#KD树的构建（二维平面）" class="headerlink" title="KD树的构建（二维平面）"></a>KD树的构建（二维平面）</h3><ol>
<li>确定split域：按照x&#x2F;y轴进行分割，根据x轴以及y轴数据上的方差，方差大的为split域。</li>
<li>确定Node-Data域：按照split值排序，取中间的点作为Node-Data点</li>
<li>确定左子空间和右子空间：按照Node-Data的x&#x2F;y坐标进行点的分割</li>
</ol>
<p>详细步骤：</p>
<blockquote>
<p>给定一个二维空间数据集：T&#x3D;{(2,3),(5,4),(9,6),(4,7),(8,1),(7,2)}，构造一个平衡kd树。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250606154437192.png" alt="image-20250606154437192"></p>
<ol>
<li>确定：split域&#x3D;x。具体是：6个数据点在x，y维度上的数据方差分别为39，28.63，在x轴上方差更大，故split域值为x；</li>
<li>确定：Node-data &#x3D; （7,2）。具体是：根据x维上的值将数据排序，6个数据的中值(所谓中值，即中间大小的值)为5,7的平均値，但是没有6，所以往后进一到7（暂时没搞懂）！！所以Node-data域位数据点（7,2）。</li>
<li>确定：左子空间和右子空间。具体是：分割超平面x&#x3D;7将整个空间分为两部分：x&lt;&#x3D;7的部分为左子空间，包含3个节点&#x3D;{(2,3),(5,4),(4,7)}；另一部分为右子空间，包含2个节点&#x3D;{(9,6)，(8,1)}；</li>
<li>如上算法所述，kd树的构建是一个递归过程，我们对左子空间和右子空间内的数据重复根节点的过程就可以得到一级子节点**（5,4）<strong>和</strong>（9,6）**，同时将空间和数据集进一步细分，如此往复直到空间中只包含一个数据点。</li>
</ol>
<h3 id="KD树的快速最近邻搜索算法"><a href="#KD树的快速最近邻搜索算法" class="headerlink" title="KD树的快速最近邻搜索算法"></a>KD树的<strong>快速最近邻搜索</strong>算法</h3><blockquote>
<p>假设标记为星星的点是 test point， 绿色的点是找到的近似点，在回溯过程中，需要用到一个队列，存储需要回溯的点，在判断其他子节点空间中是否有可能有距离查询点更近的数据点时，做法是以查询点为圆心，以当前的最近距离为半径画圆，这个圆称为候选超球（candidate hypersphere），如果圆与回溯点的轴相交，则需要将轴另一边的节点都放到回溯队列里面来。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250606155816616.png" alt="image-20250606155816616"></p>
<p>样本集{(2,3),(5,4), (9,6), (4,7), (8,1), (7,2)}</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250606160159584.png" alt="image-20250606160159584"></p>
<ul>
<li><p>查找点(2.1,3.1)</p>
</li>
<li><p>确定Search_Path为&lt;(7,2),(5,4), (2,3)&gt;；从search_path中取出(2,3)作为当前最佳结点nearest，dist为0.141</p>
<ul>
<li>然后回溯至(5,4)，以(2.1,3.1)为圆心，以dist&#x3D;0.141为半径画一个圆，并不和超平面y&#x3D;4相交，如上图，所以不必跳到结点(5,4)的右子空间去搜索，因为右子空间中不可能有更近样本点了。</li>
</ul>
</li>
<li><p>于是再回溯至(7,2)，同理，以(2.1,3.1)为圆心，以dist&#x3D;0.141为半径画一个圆并不和超平面x&#x3D;7相交，所以也不用跳到结点(7,2)的右子空间去搜索。</p>
<ul>
<li>至此，search_path为空，结束整个搜索，返回nearest(2,3)作为(2.1,3.1)的最近邻点，最近距离为0.141。</li>
</ul>
</li>
<li><p>查找点(2,4.5)</p>
<ul>
<li>在(7,2)处测试到达(5,4)，在(5,4)处测试到达(4,7)【优先选择在本域搜索】，然后search_path中的结点为&lt;(7,2),(5,4), (4,7)&gt;，从search_path中取出(4,7)作为当前最佳结点nearest, dist为3.202；</li>
<li>然后回溯至(5,4)，以(2,4.5)为圆心，以dist&#x3D;3.202为半径画一个圆与超平面y&#x3D;4相交，所以需要跳到(5,4)的左子空间去搜索。所以要将(2,3)加入到search_path中，现在search_path中的结点为&lt;(7,2),(2, 3)&gt;；另外，(5,4)与(2,4.5)的距离为3.04 &lt; dist &#x3D; 3.202，所以将(5,4)赋给nearest，并且dist&#x3D;3.04。</li>
<li>回溯至(2,3)，(2,3)是叶子节点，直接平判断(2,3)是否离(2,4.5)更近，计算得到距离为1.5，所以nearest更新为(2,3)，dist更新为(1.5)</li>
<li>回溯至(7,2)，同理，以(2,4.5)为圆心，以dist&#x3D;1.5为半径画一个圆并不和超平面x&#x3D;7相交, 所以不用跳到结点(7,2)的右子空间去搜索。</li>
<li>至此，search_path为空，结束整个搜索，返回nearest(2,3)作为(2,4.5)的最近邻点，最近距离为1.5。</li>
</ul>
</li>
</ul>
<h3 id="KD树的插入"><a href="#KD树的插入" class="headerlink" title="KD树的插入"></a>KD树的插入</h3><ul>
<li>在现有KD树中插入点 <code>(3, 6)</code>）：<ul>
<li><strong>现有树结构</strong>（按 <code>x→y→x...</code> 划分）：</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">          (5,4)        ← 根节点（x轴分割）</span><br><span class="line">         /     \</span><br><span class="line">    (2,3)       (8,1)   ← 第2层（y轴分割）</span><br><span class="line">     /   \       /   \</span><br><span class="line">(1,7) (4,2) (7,9) (9,5)</span><br></pre></td></tr></table></figure>

<p><strong>插入步骤</strong>：</p>
<ol>
<li>比较根节点 <code>(5,4)</code>（第1层，x轴）：<ul>
<li>插入点 <code>(3,6)</code> 的x值 <code>3 &lt; 5</code> → 进入左子树。</li>
</ul>
</li>
<li>比较 <code>(2,3)</code>（第2层，y轴）：<ul>
<li>插入点y值 <code>6 &gt; 3</code> → 进入右子树。</li>
</ul>
</li>
<li>到达叶子节点 <code>(4,2)</code>：<ul>
<li>第3层按x轴比较，<code>3 &lt; 4</code> → 作为 <code>(4,2)</code> 的左子节点插入。</li>
</ul>
</li>
</ol>
<p><strong>插入后树结构</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">          (5,4)</span><br><span class="line">         /     \</span><br><span class="line">    (2,3)       (8,1)</span><br><span class="line">     /   \       /   \</span><br><span class="line">(1,7) (4,2) (7,9) (9,5)</span><br><span class="line">     /</span><br><span class="line">  (3,6)       ← 新插入节点</span><br></pre></td></tr></table></figure>



<h3 id="KD树的删除"><a href="#KD树的删除" class="headerlink" title="KD树的删除"></a>KD树的删除</h3><ul>
<li><p><strong>核心逻辑</strong>：找到待删除节点后，按以下规则处理：</p>
<ul>
<li><p><strong>情况1</strong>：若为叶子节点 → 直接删除。</p>
</li>
<li><p><strong>情况2</strong>：若非叶子节点 → 找到子树中同分割轴的最优替代节点（类似二叉搜索树的中序后继）。</p>
</li>
</ul>
</li>
</ul>
<p>删除节点 <code>(5,4)</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">          (5,4)</span><br><span class="line">         /     \</span><br><span class="line">    (2,3)       (8,1)</span><br><span class="line">     /   \       /   \</span><br><span class="line">(1,7) (4,2) (7,9) (9,5)</span><br><span class="line">     /</span><br><span class="line">  (3,6) </span><br></pre></td></tr></table></figure>

<p><strong>步骤</strong>：</p>
<ol>
<li>定位节点 <code>(5,4)</code>：<ul>
<li>根节点，分割轴为x轴。</li>
</ul>
</li>
<li>寻找替代节点：<ul>
<li>在右子树中找x轴最小的点（即中序后继）→ <code>(7,9)</code>。</li>
</ul>
</li>
<li>替换并递归删除：<ul>
<li>用 <code>(7,9)</code> 替换 <code>(5,4)</code>，再递归删除 <code>(7,9)</code> 的原位置。</li>
</ul>
</li>
</ol>
<p><strong>删除后树结构</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">          (7,9)        ← 原(5,4)被替换</span><br><span class="line">         /     \</span><br><span class="line">    (2,3)       (8,1)</span><br><span class="line">     /   \       /   \</span><br><span class="line">(1,7) (4,2) (9,5)     ← (7,9)从原位置删除</span><br><span class="line">     /</span><br><span class="line">  (3,6)</span><br></pre></td></tr></table></figure>

<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>线性回归(Linear regression)是利用**回归方程(函数)**对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式。</p>
<p>只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/zhouaho2010/article/details/102756411">详解梯度下降</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/641618528">过拟合正则化解决</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/137713040">梯度下降</a>⭐️⭐️</li>
</ul>
<h2 id="一元线性回归"><a href="#一元线性回归" class="headerlink" title="一元线性回归"></a>一元线性回归</h2><blockquote>
<p>目标值只与一个因变量有关系，例如在某些情况下体重只跟身高有关系。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20230901102857178-20250608222015711.png" alt="image-20230901102857178"></p>
<p>$$<br>Y &#x3D; wx + b<br>$$</p>
<h3 id="回归方程解法（最小二乘法）"><a href="#回归方程解法（最小二乘法）" class="headerlink" title="回归方程解法（最小二乘法）"></a>回归方程解法（最小二乘法）</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250708153912662.png" alt="image-20250708153912662"></p>
<blockquote>
<p>由于有很多点，且并不正好落在一条直线上，这么多点每两点都能确定一条直线，这到底要怎么确定选哪条直线呢？</p>
</blockquote>
<p>刚好我们知道残差平方和的公式：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250710143356486.png"></p>
<p>可以发现上述自变量有$b$和$w$，利用微积分知识，导数为0时，Q取最小值，因此分别对$b$和$w$求偏导并令其为0：<br>$$<br>\frac{\partial Q}{\partial b}&#x3D;2\sum_{1}^{n}{(y_{i}-\hat{b}-\hat{w}x_{i})}&#x3D;0<br>$$</p>
<p>$$<br>\frac{\partial Q}{\partial w}&#x3D;2\sum_{1}^{n}{(y_{i}-\hat{b}-\hat{w}x_{i})x_{i}}&#x3D;0<br>$$</p>
<p>$x_{i}$，$y_{i}$（i&#x3D;1,2,…n）都是已知的，全部代入上面两个式子，可求得$w$和$b$的值。这就是最小二乘法，<code>二乘是平方的意思</code>。</p>
<h2 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h2><blockquote>
<p>目标值只与多个因变量有关系</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20230901103000204-20250608222015734.png" alt="image-20230901103000204"></p>
<p>$$<br>y &#x3D; \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p<br>$$</p>
<p>可以简写为矩阵形式（一般加粗表示矩阵或向量）： $\boldsymbol{Y}&#x3D;\boldsymbol{X}\boldsymbol{\beta}$</p>
<h3 id="回归方程解法（正规方程法）"><a href="#回归方程解法（正规方程法）" class="headerlink" title="回归方程解法（正规方程法）"></a>回归方程解法（正规方程法）</h3><p>一元线性回归的损失函数可以用残差平方和：<br>$$<br>Q&#x3D;\sum_{i&#x3D;1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}<br>$$</p>
<p>代入多元线性回归方程：<br>$$<br>Q&#x3D;\sum_{i&#x3D;1}^{n}\left(y_{i}-\hat\beta_{0}-\hat\beta_{1} x_{i 1}-\ldots-\hat\beta_{p} x_{i p}\right)^{2}<br>$$</p>
<p>用矩阵形式表示（$AA^{T}$&#x3D; A的L2范数）：<br>$$<br>Q &#x3D; (\boldsymbol{X}\boldsymbol{\beta}-\boldsymbol{Y})^{T}(\boldsymbol{X}\boldsymbol{\beta}-\boldsymbol{Y})<br>$$</p>
<p>由矩阵的运算规律 $(A B)^{T}&#x3D;B^{T} A^{T}$得：<br>$$<br>Q &#x3D; \left(\boldsymbol{\beta}^{T}\boldsymbol{X}^{T}-\boldsymbol{Y}^{T}\right)(\boldsymbol{X}\boldsymbol{\beta}-\boldsymbol{Y})<br>$$<br>由矩阵乘法分配律得：<br>$$<br>Q&#x3D;{\beta}^{T}{X}^{T}{X}{\beta}-{\beta}^{T}{X}^{T}{Y}-{Y}^{T}{X}{\beta}+ {Y}^{T}{Y}<br>$$<br>简化得：<br>$$<br>Q&#x3D;{\beta}^{T}{X}^{T}{X}{\beta}-2{\beta}^{T}{X}^{T}{Y}+{Y}^{T}{Y}<br>$$<br>按照一元线性回归求解析解的思路，现在要对Q求导并令导数为0：</p>
<p>根据公理：$\frac{\partial}{\partial \beta}(A\beta)^T(B\beta) &#x3D; A^TB + B^TA$得：<br>$$<br>\frac{\partial Q}{\partial \beta} &#x3D; 2X^{T}X\beta - X^{T}Y - X^{T}Y &#x3D; 0<br>$$</p>
<p>合并同类项化简得到对称形式：<br>$$<br>2X^{T}X\beta - 2X^{T}Y &#x3D; 0<br>$$</p>
<p>移项整理：<br>$$<br>X^{T}X\beta &#x3D; X^{T}Y<br>$$</p>
<p><code>最终解得</code>：<br>$$<br>\beta &#x3D; \left(X^{T}X\right)^{-1}X^{T}Y<br>$$</p>
<h3 id="正规方程应用"><a href="#正规方程应用" class="headerlink" title="正规方程应用"></a>正规方程应用</h3><p>通过多元线性回归方程求解：</p>
<p>公式$w&#x3D;(X^{T}X)^{-1}X^{T}Y$</p>
<ul>
<li>X为特征值矩阵，Y为目标值向量，w为模型参数的向量形式</li>
</ul>
<p>例如根据房子的空间、卧室数量、层数以及房子年龄预测房子的价格：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250708153515253.png" alt="image-20250708153515253"></p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><blockquote>
<p>一元线性回归、多元线性回归的损失函数可以用如下公式进行判断</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250708143849186.png" alt="image-20250708143849186"></p>
<p>计算每个点的残差 $e_i&#x3D;y_i-\hat{y_i}$ 的值，再将其平方（消除负号），再将所有的 $e_{i}^{2}$相加，量化出预测值和真实值的误差。</p>
<p>得到残差平方和SSE（Sum of Squares for Error）：</p>
<p><a href="https://liamjohnson-w.github.io/2024/08/08/2024.08.08/">Latex传送门</a>：时刻注意下划线_以及尖角号^导致公式解析异常。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250710143356486.png" alt="image-20250710143356486"></p>
<p>也就是回归问题的损失函数。</p>
<h3 id="绝对值误差函数-Mean-Absolute-Error-MAE-："><a href="#绝对值误差函数-Mean-Absolute-Error-MAE-：" class="headerlink" title="绝对值误差函数(Mean Absolute Error, MAE)："></a>绝对值误差函数(Mean Absolute Error, MAE)：</h3><p>$$<br> \text{MAE} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} \left| Y_i - \hat{Y}_i \right|<br>$$</p>
<ul>
<li><p>上面的公式中：n 为样本数量, y 为实际值, $\hat{y}$ 为预测值</p>
</li>
<li><p>MAE 越小模型预测约准确</p>
</li>
</ul>
<p>Sklearn 中MAE的API</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line">mean_absolute_error(y_test,y_predict)</span><br></pre></td></tr></table></figure>



<h3 id="均方误差函数-Mean-Squared-Error-MSE-："><a href="#均方误差函数-Mean-Squared-Error-MSE-：" class="headerlink" title="均方误差函数(Mean Squared Error, MSE)："></a>均方误差函数(Mean Squared Error, MSE)：</h3><p>$$<br>\text{MSE} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} \left(Y_i - \hat{Y}_i\right)^2<br>$$</p>
<ul>
<li>上面的公式中：n 为样本数量, y 为真实值, $\hat{y}$ 为预测值</li>
<li>MSE 越小模型预测约准确</li>
</ul>
<p>Sklearn 中MSE的API</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line">mean_squared_error(y_test,y_predict)</span><br></pre></td></tr></table></figure>



<h3 id="均方根误差函数-Root-Mean-Squared-Error-RMSE"><a href="#均方根误差函数-Root-Mean-Squared-Error-RMSE" class="headerlink" title="均方根误差函数(Root Mean Squared Error ,RMSE)"></a>均方根误差函数(Root Mean Squared Error ,RMSE)</h3><p>$$<br>RMSE &#x3D; \sqrt{\frac{1}{n}\sum_{i&#x3D;1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}}<br>$$</p>
<ul>
<li>上面的公式中：n 为样本数量, y 为真实值, $\hat{y}$ 为预测值</li>
<li>RMSE 越小模型预测约准确</li>
</ul>
<h2 id="梯度下降算法⭐️⭐️⭐️"><a href="#梯度下降算法⭐️⭐️⭐️" class="headerlink" title="梯度下降算法⭐️⭐️⭐️"></a>梯度下降算法⭐️⭐️⭐️</h2><p>算法思想：<code>沿着梯度下降的方向求解极小值</code>，也就是通过一步步迭代，让所有偏导函数都下降到最低</p>
<p><code>梯度就是导数+方向</code></p>
<p>单变量函数中，<code>梯度就是某一点切线斜率(某一点的导数)</code></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250608163506950.png" alt="image-20250608163506950"></p>
<p>多变量函数中，<code>梯度就是某一个点的偏导数</code></p>
<p>$L(a,b,c)$的梯度为<br>$$<br>\left(\frac{\partial L}{\partial a}, \frac{\partial L}{\partial b}, \frac{\partial L}{\partial c}\right)<br>$$</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250608163451996.png" alt="image-20250608163451996"></p>
<p>梯度下降公式：<br>$$<br>\theta_{i+1} &#x3D; \theta_{i} - \alpha \frac{\partial}{\partial \theta_{i}} J(\theta)<br>$$</p>
<ul>
<li>$\alpha$（或$\eta$）：学习率（步长）<ul>
<li>机器学习常用范围：$0.001 \sim 0.01$</li>
<li>深度学习常用范围：$10^{-6}$量级</li>
<li><code>学习率太小，下降的速度会变慢；学习率太大，容易错过最低点，产生下降过程中的震荡，甚至梯度爆炸</code>。</li>
</ul>
</li>
<li>负号的意义：<code>梯度是上升最快的方向(想象斜率为正，函数呈上升趋势)，我们需要下降最快的方向，因此需要加负号</code></li>
<li>$\alpha$后面的是损失函数对某个特征求偏导</li>
</ul>
<h3 id="单变量梯度下降示例"><a href="#单变量梯度下降示例" class="headerlink" title="单变量梯度下降示例"></a>单变量梯度下降示例</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250708174010262.png" alt="image-20250708174010262"></p>
<p>求函数 $J(\theta) &#x3D; \theta^{2}$ 的最小值，即确定 $\theta$ 为何值时 $J(\theta)$ 最小。</p>
<ul>
<li><p>函数导数：$\frac{\partial J(\theta)}{\partial \theta} &#x3D; 2\theta$ ，于是</p>
<ul>
<li>梯度下降更新公式：$\theta_{new} &#x3D; \theta - \alpha \cdot 2\theta$</li>
</ul>
</li>
<li><p>令：初始值：$\theta_0 &#x3D; 1$、学习率：$\alpha &#x3D; 0.4$</p>
</li>
<li><p>经过5次迭代后，$\theta$ 值从1收敛到0.0016</p>
</li>
</ul>
<p>迭代过程</p>
<table>
<thead>
<tr>
<th>步骤</th>
<th>计算公式</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td>第一步</td>
<td>$\theta &#x3D; 1$</td>
<td>1.0</td>
</tr>
<tr>
<td>第二步</td>
<td>$1 - 0.4 \times (2 \times 1)$</td>
<td>0.2</td>
</tr>
<tr>
<td>第三步</td>
<td>$0.2 - 0.4 \times (2 \times 0.2)$</td>
<td>0.04</td>
</tr>
<tr>
<td>第四步</td>
<td>$0.04 - 0.4 \times (2 \times 0.04)$</td>
<td>0.008</td>
</tr>
<tr>
<td>第五步</td>
<td>$0.008 - 0.4 \times (2 \times 0.008)$</td>
<td>0.0016</td>
</tr>
</tbody></table>
<h3 id="多元线性回归梯度算法"><a href="#多元线性回归梯度算法" class="headerlink" title="多元线性回归梯度算法"></a>多元线性回归梯度算法</h3><p>回到前面的多元线性回归，用梯度下降算法求损失函数的最小值。</p>
<p>首先，求梯度，前面已经给出求偏导的公式：<br>$$<br>\frac{\partial Q}{\partial \beta}&#x3D;2 X^{T} X \beta-2X^{T} Y&#x3D;2 X^{T} (X \beta-Y)<br>$$</p>
<p>将梯度代入随机梯度下降公式：<br>$$<br>\Theta_{k+1}&#x3D;\Theta_{k}-\alpha \cdot 2 X^{T} (X \beta-Y)<br>$$</p>
<p>这个式子中，X矩阵和Y向量都是已知的，步长是人为设定的一个值，只有参数 $\beta$ 是未知的，而每一步的 $\Theta$ 是由 $\beta$ 决定的，也就是每一步的点坐标。</p>
<p><strong>算法过程：</strong></p>
<ol>
<li>初始化 $\beta$ 向量的值，即 $\Theta_{0}$ ，将其代入 $\frac{\partial Q}{\partial \beta}$ 得到当前位置的梯度；</li>
<li>用步长 $\alpha$ 乘以当前梯度，得到从当前位置下降的距离；</li>
<li>更新 $\Theta_1$ ，其更新表达式为 $\Theta_1&#x3D;\Theta_0-\alpha \cdot 2 X^{T} (X \Theta_0-Y)$ ；</li>
<li>重复以上步骤，直到更新到某个 $\Theta_k$ ，达到停止条件，这个 $\Theta_k$ 就是我们求解的参数向量。</li>
</ol>
<h3 id="梯度下降算法调优"><a href="#梯度下降算法调优" class="headerlink" title="梯度下降算法调优"></a>梯度下降算法调优</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250708174829651.png" alt="image-20250708174829651"></p>
<p>梯度下降公式：<br>$$<br>\theta_{i+1} &#x3D; \theta_{i} - \alpha \frac{\partial}{\partial \theta_{i}} J(\theta)<br>$$</p>
<p>其中，步长：$\theta_0$，学习率：$\alpha$，$\frac{\partial}{\partial \theta_{i}} J(\theta)$为回归函数的对变量的导数。</p>
<p>在使用梯度下降时，需要进行调优</p>
<ul>
<li><p><code>算法的步长选择。步长太大，会导致迭代过快，有可能错过最优解。步长太小，收敛速度过慢</code>。需要多次运行后才能得到一个较为优的值。</p>
</li>
<li><p><code>算法参数的初始值选择</code>。由于有局部最优解的风险，需要多次用不同初始值运行算法，选择损失函数最小化的初值。</p>
</li>
<li><p>归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化。新期望为0，新方差为1，迭代速度可以大大加快。</p>
</li>
</ul>
<p>$$<br>\frac{x - \overline{x}}{\operatorname{std}(x)}<br>$$</p>
<h3 id="改进的梯度算法"><a href="#改进的梯度算法" class="headerlink" title="改进的梯度算法"></a>改进的梯度算法</h3><ul>
<li>批量梯度下降法BGD（Batch Gradient Descent）</li>
</ul>
<p>$$<br>\theta_{j} &#x3D; \theta_{j} - \alpha \left(<br>\frac{1}{N} \sum_{i&#x3D;1}^{N} \left(<br>h_{\theta}\left(x_{0}^{(i)}, x_{1}^{(i)}, \cdots, x_{n}^{(i)}\right) - y^{(i)}<br>\right) x_{j}^{(i)}<br>\right)<br>$$</p>
<p>批量梯度下降法，<code>是梯度下降法最常用的形式</code></p>
<p>就是在更新参数时<code>使用所有的样本</code>来进行更新，这个方法就是之前的线性回归的梯度下降算法。</p>
<ul>
<li>随机梯度下降法SGD（Stochastic Gradient Descent）</li>
</ul>
<p>$$<br>\theta_{j} &#x3D; \theta_{j} - \alpha \left(<br>h_{\theta}\left(x_{0}^{(i)}, x_{1}^{(i)}, \cdots, x_{n}^{(i)}\right) - y^{(i)}<br>\right) x_{j}^{(i)}<br>$$</p>
<p>就是在与求梯度时没有用所有的N个样本的数据，而是仅仅选取一个样本j来求梯度。</p>
<blockquote>
<p>随机梯度下降法，和批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。</p>
<p>自然各自的优缺点都非常突出。</p>
<p>训练速度，随机梯度下降法仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度很慢。</p>
<p>准确度，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。</p>
<p>收敛速度，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。<br>那么，有没有一个中庸的办法能够结合两种方法的优点呢？有！这小批量梯度下降法MBGD。</p>
</blockquote>
<ul>
<li>小批量梯度下降法MBGD（Mini-batch Gradient Descent）</li>
</ul>
<p>$$<br>\theta_{j}&#x3D; \theta_{j} - \alpha \left(<br>\frac{1}{x} \sum_{i&#x3D;t}^{t+x-1} \left(<br>h_{\theta}\left(x_{0}^{(i)}, x_{1}^{(i)}, \cdots, x_{n}^{(i)}\right) - y^{(i)}<br>\right) x_{j}^{(i)}<br>\right)<br>$$</p>
<p>小批量梯度下降法是<code>批量梯度下降法和随机梯度下降法的折中</code></p>
<p>也就是对于N个样本，采用x个样本来迭代，1&lt;x&lt;N。一般可以取x&#x3D;10，当然根据样本的数据，可以调整这个x的值。</p>
<p><code>若batch_size=1则变成了SGD，若batch_size=n则变成了FGD</code></p>
<ul>
<li>随机平均梯度下降算法 SAG</li>
</ul>
<p>$$<br>\theta_{i+1} &#x3D; \theta_{i} - \frac{\alpha}{n} \sum_{j&#x3D;1}^{n} \left( h_{\theta}\left(x_{0}^{(j)}, x_{1}^{(j)},\ldots,x_{n}^{(j)}\right) - y_{j} \right) x_{i}^{(j)}<br>$$</p>
<p>每次迭代时，<code>随机选择一个样本的梯度值和以往样本的梯度值的均值进行参数更新。</code></p>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><blockquote>
<p><code>L1正则化：将不重要的特征的参数为0</code><br><code>L2正则化：将不重要的特征参数趋向于0</code></p>
</blockquote>
<p>参考链接：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/62457875">线性回归中的正则化</a></li>
</ul>
<h3 id="过拟合如何解决"><a href="#过拟合如何解决" class="headerlink" title="过拟合如何解决"></a>过拟合如何解决</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250708204038413.png" alt="image-20250708204038413"></p>
<p>使用正则化项，也就是给loss function加上一个参数项，正则化项有<strong>L1正则化、L2正则化、ElasticNet</strong>。加入这个正则化项好处：</p>
<ul>
<li>控制参数幅度，不让模型“无法无天”。</li>
<li>限制参数搜索空间</li>
<li>解决欠拟合与过拟合的问题。</li>
</ul>
<h3 id="L2正则化-Ridge岭回归"><a href="#L2正则化-Ridge岭回归" class="headerlink" title="L2正则化(Ridge岭回归)"></a>L2正则化(Ridge岭回归)</h3><ul>
<li>数学定义：在原始损失函数中添加权重的平方和作为惩罚项：</li>
</ul>
<p>$$<br>J(\mathbf{w}) &#x3D; \text{Loss}(\mathbf{y}, \hat{\mathbf{y}}) + \lambda \sum_{i&#x3D;1}^n w_i^2<br>$$</p>
<p>$$<br>J &#x3D; J_{0} + \lambda \sum_{w} w^{2}<br>$$</p>
<ul>
<li><p>$J$：加入正则化后的总损失函数</p>
</li>
<li><p>$J_0$：原始损失函数（如MSE&#x2F;交叉熵）</p>
</li>
<li><p>$\lambda$：正则化强度超参数（$\lambda &gt; 0$）</p>
</li>
<li><p>$w$：模型权重参数</p>
</li>
</ul>
<p>损失函数<br>$$<br>J_0&#x3D;\sum_{i&#x3D;1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}<br>$$</p>
<p>$J_0$表示上面的 loss function ，在loss function的基础上加入$w$参数的平方和乘以$\lambda$，假设:<br>$$<br>L &#x3D; \lambda(w_{1}^{2} + w_{2}^{2})<br>$$<br>回忆以前学过的单位元的方程：</p>
<p>$$<br> x^{2} + y^{2} &#x3D; 1<br>$$</p>
<p>和L2正则化项一样，此时我们的任务变成在L约束下求出J取最小值的解。求解J0的过程可以画出等值线。同时L2正则化的函数L也可以在w1w2的二维平面上画出来。如下图：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250616200929180.png" alt="image-20250616200929180"></p>
<blockquote>
<p>什么场景下用L2正则化？</p>
<p>只要数据线性相关，用LinearRegression拟合的不是很好，<strong>需要正则化</strong>，可以考虑使用岭回归(L2), 如何输入特征的维度很高,而且是稀疏线性关系的话， 岭回归就不太合适,考虑使用Lasso回归。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">L2正则化</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 0.导入工具包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression,Lasso,Ridge</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.准备数据</span></span><br><span class="line">np.random.seed(<span class="number">22</span>)</span><br><span class="line">x = np.random.uniform(-<span class="number">3</span>, <span class="number">3</span>, size=<span class="number">100</span>)</span><br><span class="line"><span class="comment">#print(x)</span></span><br><span class="line">y = <span class="number">0.5</span> * x ** <span class="number">2</span> + x + <span class="number">2</span> + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># print(y)</span></span><br><span class="line"><span class="comment"># 2.模型训练</span></span><br><span class="line">model = Ridge(alpha=<span class="number">0.1</span>)</span><br><span class="line">X = x.reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">X3 = np.hstack([X,X**<span class="number">2</span>,X**<span class="number">3</span>,X**<span class="number">4</span>,X**<span class="number">5</span>,X**<span class="number">6</span>,X**<span class="number">7</span>,X**<span class="number">9</span>,X**<span class="number">10</span>])</span><br><span class="line">model.fit(X3,y)</span><br><span class="line"><span class="built_in">print</span>(model.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.模型预测</span></span><br><span class="line">y_predict  = model.predict(X3)</span><br><span class="line"><span class="built_in">print</span>(mean_squared_error(y_true=y,y_pred=y_predict))</span><br><span class="line"><span class="comment"># 4.展示效果</span></span><br><span class="line">plt.scatter(x,y)</span><br><span class="line">plt.plot(np.sort(x),y_predict[np.argsort(x)],color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[ 1.41372496e+00  1.24305713e+00 -9.90023788e-01 -3.35689918e-01</span></span><br><span class="line"><span class="string">  4.82122518e-01  4.60105075e-02 -8.41885140e-02  4.75191054e-03</span></span><br><span class="line"><span class="string"> -2.35177926e-04]</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">0.8227587479762861</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h3 id="L1正则化-Lasso回归"><a href="#L1正则化-Lasso回归" class="headerlink" title="L1正则化(Lasso回归)"></a>L1正则化(Lasso回归)</h3><p>Lasso(Least Absolute Shrinkage and Selection Operator)</p>
<p>L1正则化与L2正则化的区别在于惩罚项的不同：</p>
<p>$$<br>J(\mathbf{w}) &#x3D; \text{Loss}(\mathbf{y}, \hat{\mathbf{y}}) + \lambda \sum_{i&#x3D;1}^n |w_i|<br>$$</p>
<p>$$<br>J &#x3D; J_{0} + \lambda \left( |w_{1}| + |w_{2}| \right)<br>$$</p>
<p>求解J0的过程可以画出等值线。同时L1正则化的函数也可以在w1w2的二维平面上画出来。如下图：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250616201532812.png" alt="image-20250616201532812"></p>
<p>惩罚项表示为图中的黑色棱形，随着梯度下降法的不断逼近，与棱形第一次产生交点，而这个交点很容易出现在坐标轴上。<strong>这就说明了L1正则化容易得到稀疏矩阵。</strong></p>
<blockquote>
<p>L1正则化使用场景</p>
<p><strong>L1正则化(Lasso回归)可以使得一些特征的系数变小,甚至还使一些绝对值较小的系数直接变为0</strong>，从而增强模型的泛化能力 。对于高的特征数据,尤其是线性关系是稀疏的，就采用L1正则化(Lasso回归),或者是要在一堆特征里面找出主要的特征，那么L1正则化(Lasso回归)更是首选了。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">L1正则化</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 0.导入工具包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression,Lasso</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.准备数据</span></span><br><span class="line">np.random.seed(<span class="number">22</span>)</span><br><span class="line">x = np.random.uniform(-<span class="number">3</span>, <span class="number">3</span>, size=<span class="number">100</span>)</span><br><span class="line"><span class="comment">#print(x)</span></span><br><span class="line">y = <span class="number">0.5</span> * x ** <span class="number">2</span> + x + <span class="number">2</span> + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># print(y)</span></span><br><span class="line"><span class="comment"># 2.模型训练</span></span><br><span class="line">model = Lasso(alpha=<span class="number">0.1</span>)</span><br><span class="line">X = x.reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">X3 = np.hstack([X,X**<span class="number">2</span>,X**<span class="number">3</span>,X**<span class="number">4</span>,X**<span class="number">5</span>,X**<span class="number">6</span>,X**<span class="number">7</span>,X**<span class="number">9</span>,X**<span class="number">10</span>])</span><br><span class="line">model.fit(X3,y)</span><br><span class="line"><span class="built_in">print</span>(model.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.模型预测</span></span><br><span class="line">y_predict  = model.predict(X3)</span><br><span class="line"><span class="built_in">print</span>(mean_squared_error(y_true=y,y_pred=y_predict))</span><br><span class="line"><span class="comment"># 4.展示效果</span></span><br><span class="line">plt.scatter(x,y)</span><br><span class="line">plt.plot(np.sort(x),y_predict[np.argsort(x)],color=<span class="string">&quot;r&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[ 2.68230387e-01  2.17826117e-01  3.65595593e-01  7.28745107e-02</span></span><br><span class="line"><span class="string"> -4.14761830e-02 -1.80435345e-03 -5.28230167e-03  7.29378060e-04</span></span><br><span class="line"><span class="string"> -5.27239512e-05]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">0.94157770743677</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h3 id="ElasticNet回归"><a href="#ElasticNet回归" class="headerlink" title="ElasticNet回归"></a>ElasticNet回归</h3><p><strong>ElasticNet综合了L1正则化项和L2正则化项</strong>，以下是它的公式：<br>$$<br>J(\mathbf{w}) &#x3D; \text{Loss}(\mathbf{y}, \hat{\mathbf{y}}) + \lambda_1 \sum_{i&#x3D;1}^n |w_i| + \lambda_2 \sum_{i&#x3D;1}^n w_i^2<br>$$</p>
<p>$$<br>\min \left(<br>\frac{1}{2m} \left[ \sum_{i&#x3D;1}^{m}(y_i’ - y_i)^2 \right]<br>+ \lambda \sum_{j&#x3D;1}^{n} \theta_j^2<br>+ \lambda \sum_{j&#x3D;1}^{n} |\theta_j|<br>\right)<br>$$</p>
<blockquote>
<p>ElasticNet回归的使用场景</p>
<p>ElasticNet在我们发现用Lasso回归太过(太多特征被稀疏为0),而岭回归也正则化的不够(回归系数衰减太慢)的时候，可以考虑使用ElasticNet回归来综合，得到比较好的结果。</p>
</blockquote>
<h3 id="线性回归要求因变量服从正态分布？"><a href="#线性回归要求因变量服从正态分布？" class="headerlink" title="线性回归要求因变量服从正态分布？"></a>线性回归要求因变量服从正态分布？</h3><p>我们假设线性回归的噪声服从均值为0的正态分布。 当噪声符合正态分布$N(0,delta^2)$时，因变量则符合正态分布$N(ax(i)+b,delta^2)$，其中预测函数$y&#x3D;ax^{(i)}+b$。这个结论可以由正态分布的概率密度函数得到。也就是说当噪声符合正态分布时，其因变量必然也符合正态分布。</p>
<p>在用线性回归模型拟合数据之前，首先要求数据应符合或近似符合正态分布，否则得到的拟合函数不正确。</p>
<h2 id="回归模型评估方法⭐️"><a href="#回归模型评估方法⭐️" class="headerlink" title="回归模型评估方法⭐️"></a>回归模型评估方法⭐️</h2><table>
<thead>
<tr>
<th>特性</th>
<th>MAE</th>
<th>MSE</th>
<th>RMSE</th>
</tr>
</thead>
<tbody><tr>
<td><strong>误差敏感性</strong></td>
<td>对异常值不敏感</td>
<td>对异常值敏感</td>
<td>对异常值敏感</td>
</tr>
<tr>
<td><strong>量纲</strong></td>
<td>与原数据一致</td>
<td>原数据平方量纲</td>
<td>与原数据一致</td>
</tr>
<tr>
<td><strong>数学性质</strong></td>
<td>不可导</td>
<td>处处可导</td>
<td>处处可导</td>
</tr>
<tr>
<td><strong>优化方向</strong></td>
<td>中位数</td>
<td>平均值</td>
<td>平均值</td>
</tr>
<tr>
<td><strong>计算复杂度</strong></td>
<td>低</td>
<td>中等</td>
<td>中等（需开方）</td>
</tr>
</tbody></table>
<ol>
<li><strong>MAE适用场景</strong>：<ul>
<li>需要解释性强的场景</li>
<li>数据存在少量异常值时</li>
<li>例如：房价预测、零售销量预测</li>
</ul>
</li>
<li><strong>MSE&#x2F;RMSE适用场景</strong>：<ul>
<li>需要惩罚大误差的场景</li>
<li>数据质量较高且分布均匀时</li>
<li>例如：金融风险评估、科学实验分析</li>
</ul>
</li>
<li><strong>特殊注意事项</strong>：<ul>
<li>MSE值通常比MAE大1-2个数量级</li>
<li>RMSE在量纲上与MAE可比，但数值通常更大</li>
<li>当误差分布呈高斯分布时，MSE是最优</li>
</ul>
</li>
</ol>
<h2 id="波士顿房价案例-线性回归及API"><a href="#波士顿房价案例-线性回归及API" class="headerlink" title="波士顿房价案例(线性回归及API)"></a>波士顿房价案例(线性回归及API)</h2><p>波士顿房价数据集是机器学习中经典的回归问题数据集，包含506条样本数据，每条数据有13个特征变量和1个目标变量(房价中位数)。数据来源于1978年美国人口普查局收集的波士顿地区房价信息。</p>
<h3 id="数据集属性"><a href="#数据集属性" class="headerlink" title="数据集属性"></a>数据集属性</h3><p>数据集包含以下特征：</p>
<ul>
<li>CRIM：城镇人均犯罪率</li>
<li>ZN：占地面积超过2.5万平方英尺的住宅用地比例</li>
<li>INDUS：城镇非零售业务地区的比例</li>
<li>CHAS：查尔斯河虚拟变量（1表示靠近河流，0表示不靠近）</li>
<li>NOX：氮氧化物浓度（每千万分之一）</li>
<li>RM：平均每户住房房间数</li>
<li>AGE：1940年以前建造的自住房屋比例</li>
<li>DIS：到波士顿五个就业中心的加权距离</li>
<li>RAD：径向公路可达性指数</li>
<li>TAX：每10,000美元的全额财产税税率</li>
<li>PTRATIO：城镇师生比例</li>
<li>B：黑人比例</li>
<li>LSTAT：人口中低收入阶层百分比</li>
<li>MEDV：自住房屋房价中位数（目标变量）</li>
</ul>
<h3 id="案例分析步骤"><a href="#案例分析步骤" class="headerlink" title="案例分析步骤"></a>案例分析步骤</h3><ol>
<li>数据获取与分割</li>
<li>特征标准化处理</li>
<li>模型训练（正规方程和梯度下降两种方法）</li>
<li>模型评估</li>
</ol>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0.导包</span></span><br><span class="line"><span class="comment"># from sklearn.datasets import load_boston</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression,SGDRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error,mean_absolute_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.加载数据</span></span><br><span class="line"><span class="comment"># boston = load_boston()</span></span><br><span class="line"><span class="comment"># print(boston)</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_url = <span class="string">&quot;http://lib.stat.cmu.edu/datasets/boston&quot;</span></span><br><span class="line">raw_df = pd.read_csv(data_url, sep=<span class="string">&quot;\s+&quot;</span>, skiprows=<span class="number">22</span>, header=<span class="literal">None</span>)</span><br><span class="line">data = np.hstack([raw_df.values[::<span class="number">2</span>, :], raw_df.values[<span class="number">1</span>::<span class="number">2</span>, :<span class="number">2</span>]])</span><br><span class="line">target = raw_df.values[<span class="number">1</span>::<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.数据集划分</span></span><br><span class="line">x_train,x_test,y_train,y_test =train_test_split(data,target,test_size=<span class="number">0.2</span>,random_state=<span class="number">22</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.标准化</span></span><br><span class="line">process=StandardScaler()</span><br><span class="line">x_train=process.fit_transform(x_train)</span><br><span class="line">x_test=process.transform(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.模型训练</span></span><br><span class="line"><span class="comment"># 4.1 实例化(正规方程)</span></span><br><span class="line"><span class="comment"># 随机梯度下降法</span></span><br><span class="line">model = SGDRegressor(learning_rate=<span class="string">&#x27;constant&#x27;</span>,eta0=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 4.2 fit</span></span><br><span class="line">model.fit(x_train,y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(model.coef_)</span></span><br><span class="line"><span class="comment"># print(model.intercept_)</span></span><br><span class="line"><span class="comment"># 5.预测</span></span><br><span class="line">y_predict=model.predict(x_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.模型评估</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    MSE: 23.39259636923585</span></span><br><span class="line"><span class="string">    RMSE: 4.836589332291491</span></span><br><span class="line"><span class="string">    MAE: 3.562918092849904</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MSE:&quot;</span>,mean_squared_error(y_test,y_predict))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;RMSE:&quot;</span>,mean_squared_error(y_test,y_predict) ** <span class="number">0.5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MAE:&quot;</span>,mean_absolute_error(y_test,y_predict))</span><br></pre></td></tr></table></figure>



<h3 id="正规方程法"><a href="#正规方程法" class="headerlink" title="正规方程法"></a>正规方程法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0.导包</span></span><br><span class="line"><span class="comment"># from sklearn.datasets import load_boston</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression,SGDRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error,mean_absolute_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.加载数据</span></span><br><span class="line"><span class="comment"># boston = load_boston()</span></span><br><span class="line"><span class="comment"># print(boston)</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入数据</span></span><br><span class="line">data_url = <span class="string">&quot;http://lib.stat.cmu.edu/datasets/boston&quot;</span></span><br><span class="line">raw_df = pd.read_csv(data_url, sep=<span class="string">&quot;\s+&quot;</span>, skiprows=<span class="number">22</span>, header=<span class="literal">None</span>)</span><br><span class="line">data = np.hstack([raw_df.values[::<span class="number">2</span>, :], raw_df.values[<span class="number">1</span>::<span class="number">2</span>, :<span class="number">2</span>]])</span><br><span class="line">target = raw_df.values[<span class="number">1</span>::<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.数据集划分</span></span><br><span class="line">x_train,x_test,y_train,y_test =train_test_split(data,target,test_size=<span class="number">0.2</span>,random_state=<span class="number">22</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.标准化</span></span><br><span class="line">process=StandardScaler()</span><br><span class="line">x_train=process.fit_transform(x_train)</span><br><span class="line">x_test=process.transform(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.模型训练 - 采用线性回归模型</span></span><br><span class="line"><span class="comment"># 4.1 实例化(正规方程)</span></span><br><span class="line"><span class="comment"># 默认采用的就是正规方程法</span></span><br><span class="line">model =LinearRegression(fit_intercept=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 4.2 fit</span></span><br><span class="line">model.fit(x_train,y_train)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">coef:</span></span><br><span class="line"><span class="string">    [-0.73088157  1.13214851 -0.14177415  0.86273811 -2.02555721  2.72118285</span></span><br><span class="line"><span class="string"> -0.1604136  -3.36678479  2.5618082  -1.68047903 -1.67613468  0.91214657</span></span><br><span class="line"><span class="string"> -3.79458347]</span></span><br><span class="line"><span class="string">intercept:</span></span><br><span class="line"><span class="string"> 22.57970297029704</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(model.coef_)</span><br><span class="line"><span class="built_in">print</span>(model.intercept_)</span><br><span class="line"><span class="comment"># 5.预测</span></span><br><span class="line">y_predict=model.predict(x_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.模型评估</span></span><br><span class="line"><span class="comment"># 使用MSE进行评估</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    MSE: 20.77068478427006</span></span><br><span class="line"><span class="string">    RMSE: 4.557486674063903</span></span><br><span class="line"><span class="string">    MAE: 3.425181871853366</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MSE:&quot;</span>,mean_squared_error(y_test,y_predict))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;RMSE:&quot;</span>,mean_squared_error(y_test,y_predict) ** <span class="number">0.5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MAE:&quot;</span>,mean_absolute_error(y_test,y_predict))</span><br></pre></td></tr></table></figure>



<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p><code>逻辑回归是用来做分类算法的</code>，比如一元线性回归，一般形式是$Y&#x3D;aX+b$，$Y$的取值范围是$[-∞, +∞]$，有这么多取值，怎么进行分类呢？不用担心，伟大的数学家已经为我们找到了一个方法。</p>
<p>也就是把Y的结果带入一个非线性变换的Sigmoid函数中，即可得到[0,1]之间取值范围的数S，S可以把它看成是一个概率值，如果我们设置概率阈值为0.5，那么S大于0.5可以看成是正样本，小于0.5看成是负样本，就可以进行分类了。业界经常用它来预测：<code>客户是否会购买某个商品，借款人是否会违约等等。</code></p>
<p>Logistic Regression 虽然被称为回归，但其实际上是分类模型，并常用于二分类。其本质是：<code>假设数据服从这个分布，然后使用极大似然估计做参数的估计。</code></p>
<p>参考链接：<a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/2.Logistics%20Regression/2.Logistics%20Regression.md">NLP-LOVE</a></p>
<h2 id="Sigmoid-函数"><a href="#Sigmoid-函数" class="headerlink" title="Sigmoid 函数"></a>Sigmoid 函数</h2><p>函数公式如下：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250616143820238.png" alt="image-20250616143820238"></p>
<blockquote>
<p>函数中t无论取什么值，其结果都在[0,-1]的区间内，回想一下，一个分类问题就有两种答案，一种是“是”，一种是“否”，那0对应着“否”，1对应着“是”，那又有人问了，你这不是[0,1]的区间吗，怎么会只有0和1呢？这个问题问得好，我们假设分类的<strong>阈值</strong>是0.5，那么超过0.5的归为1分类，低于0.5的归为0分类，<code>阈值是可以自己设定的。</code></p>
</blockquote>
<p>好了，接下来我们把$aX+b$带入t中就得到了逻辑回归的一般模型方程：</p>
<p>$$<br>H(a,b) &#x3D; \frac{1}{1 + e^{(aX + b)}}<br>$$</p>
<p>结果P也可以理解为概率，换句话说概率大于0.5的属于1分类，概率小于0.5的属于0分类，这就达到了分类的目的。</p>
<p>其分布函数和密度函数分别为：</p>
<p>$$<br>F(x) &#x3D; P(X\leq{x})&#x3D;\frac{1}{1 + e^{-(x-u)&#x2F;\gamma}}<br>$$</p>
<p>$$<br>f(x) &#x3D; F’(X\leq{x})&#x3D;\frac{e^{-(x-u)&#x2F;\gamma}}{\gamma(1 + e^{-(x-u)&#x2F;\gamma})^{2}}<br>$$</p>
<p>其中， $\mu$ 表示<strong>位置参数</strong>， $\gamma&gt;0$ 为<strong>形状参数</strong>，可以看下其图像特征：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250609160632664.png" alt="image-20250609160632664"></p>
<p>Logistic 分布是由其位置和尺度参数定义的连续分布。Logistic 分布的形状与正态分布的形状相似，但是 Logistic 分布的尾部更长，所以我们可以使用 Logistic 分布来建模比正态分布具有更长尾部和更高波峰的数据分布。</p>
<p>在深度学习中常用到的 Sigmoid 函数就是 Logistic 的分布函数在 $\mu&#x3D;0, \gamma&#x3D;1$ 的特殊形式。</p>
<h2 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h2><blockquote>
<p>在机器学习领域，总是避免不了谈论损失函数这一概念。</p>
<p><code>损失函数是用于衡量预测值与实际值的偏离程度</code>，即模型预测的错误程度。</p>
<p>也就是说，这个值越小，认为模型效果越好，举个极端例子，<code>如果预测完全精确，则损失函数值为0。</code></p>
<p>假设：有 0、1 两个类别，<code>某个样本被分为 1 类的概率为 p,则分为 0 类的概率为 1-p</code>，则每一个样本分类正确的概率为：</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250711204140565.png" alt="image-20250711204140565"></p>
<p>对上述公式进行合式：<br>$$<br>p^y(1-p)^{1-y}<br>$$<br>咱们对损失函数的期望是：</p>
<p>当样本是1类别，模型预测的p越大越好；</p>
<p>当样本是0类别，模型预测的p越小越好；<br>$$<br>P(x_2)…P(y_n|x_n) &#x3D; \prod_{i&#x3D;1}^{n} p^{y_i}(1-p)^{1-y_i}<br>$$</p>
<p>取对数转换连乘为连加<br>$$<br>\log L &#x3D; \log\left(\prod_{i&#x3D;1}^{m}\left[p_{i}^{y_i}\left(1-p_{i}\right)^{1-y_i}\right]\right)<br>$$</p>
<p>$$<br>&#x3D; \sum_{i&#x3D;1}^{m} \log \left(p_{i}^{y_i}\left(1-p_{i}\right)^{1-y_i}\right)<br>$$</p>
<p>$$<br>&#x3D; \sum_{i&#x3D;1}^{m}\left[\log\left(p_{i}^{y_i}\right)+\log\left(\left(1-p_{i}\right)^{1-y_i}\right)\right]<br>$$</p>
<p><em>（对数性质：$\log(CD) &#x3D; \log C + \log D$）</em><br>$$<br>&#x3D; \sum_{i&#x3D;1}^{m}\left[y_i \log \left(p_{i}\right) + \left(1-y_i\right) \log \left(1-p_{i}\right)\right]<br>$$<br><em>（对数性质：$\log(A^B) &#x3D; B \log A$）</em><br>$$<br>\log L &#x3D; \sum_{i&#x3D;1}^{m} \left[ y_i \log(p_i) + (1-y_i)\log(1-p_i) \right]<br>$$</p>
<p>于是得到公式：</p>
<p>$$<br> Loss(L) &#x3D; -\sum_{i&#x3D;1}^{m} \left( y_{i}\log(p_{i}) + (1 - y_{i})\log(1 - p_{i}) \right)<br>$$</p>
<p>其中$$ p_{i} &#x3D; \operatorname{sigmoid}(w^{T}x + b) $$是逻辑回归的输出结果</p>
<ul>
<li>损失函数的工作原理：每个样本预测值有A、B两个类别，真实类别对应的位置，概率值域越大越好</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250609185456031.png" alt="image-20250609185456031"></p>
<h2 id="逻辑回归API"><a href="#逻辑回归API" class="headerlink" title="逻辑回归API"></a>逻辑回归API</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.linear_model.LogisticRegression(solver=<span class="string">&#x27;liblinear&#x27;</span>, penalty=‘l2’, C = <span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>solver</strong> ：<strong>损失函数优化方法</strong>:</p>
<ul>
<li><p>训练速度:liblinear 对小数据集场景训练速度更快，sag 和 saga 对大数据集更快一些。 </p>
</li>
<li><p>正则化:</p>
<ul>
<li>newton-cg、lbfgs、sag、saga 支持 L2 正则化或者没有正则化</li>
<li>2liblinear 和 saga 支持 L1 正则化</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>penalty</strong>：正则化的种类</p>
<ul>
<li>L1 正则化或者  L2 正则化</li>
</ul>
</li>
<li><p><strong>C</strong>：正则化力度</p>
<ul>
<li>默认将类别数量少的当做正例</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.获取数据</span></span><br><span class="line">names = [<span class="string">&#x27;Sample code number&#x27;</span>, <span class="string">&#x27;Clump Thickness&#x27;</span>, <span class="string">&#x27;Uniformity of Cell Size&#x27;</span>, </span><br><span class="line">         <span class="string">&#x27;Uniformity of Cell Shape&#x27;</span>, <span class="string">&#x27;Marginal Adhesion&#x27;</span>, <span class="string">&#x27;Single Epithelial Cell Size&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;Bare Nuclei&#x27;</span>, <span class="string">&#x27;Bland Chromatin&#x27;</span>, <span class="string">&#x27;Normal Nucleoli&#x27;</span>, <span class="string">&#x27;Mitoses&#x27;</span>, <span class="string">&#x27;Class&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.基本数据处理</span></span><br><span class="line"><span class="comment"># 2.1 加载数据并指定缺失值标记</span></span><br><span class="line">data = pd.read_csv(</span><br><span class="line">    <span class="string">&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data&quot;</span>,</span><br><span class="line">    names=names,</span><br><span class="line">    na_values=<span class="string">&#x27;?&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.2 缺失值处理 - 删除包含缺失值的行</span></span><br><span class="line">data = data.dropna()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.3 确定特征值和目标值</span></span><br><span class="line">x = data.iloc[:, <span class="number">1</span>:<span class="number">10</span>]  <span class="comment"># 选取第2到10列作为特征</span></span><br><span class="line">y = data[<span class="string">&quot;Class&quot;</span>]       <span class="comment"># 选取Class列作为目标值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.4 分割数据集</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">22</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.特征工程(标准化)</span></span><br><span class="line">transfer = StandardScaler()</span><br><span class="line">x_train = transfer.fit_transform(x_train)</span><br><span class="line">x_test = transfer.transform(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.机器学习(逻辑回归)</span></span><br><span class="line">estimator = LogisticRegression(solver=<span class="string">&#x27;liblinear&#x27;</span>, penalty=<span class="string">&#x27;l2&#x27;</span>, C=<span class="number">1.0</span>)</span><br><span class="line">estimator.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.模型评估</span></span><br><span class="line"><span class="comment"># 5.1 预测测试集</span></span><br><span class="line">y_predict = estimator.predict(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.2 计算准确率</span></span><br><span class="line">accuracy = estimator.score(x_test, y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型准确率:&quot;</span>, accuracy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可选：输出预测结果和真实值的对比</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预测结果示例:&quot;</span>, y_predict[:<span class="number">10</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;真实值示例:&quot;</span>, y_test.values[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>



<h2 id="可以进行多分类吗？"><a href="#可以进行多分类吗？" class="headerlink" title="可以进行多分类吗？"></a>可以进行多分类吗？</h2><p>可以的，其实我们可以从二分类问题过度到多分类问题(one vs rest)，思路步骤如下：</p>
<p>1.将类型class1看作正样本，其他类型全部看作负样本，然后我们就可以得到样本标记类型为该类型的概率$p_1$。</p>
<p>2.然后再将另外类型class2看作正样本，其他类型全部看作负样本，同理得到$p_2$。</p>
<p>3.以此循环，我们可以得到该待预测样本的标记类型分别为类型class i时的概率$p_i$，最后我们取$p_i$中最大的那个概率对应的样本标记类型作为我们的待预测样本类型。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250616174858019.png" alt="image-20250616174858019"></p>
<h2 id="分类评估方法⭐️"><a href="#分类评估方法⭐️" class="headerlink" title="分类评估方法⭐️"></a>分类评估方法⭐️</h2><h3 id="混淆矩阵及其构建"><a href="#混淆矩阵及其构建" class="headerlink" title="混淆矩阵及其构建"></a>混淆矩阵及其构建</h3><blockquote>
<p>简单根据象限来记忆：TP、FN、FP、TN（从上到下、从左到右都是正、假）</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250609202123066.png" alt="image-20250609202123066"></p>
<p>混淆矩阵作用在测试集样本集中：</p>
<ol>
<li>真实值是 <strong>正例</strong> 的样本中，被分类为 <strong>正例</strong> 的样本数量有多少，这部分样本叫做真正例（TP，True Positive）</li>
<li>真实值是 <strong>正例</strong> 的样本中，被分类为 <strong>假例</strong> 的样本数量有多少，这部分样本叫做伪反例（FN，False Negative）</li>
<li>真实值是 <strong>假例</strong> 的样本中，被分类为 <strong>正例</strong> 的样本数量有多少，这部分样本叫做伪正例（FP，False Positive）</li>
<li>真实值是 <strong>假例</strong> 的样本中，被分类为 <strong>假例</strong> 的样本数量有多少，这部分样本叫做真反例（TN，True Negative）</li>
</ol>
<h3 id="模型预测正例假例举例："><a href="#模型预测正例假例举例：" class="headerlink" title="模型预测正例假例举例："></a><strong>模型预测正例假例举例：</strong></h3><p>样本集中有 6 个恶性肿瘤样本，4 个良性肿瘤样本，假设恶性肿瘤为正例，则：</p>
<p><strong>模型 A：</strong> 预测对了 3 个恶性肿瘤样本，4 个良性肿瘤样本</p>
<ol>
<li>真正例 TP 为：3 </li>
<li>伪反例 FN 为：3</li>
<li>假正例 FP 为：0</li>
<li>真反例 TN：4</li>
<li><strong>精准率：3&#x2F;(3+0) &#x3D; 100%</strong></li>
<li><strong>召回率：3&#x2F;(3+3)&#x3D;50%</strong></li>
<li><strong>F1-score：(2*3)&#x2F;(2*3+3+0)&#x3D;67%</strong></li>
</ol>
<p><strong>模型 B：</strong> 预测对了 6 个恶性肿瘤样本，1个良性肿瘤样本</p>
<ol>
<li>真正例 TP 为：6</li>
<li>伪反例 FN 为：0</li>
<li>假正例 FP 为：3</li>
<li>真反例 TN：1</li>
<li><strong>精准率：6&#x2F;(6+3) &#x3D; 67%</strong></li>
<li><strong>召回率：6&#x2F;(6+0)&#x3D; 100%</strong> </li>
<li><strong>F1-score：(2*6)&#x2F;(2*6+0+3)&#x3D;80%</strong></li>
</ol>
<p><code>TP+FN+FP+TN = 总样本数量</code></p>
<h3 id="Precision（精确率）"><a href="#Precision（精确率）" class="headerlink" title="Precision（精确率）"></a>Precision（精确率）</h3><blockquote>
<p>精确率也叫做查准率，指的是对正例样本的预测准确率。</p>
<p>比如：我们把恶性肿瘤当做正例样本，则我们就需要知道$模型对恶性肿瘤的预测准确率。</p>
</blockquote>
<p>公式：<br>$$<br> P &#x3D; \frac{TP}{TP + FP}<br>$$</p>
<h3 id="Recall（召回率）"><a href="#Recall（召回率）" class="headerlink" title="Recall（召回率）"></a>Recall（召回率）</h3><blockquote>
<p>召回率也叫做查全率，指的是预测为真正例样本占所有真实正例样本的比重。</p>
<p>例如：我们把恶性肿瘤当做正例样本，则我们想知道<code>模型是否能把所有的恶性肿瘤患者都预测出来</code>。</p>
</blockquote>
<p>公式：<br>$$<br> P &#x3D; \frac{TP}{TP + FN}<br>$$</p>
<h3 id="F1-score"><a href="#F1-score" class="headerlink" title="F1-score"></a>F1-score</h3><blockquote>
<p>如果我们对模型的精度、召回率都有要求，希望知道模型在这两个评估方向的综合预测能力如何？则可以使用 F1-score 指标。</p>
</blockquote>
<p>$$<br> F1 &#x3D; \frac{2TP}{2TP + FN + FP} &#x3D; \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}<br>$$</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建混淆矩阵</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, precision_score, recall_score, f1_score</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建数据:真实值,预测值</span></span><br><span class="line">y_true = [<span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;良性&#x27;</span>, <span class="string">&#x27;良性&#x27;</span>, <span class="string">&#x27;良性&#x27;</span>, <span class="string">&#x27;良性&#x27;</span>]</span><br><span class="line"><span class="comment"># 模型A的预测值</span></span><br><span class="line">y_pre_A = [<span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;良性&#x27;</span>, <span class="string">&#x27;良性&#x27;</span>, <span class="string">&#x27;良性&#x27;</span>, <span class="string">&#x27;良性&#x27;</span>, <span class="string">&#x27;良性&#x27;</span>, <span class="string">&#x27;良性&#x27;</span>, <span class="string">&#x27;良性&#x27;</span>]</span><br><span class="line"><span class="comment"># 模型B的预测值</span></span><br><span class="line">y_pre_B = [<span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;良性&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 混淆矩阵</span></span><br><span class="line"><span class="comment"># 传入真实值和预测值以及标签（正负样本）</span></span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(y_true, y_pre_A, labels=[<span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;良性&#x27;</span>]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(y_true, y_pre_B, labels=[<span class="string">&#x27;恶性&#x27;</span>, <span class="string">&#x27;良性&#x27;</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 精确率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-----------------精确率-------------------------------&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(precision_score(y_true, y_pre_A, pos_label=<span class="string">&#x27;恶性&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(precision_score(y_true, y_pre_B, pos_label=<span class="string">&#x27;恶性&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 召回率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-----------------召回率-------------------------------&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(recall_score(y_true, y_pre_A, pos_label=<span class="string">&#x27;恶性&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(recall_score(y_true, y_pre_B, pos_label=<span class="string">&#x27;恶性&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-----------------f1-score-------------------------------&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(f1_score(y_true, y_pre_A, pos_label=<span class="string">&#x27;恶性&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(f1_score(y_true, y_pre_B, pos_label=<span class="string">&#x27;恶性&#x27;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="ROC曲线和AUC指标"><a href="#ROC曲线和AUC指标" class="headerlink" title="ROC曲线和AUC指标"></a>ROC曲线和AUC指标</h3><blockquote>
<p>TPR （True Positive Rate）：正样本中被预测为正样本的概率 (召回率)</p>
<p>FPR （False Positive Rate）：负样本中被预测为正样本的概率</p>
<p><code>AUC 是 ROC 曲线下面的面积，该值越大，则模型的辨别能力就越强</code></p>
<p><code>AUC 值主要评估模型对正例样本、负例样本的辨别能力</code></p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250711213215389.png" alt="image-20250711213215389"></p>
<p><code>TPR越大越好；FPR越小越好</code></p>
<p><code>ROC取下越往左上，越好，线下面积越大。</code></p>
<h2 id="电信客户流失预测"><a href="#电信客户流失预测" class="headerlink" title="电信客户流失预测"></a>电信客户流失预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入必要的库</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV, StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"><span class="keyword">import</span> sklearn.metrics <span class="keyword">as</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置绘图风格</span></span><br><span class="line">%matplotlib inline</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&quot;whitegrid&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修复中文显示异常Mac</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;Arial Unicode MS&#x27;</span>]</span><br><span class="line"><span class="comment"># 如果是Mac本, 不支持SimHei的时候, 可以修改为 &#x27;Microsoft YaHei&#x27; 或者 &#x27;Arial Unicode MS&#x27;\n</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 数据加载与初步探索</span></span><br><span class="line">churn = pd.read_csv(<span class="string">&#x27;data/churn.csv&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据形状:&quot;</span>, churn.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n数据信息:&quot;</span>)</span><br><span class="line">churn.info()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n前5行数据:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(churn.head())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 数据预处理</span></span><br><span class="line"><span class="comment"># 将类别型变量进行one-hot编码</span></span><br><span class="line">churn = pd.get_dummies(churn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据整理</span></span><br><span class="line">churn.drop([<span class="string">&#x27;Churn_No&#x27;</span>, <span class="string">&#x27;gender_Male&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统一列名为小写</span></span><br><span class="line">churn.columns = churn.columns.<span class="built_in">str</span>.lower()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重命名目标变量</span></span><br><span class="line">churn = churn.rename(columns=&#123;<span class="string">&#x27;churn_yes&#x27;</span>: <span class="string">&#x27;flag&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看目标变量分布</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n目标变量分布:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(churn.flag.value_counts())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n目标变量比例:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(churn.flag.value_counts(normalize=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 数据分析</span></span><br><span class="line"><span class="comment"># 按标签分组查看均值</span></span><br><span class="line">summary = churn.groupby(<span class="string">&#x27;flag&#x27;</span>).mean()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n按流失分组后的均值:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(summary)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化分析</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">sns.countplot(y=<span class="string">&#x27;contract_month&#x27;</span>, hue=<span class="string">&#x27;flag&#x27;</span>, data=churn)</span><br><span class="line">plt.title(<span class="string">&#x27;按月合约客户的流失情况&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 特征选择与建模</span></span><br><span class="line"><span class="comment"># 选择特征和目标变量</span></span><br><span class="line">y = churn[<span class="string">&#x27;flag&#x27;</span>]</span><br><span class="line">x = churn[[<span class="string">&#x27;contract_month&#x27;</span>, <span class="string">&#x27;internet_other&#x27;</span>, <span class="string">&#x27;streamingtv&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(</span><br><span class="line">    x, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.1 基础逻辑回归模型</span></span><br><span class="line">lr = linear_model.LogisticRegression()</span><br><span class="line">lr.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型评估</span></span><br><span class="line">y_pred_train = lr.predict(x_train)</span><br><span class="line">y_pred_test = lr.predict(x_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n基础模型训练集准确率:&quot;</span>, metrics.accuracy_score(y_train, y_pred_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;基础模型测试集准确率:&quot;</span>, metrics.accuracy_score(y_test, y_pred_test))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;基础模型AUC分数:&quot;</span>, roc_auc_score(y_test, y_pred_test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.2 考虑类别不平衡的模型</span></span><br><span class="line">lr_balanced = linear_model.LogisticRegression(class_weight=<span class="string">&#x27;balanced&#x27;</span>)</span><br><span class="line">lr_balanced.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">y_pred_balanced = lr_balanced.predict(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n平衡模型测试集准确率:&quot;</span>, metrics.accuracy_score(y_test, y_pred_balanced))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;平衡模型AUC分数:&quot;</span>, roc_auc_score(y_test, y_pred_balanced))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.3 网格搜索优化参数</span></span><br><span class="line">kfold = StratifiedKFold(n_splits=<span class="number">5</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">lr = linear_model.LogisticRegression()</span><br><span class="line"></span><br><span class="line">param_grid = &#123;</span><br><span class="line">    <span class="string">&#x27;solver&#x27;</span>: [<span class="string">&#x27;newton-cg&#x27;</span>, <span class="string">&#x27;lbfgs&#x27;</span>, <span class="string">&#x27;liblinear&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;C&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>],</span><br><span class="line">    <span class="string">&#x27;class_weight&#x27;</span>: [<span class="string">&#x27;balanced&#x27;</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">search = GridSearchCV(lr, param_grid, cv=kfold)</span><br><span class="line">best_lr = search.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n最佳参数组合:&quot;</span>, best_lr.best_params_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用最佳模型</span></span><br><span class="line">y_pred_best = best_lr.predict(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n优化模型测试集准确率:&quot;</span>, metrics.accuracy_score(y_test, y_pred_best))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;优化模型AUC分数:&quot;</span>, roc_auc_score(y_test, y_pred_best))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看模型系数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n模型系数:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;contract_month:&quot;</span>, best_lr.best_estimator_.coef_[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;internet_other:&quot;</span>, best_lr.best_estimator_.coef_[<span class="number">0</span>][<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;streamingtv:&quot;</span>, best_lr.best_estimator_.coef_[<span class="number">0</span>][<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 模型评估报告</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n分类报告:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(metrics.classification_report(y_test, y_pred_best))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制ROC曲线</span></span><br><span class="line">y_pred_prob = best_lr.predict_proba(x_test)[:, <span class="number">1</span>]</span><br><span class="line">fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(fpr, tpr, label=<span class="string">&#x27;ROC曲线 (AUC = %0.2f)&#x27;</span> % roc_auc_score(y_test, y_pred_best))</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">&#x27;k--&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;假阳性率&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;真阳性率&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;ROC曲线&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;lower right&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><p>参考链接：<a href="https://liamjohnson-w.github.io/2023/08/09/2023.08.09/">https://liamjohnson-w.github.io/2023/08/09/2023.08.09/</a></p>
<p><code>决策树</code>算法是一种监督学习算法，英文是Decision tree。</p>
<p><code>决策树</code>是一个类似于流程图的树结构：</p>
<ul>
<li>其中，<code>每个内部结点表示一个特征</code>或属性。</li>
<li>而<code>每个树叶结点代表一个标签(分类)</code>。</li>
<li>树的最顶层是根结点。<code>使用决策树分类时就是将实例分配到叶节点的类中。该叶节点所属的类就是该节点的标签(分类)</code>。</li>
</ul>
<p><code>决策树</code>思想的来源非常朴素，试想每个人的大脑都有类似于if-else这样的逻辑判断，这其中的if表示的是条件，if之后的then就是一种选择或决策。程序设计中的条件分支结构就是if-then结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法。</p>
<h2 id="构建决策树"><a href="#构建决策树" class="headerlink" title="构建决策树"></a>构建决策树</h2><blockquote>
<p>构建决策树包括三个步骤：</p>
<ul>
<li>特征选择：选取有较强分类能力的特征。</li>
<li>决策树生成：根据选择的特征生成决策树。典型的算法有ID3、C4.5、CART，它们生成决策树过程相似，ID3是采用<code>信息增益</code>作为特征选择度量，而C4.5采用<code>信息增益率</code>、CART<code>基尼指数</code>。</li>
<li>决策树剪枝：决策树也易过拟合，采用剪枝的方法缓解过拟合。剪枝原因是决策树生成算法生成的树对训练数据的预测很准确，但是对于未知数据分类很差，这就产生了<code>过拟合</code>的现象。</li>
</ul>
</blockquote>
<table>
<thead>
<tr>
<th><strong>名称</strong></th>
<th><strong>提出时间</strong></th>
<th><strong>分支方式</strong></th>
<th><strong>特点</strong></th>
</tr>
</thead>
<tbody><tr>
<td>ID3</td>
<td>1975</td>
<td>信息增益</td>
<td>1.ID3只能对离散属性的数据集构成决策树  2.倾向于选择取值较多的属性</td>
</tr>
<tr>
<td>C4.5</td>
<td>1993</td>
<td>信息增益率</td>
<td>1.缓解了ID3分支过程中总喜欢偏向选择值较多的属性  2.可处理连续数值型属性，也增加了对缺失值的处理方法  3.只适合于能够驻留于内存的数据集,大数据集无能为力</td>
</tr>
<tr>
<td>CART</td>
<td>1984</td>
<td>基尼指数</td>
<td>1.可以进行分类和回归，可处理离散属性，也可以处理连续属性  2.采用基尼指数，计算量减小  3.一定是二叉树</td>
</tr>
</tbody></table>
<h3 id="Notice"><a href="#Notice" class="headerlink" title="Notice"></a>Notice</h3><p><code>信息增益（ID3）、信息增益率值越大（C4.5），则说明优先选择该特征。</code></p>
<p><code>基尼指数值越小（cart），则说明优先选择该特征。</code></p>
<h2 id="ID3算法构建决策树⭐️"><a href="#ID3算法构建决策树⭐️" class="headerlink" title="ID3算法构建决策树⭐️"></a>ID3算法构建决策树⭐️</h2><blockquote>
<p>ID3 树是基于<code>信息增益</code>构建的决策树.</p>
<p><code>使用信息增益最大的特征作为决策树的一个分裂节点。</code></p>
</blockquote>
<h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><p>ID3 树是基于信息增益构建的决策树.</p>
<p>定义</p>
<ul>
<li>熵在信息论中代表随机变量不确定度的度量。</li>
<li>熵越大，数据的不确定性度越高 </li>
<li>熵越小，数据的不确定性越低</li>
</ul>
<p>公式</p>
<p>$$<br>\large<br>H &#x3D; -\sum_{i&#x3D;1}^{k}p_i\log_{b}(p_i)<br>$$</p>
<p><strong>参数说明</strong></p>
<ul>
<li>$b&#x3D;2$（默认）：单位为 比特（bits）（信息论常用）。</li>
<li>$b&#x3D;e$（自然对数）：单位为 纳特（nats）。</li>
<li>$b&#x3D;10$：单位为 哈特莱（hartleys）。</li>
</ul>
<p>例子1：假如有三个类别，分别占比为：{1&#x2F;3,1&#x2F;3,1&#x2F;3}，信息熵计算结果为：</p>
<p>$H&#x3D;-\frac{1}{3}\log_{2}(\frac{1}{3})-\frac{1}{3}\log_{2}(\frac{1}{3})-\frac{1}{3}\log_{2}(\frac{1}{3})&#x3D;1.0986$</p>
<p>例子2：假如有三个类别，分别占比为：{1&#x2F;10,2&#x2F;10,7&#x2F;10}，信息熵计算结果为：</p>
<p>$H&#x3D;-\frac{1}{10}\log_{2}(\frac{1}{10})-\frac{2}{10}\log_{2}(\frac{2}{10})-\frac{7}{10}\log_{2}(\frac{7}{10})&#x3D;0.8018$</p>
<p>熵越大，表示整个系统不确定性越大，越随机，反之确定性越强。</p>
<p>例子3：假如有三个类别，分别占比为：{1,0,0}，信息熵计算结果为：</p>
<p>$H&#x3D;-1\log_{2}(1)&#x3D;0$</p>
<h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>特征$A$对 数据集D的信息增益$g(D,A)$，定义为集合$D$的熵$H(D)$与特征A给定条件下D的熵$H(D|A)$之差。即</p>
<p>$$<br>\large<br>g(D,A)&#x3D;H(D)-H(D|A)<br>$$</p>
<p>根据信息增益选择特征方法是：对训练数据集D，计算其每个特征的信息增益，并比较它们的大小，并选择信息增益最大的特征进行划分。<strong>表示由于特征$A$而使得对数据D的分类不确定性减少的程度。</strong></p>
<p>算法：</p>
<p>设训练数据集为D，$\mid D\mid$表示其样本个数。设有$K$个类$C_k$，$k&#x3D;1,2,\cdots,K$，$\mid C_k\mid$为属于类$C_k$的样本个数，$\sum\limits_{k&#x3D;1}^{K}&#x3D;\mid{D}\mid$。设特征A有$n$个不同取值${a_1, a_2, \cdots,a_n}$，根据特征A的取值将D划分为$n$个子集$D_1, D_2, \cdots,D_n$，$\mid D_i\mid$为$D_i$样本个数，$\sum\limits_{i&#x3D;1}^n\mid D_i\mid&#x3D;\mid D\mid$。子集中属于类$C_k$的样本集合为$D_{ik}$，即$D_{ik}&#x3D;D_i\bigcap C_k$，$\mid D_{ik}\mid$为$D_{ik}$的样本个数。信息增益算法如下：</p>
<ul>
<li><p>输入：训练数据集$D$和特征$A$；</p>
</li>
<li><p>输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$</p>
</li>
<li><p>(1) 计算数据集$D$的经验熵$H(D)$</p>
<p>$H(D)&#x3D;-\sum\limits_{k&#x3D;1}^{K}\frac{\mid C_k\mid}{\mid D\mid}\log_2\frac{\mid C_k\mid}{\mid D\mid}$</p>
</li>
<li><p>(2) 计算特征$A$对数据集$D$的经验条件熵$H(D\mid A)$</p>
<p>$H(D\mid A)&#x3D;\sum\limits_{i&#x3D;1}^{n}\frac{\mid D_i\mid}{\mid D\mid}H(D_i)&#x3D;-\sum\limits_{i&#x3D;1}^{n}\frac{\mid D_i\mid}{\mid D\mid}\sum_\limits{k&#x3D;1}^{K}\frac{\mid D_{ik}\mid}{\mid D_i\mid}\log_2\frac{\mid D_{ik}\mid}{\mid D_i\mid}$</p>
</li>
<li><p>(3) 计算信息增益</p>
<p>$g(D,A)&#x3D;H(D)-H(D|A)$</p>
</li>
</ul>
<h3 id="信息增益计算案例"><a href="#信息增益计算案例" class="headerlink" title="信息增益计算案例"></a>信息增益计算案例</h3><p>例子：已知6个样本，根据特征a：</p>
<table>
<thead>
<tr>
<th>特征a</th>
<th>目标值</th>
</tr>
</thead>
<tbody><tr>
<td>α</td>
<td>A</td>
</tr>
<tr>
<td>α</td>
<td>A</td>
</tr>
<tr>
<td>β</td>
<td>B</td>
</tr>
<tr>
<td>α</td>
<td>A</td>
</tr>
<tr>
<td>β</td>
<td>B</td>
</tr>
<tr>
<td>α</td>
<td>B</td>
</tr>
</tbody></table>
<p>$α$ 部分对应的目标值为： $AAAB$</p>
<p>$β $部分对应的目标值为：$BB$</p>
<ul>
<li>条件为$α$熵：$-\frac{3}{4} * log_{2}(\frac{3}{4}) - \frac{1}{4} * log_{2}(\frac{1}{4}) &#x3D; 0.81 $</li>
<li>条件为$β$熵：$ -(\frac{2}{2} * log_{2}(\frac{2}{2})) &#x3D; 0$</li>
<li>条件熵：$α $部分占了 $\frac{4}{6}$，$β$ 部分 占了 $\frac{2}{6}$<ul>
<li>$(\frac{4}{6}) * 0.81 + (\frac{2}{6}）* 0 &#x3D; 0.54$</li>
</ul>
</li>
<li>熵：$-\frac{3}{6} * log_{2}(\frac{3}{6}) – \frac{3}{6} * log_{2}(\frac{3}{6}) &#x3D; 1$</li>
<li>信息增益：熵 – 条件熵： $1.0 – 0.54 &#x3D; 0.46$</li>
</ul>
<h3 id="ID3树构建流程"><a href="#ID3树构建流程" class="headerlink" title="ID3树构建流程"></a>ID3树构建流程</h3><p><strong>构建流程：</strong></p>
<ol>
<li>计算每个特征的信息增益</li>
<li>使用信息增益最大的特征将数据集 $S $拆分为子集</li>
<li>使用该特征（信息增益最大的特征）作为决策树的一个节点</li>
<li>使用剩余特征对子集重复上述过程</li>
</ol>
<p><strong>案例：</strong></p>
<p>已知：某一个论坛客户流失率数据</p>
<p>需求：考察性别、活跃度特征哪一个特征对流失率的影响更大</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610200825130.png" alt="image-20250610200825130"></p>
<p>分析：</p>
<p>15条样本：5正样本、10个负样本</p>
<ul>
<li>计算熵</li>
</ul>
<p>$$ H(D) &#x3D; \left(-\frac{5}{15}\log_{2}\frac{5}{15}\right) + \left(-\frac{10}{15}\log_{2}\frac{10}{15}\right) &#x3D; 0.9812 $$</p>
<ul>
<li>计算性别条件熵(a&#x3D;”性别”)：</li>
</ul>
<p>$$<br>\begin{aligned}<br>H(D, \text{性别}) &amp;&#x3D; \sum_{v&#x3D;1}^{n} \frac{D^{v}}{D} H(D^{v})<br>\end{aligned}<br>$$</p>
<p>$$&#x3D; \left(\frac{8}{15}\right)\left(-\frac{3}{8} \log_{2} \frac{3}{8} - \frac{5}{8} \log_{2} \frac{5}{8}\right) + \left(\frac{7}{15}\right)\left(-\frac{2}{7} \log_{2} \frac{2}{7} - \frac{5}{7} \log_{2} \frac{5}{7}\right)$$</p>
<ul>
<li>计算性别信息增益(a&#x3D;”性别”)</li>
</ul>
<p>$$<br>\begin{aligned}<br>g(D, \alpha) &amp;&#x3D; H(D) - H(D \mid \alpha) \<br>&amp;&#x3D; 0.9812 - \left[<br>    \left(\frac{8}{15}\right)\left(-\frac{3}{8}\log_{2}\frac{3}{8} - \frac{5}{8}\log_{2}\frac{5}{8}\right)<br>    + \left(\frac{7}{15}\right)\left(-\frac{2}{7}\log_{2}\frac{2}{7} - \frac{5}{7}\log_{2}\frac{5}{7}\right)<br>\right] \<br>&amp;&#x3D; 0.0064<br>\end{aligned}<br>$$</p>
<ul>
<li>计算活跃度条件熵(a&#x3D;“活跃度”)</li>
</ul>
<p>$$<br>\begin{aligned}<br>H(D, \text{活跃度}) &amp;&#x3D; \sum_{v&#x3D;1}^{n} \frac{D^{v}}{D} H(D^{v}) \<br>&amp;&#x3D; \left(\frac{6}{15}\right)(0) + \left(\frac{5}{15}\right)\left(-\frac{1}{5}\log_{2}\frac{1}{5} - \frac{4}{5}\log_{2}\frac{4}{5}\right) + \left(\frac{4}{15}\right)(0)<br>\end{aligned}<br>$$</p>
<ul>
<li>计算活跃度信息增益(a&#x3D;活跃度”)</li>
</ul>
<p>$$<br>\begin{aligned}<br>g(D, \alpha) &amp;&#x3D; H(D) - H(D \mid \alpha) \<br>&amp;&#x3D; 0.9812 - \left[<br>    \left(\frac{6}{15}\right)(0) +<br>    \left(\frac{5}{15}\right)\left(-\frac{1}{5}\log_{2}\frac{1}{5} - \frac{4}{5}\log_{2}\frac{4}{5}\right) +<br>    \left(\frac{4}{15}\right)(0)<br>\right] \<br>&amp;&#x3D; 0.6776<br>\end{aligned}<br>$$</p>
<p><strong>结论</strong>：活跃度的信息增益比性别的信息增益大，对用户流失的影响比性别大。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250713083502968.png" alt="image-20250713083502968"></p>
<h2 id="C4-5算法构建决策树"><a href="#C4-5算法构建决策树" class="headerlink" title="C4.5算法构建决策树"></a>C4.5算法构建决策树</h2><h3 id="信息增益率"><a href="#信息增益率" class="headerlink" title="信息增益率"></a>信息增益率</h3><blockquote>
<p><code>根据信息增益率，选择特征的信息增益率大作为分裂特征。</code></p>
</blockquote>
<p>$$<br>\begin{aligned}<br>\text{Gain_Ratio}(D, a) &amp;&#x3D; \frac{\text{Gain}(D, a)}{IV(a)} \<br>\end{aligned}<br>$$</p>
<p>$$<br>\begin{aligned} IV(a) &amp;&#x3D; -\sum_{v&#x3D;1}^{V} \frac{D^{v}}{D} \log \frac{D^{v}}{D} \end{aligned}<br>$$</p>
<ol>
<li>Gain_Ratio 表示信息增益率</li>
<li>IV 表示分裂信息、特征熵</li>
<li>信息增益率&#x3D;特征的信息增益 ➗ 特征熵</li>
</ol>
<p>信息增益比本质： 是在信息增益的基础之上乘上一个<code>惩罚参数。</code></p>
<p><code>特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大</code>。</p>
<p>惩罚参数：数据集D以特征A作为随机变量的熵的倒数。</p>
<h3 id="C4-5决策树构建流程"><a href="#C4-5决策树构建流程" class="headerlink" title="C4.5决策树构建流程"></a>C4.5决策树构建流程</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610202930336.png" alt="image-20250610202930336"></p>
<p>特征a的信息增益率：</p>
<ol>
<li><strong>信息增益</strong>：</li>
</ol>
<p>$$<br>\begin{aligned}<br>&amp;\left(-\frac{3}{6}\log_{2}\frac{3}{6} - \frac{3}{6}\log_{2}\frac{3}{6}\right) \<br>&amp;- \left[ \frac{4}{6}\left(-\frac{3}{4}\log_{2}\frac{3}{4} - \frac{1}{4}\log_{2}\frac{1}{4}\right) + \frac{2}{6}(0) \right] \<br>&amp;&#x3D; 1 - 0.54 &#x3D; 0.46<br>\end{aligned}<br>$$</p>
<ol start="2">
<li><strong>IV信息熵</strong>：</li>
</ol>
<p>$$<br>-\frac{4}{6}\log_{2}\frac{4}{6} - \frac{2}{6}\log_{2}\frac{2}{6} &#x3D; 0.92<br>$$</p>
<ol start="3">
<li><strong>信息增益率</strong>：</li>
</ol>
<p>$$<br>\frac{0.46}{0.92} &#x3D; 0.5<br>$$</p>
<p>特征b的信息增益率：</p>
<ol>
<li><strong>信息增益</strong>：</li>
</ol>
<p>$$<br>-\frac{3}{6}\log_{2}\frac{3}{6} - \frac{3}{6}\log_{2}\frac{3}{6} - 6 \times 0 &#x3D; 1<br>$$</p>
<ol start="2">
<li><strong>IV信息熵</strong>：</li>
</ol>
<p>$$<br>-\frac{1}{6}\log_{2}\frac{1}{6} \times 6 &#x3D; 2.58<br>$$</p>
<ol start="3">
<li><strong>信息增益率</strong>：</li>
</ol>
<p>$$<br>\frac{1}{2.58} &#x3D; 0.39<br>$$</p>
<p>结论：特征a的信息增益率大于特征b的信息增益率，根据信息增益率，应该选择特征a作为分裂特征</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250713150757361.png" alt="image-20250713150757361"></p>
<h2 id="CART算法构建决策树⭐️"><a href="#CART算法构建决策树⭐️" class="headerlink" title="CART算法构建决策树⭐️"></a>CART算法构建决策树⭐️</h2><h3 id="Cart树简介"><a href="#Cart树简介" class="headerlink" title="Cart树简介"></a>Cart树简介</h3><blockquote>
<p>Cart模型是一种决策树模型，它即可以用于分类，也可以用于回归。</p>
<p>分类和回归树模型采用不同的最优化策略。Cart回归树使用平方误差最小化策略，<code>Cart分类生成树采用的基尼指数最小化策略。</code></p>
</blockquote>
<h3 id="基尼指数计算案例"><a href="#基尼指数计算案例" class="headerlink" title="基尼指数计算案例"></a>基尼指数计算案例</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610203514367.png" alt="image-20250610203514367"></p>
<h4 id="是否有房"><a href="#是否有房" class="headerlink" title="是否有房"></a>是否有房</h4><p>计算过程如下：根据是否有房将目标值划分为两部分：</p>
<ol>
<li><p>计算有房子的基尼值： <strong>有房子有 1、4、7 共计三个样本，对应：3个no、0个yes</strong></p>
<p>$G i n i(\text {是否有房，yes })&#x3D;1-\left(\frac{0}{3}\right)^{2}-\left(\frac{3}{3}\right)^{2}&#x3D;0$</p>
</li>
<li><p>计算无房子的基尼值：<strong>无房子有 2、3、5、6、8、9、10 共七个样本，对应：4个no、3个yes</strong></p>
<p>$\operatorname{Gini}(\text {是否有房，no })&#x3D;1-\left(\frac{3}{7}\right)^{2}-\left(\frac{4}{7}\right)^{2}&#x3D;0.4898$</p>
</li>
<li><p>计算基尼指数：<strong>第一部分样本数量占了总样本的 3&#x2F;10、第二部分样本数量占了总样本的 7&#x2F;10：</strong></p>
<p>$\operatorname{Gini_{-}} i n \operatorname{dex}(D, \text { 是否有房 })&#x3D;\frac{7}{10} * 0.4898+\frac{3}{10} * 0&#x3D;0.343$</p>
</li>
</ol>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250713083702157.png" alt="image-20250713083702157"></p>
<h4 id="婚姻状况"><a href="#婚姻状况" class="headerlink" title="婚姻状况"></a>婚姻状况</h4><ol>
<li><p>计算  <strong>{married} 和 {single,divorced}</strong> 情况下的基尼指数：</p>
<p>结婚的基尼值，有 2、4、6、9 共 4 个样本，并且对应目标值全部为 no：</p>
<p>$\operatorname{Gini_index}(D,\text)&#x3D;0$</p>
<p>不结婚的基尼值，有 1、3、5、7、8、10 共 6 个样本，并且对应 3 个 no，3 个 yes：</p>
<p>$\operatorname{Gini_index}(D, \text { {single,divorced} })&#x3D;1-\left(\frac{3}{6}\right)^{2}-\left(\frac{3}{6}\right)^{2}&#x3D;0.5$</p>
<p>以 married 作为分裂点的基尼指数：</p>
<p>$\operatorname{Gini_index}(D, \text { married })&#x3D;\frac{4}{10} * 0+\frac{6}{10} *\left[1-\left(\frac{3}{6}\right)^{2}-\left(\frac{3}{6}\right)^{2}\right]&#x3D;0.3$</p>
</li>
<li><p>计算  <strong>{single} | {married,divorced}</strong> 情况下的基尼指数</p>
<p>$\operatorname{Gini_index}(D,\text{婚姻状况})&#x3D;\frac{4}{10} * 0.5+\frac{6}{10} *\left[1-\left(\frac{1}{6}\right)^{2}-\left(\frac{5}{6}\right)^{2}\right]&#x3D;0.367$</p>
</li>
<li><p>计算  <strong>{divorced} | {single,married}</strong> 情况下基尼指数</p>
<p>$\operatorname{Gini_index}(D, \text { 婚姻状况 })&#x3D;\frac{2}{10} * 0.5+\frac{8}{10} *\left[1-\left(\frac{2}{8}\right)^{2}-\left(\frac{6}{8}\right)^{2}\right]&#x3D;0.4$</p>
</li>
<li><p>最终：该特征的基尼值为 0.3，并且预选分裂点为：<strong>{married} 和 {single,divorced}</strong></p>
</li>
</ol>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250713083647358.png" alt="image-20250713083647358"></p>
<h4 id="年收入"><a href="#年收入" class="headerlink" title="年收入"></a>年收入</h4><p>先将数值型属性升序排列，以相邻中间值作为待确定分裂点：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20230905153041427.png" alt="image-20230905153041427"></p>
<p>以年收入 65 将样本分为两部分，计算基尼指数:</p>
<p>$节点为65时:{年收入}&#x3D;\frac{1}{10} * 0 + \frac{9}{10} *\left[1-\left(\frac{6}{9}\right)^{2}-\left(\frac{3}{9}\right)^{2}\right]&#x3D;0.4$</p>
<p>以此类推计算所有分割点的基尼指数，我们发现最小的基尼指数为 0.3。</p>
<p>此时，我们发现：</p>
<ol>
<li>以是否有房作为分裂点的基尼指数为：0.343</li>
<li>以婚姻状况为分裂特征、以 married 作为分裂点的基尼指数为：0.3</li>
<li>以年收入作为分裂特征、以 97.5 作为分裂点的的基尼指数为：0.3</li>
</ol>
<p>最小基尼指数有两个分裂点，我们随机选择一个即可，假设婚姻状况，则可确定决策树如下：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610203859367.png" alt="image-20250610203859367"></p>
<h3 id="Cart案例（泰坦尼克号生存案例"><a href="#Cart案例（泰坦尼克号生存案例" class="headerlink" title="Cart案例（泰坦尼克号生存案例"></a>Cart案例（泰坦尼克号生存案例</h3><p>API</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">sklearn</span>.tree.DecisionTreeClassifier(criterion=’gini’,max_depth=<span class="literal">None</span>, random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>



<ul>
<li>criterion<ul>
<li>特征选择标准</li>
<li>“gini”或者”entropy”，前者代表基尼系数，后者代表信息增益。一默认”gini”，即CART算法。</li>
</ul>
</li>
<li>min_samples_split<ul>
<li>内部节点再划分所需最小样本数</li>
<li>这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。我之前的一个项目例子，有大概10万样本，建立决策树时，我选择了min_samples_split&#x3D;10。可以作为参考。</li>
</ul>
</li>
<li>min_samples_leaf<ul>
<li>叶子节点最少样本数</li>
<li>这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。之前的10万样本项目使用min_samples_leaf的值为5，仅供参考。</li>
</ul>
</li>
<li>max_depth<ul>
<li>决策树最大深度</li>
<li>决策树的最大深度，默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间</li>
</ul>
</li>
<li>random_state<ul>
<li>随机数种子</li>
</ul>
</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250713200345623.png" alt="image-20250713200345623"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier, DecisionTreeRegressor, BaseDecisionTree, plot_tree</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler, StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;Arial Unicode MS&#x27;</span>]</span><br><span class="line"><span class="comment"># 如果是Mac本, 不支持SimHei的时候, 可以修改为 &#x27;Microsoft YaHei&#x27; 或者 &#x27;Arial Unicode MS&#x27;\n</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;./train.csv&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="comment"># 查看数据情况，发现有空值 于是需要用平均值填充</span></span><br><span class="line">df[<span class="string">&#x27;Age&#x27;</span>].fillna(df[<span class="string">&#x27;Age&#x27;</span>].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">x = df[[<span class="string">&quot;Pclass&quot;</span>, <span class="string">&quot;Age&quot;</span>, <span class="string">&quot;Sex&quot;</span>]]</span><br><span class="line">x = pd.get_dummies(x).rename(columns=&#123;<span class="string">&#x27;Sex_female&#x27;</span>:<span class="string">&#x27;Sex&#x27;</span>&#125;)[[<span class="string">&quot;Pclass&quot;</span>, <span class="string">&quot;Age&quot;</span>, <span class="string">&quot;Sex&quot;</span>]]</span><br><span class="line">y = df[[<span class="string">&#x27;Survived&#x27;</span>]]</span><br><span class="line"><span class="built_in">print</span>(x.head(<span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(y.head(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 有了x，y就可以划分训练集和测试集了</span></span><br><span class="line">x_train, x_test, y_train, y_test= train_test_split(x, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">12</span>)</span><br><span class="line">es_model = DecisionTreeClassifier(criterion=<span class="string">&#x27;entropy&#x27;</span>, max_depth=<span class="number">10</span>)</span><br><span class="line">es_model.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">y_predict = es_model.predict(x_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;准确率：&quot;</span>, accuracy_score(y_test, y_predict))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 决策树可视化</span></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">50</span>, <span class="number">50</span>))</span><br><span class="line">plot_tree(es_model,</span><br><span class="line">          ax=ax,</span><br><span class="line">          max_depth=<span class="number">3</span>,</span><br><span class="line">          filled=<span class="literal">True</span>,</span><br><span class="line">          feature_names=[<span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Pclass_1&#x27;</span>, <span class="string">&#x27;Pclass_2&#x27;</span>, <span class="string">&#x27;Pclass_3&#x27;</span>, <span class="string">&#x27;Sex_female&#x27;</span>, <span class="string">&#x27;Sex_male&#x27;</span>],</span><br><span class="line">          class_names=[<span class="string">&#x27;生存&#x27;</span>, <span class="string">&#x27;不生存&#x27;</span>])</span><br><span class="line">plt.savefig(<span class="string">&#x27;b.png&#x27;</span>, dpi=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>



<h2 id="Cart回归决策树⭐️"><a href="#Cart回归决策树⭐️" class="headerlink" title="Cart回归决策树⭐️"></a>Cart回归决策树⭐️</h2><h3 id="回归决策树构建原理"><a href="#回归决策树构建原理" class="headerlink" title="回归决策树构建原理"></a>回归决策树构建原理</h3><p>CART 回归树和 CART 分类树的不同之处在于:</p>
<ol>
<li>CART 分类树预测<code>输出的是一个离散值</code>，CART 回归树预测<code>输出的是一个连续值</code>。</li>
<li><code>CART 分类树使用基尼指数作为划分、构建树的依据，CART 回归树使用平方损失</code>。</li>
<li>分类树使用<code>叶子节点里出现更多次数的类别</code>作为预测类别，回归树则采用<code>叶子节点里均值</code>作为预测输出</li>
</ol>
<h3 id="CART-回归树构建"><a href="#CART-回归树构建" class="headerlink" title="CART 回归树构建:"></a><strong>CART 回归树构建:</strong></h3><p>$$<br>\operatorname{Loss}(y, f(x))&#x3D;(f(x)-y)^{2}<br>$$</p>
<p><strong>例子：</strong></p>
<p>假设：数据集只有 1 个特征 x, 目标值值为 y，如下图所示：</p>
<table>
<thead>
<tr>
<th>x</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody><tr>
<td>y</td>
<td>5.56</td>
<td>5.7</td>
<td>5.91</td>
<td>6.4</td>
<td>6.8</td>
<td>7.05</td>
<td>8.9</td>
<td>8.7</td>
<td>9</td>
<td>9.05</td>
</tr>
</tbody></table>
<p>由于只有 1 个特征，所以只需要选择该特征的最优划分点，并不需要计算其他特征。</p>
<ol>
<li><p><strong>先将特征 x 的值排序，并取相邻元素均值作为待划分点，如下图所示：</strong></p>
<table>
<thead>
<tr>
<th>s</th>
<th>1.5</th>
<th>2.5</th>
<th>3.5</th>
<th>4.5</th>
<th>5.5</th>
<th>6.5</th>
<th>7.5</th>
<th>8.5</th>
<th>9.5</th>
</tr>
</thead>
</table>
</li>
<li><p><strong>计算每一个划分点的平方损失，例如：1.5 的平方损失计算过程为：</strong></p>
<p>R1 为 小于 1.5 的样本个数，样本数量为：1，其输出值为：5.56</p>
<p>$R_1 &#x3D;5.56$</p>
<p>R2 为 大于 1.5 的样本个数，样本数量为：9 ，其输出值为：</p>
<p>$R_2&#x3D;(5.7+5.91+6.4+6.8+7.05+8.9+8.7+9+9.05) &#x2F; 9&#x3D;7.50$</p>
<p>该划分点的平方损失：</p>
<p>$L(1.5)&#x3D;(5.56-5.56)^{2}+\left[(5.7-7.5)^{2}+(5.91-7.5)^{2}+\ldots+(9.05-7.5)^{2}\right]&#x3D;0+15.72&#x3D;15.72$</p>
</li>
<li><p><strong>以此方式计算 2.5、3.5… 等划分点的平方损失，结果如下所示：</strong></p>
<table>
<thead>
<tr>
<th>s</th>
<th>1.5</th>
<th>2.5</th>
<th>3.5</th>
<th>4.5</th>
<th>5.5</th>
<th>6.5</th>
<th>7.5</th>
<th>8.5</th>
<th>9.5</th>
</tr>
</thead>
<tbody><tr>
<td>m(s)</td>
<td>15.72</td>
<td>12.07</td>
<td>8.36</td>
<td>5.78</td>
<td>3.91</td>
<td><strong>1.93</strong></td>
<td>8.01</td>
<td>11.73</td>
<td>15.74</td>
</tr>
</tbody></table>
</li>
<li><p><strong>当划分点 s&#x3D;6.5 时，m(s) 最小。因此，第一个划分变量：特征为 X, 切分点为 6.5，即：j&#x3D;x,  s&#x3D;6.5</strong></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/cart1.png" alt="image-20220305183857165"></p>
</li>
<li><p><strong>对左子树的 6 个结点计算每个划分点的平方式损失，找出最优划分点：</strong></p>
<table>
<thead>
<tr>
<th>x</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody><tr>
<td>y</td>
<td>5.56</td>
<td>5.7</td>
<td>5.91</td>
<td>6.4</td>
<td>6.8</td>
<td>7.05</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>s</th>
<th>1.5</th>
<th>2.5</th>
<th>3.5</th>
<th>4.5</th>
<th>5.5</th>
</tr>
</thead>
<tbody><tr>
<td>c1</td>
<td>5.56</td>
<td>5.63</td>
<td>5.72</td>
<td>5.89</td>
<td>6.07</td>
</tr>
<tr>
<td>c2</td>
<td>6.37</td>
<td>6.54</td>
<td>6.75</td>
<td>6.93</td>
<td>7.05</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>s</th>
<th>1.5</th>
<th>2.5</th>
<th>3.5</th>
<th>4.5</th>
<th>5.5</th>
</tr>
</thead>
<tbody><tr>
<td>m(s)</td>
<td>1.3087</td>
<td>0.754</td>
<td>0.2771</td>
<td>0.4368</td>
<td>1.0644</td>
</tr>
</tbody></table>
</li>
<li><p><strong>s&#x3D;3.5时，m(s) 最小，所以左子树继续以 3.5 进行分裂:</strong></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/cart2.png"></p>
</li>
<li><p><strong>假设在生成3个区域</strong> 之后停止划分，以上就是回归树。每一个叶子结点的输出为：挂在该结点上的所有样本均值。</p>
</li>
</ol>
<table>
<thead>
<tr>
<th>x</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody><tr>
<td>y</td>
<td>5.56</td>
<td>5.7</td>
<td>5.91</td>
<td>6.4</td>
<td>6.8</td>
<td>7.05</td>
<td>8.9</td>
<td>8.7</td>
<td>9</td>
<td>9.05</td>
</tr>
</tbody></table>
<p>1号样本真实值  5.56 预测结果：5.72</p>
<p>2号样本真实值是 5.7 预测结果：5.72</p>
<p>3 号样本真实值是 5.91 预测结果 5.72</p>
<p>CART 回归树构建过程如下：</p>
<ol>
<li>选择第一个特征，将该特征的值进行排序，取相邻点计算均值作为待划分点</li>
<li>根据所有划分点，将数据集分成两部分：R1、R2</li>
<li>R1 和 R2 两部分的平方损失相加作为该切分点平方损失</li>
<li>取最小的平方损失的划分点，作为当前特征的划分点</li>
<li>以此计算其他特征的最优划分点、以及该划分点对应的损失值</li>
<li>在所有的特征的划分点中，选择出最小平方损失的划分点，作为当前树的分裂点</li>
</ol>
<h2 id="决策树剪枝⭐️"><a href="#决策树剪枝⭐️" class="headerlink" title="决策树剪枝⭐️"></a>决策树剪枝⭐️</h2><h3 id="什么是剪枝"><a href="#什么是剪枝" class="headerlink" title="什么是剪枝?"></a>什么是剪枝?</h3><blockquote>
<p>剪枝 (pruning)是决策树学习算法对付 <strong>过拟合</strong> 的主要手段。</p>
<p>在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，这时就可能因训练样本学得”太好”了，以致于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合。因此，可通过主动去掉一些分支来降低过拟合的风险。</p>
</blockquote>
<p>剪枝是指将一颗子树的子节点全部删掉，利用叶子节点替换子树(实质上是后剪枝技术)，也可以（假定当前对以root为根的子树进行剪枝）只保留根节点本身而删除所有的叶子，以下图为例：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610170109157.png" alt="image-20250610170109157"></p>
<h3 id="常见减枝方法"><a href="#常见减枝方法" class="headerlink" title="常见减枝方法"></a>常见减枝方法</h3><p>决策树剪枝的基本策略有”预剪枝” (pre-pruning）和”后剪枝”（post- pruning) 。</p>
<ol>
<li>预剪枝是指<code>在决策树生成过程中</code>，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点;</li>
<li>后剪枝则是先从训练集<code>生成一棵完整的决策树</code>，然后<code>自底向上地对非叶结点</code>进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。</li>
</ol>
<h3 id="剪枝方法对比"><a href="#剪枝方法对比" class="headerlink" title="剪枝方法对比"></a>剪枝方法对比</h3><p>预剪枝优点：</p>
<ul>
<li>预剪枝使决策树的很多分支没有展开，不单<code>降低了过拟合风险，还显著减少了决策树的训练、测试时间开销</code></li>
</ul>
<p>预剪枝缺点：</p>
<ul>
<li>有些分支的当前划分虽不能提升泛化性能，甚至会导致泛化性能降低，但<code>在其基础上进行的后续划分却有可能导致性能的显著提高</code></li>
<li>预剪枝决策树也带来了<code>欠拟合</code>的风险</li>
</ul>
<p>后剪枝优点：</p>
<ul>
<li>比预剪枝保留了更多的分支。一般情况下，后剪枝决策树的<code>欠拟合风险很小，泛化性能往往优于预剪枝</code></li>
</ul>
<p>后剪枝缺点：</p>
<ul>
<li>但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中所有非叶子节点进行逐一考察，因此在<code>训练时间开销比未剪枝的决策树和预剪枝的决策树都要大得多。</code></li>
</ul>
<h3 id="预剪枝-pre-pruning）举例"><a href="#预剪枝-pre-pruning）举例" class="headerlink" title="预剪枝 (pre-pruning）举例"></a>预剪枝 (pre-pruning）举例</h3><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/17.png" />

<ol>
<li><p>假设: 当前树只有一个结点, 即编号为1的结点. 此时, 所有的样本预测类别为: 其类别标记为训练样例数最多的类别，假设我们将这个叶结点标记为 “好瓜”。此时, 在验证集上所有的样本都会被预测为 “好瓜”, 此时的准确率为: 3&#x2F;7</p>
</li>
<li><p>如果进行此次分裂, 则树的深度为 2, 有三个分支. 在用属性”脐部”划分之后，上图中的结点2、3、4分别包含编号为 {1，2，3， 14}、 {6，7， 15， 17}、 {10， 16} 的训练样例，因此这 3 个结点分别被标记为叶结点”好瓜”、 “好瓜”、 “坏瓜”。此时, 在验证集上 4、5、8、11、12 样本预测正确，准确率为: 5&#x2F;7。很显然, 通过此次分裂准确率有所提升, 值得分裂.</p>
</li>
<li><p>接下来，对结点2进行划分，基于信息增益准则将挑选出划分属性”色泽”。然而，在使用”色泽”划分后，编号为 {5} 的验证集样本分类结果会由正确转为错误，使得验证集精度下降为 57.1%。于是，预剪枝策略将禁止结点2被划分。</p>
</li>
<li><p>对结点3，最优划分属性为”根蒂”，划分后验证集精度仍为 5&#x2F;7. 这个 划分不能提升验证集精度，于是，预剪枝策略禁止结点3被划分。</p>
</li>
<li><p>对结点4，其所含训练样例己属于同一类，不再进行划分.</p>
</li>
</ol>
<p>于是，基于预剪枝策略从上表数据所生成的决策树如上图所示，其验证集精度为 71.4%. 这是一棵仅有一层划分的决策树。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610204200236.png" alt="image-20250610204200236">后剪枝(post- pruning)举例</p>
<p> 后剪枝先从训练集生成一棵完整决策树，继续使用上面的案例，从前面计算，我们知前面构造的决策树的验证集精度为42.9%。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20230905171420720.png" alt="image-20230905171420720"></p>
<ol>
<li>首先考察结点6，若将其领衔的分支剪除则相当于把6替换为叶结点。替换后的叶结点包含编号为 {7， 15} 的训练样本，于是该叶结点的类别标记为”好瓜”, 此时决策树的验证集精度提高至 57.1%。</li>
</ol>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610204247674.png" alt="image-20250610204247674"></p>
<ol start="2">
<li>然后考察结点5，若将其领衔的子树替换为叶结点，则替换后的叶结点包含编号为 {6，7，15}的训练样例，叶结点类别标记为”好瓜’；此时决策树验证集精度仍为 57.1%. 于是，可以不进行剪枝.</li>
<li>对结点2，若将其领衔的子树替换为叶结点，则替换后的叶结点包含编号 为 {1， 2， 3， 14} 的训练样例，叶结点标记为”好瓜”此时决策树的验证集精度提高至 71.4%. 于是，后剪枝策略决定剪枝.</li>
<li>对结点3和1，若将其领衔的子树替换为叶结点，则所得决策树的验证集 精度分别为 71.4% 与 42.9%，均未得到提高，于是它们被保留。</li>
<li>最终, 基于后剪枝策略生成的决策树如上图所示, 其验证集精度为 71.4%。</li>
</ol>
<h1 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h1><h2 id="集成学习是什么？"><a href="#集成学习是什么？" class="headerlink" title="集成学习是什么？"></a>集成学习是什么？</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610171858319.png" alt="image-20250610171858319"></p>
<p>集成学习是机器学习中的一种思想，它通过多个模型的组合形成一个精度更高的模型，参与组合的模型成为弱学习器（基学习器）。训练时，使用训练集依次训练出这些弱学习器，对未知的样本进行预测时，使用这些弱学习器联合进行预测。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610171937011.png" alt="image-20250610171937011"></p>
<p>传统机器学习算法 (例如：决策树，逻辑回归等) 的目标都是寻找一个最优分类器尽可能的将训练数据分开。集成学习 (Ensemble Learning) 算法的基本思想就是将多个分类器组合，从而实现一个预测效果更好的集成分类器。集成算法可以说从一方面验证了中国的一句老话：三个臭皮匠，赛过诸葛亮.</p>
<p>集成学习通过建立几个模型来解决单一预测问题。它的工作原理是 生成多个分类器&#x2F;模型，各自独立地学习和作出预测。这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测</p>
<p>集成学习算法一般分为：bagging和boosting。</p>
<h2 id="Bagging集成算法"><a href="#Bagging集成算法" class="headerlink" title="Bagging集成算法"></a>Bagging集成算法</h2><blockquote>
<p>Baggging 框架通过有放回的抽样产生不同的训练集，从而训练具有差异性的弱学习器，然后通过平权投票、多数表决的方式决定预测结果。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610172549709.png" alt="image-20250610172549709"></p>
<h2 id="Boosting-集成算法"><a href="#Boosting-集成算法" class="headerlink" title="Boosting 集成算法"></a>Boosting 集成算法</h2><blockquote>
<p>Boosting 体现了提升思想，每一个训练器重点关注前一个训练器不足的地方进行训练，通过加权投票的方式，得出预测结果。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610172643438.png" alt="image-20250610172643438"></p>
<p>Boosting是一组可将弱学习器升为强学习器算法。这类算法的工作机制类似：</p>
<p>1.先从初始训练集训练出一个基学习器</p>
<p>2.在根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续得到最大的关注。</p>
<p>3.然后基于调整后的样本分布来训练下一个基学习器；</p>
<p>4.如此重复进行，直至基学习器数目达到实现指定的值T为止。</p>
<p>5.再将这T个基学习器进行加权结合得到集成学习器。</p>
<p><strong>简而言之：每新加入一个弱学习器，整体能力就会得到提升</strong></p>
<h2 id="Bagging-与-Boosting区别"><a href="#Bagging-与-Boosting区别" class="headerlink" title="Bagging 与 Boosting区别"></a>Bagging 与 Boosting区别</h2><blockquote>
<p>下面提到的四种算法中，<code>只有随机森林算法是基于Bagging，其余都是基于Boosting</code>。</p>
</blockquote>
<p><strong>区别一:数据方面</strong></p>
<ul>
<li>Bagging：<code>有放回采样</code></li>
<li>Boosting：<code>全部数据集, 重点关注前一个弱学习器不足</code></li>
</ul>
<p><strong>区别二:投票方面</strong></p>
<ul>
<li>Bagging：<code>平权投票</code></li>
<li>Boosting：<code>加权投票</code></li>
</ul>
<p><strong>区别三:学习顺序</strong></p>
<ul>
<li>Bagging的<code>学习是并行的，每个学习器没有依赖关系</code></li>
<li>Boosting<code>学习是串行，学习有先后顺序</code></li>
</ul>
<h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><p>随机森林是基于 Bagging 思想实现的一种集成学习算法，它采用决策树模型为每一个基学习器。其构造过程：</p>
<ol>
<li>训练：<ol>
<li><code>从原来的N个训练样本中有放回地随机抽取m个样本(包括可能重复样本)</code></li>
<li><code>随机挑选 n 个特征（n 小于总特征数量）</code></li>
</ol>
</li>
<li><code>预测：平权投票，多数表决输出预测结果</code></li>
</ol>
<blockquote>
<p>具体来讲就是每次从原来的N个训练样本中有放回地随机抽取m个样本(包括可能重复样本)。</p>
<p>然后，从候选的特征中随机抽取k个特征，作为当前节点下决策的备选特征，从这些特征中选择最好地划分训练样本的特征。用每个样本集作为训练样本构造决策树。单个决策树在产生样本集和确定特征后，使用<code>CART算法</code>计算，不剪枝。</p>
<p>最后，得到所需数目的决策树后，随机森林方法对这些树的输出进行投票，以得票最多的类作为随机森林的决策。</p>
</blockquote>
<h3 id="随机森林不会产生过拟合原因："><a href="#随机森林不会产生过拟合原因：" class="headerlink" title="随机森林不会产生过拟合原因："></a>随机森林不会产生过拟合原因：</h3><p>（1）随机森林的方法<code>即对训练样本进行了采样，又对特征进行了采样</code>，充分保证了所构建的每个树之间的独立性，使得投票结果更准确。</p>
<p>（2）随机森林的随机性体现在<code>每棵树的训练样本是随机的</code>，<code>树中每个节点的分裂属性也是随机选择的</code>。有了这2个随机因素，即使每棵决策树没有进行剪枝，随机森林也不会产生过拟合的现象。</p>
<p>随机森林中有两个可控制参数：</p>
<ul>
<li><p>森林中树的数量（一般选取值较大）</p>
</li>
<li><p>抽取的属性值m的大小。</p>
</li>
</ul>
<h3 id="随机森林API-泰坦尼克号案例"><a href="#随机森林API-泰坦尼克号案例" class="headerlink" title="随机森林API(泰坦尼克号案例)"></a>随机森林API(泰坦尼克号案例)</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250611180801848.png" alt="image-20250611180801848"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.ensemble.RandomForestClassifier()</span><br></pre></td></tr></table></figure>



<p><strong>n_estimators</strong>：决策树数量，(default &#x3D; 10) </p>
<p><strong>Criterion</strong>：entropy、或者 gini, (default &#x3D; gini)</p>
<p><strong>max_depth</strong>：指定树的最大深度，(default &#x3D; None 表示树会尽可能的生长)</p>
<p><strong>max_features&#x3D;”auto”</strong>, 决策树构建时使用的最大特征数量</p>
<ul>
<li>If “auto”, then <code>max_features=sqrt(n_features)</code>.</li>
<li>If “sqrt”, then <code>max_features=sqrt(n_features)</code>(same as “auto”).</li>
<li>If “log2”, then <code>max_features=log2(n_features)</code>.</li>
<li>If None, then <code>max_features=n_features</code>.</li>
</ul>
<p><strong>bootstrap</strong>：是否采用有放回抽样，如果为 False 将会使用全部训练样本，(default &#x3D; True)</p>
<p><strong>min_samples_split：</strong> 结点分裂所需最小样本数，(default &#x3D; 2)</p>
<ul>
<li>如果节点样本数少于min_samples_split，则不会再进行划分.</li>
<li>如果样本量不大，不需要设置这个值.</li>
<li>如果样本量数量级非常大，则推荐增大这个值.</li>
</ul>
<p><strong>min_samples_leaf：</strong> 叶子节点的最小样本数，(default &#x3D; 1)</p>
<ul>
<li>如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝.</li>
<li>较小的叶子结点样本数量使模型更容易捕捉训练数据中的噪声.</li>
</ul>
<p><strong>min_impurity_split:</strong> 节点划分最小不纯度</p>
<ul>
<li>如果某节点的不纯度(基尼系数，均方差)小于这个阈值，则该节点不再生成子节点，并变为叶子节点.</li>
<li>一般不推荐改动默认值1e-7。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0.导入工具包</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> plot_tree</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.读取数据</span></span><br><span class="line">data =pd.read_csv(<span class="string">&#x27;./data/titanic/train.csv&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(data.head())</span><br><span class="line"><span class="built_in">print</span>(data.info())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.数据处理</span></span><br><span class="line"><span class="comment">## 2.1 特征选择</span></span><br><span class="line">x = data[[<span class="string">&#x27;Pclass&#x27;</span>,<span class="string">&#x27;Sex&#x27;</span>,<span class="string">&#x27;Age&#x27;</span>]].copy() <span class="comment"># 数据选择</span></span><br><span class="line">y = data[<span class="string">&#x27;Survived&#x27;</span>].copy() <span class="comment"># 目标值选择</span></span><br><span class="line"><span class="comment"># print(x.head(10))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.2 缺失值处理</span></span><br><span class="line"><span class="comment"># 数值型缺失值 - 采用平均值或者是中位数进行填充</span></span><br><span class="line"><span class="comment"># 日期值，可以使用缺失值前后的值进行填充</span></span><br><span class="line">x[<span class="string">&#x27;Age&#x27;</span>].fillna(x[<span class="string">&#x27;Age&#x27;</span>].mean(),inplace = <span class="literal">True</span>)</span><br><span class="line"><span class="comment"># print(x.head(10))</span></span><br><span class="line">x=pd.get_dummies(x)</span><br><span class="line"><span class="built_in">print</span>(x.head(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 2.3 训练集和测试集划分</span></span><br><span class="line">x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.模型训练</span></span><br><span class="line"><span class="comment"># 3.1 决策树</span></span><br><span class="line">tree =DecisionTreeClassifier()</span><br><span class="line">tree.fit(x_train,y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.2 随机森林</span></span><br><span class="line">rf=RandomForestClassifier()</span><br><span class="line">rf.fit(x_train,y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.3 网格搜索交叉验证</span></span><br><span class="line">params = &#123;<span class="string">&#x27;n_estimators&#x27;</span>:[<span class="number">10</span>,<span class="number">20</span>],<span class="string">&#x27;max_depth&#x27;</span>:[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]&#125;</span><br><span class="line">model =GridSearchCV(estimator=rf,param_grid=params,cv=<span class="number">3</span>)</span><br><span class="line">model.fit(x_train,y_train)</span><br><span class="line"><span class="built_in">print</span>(model.best_estimator_)</span><br><span class="line"></span><br><span class="line">rfs = RandomForestClassifier(max_depth=<span class="number">4</span>, n_estimators=<span class="number">10</span>)</span><br><span class="line">rfs.fit(x_train,y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.模型评估</span></span><br><span class="line"><span class="comment"># 4.1 决策树</span></span><br><span class="line"><span class="built_in">print</span>(tree.score(x_test,y_test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.2 随机森林</span></span><br><span class="line"><span class="built_in">print</span>(rf.score(x_test,y_test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.3 网格搜索交叉验证</span></span><br><span class="line"><span class="built_in">print</span>(rfs.score(x_test,y_test))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h2><blockquote>
<p>Adaptive Boosting(自适应提升)基于 Boosting思想实现的一种集成学习算法核心思想是通过逐步提高那些被前一步分类错误的样本的权重来训练一个强分类器。弱分类器的性能比随机猜测强就行，即可构造出一个非常准确的强分类器。其特点是：<strong>训练时，样本具有权重，并且在训练过程中动态调整。被分错的样本的样本会加大权重，算法更加关注难分的样本。</strong></p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250611175112448.png" alt="image-20250611175112448"></p>
<h3 id="AdaBoost推导过程及案例"><a href="#AdaBoost推导过程及案例" class="headerlink" title="AdaBoost推导过程及案例"></a>AdaBoost推导过程及案例</h3><blockquote>
<p>整个Adaboost 迭代算法就3步：</p>
<p>1.初始化训练数据权值。<br><code>如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N</code>。</p>
<p>2.训练弱分类器。<br>具体训练过程中，<code>如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低</code>；<code>相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高</code>。</p>
<p>然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。</p>
<p>3.<code>将各个训练得到的多个弱分类器组合成一个强分类器</code>。<br>各个弱分类器的训练过程结束后，<code>加大分类误差率小的弱分类器的权重</code>，使其在最终的分类函数中起着较大的决策作用，<code>而降低分类误差率大的弱分类器的权重</code>，使其在最终的分类函数中起着较小的决策作用。</p>
<p>换言之，<code>误差率低的弱分类器在最终分类器中占的权重较大</code>，否则较小。</p>
<p>参考链接：</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/41536315">Adaboost算法原理</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41194171/article/details/85014669">【 Adaboost算法】 机器学习实例推导计算+公式详细过程</a></p>
</li>
</ul>
</blockquote>
<h3 id="Adaboost算法流程"><a href="#Adaboost算法流程" class="headerlink" title="Adaboost算法流程"></a>Adaboost算法流程</h3><p>模型权重计算公式：<br>$$<br>\alpha_t &#x3D; \frac{1}{2} \ln \left( \frac{1 - \epsilon_t}{\epsilon_t} \right)<br>$$<br>$\alpha_t$为模型权重，$\epsilon_t$表示第m个弱学习器的错误率。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250714201452809.png" alt="image-20250714201452809"></p>
<p>设训练集样本数为 $N$，初始化每个样本权重：$w_i &#x3D; \frac{1}{N}$（即均匀分布）。</p>
<p>迭代训练弱分类器（共 $M$轮）</p>
<p>训练弱分类器：使用<strong>当前样本权重分布</strong>训练弱分类器 $G_m(x)$（如决策树桩）。</p>
<p>计算分类错误率 $\epsilon_m$：$\epsilon_m &#x3D; \sum_{i&#x3D;1}^N w_i \cdot I(y_i \neq G_m(x_i))$<br>（即被错误分类样本的权重之和）。</p>
<p>$I$指示函数，预测错误时值为1，否则为0</p>
<p>计算弱分类器权重 $\alpha_m$：$\alpha_m &#x3D; \frac{1}{2} \ln \left( \frac{1 - \epsilon_m}{\epsilon_m} \right)$</p>
<p>错误率 $\epsilon_m$ 越低，$\alpha_m$ 越大（分类器贡献越大）。</p>
<p>更新样本权重</p>
<ul>
<li><p>正确分类样本：权重乘以 $e^{-\alpha_m}$（权重减小）</p>
</li>
<li><p>错误分类样本：权重乘以$e^{\alpha_m}$（权重增大）</p>
</li>
<li><p>标准化权重：$w_i \leftarrow \frac{w_i}{\sum w_i}$<br>（下一轮分类器更关注难分样本）。</p>
</li>
</ul>
<p>组合弱分类器</p>
<ul>
<li>最终强分类器：$H(x) &#x3D; \text{sign} \left( \sum_{m&#x3D;1}^M \alpha_m G_m(x) \right)$。</li>
</ul>
<h3 id="Adaboost算法案例"><a href="#Adaboost算法案例" class="headerlink" title="Adaboost算法案例"></a>Adaboost算法案例</h3><p>数据集与参数（5个样本）：</p>
<ul>
<li><strong>弱分类器</strong>：单层决策树（决策树桩）</li>
<li><strong>迭代次数</strong>：$M &#x3D; 3$。</li>
</ul>
<table>
<thead>
<tr>
<th>样本</th>
<th>特征 $X_1$</th>
<th>特征$X_2$</th>
<th>标签 $y$</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>1.0</td>
<td>2.1</td>
<td>+1</td>
</tr>
<tr>
<td>2</td>
<td>2.0</td>
<td>1.1</td>
<td>+1</td>
</tr>
<tr>
<td>3</td>
<td>1.3</td>
<td>1.0</td>
<td>-1</td>
</tr>
<tr>
<td>4</td>
<td>1.0</td>
<td>1.0</td>
<td>-1</td>
</tr>
<tr>
<td>5</td>
<td>2.0</td>
<td>1.0</td>
<td>+1</td>
</tr>
</tbody></table>
<p><strong>第一轮迭代（$m&#x3D;1$）</strong></p>
<ul>
<li><p><strong>初始权重</strong>：$w_i &#x3D; 0.2$（均匀分布）</p>
</li>
<li><p>弱分类器 $G_1(x)$：以 $X_1 \leq 1.5$ 为阈值（分类规则：若 $X_1 \leq 1.5$则预测为 $-1$，否则 $+1$）。</p>
</li>
<li><p><strong>错误样本</strong>：样本1（被错分为 $-1$），错误率 $\epsilon_1 &#x3D; 0.2$</p>
</li>
<li><p><strong>分类器权重</strong>：$\alpha_1 &#x3D; \frac{1}{2} \ln \frac{0.8}{0.2} \approx 0.693$</p>
</li>
<li><p>更新样本权重：</p>
<ul>
<li><p>样本1权重更新为$0.2 \times e^{0.693} \approx 0.4$</p>
</li>
<li><p>其他样本权重更新为 $0.2 \times e^{-0.693} \approx 0.1$</p>
</li>
<li><p>标准化后权重：</p>
<table>
<thead>
<tr>
<th>样本</th>
<th>新权重 $w_i$</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>0.4</td>
</tr>
<tr>
<td>2-5</td>
<td>0.15</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ul>
<p><strong>第二轮迭代（$m&#x3D;2$）</strong></p>
<ul>
<li><p>弱分类器 $G_2(x)$：以 $X_2 \leq 1.0$为阈值（规则：$X_2 \leq 1.0$则 $-1$，否则 $+1$）。</p>
</li>
<li><p><strong>错误样本</strong>：样本5（被错分为 $-1$），错误率 $\epsilon_2 &#x3D; 0.15$</p>
</li>
<li><p><strong>分类器权重</strong>：$\alpha_2 &#x3D; \frac{1}{2} \ln \frac{0.85}{0.15} \approx 0.875$</p>
</li>
<li><p>更新样本权重：</p>
<ul>
<li><p>样本5权重更新为 $0.15 \times e^{0.875} \approx 0.35$</p>
</li>
<li><p>其他样本权重减小，标准化后：</p>
<table>
<thead>
<tr>
<th>样本</th>
<th>新权重 $w_i$</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>0.28</td>
</tr>
<tr>
<td>5</td>
<td>0.35</td>
</tr>
<tr>
<td>2,3,4</td>
<td>0.11</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ul>
<p><strong>第三轮迭代（$m&#x3D;3$）</strong></p>
<ul>
<li><p>弱分类器 $G_3(x)$：以 $X_1 \leq 2.0$为阈值（规则：$X_1 \leq 2.0$则 $+1$，否则 $-1$）。</p>
</li>
<li><p><strong>错误样本</strong>：样本3（被错分为 $+1$），错误率 $\epsilon_3 &#x3D; 0.11$</p>
</li>
<li><p><strong>分类器权重</strong>：$\alpha_3 &#x3D; \frac{1}{2} \ln \frac{0.89}{0.11} \approx 1.12$</p>
</li>
<li><p><strong>样本权重更新略</strong>（原理同上）。</p>
</li>
<li><p>组合强分类器</p>
</li>
</ul>
<p>$$H(x) &#x3D; \text{sign} \left( 0.693 \cdot G_1(x) + 0.875 \cdot G_2(x) + 1.12 \cdot G_3(x) \right)$$</p>
<ul>
<li>分类结果：<ul>
<li>样本1：$G_1&#x3D;-1, G_2&#x3D;+1, G_3&#x3D;+1$ → 加权和 $&gt;0$ → 预测 $+1$ ✓</li>
<li>样本3：$G_1&#x3D;-1, G_2&#x3D;-1, G_3&#x3D;+1$ → 加权和 $&lt;0$→ 预测 $-1$ ✓</li>
<li><strong>所有样本分类正确</strong>（原始单层决策树错误率40%，集成后达100%）。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入必要的库</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 数据准备</span></span><br><span class="line"><span class="comment"># 获取数据</span></span><br><span class="line">df_wine = pd.read_csv(<span class="string">&#x27;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&#x27;</span>, header=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改列名</span></span><br><span class="line">df_wine.columns = [<span class="string">&#x27;Class label&#x27;</span>, <span class="string">&#x27;Alcohol&#x27;</span>, <span class="string">&#x27;Malic acid&#x27;</span>, <span class="string">&#x27;Ash&#x27;</span>, </span><br><span class="line">                   <span class="string">&#x27;Alcalinity of ash&#x27;</span>, <span class="string">&#x27;Magnesium&#x27;</span>, <span class="string">&#x27;Total phenols&#x27;</span>, </span><br><span class="line">                   <span class="string">&#x27;Flavanoids&#x27;</span>, <span class="string">&#x27;Nonflavanoid phenols&#x27;</span>, <span class="string">&#x27;Proanthocyanins&#x27;</span>, </span><br><span class="line">                   <span class="string">&#x27;Color intensity&#x27;</span>, <span class="string">&#x27;Hue&#x27;</span>, <span class="string">&#x27;OD280/OD315 of diluted wines&#x27;</span>, <span class="string">&#x27;Proline&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去掉一类(1,2,3)只保留两类</span></span><br><span class="line">df_wine = df_wine[df_wine[<span class="string">&#x27;Class label&#x27;</span>] != <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取特征值和目标值</span></span><br><span class="line">X = df_wine[[<span class="string">&#x27;Alcohol&#x27;</span>, <span class="string">&#x27;Hue&#x27;</span>]].values</span><br><span class="line">y = df_wine[<span class="string">&#x27;Class label&#x27;</span>].values</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 数据预处理</span></span><br><span class="line"><span class="comment"># 类别转化 (2,3)=&gt;(0,1)</span></span><br><span class="line">le = LabelEncoder()</span><br><span class="line">y = le.fit_transform(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.4</span>, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 模型训练与评估</span></span><br><span class="line"><span class="comment"># 创建决策树和AdaBoost分类器</span></span><br><span class="line">tree = DecisionTreeClassifier(criterion=<span class="string">&#x27;entropy&#x27;</span>, max_depth=<span class="number">1</span>, random_state=<span class="number">0</span>)</span><br><span class="line">ada = AdaBoostClassifier(base_estimator=tree, n_estimators=<span class="number">500</span>, learning_rate=<span class="number">0.1</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 决策树性能评估</span></span><br><span class="line">tree = tree.fit(X_train, y_train)</span><br><span class="line">y_train_pred = tree.predict(X_train)</span><br><span class="line">y_test_pred = tree.predict(X_test)</span><br><span class="line">tree_train = accuracy_score(y_train, y_train_pred)</span><br><span class="line">tree_test = accuracy_score(y_test, y_test_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Decision tree train/test accuracies %.3f/%.3f&#x27;</span> % (tree_train, tree_test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># AdaBoost性能评估</span></span><br><span class="line">ada = ada.fit(X_train, y_train)</span><br><span class="line">y_train_pred = ada.predict(X_train)</span><br><span class="line">y_test_pred = ada.predict(X_test)</span><br><span class="line">ada_train = accuracy_score(y_train, y_train_pred)</span><br><span class="line">ada_test = accuracy_score(y_test, y_test_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Adaboost train/test accuracies %.3f/%.3f&#x27;</span> % (ada_train, ada_test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 可视化决策边界</span></span><br><span class="line">x_min, x_max = X_train[:, <span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">1</span>, X_train[:, <span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">y_min, y_max = X_train[:, <span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">1</span>, X_train[:, <span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">0.1</span>),</span><br><span class="line">                     np.arange(y_min, y_max, <span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line">f, axarr = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, sharex=<span class="string">&#x27;col&#x27;</span>, sharey=<span class="string">&#x27;row&#x27;</span>, figsize=(<span class="number">8</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx, clf, tt <span class="keyword">in</span> <span class="built_in">zip</span>([<span class="number">0</span>, <span class="number">1</span>], [tree, ada], [<span class="string">&#x27;Decision Tree&#x27;</span>, <span class="string">&#x27;AdaBoost&#x27;</span>]):</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    axarr[idx].contourf(xx, yy, Z, alpha=<span class="number">0.3</span>)</span><br><span class="line">    axarr[idx].scatter(X_train[y_train==<span class="number">0</span>, <span class="number">0</span>], X_train[y_train==<span class="number">0</span>, <span class="number">1</span>], c=<span class="string">&#x27;blue&#x27;</span>, marker=<span class="string">&#x27;^&#x27;</span>)</span><br><span class="line">    axarr[idx].scatter(X_train[y_train==<span class="number">1</span>, <span class="number">0</span>], X_train[y_train==<span class="number">1</span>, <span class="number">1</span>], c=<span class="string">&#x27;red&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">    axarr[idx].set_title(tt)</span><br><span class="line">    axarr[idx].set_xlabel(<span class="string">&#x27;Alcohol&#x27;</span>)</span><br><span class="line">    axarr[idx].set_ylabel(<span class="string">&#x27;Hue&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><blockquote>
<p>图解GBDT：<a target="_blank" rel="noopener" href="https://blog.csdn.net/ShowMeAI/article/details/123402422">https://blog.csdn.net/ShowMeAI/article/details/123402422</a></p>
<p>GBDT（Gradient Boosting Decision Tree，梯度提升决策树）是一种基于集成学习的迭代算法，通过组合多个弱学习器（决策树）提升模型预测能力。梯度提升树（Gradient Boosting Decision Tre）是提升树（Boosting Decision Tree）的一种改进算法。<code>GBDT通过迭代拟合残差（预测误差）逐步优化模型</code>。<code>每轮新增的决策树学习前一模型预测值与真实值的残差</code>，最终将所有树的<code>预测结果加权求和作为最终输出</code>。</p>
</blockquote>
<p>目标函数：最小化损失函数 $L(y, F(x))$，其中 $F(x)$为模型预测值：$F^*(x) &#x3D; \arg \min_{F} \sum_{i&#x3D;1}^n L(y_i, F(x_i))$</p>
<p>迭代更新：第 $m$ 轮模型更新公式为：$F_{m}(x) &#x3D; F_{m-1}(x) + \gamma_m h_m(x)$</p>
<ul>
<li>其中 $\gamma_m$ 为学习率</li>
<li>$h_m(x)$ 为新决策树。</li>
</ul>
<p>负梯度拟合：每轮计算损失函数的负梯度（伪残差）作为新树的学习目标：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250714202314363.png" alt="image-20250714202314363"></p>
<p>新树 $h_m(x)$ 的任务是拟合 $r_i^{(m)}$</p>
<ul>
<li><p>叶子节点权重：对叶子节点区域 $R_{mj}$，计算最优权重 ：$c_{mj}$</p>
<p>$$<br>c_{mj} &#x3D; \arg\min_{c} \sum_{x_i \in R_{mj}} L(y_i, F_{m-1}(x_i) + c)<br>$$</p>
<p>对于平方损失函数，$c_{mj}$ 即节点内样本残差的均值</p>
</li>
</ul>
<p>不同任务使用不同损失函数：</p>
<table>
<thead>
<tr>
<th><strong>任务类型</strong></th>
<th><strong>损失函数</strong></th>
<th><strong>负梯度形式</strong></th>
</tr>
</thead>
<tbody><tr>
<td>回归（如MSE）</td>
<td>$\frac{1}{2}(y_i - \hat{y_i})^2$</td>
<td>$y_i - \hat{y_i}$</td>
</tr>
<tr>
<td>二分类（如Log Loss）</td>
<td>$-\left[ y_i \ln(\hat{y_i}) + (1-y_i)\ln(1-\hat{y_i}) \right]$</td>
<td>$y_i - \hat{y_i}$</td>
</tr>
</tbody></table>
<h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><ol>
<li><p>初始化：用常量初始化模型（如回归问题取标签均值）：</p>
<p>$$<br>F_0(x) &#x3D; \arg\min_{c} \sum_{i&#x3D;1}^n L(y_i, c)<br>$$</p>
</li>
<li><p>迭代训练（共 $M$轮）：</p>
<ul>
<li>计算负梯度（残差） $r_i^{(m)}$；</li>
<li>训练新决策树 $h_m(x)$ 拟合 $r_i^{(m)}$，生成叶子节点区域 $R_{mj}$；</li>
<li>计算每个叶子节点的权重 $c_{mj}$；</li>
<li>更新模型： $F_m(x) &#x3D; F_{m-1}(x) + \gamma \sum_{j} c_{mj} \mathbf{1}(x \in R_{mj})$。</li>
</ul>
</li>
<li><p>输出最终模型：</p>
<p>$$<br>F_M(x) &#x3D; F_0(x) + \sum_{m&#x3D;1}^M \gamma_m h_m(x)<br>$$</p>
</li>
</ol>
<h3 id="GBDT算法案例"><a href="#GBDT算法案例" class="headerlink" title="GBDT算法案例"></a>GBDT算法案例</h3><ul>
<li><p><strong>年龄预测（回归问题）</strong></p>
<ul>
<li><p><strong>数据</strong>：4人样本（A:14岁, B:16岁, C:24岁, D:26岁）。</p>
</li>
<li><p><strong>参数</strong>：学习率 $\gamma &#x3D; 0.1$，树深度为2，迭代5轮。</p>
</li>
</ul>
</li>
</ul>
<p><strong>步骤</strong>：</p>
<ol>
<li><p><strong>初始化</strong>：取年龄均值 $F_0(x) &#x3D; (14+16+24+26)&#x2F;4 &#x3D; 20$。</p>
</li>
<li><p>第一轮迭代：</p>
<ul>
<li><p>计算残差：</p>
<table>
<thead>
<tr>
<th>样本</th>
<th>真实值</th>
<th>预测值</th>
<th>残差</th>
</tr>
</thead>
<tbody><tr>
<td>A</td>
<td>14</td>
<td>20</td>
<td>-6</td>
</tr>
<tr>
<td>B</td>
<td>16</td>
<td>20</td>
<td>-4</td>
</tr>
<tr>
<td>C</td>
<td>24</td>
<td>20</td>
<td>4</td>
</tr>
<tr>
<td>D</td>
<td>26</td>
<td>20</td>
<td>6</td>
</tr>
</tbody></table>
</li>
<li><p>训练树1：按特征划分（如年龄≤20），左节点（A,B）残差均值&#x3D; -5，右节点（C,D）残差均值&#x3D;5。</p>
</li>
<li><p>更新模型：$F_1(x) &#x3D; 20 + 0.1 \times \text{树1输出}$。</p>
</li>
</ul>
</li>
<li><p>后续迭代：每轮新树拟合上一轮残差，逐步逼近真实值。</p>
<ul>
<li>最终预测：A的年龄&#x3D; $20 + 0.1 \times (-5) + 0.1 \times (-0.5) + \cdots \approx 14$。</li>
</ul>
</li>
</ol>
<p>关键参数</p>
<table>
<thead>
<tr>
<th><strong>参数</strong></th>
<th><strong>作用</strong></th>
</tr>
</thead>
<tbody><tr>
<td>n_estimators</td>
<td>树的数量，过多易过拟合，过少欠拟合</td>
</tr>
<tr>
<td>learning_rate</td>
<td>学习率（缩减因子），越小需更多树但模型更稳定</td>
</tr>
<tr>
<td>max_depth</td>
<td>单棵树深度，控制模型复杂度</td>
</tr>
<tr>
<td>min_samples_split</td>
<td>节点分裂所需最小样本数，防止过拟合</td>
</tr>
</tbody></table>
<blockquote>
<p>案例代码实现可参考Scikit-learn库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier  </span><br><span class="line">model = GradientBoostingClassifier(learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">100</span>, max_depth=<span class="number">3</span>)  </span><br><span class="line">model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure></blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.数据导入</span></span><br><span class="line"><span class="comment">#1.1导入数据</span></span><br><span class="line"><span class="keyword">import</span>  pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment">#1.2.利用pandas的read.csv模块从互联网中收集泰坦尼克号数据集</span></span><br><span class="line">titanic=pd.read_csv(<span class="string">&quot;data/titanic.csv&quot;</span>)</span><br><span class="line">titanic.info() <span class="comment">#查看信息</span></span><br><span class="line"><span class="comment">#2人工选择特征pclass,age,sex</span></span><br><span class="line">X=titanic[[<span class="string">&#x27;pclass&#x27;</span>,<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;sex&#x27;</span>]]</span><br><span class="line">y=titanic[<span class="string">&#x27;survived&#x27;</span>]</span><br><span class="line"><span class="comment">#3.特征工程</span></span><br><span class="line"><span class="comment">#数据的填补</span></span><br><span class="line">X[<span class="string">&#x27;age&#x27;</span>].fillna(X[<span class="string">&#x27;age&#x27;</span>].mean(),inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据的切分</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test =train_test_split(X,y,test_size=<span class="number">0.25</span>,random_state=<span class="number">22</span>)</span><br><span class="line"><span class="comment">#将数据转化为特征向量</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line">vec=DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line">X_train=vec.fit_transform(X_train.to_dict(orient=<span class="string">&#x27;records&#x27;</span>))</span><br><span class="line">X_test=vec.transform(X_test.to_dict(orient=<span class="string">&#x27;records&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.使用单一的决策树进行模型的训练及预测分析</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">dtc=DecisionTreeClassifier()</span><br><span class="line">dtc.fit(X_train,y_train)</span><br><span class="line">dtc_y_pred=dtc.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;score&quot;</span>,dtc.score(X_test,y_test))</span><br><span class="line"><span class="comment">#5.随机森林进行模型的训练和预测分析</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">rfc=RandomForestClassifier(random_state=<span class="number">9</span>)</span><br><span class="line">rfc.fit(X_train,y_train)</span><br><span class="line">rfc_y_pred=rfc.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;score:forest&quot;</span>,rfc.score(X_test,y_test))</span><br><span class="line"><span class="comment">#6.GBDT进行模型的训练和预测分析</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line">gbc=GradientBoostingClassifier()</span><br><span class="line">gbc.fit(X_train,y_train)</span><br><span class="line">gbc_y_pred=gbc.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;score:GradientBoosting&quot;</span>,gbc.score(X_test,y_test))</span><br><span class="line"></span><br><span class="line"><span class="comment">#7.性能评估</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;dtc_report:&quot;</span>,classification_report(dtc_y_pred,y_test))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;rfc_report:&quot;</span>,classification_report(rfc_y_pred,y_test))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;gbc_report:&quot;</span>,classification_report(gbc_y_pred,y_test))</span><br></pre></td></tr></table></figure>



<h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><blockquote>
<p>XGBoost的<strong>核心算法思想</strong>，基本就是：</p>
<ol>
<li>不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数<strong>f(x)</strong>，去拟合上次预测的残差。</li>
<li>当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数</li>
<li>最后只需要将每棵树对应的分数加起来就是该样本的预测值。</li>
</ol>
<p>参考链接：</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/Mephostopheles/p/18397154">https://www.cnblogs.com/Mephostopheles/p/18397154</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/mantch/p/11164221.html">终于有人说清楚了–XGBoost算法</a></p>
</li>
</ul>
</blockquote>
<h3 id="XGBoost与GBDT有什么不同"><a href="#XGBoost与GBDT有什么不同" class="headerlink" title="XGBoost与GBDT有什么不同"></a>XGBoost与GBDT有什么不同</h3><p>除了算法上与传统的GBDT有一些不同外，XGBoost还在工程实现上做了大量的优化。总的来说，两者之间的区别和联系可以总结成以下几个方面。</p>
<ol>
<li><code>GBDT是机器学习算法，XGBoost是该算法的工程实现。</code></li>
<li>在使用CART作为基分类器时，XGBoost显式地加入了<code>正则项来控制模型的复杂度</code>，<code>有利于防止过拟合，从而提高模型的泛化能力</code>。</li>
<li>GBDT在模型训练时只使用了<code>代价函数的一阶导数</code>信息，<code>XGBoost对代价函数进行二阶泰勒展开</code>，可以同时使用一阶和二阶导数。</li>
<li>传统的GBDT采用CART作为基分类器，XGBoost<code>支持多种类型的基分类器</code>，比如线性分类器。</li>
<li>传统的GBDT在每轮迭代时使用全部的数据，XGBoost则<code>采用了与随机森林相似的策略</code>，支持对数据进行采样。</li>
<li>传统的GBDT没有设计对缺失值进行处理，XGBoost能够<code>自动学习出缺失值的处理策略</code>。</li>
</ol>
<h3 id="在XGBoost目标函数泰勒展开："><a href="#在XGBoost目标函数泰勒展开：" class="headerlink" title="在XGBoost目标函数泰勒展开："></a><strong>在XGBoost目标函数泰勒展开</strong>：</h3><p>$Obj^{(t)} &#x3D; \sum_{i&#x3D;1}^{n} \textcolor{red}{l\left(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)\right)} + \Omega(f_t) + \text{constant}  $</p>
<ul>
<li>$y_i$：第 $i$ 个样本的真实标签</li>
<li>$\hat{y}_i^{(t-1)}$：前 $t-1$ 轮模型的累积预测值</li>
<li>$f_t(x_i)$：第 $t$ 轮新加入的决策树的预测值</li>
<li>$l$ 是一个二元函数：</li>
<li>$\textcolor{red}{ l(\text{真实值 } y_i, \text{ 预测值 } \hat{y}_i) }$</li>
<li>衡量模型预测值 $\hat{y}_i$ 与真实标签 $y_i$ 之间的差异（误差）</li>
</ul>
<h3 id="不同任务中的常见损失函数形式"><a href="#不同任务中的常见损失函数形式" class="headerlink" title="不同任务中的常见损失函数形式"></a><strong>不同任务中的常见损失函数形式</strong></h3><table>
<thead>
<tr>
<th><strong>任务类型</strong></th>
<th><strong>损失函数 $l$ 的表达式</strong></th>
<th><strong>场景示例</strong></th>
</tr>
</thead>
<tbody><tr>
<td>回归任务</td>
<td>均方误差（MSE）: $\frac{1}{2}(y_i - \hat{y}_i)^2$</td>
<td>房价预测、销量预测</td>
</tr>
<tr>
<td>二分类任务</td>
<td>对数损失（Log Loss）: $-y_i \ln(\hat{y}_i) - (1-y_i)\ln(1-\hat{y}_i)$</td>
<td>疾病诊断、点击率预测</td>
</tr>
<tr>
<td>多分类任务</td>
<td>交叉熵损失（Cross-Entropy）: $-\sum_c y_{i,c} \ln(\hat{y}_{i,c})$</td>
<td>图像分类、文本分类</td>
</tr>
</tbody></table>
<ol>
<li><p>泰勒展开公式：</p>
<p>$$<br>f(x + \Delta x) \simeq f(x) + f’(x) \Delta x + \frac{1}{2} f’’(x) \Delta x^2<br>$$</p>
</li>
<li><p>应用到损失函数：</p>
<ul>
<li><p>令 $x &#x3D; \hat{y}_i^{(t-1)}$, $\Delta x &#x3D; f_t(x_i)$</p>
</li>
<li><p>定义梯度：</p>
<p>$$<br>g_i &#x3D; \frac{\partial l}{\partial \hat{y}^{(t-1)}} \quad \text{(一阶导数)}<br>$$</p>
</li>
<li><p>定义Hessian：</p>
<p>$$<br>h_i &#x3D; \frac{\partial^2 l}{\partial \left( \hat{y}^{(t-1)} \right)^2} \quad \text{(二阶导数)}<br>$$</p>
</li>
</ul>
</li>
<li><p>近似目标函数：</p>
<p>$$<br>Obj^{(t)} \simeq \sum_{i&#x3D;1}^{n} \left[ \textcolor{red}{l(y_i, \hat{y}_i^{(t-1)})} + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i) \right] + \Omega(f_t)<br>$$</p>
<blockquote>
<p><strong>关键点</strong>：泰勒展开将复杂的损失函数 $l$ 转换为 <strong>常数项</strong> + <strong>一阶梯度项</strong> + <strong>二阶Hessian项</strong>，使优化过程可解析求解。</p>
</blockquote>
</li>
</ol>
<h3 id="为什么需要二阶展开？"><a href="#为什么需要二阶展开？" class="headerlink" title="为什么需要二阶展开？"></a><strong>为什么需要二阶展开？</strong></h3><ul>
<li><strong>精度提升</strong>：<br> 二阶导数（Hessian $h_i$）捕捉损失函数的曲率信息，比一阶方法（如GBDT）收敛更快、精度更高。</li>
<li><strong>正则化控制</strong>：<br> 二阶项 $h_i f_t^2(x_i)$ 隐含正则化作用，抑制决策树权重的极端值（防止过拟合）。</li>
</ul>
<h3 id="XGBoost算法流程"><a href="#XGBoost算法流程" class="headerlink" title="XGBoost算法流程"></a>XGBoost算法流程</h3><p>初始化模型</p>
<ul>
<li><p><strong>目标</strong>：构建初始预测值（常数值）</p>
</li>
<li><p>参数作用：</p>
<ul>
<li>base_score：初始预测值（回 归任务取标签均值，分类任务取先验概率）</li>
</ul>
</li>
<li><p><strong>迭代训练（共M轮）</strong></p>
</li>
</ul>
<p><strong>步骤1：计算梯度与Hessian</strong></p>
<p>数学形式：</p>
<ul>
<li>一阶梯度：</li>
</ul>
<p>$$<br>g_i &#x3D; \frac{\partial L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)}<br>$$</p>
<ul>
<li>二阶Hessian：</li>
</ul>
<p>$$<br>h_i &#x3D; \frac{\partial^2 L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)^2}<br>$$</p>
<ul>
<li>参数控制：<ul>
<li>$objective$：定义损失函数（如$reg:squarederror$、$binary:logistic$）</li>
</ul>
</li>
</ul>
<p><strong>步骤2：构建决策树（关键参数解析）</strong></p>
<table>
<thead>
<tr>
<th><strong>参数类别</strong></th>
<th><strong>参数名</strong></th>
<th><strong>数学含义</strong></th>
<th><strong>作用</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>树结构控制</strong></td>
<td>max_depth</td>
<td>树的最大深度</td>
<td>限制复杂度，防止过拟合（典型值：3-10）</td>
</tr>
<tr>
<td></td>
<td>min_child_weight</td>
<td>叶节点样本权重和的最小值</td>
<td>控制分裂粒度（值越大越保守）</td>
</tr>
<tr>
<td></td>
<td>gamma</td>
<td>分裂所需最小损失减少量</td>
<td>增益阈值（$\gamma \uparrow$ → 模型更简单）</td>
</tr>
<tr>
<td><strong>随机化控制</strong></td>
<td>subsample</td>
<td>样本采样比例（默认1）</td>
<td>降低方差（典型值：0.5-0.8）</td>
</tr>
<tr>
<td></td>
<td>colsample_bytree</td>
<td>特征采样比例（默认1）</td>
<td>减少特征相关性影响</td>
</tr>
<tr>
<td><strong>正则化</strong></td>
<td>reg_alpha</td>
<td>L1正则项系数（$\alpha$）</td>
<td>稀疏特征优化</td>
</tr>
<tr>
<td></td>
<td>reg_lambda</td>
<td>L2正则项系数（$\lambda$）</td>
<td>抑制权重过大</td>
</tr>
</tbody></table>
<ul>
<li><p>节点分裂增益公式：</p>
<p>$$<br>\text{Gain} &#x3D; \frac{1}{2} \left[ \frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{(G_L+G_R)^2}{H_L+H_R + \lambda} \right] - \gamma<br>$$</p>
<p>其中 $G$ 为梯度之和，$H$ 为Hessian之和</p>
</li>
</ul>
<p><strong>步骤3：计算叶节点权重</strong></p>
<ul>
<li><p>权重公式：</p>
<p>$$<br>w_j &#x3D; -\frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda}<br>$$</p>
</li>
</ul>
<p><strong>步骤4：更新模型</strong></p>
<ul>
<li>参数控制：<ul>
<li>$eta$（$learning_rate$）：收缩权重（默认0.3），防止过拟合</li>
<li>更新公式：</li>
</ul>
</li>
</ul>
<p>$$<br>F_m(x) &#x3D; F_{m-1}(x) + \eta \cdot w_{q(x)}<br>$$</p>
<h3 id="XGBoost案例：乳腺癌分类"><a href="#XGBoost案例：乳腺癌分类" class="headerlink" title="XGBoost案例：乳腺癌分类"></a>XGBoost案例：乳腺癌分类</h3><ul>
<li><strong>数据与参数配置</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X, y = data.data, data.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集/测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为DMatrix格式（加速计算）</span></span><br><span class="line">dtrain = xgb.DMatrix(X_train, label=y_train)</span><br><span class="line">dtest = xgb.DMatrix(X_test, label=y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数配置（关键参数释义）</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary:logistic&#x27;</span>,  <span class="comment"># 二分类逻辑损失</span></span><br><span class="line">    <span class="string">&#x27;eval_metric&#x27;</span>: <span class="string">&#x27;logloss&#x27;</span>,        <span class="comment"># 评估指标</span></span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">4</span>,                  <span class="comment"># 树深度限制</span></span><br><span class="line">    <span class="string">&#x27;eta&#x27;</span>: <span class="number">0.1</span>,                      <span class="comment"># 学习率</span></span><br><span class="line">    <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.8</span>,                <span class="comment"># 80%样本采样</span></span><br><span class="line">    <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.7</span>,          <span class="comment"># 70%特征采样</span></span><br><span class="line">    <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">1.0</span>,                <span class="comment"># L2正则</span></span><br><span class="line">    <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0.3</span>,                     <span class="comment"># 分裂最小增益阈值</span></span><br><span class="line">    <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">1</span>             <span class="comment"># 叶节点最小样本权重和</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>算法流程验证</strong></li>
</ul>
<ol>
<li><p><strong>初始化</strong>：</p>
<ul>
<li>初始预测值 &#x3D; 良性样本比例（62.5%）</li>
</ul>
</li>
<li><p><strong>第一轮迭代</strong>：</p>
<ul>
<li>计算梯度与Hessian（对数损失函数）</li>
<li>按特征分裂（如$worst radius$），选择增益最大分裂点</li>
<li>计算叶节点权重（如左叶权重&#x3D;0.2，右叶权重&#x3D;-0.3）</li>
<li>更新模型：$F_1(x) &#x3D; 0.625 + 0.1 \times \text{树输出}$</li>
</ul>
</li>
<li><p><strong>早停机制</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">evals = [(dtrain, <span class="string">&#x27;train&#x27;</span>), (dtest, <span class="string">&#x27;eval&#x27;</span>)]</span><br><span class="line">model = xgb.train(</span><br><span class="line">    params, </span><br><span class="line">    dtrain,</span><br><span class="line">    num_boost_round=<span class="number">100</span>,</span><br><span class="line">    evals=evals,</span><br><span class="line">    early_stopping_rounds=<span class="number">10</span>  <span class="comment"># 连续10轮无提升则停止</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p><strong>输出</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span>] train-logloss:<span class="number">0.68</span> <span class="built_in">eval</span>-logloss:<span class="number">0.65</span></span><br><span class="line">[<span class="number">1</span>] train-logloss:<span class="number">0.63</span> <span class="built_in">eval</span>-logloss:<span class="number">0.61</span></span><br><span class="line">...</span><br><span class="line">[<span class="number">42</span>] train-logloss:<span class="number">0.11</span> <span class="built_in">eval</span>-logloss:<span class="number">0.21</span>  <span class="comment"># 停止在42轮</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="聚类算法-Clustering"><a href="#聚类算法-Clustering" class="headerlink" title="聚类算法(Clustering)"></a>聚类算法(Clustering)</h1><blockquote>
<p>一种典型的无监督学习算法，主要用于将相似的样本自动归到一个类别中。</p>
<p>在聚类算法中根据样本之间的相似性，将样本划分到不同的类别中，对于不同的相似度计算方法，会得到不同的聚类结果，常用的相似度计算方法有欧式距离法。</p>
</blockquote>
<h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.cluster.KMeans(n_clusters=<span class="number">8</span>)</span><br></pre></td></tr></table></figure>



<ul>
<li>参数:<ul>
<li>n_clusters:开始的聚类中心数量<ul>
<li>整型，缺省值&#x3D;8，生成的聚类数，即产生的质心（centroids）数。</li>
</ul>
</li>
</ul>
</li>
<li>方法:<ul>
<li>estimator.fit(x)</li>
<li>estimator.predict(x)</li>
<li>estimator.fit_predict(x)<ul>
<li>计算聚类中心并预测每个样本属于哪个类别,相当于先调用fit(x),然后再调用predict(x)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="聚类算法都是无监督学习吗"><a href="#聚类算法都是无监督学习吗" class="headerlink" title="聚类算法都是无监督学习吗?"></a>聚类算法都是无监督学习吗?</h2><p>什么是聚类算法？聚类是一种机器学习技术，它涉及到数据点的分组。给定一组数据点，我们可以使用聚类算法将每个数据点划分为一个特定的组。理论上，同一组中的数据点应该具有相似的属性和&#x2F;或特征，而不同组中的数据点应该具有高度不同的属性和&#x2F;或特征。<strong>聚类是一种无监督学习的方法</strong>，是许多领域中常用的统计数据分析技术。</p>
<p>常用的算法包括<strong>K-MEANS、高斯混合模型（Gaussian Mixed Model，GMM）、自组织映射神经网络（Self-Organizing Map，SOM）</strong></p>
<h2 id="k-means-k均值-算法"><a href="#k-means-k均值-算法" class="headerlink" title="k-means(k均值)算法"></a>k-means(k均值)算法</h2><h3 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h3><p>K-均值是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。</p>
<p>K-均值是一个迭代算法，假设我们想要将数据聚类成 n 个组，其方法为:</p>
<ul>
<li>首先选择𝐾个随机的点，称为聚类中心（cluster centroids）；</li>
<li>对于数据集中的每一个数据，按照距离𝐾个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。</li>
<li>计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。</li>
<li>重复步骤，直至中心点不再变化。</li>
</ul>
<p>用 <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/0b5a79c3fbc15569003a978573b02821f8cfdd87b2c517a0ba95151887dc3c04/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f75253545312c75253545322c2e2e2e2c752535456b"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f75253545312c75253545322c2e2e2e2c752535456b.gif" alt="img"></a>来表示聚类中心，用𝑐(1),𝑐(2),…,𝑐(𝑚)来存储与第𝑖个实例数据最近的聚类中心的索引，K-均值算法的伪代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Repeat &#123;</span><br><span class="line">    for i = 1 to m</span><br><span class="line">    c(i) := index (form 1 to K) of cluster centroid closest to x(i)</span><br><span class="line">    for k = 1 to K</span><br><span class="line">    μk := average (mean) of points assigned to cluster k</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>算法分为两个步骤，第一个 for 循环是赋值步骤，即：对于每一个样例𝑖，计算其应该属于的类。第二个 for 循环是聚类中心的移动，即：对于每一个类𝐾，重新计算该类的质心。</p>
<p>K-均值算法也可以很便利地用于将数据分为许多不同组，即使在没有非常明显区分的组群的情况下也可以。下图所示的数据集包含身高和体重两项特征构成的，利用 K-均值算法将数据分为三类，用于帮助确定将要生产的 T-恤衫的三种尺寸。</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/7934ba50374f3316d8f1b8a3dcd210cc6d9d2607c6fa640c0ba0e8883c116c43/687474703a2f2f7778332e73696e61696d672e636e2f6d773639302f30303633304465666c79316735623734367a776a676a33306668306337676e6a2e6a7067"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/687474703a2f2f7778332e73696e61696d672e636e2f6d773639302f30303633304465666c79316735623734367a776a676a33306668306337676e6a2e6a7067.jpeg" alt="img"></a></p>
<h3 id="损失函数-2"><a href="#损失函数-2" class="headerlink" title="损失函数"></a>损失函数</h3><p>K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和，因此 K-均值的代价函数（又称畸变函数 Distortion function）为：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/8fabb926634ad9f39b44abb9027b7759d7e0064b6726a3c32b40ac78cade6632/687474703a2f2f7778322e73696e61696d672e636e2f6d773639302f303036333044656667793167356275397766776f746a33306a703031747132762e6a7067"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/687474703a2f2f7778322e73696e61696d672e636e2f6d773639302f303036333044656667793167356275397766776f746a33306a703031747132762e6a7067.jpeg" alt="img"></a></p>
<p>其中 <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/9b314fc1d7ec1ea7323ca614ff4ce8f8a53638c8f5212ed85d3f58a3b9204104/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f755f25374263253545253742286929253744253744"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f755f25374263253545253742286929253744253744.gif" alt="img"></a>代表与 <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/dbddcd05608e3f574a078725039c22f3d5e57147e6ec0fa467a5cefd6958dcde/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f78253545253742286929253744"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f78253545253742286929253744.gif" alt="img"></a>最近的聚类中心点。 我们的的优化目标便是找出使得代价函数最小的 <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/e033e5ebe1090d476a87e47f73d293fff771a88daa92370ce1cae8381d9b05cd/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f632535452537422831292537442c632535452537422832292537442c2e2e2e2c63253545253742286d29253744"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f632535452537422831292537442c632535452537422832292537442c2e2e2e2c63253545253742286d29253744.gif" alt="img"></a>和 <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/1948c2fc228f4199c86eefe94761404251c7a9c850df13f53ea8e83c1bcf6f3b/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f755f312c755f322c2e2e2e2c755f6b"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f755f312c755f322c2e2e2e2c755f6b.gif" alt="img"></a>。</p>
<h3 id="k值的选择"><a href="#k值的选择" class="headerlink" title="k值的选择"></a>k值的选择</h3><p>在运行 K-均值算法的之前，我们首先要随机初始化所有的聚类中心点，下面介绍怎样做：</p>
<ol>
<li>我们应该选择𝐾 &lt; 𝑚，即聚类中心点的个数要小于所有训练集实例的数量。</li>
<li>随机选择𝐾个训练实例，然后令𝐾个聚类中心分别与这𝐾个训练实例相等K-均值的一个问题在于，它有可能会停留在一个局部最小值处，而这取决于初始化的情况。</li>
</ol>
<p>为了解决这个问题，我们通常需要多次运行 K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行 K-均值的结果，选择代价函数最小的结果。这种方法在𝐾较小的时候（2–10）还是可行的，<strong>但是如果𝐾较大，这么做也可能不会有明显地改善。</strong></p>
<p>没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。选择的时候思考我们运用 K-均值算法聚类的动机是什么。有一个可能会谈及的方法叫作**“肘部法则”**。关 于“肘部法则”，我们所需要做的是改变𝐾值，也就是聚类类别数目的总数。我们用一个聚类来运行 K 均值聚类方法。这就意味着，所有的数据都会分到一个聚类里，然后计算成本函数或者计算畸变函数𝐽。𝐾代表聚类数字。</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/2a1aa5980b6f918d6111f680fbb258057cd365778c05e2aa9d0b195452c31f02/687474703a2f2f7778322e73696e61696d672e636e2f6d773639302f30303633304465666c7931673562377062616561766a3330716f3063777463392e6a7067"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/687474703a2f2f7778322e73696e61696d672e636e2f6d773639302f30303633304465666c7931673562377062616561766a3330716f3063777463392e6a7067.jpeg" alt="img"></a></p>
<p>我们可能会得到一条类似于这样的曲线。像一个人的肘部。这就是“肘部法则”所做的，让我们来看这样一个图，看起来就好像有一个很清楚的肘在那儿。你会发现这种模式，它的畸变值会迅速下降，从 1 到 2，从 2 到 3 之后，你会在 3 的时候达到一个肘点。在此之后，畸变值就下降的非常慢，看起来就像使用 3 个聚类来进行聚类是正确的，**这是因为那个点是曲线的肘点，畸变值下降得很快，𝐾 &#x3D; 3之后就下降得很慢，那么我们就选𝐾 &#x3D; 3。**当你应用“肘部法则”的时候，如果你得到了一个像上面这样的图，那么这将是一种用来选择聚类个数的合理方法。</p>
<h3 id="KNN与K-means区别？"><a href="#KNN与K-means区别？" class="headerlink" title="KNN与K-means区别？"></a>KNN与K-means区别？</h3><p>K最近邻(k-Nearest Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。</p>
<table>
<thead>
<tr>
<th>KNN</th>
<th>K-Means</th>
</tr>
</thead>
<tbody><tr>
<td>1.KNN是分类算法 2.属于监督学习 3.训练数据集是带label的数据</td>
<td>1.K-Means是聚类算法 2.属于非监督学习 3.训练数据集是无label的数据，是杂乱无章的，经过聚类后变得有序，先无序，后有序。</td>
</tr>
<tr>
<td>没有明显的前期训练过程，属于memory based learning</td>
<td>有明显的前期训练过程</td>
</tr>
<tr>
<td>K的含义：一个样本x，对它进行分类，就从训练数据集中，在x附近找离它最近的K个数据点，这K个数据点，类别c占的个数最多，就把x的label设为c。</td>
<td>K的含义：K是人工固定好的数字，假设数据集合可以分为K个蔟，那么就利用训练数据来训练出这K个分类。</td>
</tr>
</tbody></table>
<p><strong>相似点</strong></p>
<p>都包含这样的过程，给定一个点，在数据集中找离它最近的点。即二者都用到了NN(Nears Neighbor)算法思想。</p>
<h3 id="K-Means优缺点及改进"><a href="#K-Means优缺点及改进" class="headerlink" title="K-Means优缺点及改进"></a>K-Means优缺点及改进</h3><p>k-means：在大数据的条件下，会耗费大量的时间和内存。 优化k-means的建议：</p>
<ol>
<li><p>减少聚类的数目K。因为，每个样本都要跟类中心计算距离。</p>
</li>
<li><p>减少样本的特征维度。比如说，通过PCA等进行降维。</p>
</li>
<li><p>考察其他的聚类算法，通过选取toy数据，去测试不同聚类算法的性能。</p>
</li>
<li><p>hadoop集群，K-means算法是很容易进行并行计算的。</p>
</li>
<li><p>算法可能找到局部最优的聚类，而不是全局最优的聚类。使用改进的二分k-means算法。</p>
<p>二分k-means算法：首先将整个数据集看成一个簇，然后进行一次k-means（k&#x3D;2）算法将该簇一分为二，并计算每个簇的误差平方和，选择平方和最大的簇迭代上述过程再次一分为二，直至簇数达到用户指定的k为止，此时可以达到的全局最优。</p>
</li>
</ol>
<h2 id="高斯混合模型-GMM"><a href="#高斯混合模型-GMM" class="headerlink" title="高斯混合模型(GMM)"></a>高斯混合模型(GMM)</h2><h3 id="GMM的思想"><a href="#GMM的思想" class="headerlink" title="GMM的思想"></a>GMM的思想</h3><p>高斯混合模型（Gaussian Mixed Model，GMM）也是一种常见的聚类算法，与K均值算法类似，同样使用了EM算法进行迭代计算。高斯混合模型假设每个簇的数据都是符合高斯分布（又叫正态分布）的，当前<strong>数据呈现的分布就是各个簇的高斯分布叠加在一起的结果。</strong></p>
<p>第一张图是一个数据分布的样例，如果只用一个高斯分布来拟合图中的数据，图 中所示的椭圆即为高斯分布的二倍标准差所对应的椭圆。直观来说，图中的数据 明显分为两簇，因此只用一个高斯分布来拟和是不太合理的，需要推广到用多个 高斯分布的叠加来对数据进行拟合。第二张图是用两个高斯分布的叠加来拟合得到的结果。**这就引出了高斯混合模型，即用多个高斯分布函数的线形组合来对数据分布进行拟合。**理论上，高斯混合模型可以拟合出任意类型的分布。</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/151c479443f80e236e4d8f1155fb6a379c210ebb11257d2b0dc2bdb143cdfe4e/687474703a2f2f7778342e73696e61696d672e636e2f6d773639302f30303633304465666c7931673562386d75396d6a6d6a333069753061706a75682e6a7067"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/687474703a2f2f7778342e73696e61696d672e636e2f6d773639302f30303633304465666c7931673562386d75396d6a6d6a333069753061706a75682e6a7067.jpeg" alt="img"></a></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/687474703a2f2f7778332e73696e61696d672e636e2f6d773639302f30303633304465666c7931673562386d73313031616a333069643061737768682e6a7067.jpeg" alt="img"></p>
<p>高斯混合模型的核心思想是，假设数据可以看作从多个高斯分布中生成出来 的。在该假设下，每个单独的分模型都是标准高斯模型，其均值 $u_i$ 和方差 $\sum_i$ 是待估计的参数。此外，每个分模型都还有一个参数 $\pi_i$，可以理解为权重或生成数据的概 率。高斯混合模型的公式为：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/50743a8ebf2aac441f8b77153400fc46a3df80682c15b94e5d9d94ccb136858e/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f702878293d25354373756d5f253742693d312537442535452537426b25374425354370695f694e2878253743755f692c25354373756d5f6929"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f702878293d25354373756d5f253742693d312537442535452537426b25374425354370695f694e2878253743755f692c25354373756d5f6929.gif" alt="img"></a></p>
<p>通常我们并不能直接得到高斯混合模型的参数，而是观察到了一系列 数据点，给出一个类别的数量K后，希望求得最佳的K个高斯分模型。因此，高斯 混合模型的计算，便成了最佳的均值μ，方差Σ、权重π的寻找，这类问题通常通过 最大似然估计来求解。遗憾的是，此问题中直接使用最大似然估计，得到的是一 个复杂的非凸函数，目标函数是和的对数，难以展开和对其求偏导。</p>
<p>**在这种情况下，可以用EM算法。 **EM算法是在最大化目标函数时，先固定一个变量使整体函数变为凸优化函数，求导得到最值，然后利用最优参数更新被固定的变量，进入下一个循环。具体到高 斯混合模型的求解，EM算法的迭代过程如下。</p>
<p>首先，初始随机选择各参数的值。然后，重复下述两步，直到收敛。</p>
<ul>
<li>E步骤。根据当前的参数，计算每个点由某个分模型生成的概率。</li>
<li>M步骤。使用E步骤估计出的概率，来改进每个分模型的均值，方差和权重。</li>
</ul>
<blockquote>
<p>高斯混合模型是一个生成式模型。可以这样理解数据的生成过程，假设一个最简单的情况，即只有两个一维标准高斯分布的分模型<em>N</em>(0,1)和<em>N</em>(5,1)，其权重分别为0.7和0.3。那么，在生成第一个数据点时，先按照权重的比例，随机选择一个分布，比如选择第一个高斯分布，接着从<em>N</em>(0,1)中生成一个点，如−0.5，便是第一个数据点。在生成第二个数据点时，随机选择到第二个高斯分布<em>N</em>(5,1)，生成了第二个点4.7。如此循环执行，便生成出了所有的数据点。</p>
</blockquote>
<p>也就是说，我们并不知道最佳的K个高斯分布的各自3个参数，也不知道每个 数据点究竟是哪个高斯分布生成的。所以每次循环时，先固定当前的高斯分布不 变，获得每个数据点由各个高斯分布生成的概率。然后固定该生成概率不变，根据数据点和生成概率，获得一个组更佳的高斯分布。循环往复，直到参数的不再变化，或者变化非常小时，便得到了比较合理的一组高斯分布。</p>
<h3 id="GMM与K-Means相比"><a href="#GMM与K-Means相比" class="headerlink" title="GMM与K-Means相比"></a>GMM与K-Means相比</h3><p>高斯混合模型与K均值算法的相同点是：</p>
<ul>
<li>它们都是可用于聚类的算法；</li>
<li>都需要 指定K值；</li>
<li>都是使用EM算法来求解；</li>
<li>都往往只能收敛于局部最优。</li>
</ul>
<p>而它相比于K 均值算法的优点是，可以给出一个样本属于某类的概率是多少；不仅仅可以用于聚类，还可以用于概率密度的估计；并且可以用于生成新的样本点。</p>
<h2 id="聚类算法如何评估"><a href="#聚类算法如何评估" class="headerlink" title="聚类算法如何评估"></a>聚类算法如何评估</h2><p>由于数据以及需求的多样性，没有一种算法能够适用于所有的数据类型、数 据簇或应用场景，似乎每种情况都可能需要一种不同的评估方法或度量标准。例 如，K均值聚类可以用误差平方和来评估，但是基于密度的数据簇可能不是球形， 误差平方和则会失效。在许多情况下，判断聚类算法结果的好坏强烈依赖于主观 解释。尽管如此，聚类算法的评估还是必需的，它是聚类分析中十分重要的部分之一。</p>
<p>聚类评估的任务是估计在数据集上进行聚类的可行性，以及聚类方法产生结 果的质量。这一过程又分为三个子任务。</p>
<ol>
<li><p><strong>估计聚类趋势。</strong></p>
<p>这一步骤是检测数据分布中是否存在非随机的簇结构。如果数据是基本随机 的，那么聚类的结果也是毫无意义的。我们可以观察聚类误差是否随聚类类别数 量的增加而单调变化，如果数据是基本随机的，即不存在非随机簇结构，那么聚 类误差随聚类类别数量增加而变化的幅度应该较不显著，并且也找不到一个合适 的K对应数据的真实簇数。</p>
</li>
<li><p><strong>判定数据簇数。</strong></p>
<p>确定聚类趋势之后，我们需要找到与真实数据分布最为吻合的簇数，据此判定聚类结果的质量。数据簇数的判定方法有很多，例如手肘法和Gap Statistic方 法。需要说明的是，用于评估的最佳数据簇数可能与程序输出的簇数是不同的。 例如，有些聚类算法可以自动地确定数据的簇数，但可能与我们通过其他方法确 定的最优数据簇数有所差别。</p>
</li>
<li><p><strong>测定聚类质量。</strong></p>
<p>在无监督的情况下，我们可以通过考察簇的分离情况和簇的紧 凑情况来评估聚类的效果。定义评估指标可以展现面试者实际解决和分析问题的 能力。事实上测量指标可以有很多种，以下列出了几种常用的度量指标，更多的 指标可以阅读相关文献。</p>
<p>轮廓系数、均方根标准偏差、R方（R-Square）、改进的Hubert统计。</p>
</li>
</ol>
<h1 id="附件"><a href="#附件" class="headerlink" title="附件"></a>附件</h1><h2 id="机器学习阶段参考链接"><a href="#机器学习阶段参考链接" class="headerlink" title="机器学习阶段参考链接"></a>机器学习阶段参考链接</h2><ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_51385258">机器学习-NLP阶段CSDN博客</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning">Github全阶段</a></li>
</ul>
<h2 id="公共基础知识"><a href="#公共基础知识" class="headerlink" title="公共基础知识"></a>公共基础知识</h2><h3 id="标量（Scalar）"><a href="#标量（Scalar）" class="headerlink" title="标量（Scalar）:"></a>标量（Scalar）:</h3><ul>
<li><p>零阶张量，仅包含大小（Magnitude）​ 没有方向（<code>就是单个数值</code>）</p>
</li>
<li><p>数学表示：<em>s</em>∈R（单个实数或复数）</p>
<ul>
<li>例如：质量：5kg、损失函数值：L&#x3D;0.32</li>
</ul>
</li>
</ul>
<h3 id="向量（Vector）"><a href="#向量（Vector）" class="headerlink" title="向量（Vector）"></a><strong>向量（Vector）</strong></h3><ul>
<li>一阶张量，是标量的有序组合，具有 ​<strong>​大小和方向​</strong>​。（<code>就是一个向量，线性代数里的列向量以及行向量</code>）</li>
<li>可表示空间中的点或方向，例如：<ul>
<li>坐标点：(2,3,5)</li>
<li>速度向量：<strong>v</strong>&#x3D;3<strong>i</strong>+4<strong>j</strong></li>
</ul>
</li>
</ul>
<h3 id="张量（Tensor）"><a href="#张量（Tensor）" class="headerlink" title="张量（Tensor）"></a><strong>张量（Tensor）</strong></h3><ul>
<li><strong>定义</strong>：高阶广义数组（标量是0阶，向量是1阶），可表示 ​<strong>​多维数据关系​</strong>​</li>
<li><strong>特性</strong>：<ul>
<li>阶数（Rank）：索引的自由度（如矩阵是2阶）</li>
<li>支持张量积（⊗）、收缩（Contraction）等运算</li>
<li>内存占用随维度指数增长</li>
</ul>
</li>
<li><strong>示例</strong>：<ul>
<li>矩阵（2阶张量）：$A∈R^{3×3}$(三阶方阵)</li>
<li>RGB图像（3阶张量）：$I∈R^{H×W×3}$</li>
<li>时间序列视频（4阶张量）：$V∈R^{T×H×W×3}$</li>
</ul>
</li>
</ul>
<h4 id="三者区别"><a href="#三者区别" class="headerlink" title="三者区别"></a><strong>三者区别</strong></h4><table>
<thead>
<tr>
<th align="center">特性</th>
<th align="center">标量</th>
<th align="center">向量</th>
<th align="center">张量</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>阶数</strong></td>
<td align="center">0阶</td>
<td align="center">1阶</td>
<td align="center"><em>k</em>阶（<em>k</em>≥2）</td>
</tr>
<tr>
<td align="center"><strong>方向性</strong></td>
<td align="center">无</td>
<td align="center">有</td>
<td align="center">多维方向</td>
</tr>
<tr>
<td align="center"><strong>存储结构</strong></td>
<td align="center">单个数值</td>
<td align="center">一维数组</td>
<td align="center">多维数组</td>
</tr>
<tr>
<td align="center"><strong>运算</strong></td>
<td align="center">算术运算</td>
<td align="center">线性代数运算</td>
<td align="center">张量分解&#x2F;收缩</td>
</tr>
<tr>
<td align="center"><strong>PyTorch表示</strong></td>
<td align="center"><code>torch.tensor(3)</code></td>
<td align="center"><code>torch.tensor([1,2])</code></td>
<td align="center"><code>torch.rand(2,3,4)</code></td>
</tr>
</tbody></table>
<h4 id="图形展示"><a href="#图形展示" class="headerlink" title="图形展示"></a><strong>图形展示</strong></h4><pre class="mermaid">graph LR
    A[标量] -->|升维| B[向量]
    B -->|升维| C[矩阵]
    C -->|升维| D[3阶张量]</pre>

<h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a><strong>应用场景</strong></h4><ul>
<li><strong>标量</strong>：损失值、阈值参数</li>
<li><strong>向量</strong>：特征表示、词嵌入（Word2Vec）</li>
<li>张量：<ul>
<li>计算机视觉：卷积核（4阶：<code>[out_channels, in_channels, h, w]</code>）</li>
<li>自然语言处理：注意力权重矩阵（<code>[batch_size, seq_len, seq_len]</code>）</li>
</ul>
</li>
</ul>
<h2 id="Latex数学语法支持"><a href="#Latex数学语法支持" class="headerlink" title="Latex数学语法支持"></a>Latex数学语法支持</h2><ul>
<li><a target="_blank" rel="noopener" href="https://blog.kevinchu.top/2023/09/12/hexo-supports-latex/">https://blog.kevinchu.top/2023/09/12/hexo-supports-latex/</a></li>
<li>矩阵在写MarkDown的时候可能会出现异常，用&lt;span&gt;&lt;span&gt;包裹起来就没事了</li>
</ul>
<h2 id="导数-矩阵-向量"><a href="#导数-矩阵-向量" class="headerlink" title="导数&amp;矩阵&amp;向量"></a>导数&amp;矩阵&amp;向量</h2><h3 id="常见函数的导数："><a href="#常见函数的导数：" class="headerlink" title="常见函数的导数："></a>常见函数的导数：</h3><table>
<thead>
<tr>
<th align="center">公式</th>
<th align="center">例子</th>
</tr>
</thead>
<tbody><tr>
<td align="center">$(C)^\prime&#x3D;0$</td>
<td align="center">$\left(5\right)^\prime&#x3D;0$         $\left(10\right)^\prime&#x3D;0$</td>
</tr>
<tr>
<td align="center">$\left(x^\alpha\right)^\prime&#x3D;\alpha x^{\alpha-1}$</td>
<td align="center">$\left(x^3\right)^\prime&#x3D;3 x^{2}$      $\left(x^5\right)^\prime&#x3D;5 x^{4}$</td>
</tr>
<tr>
<td align="center">$\left(a^x\right)^\prime&#x3D;a^{x}\ln{a}$</td>
<td align="center">$\left(2^x\right)^\prime&#x3D;2^x\ln{2}$      $\left(7^x\right)^\prime&#x3D;7^x\ln{7}$</td>
</tr>
<tr>
<td align="center">$\left(e^x\right)^\prime&#x3D;e^{x}$</td>
<td align="center">$\left(e^x\right)^\prime&#x3D;e^{x}$</td>
</tr>
<tr>
<td align="center">$\left(\log{_a}x\right)^\prime&#x3D;\frac{1}{x\ln{a}}$</td>
<td align="center">$\left(\log{_{10}}x\right)^\prime&#x3D;\frac{1}{x\ln{10}}$</td>
</tr>
<tr>
<td align="center">$\left(\ln{x}\right)^\prime&#x3D;\frac{1}{x}$</td>
<td align="center">$\left(\ln{x}\right)^\prime&#x3D;\frac{1}{x}$</td>
</tr>
<tr>
<td align="center">$\left(\sin{x}\right)^\prime&#x3D;\cos{x}$</td>
<td align="center">$\left(\sin{x}\right)^\prime&#x3D;\cos{x}$</td>
</tr>
<tr>
<td align="center">$\left(\cos{x}\right)^\prime&#x3D;-\sin{x}$</td>
<td align="center">$\left(\cos{x}\right)^\prime&#x3D;-\sin{x}$</td>
</tr>
</tbody></table>
<h3 id="导数的四则运算："><a href="#导数的四则运算：" class="headerlink" title="导数的四则运算："></a>导数的四则运算：</h3><table>
<thead>
<tr>
<th align="center">公式</th>
<th align="center">例子</th>
</tr>
</thead>
<tbody><tr>
<td align="center">$\left[u(x)\pm v(x)\right]^\prime&#x3D;u^\prime(x) \pm v^\prime(x)$</td>
<td align="center">$(e^x+4\ln{x})^\prime&#x3D;(e^x)^\prime+(4\ln{x})^\prime&#x3D;e^x+\frac{4}{x}$</td>
</tr>
<tr>
<td align="center">$\left[u(x)\cdot v(x)\right]^\prime&#x3D;u^\prime(x) \cdot v(x) + u(x) \cdot v^\prime(x)$</td>
<td align="center">$(\sin{x}\cdot\ln{x})^\prime&#x3D;\cos{x}\cdot\ln{x}+\sin{x}\cdot\frac{1}{x}$</td>
</tr>
<tr>
<td align="center">$\left[\frac{u(x)}{v(x)}\right]^\prime&#x3D;\frac{u^\prime(x) \cdot v(x) - u(x) \cdot v^\prime(x)}{v^2(x)}$</td>
<td align="center">$\left(\frac{e^x}{\cos{x}}\right)^\prime&#x3D;\frac{e^x\cdot\cos{x}-e^x\cdot(-\sin{x})}{cos^2(x)}$</td>
</tr>
<tr>
<td align="center">${g[h(x)]}^\prime&#x3D;g^\prime(h)*h^\prime(x)$</td>
<td align="center">$(\sin{2x})^\prime&#x3D;\cos{2x}\cdot(2x)^\prime&#x3D;2\cos(2x)$</td>
</tr>
</tbody></table>
<h3 id="复合函数求导："><a href="#复合函数求导：" class="headerlink" title="复合函数求导："></a>复合函数求导：</h3><blockquote>
<p>链式法则：先对外函数求导，再对内函数求导</p>
</blockquote>
<p>例如：计算函数 $y &#x3D; (x^{2} + 2x)^{2}$ 的导函数：<br>$$<br>\begin{aligned}<br>y’ &amp;&#x3D; 2(x^{2} + 2x)^{(2-1)} \cdot (x^{2} + 2x)’ \<br>   &amp;&#x3D; 2(x^{2} + 2x)(2x + 2) \<br>   &amp;&#x3D; 4(x^{3} + 3x^{2} + 2x) \<br>   &amp;&#x3D; 4x^{3} + 12x^{2} + 8x<br>\end{aligned}<br>$$</p>
<h2 id="对数函数"><a href="#对数函数" class="headerlink" title="对数函数"></a>对数函数</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250609111130085-20250609204542024.png" alt="image-20250609111130085"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250609110804572-20250609204542047.png" alt="image-20250609110804572"></p>
<h2 id="二元一次方程极值问题"><a href="#二元一次方程极值问题" class="headerlink" title="二元一次方程极值问题"></a>二元一次方程极值问题</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250607143836920-20250608222015992.png" alt="image-20250607143836920"></p>
<h2 id="概率"><a href="#概率" class="headerlink" title="概率"></a>概率</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250609190507660-20250609204542069.png" alt="image-20250609190507660"></p>
<h3 id="条件概率补充"><a href="#条件概率补充" class="headerlink" title="条件概率补充"></a>条件概率补充</h3><p><strong>公式：</strong>$$ P(B \mid A) &#x3D; \frac{P(AB)}{P(A)} $$</p>
<ul>
<li><p>$P(B \mid A)$：在事件A发生的条件下，事件B发生的概率 </p>
</li>
<li><p>$P(AB)$：事件A和事件B同时发生的联合概率 </p>
</li>
<li><p>$P(A)$：事件A发生的边际概率</p>
</li>
</ul>
<h3 id="全概率公式"><a href="#全概率公式" class="headerlink" title="全概率公式"></a>全概率公式</h3><blockquote>
<p>如果事件$B_{1}、B_{2}、B_{3}…B_{i}$构成一个完备事件组，即它们两两互不相容，其和为全集；并且$P(B_{i})$大于0，则对任一事件A有</p>
<p>$$ P(A) &#x3D; P(A \mid B_{1})P(B_{1}) + P(A \mid B_{2})P(B_{2}) + \cdots + P(A \mid B_{i})P(B_{i}) $$</p>
</blockquote>
<p>公式：$$ P(A) &#x3D; \sum_{i&#x3D;1}^{n} P(A|B_i) \cdot P(B_i) $$</p>
<h3 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h3><blockquote>
<p>也就是在已知结果的情况下，倒推某个事件发生的概率</p>
</blockquote>
<p>$$ P(A|B) &#x3D; \frac{P(B|A) \cdot P(A)}{P(B)} $$</p>
<p>使用全概率公式展开</p>
<p>$$ P(A|B) &#x3D; \frac{P(B|A) \cdot P(A)}{\sum_{i&#x3D;1}^{n} P(B|A_i) \cdot P(A_i)} $$</p>
<h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>参考链接：</p>
<p>极大释然估计：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41775769/article/details/113514294">https://blog.csdn.net/qq_41775769/article/details/113514294</a></p>
<h3 id="离散型统计模型"><a href="#离散型统计模型" class="headerlink" title="离散型统计模型"></a>离散型统计模型</h3><p>$$ L(\theta) &#x3D; \prod_{i&#x3D;1}^{n} P_{\theta}(X_{i}&#x3D;x_{i}) \quad i&#x3D;1,2,3,\cdots,n $$</p>
<ul>
<li>$L(\theta)$：似然函数</li>
<li>$\prod$：连乘符号</li>
<li>$P_{\theta}(X_{i}&#x3D;x_{i})$：在参数$\theta$下随机变量$X_i$取$x_i$的概率</li>
</ul>
<h3 id="连续型统计模型"><a href="#连续型统计模型" class="headerlink" title="连续型统计模型"></a>连续型统计模型</h3><p>$$ L(\theta) &#x3D; \prod_{i&#x3D;1}^{n} f(x_{i};\theta) \quad i&#x3D;1,2,3,\cdots,n $$</p>
<p><strong>参数说明</strong>：</p>
<ul>
<li>$f(x_{i};\theta)$：概率密度函数</li>
<li>其他符号含义与离散型相同</li>
</ul>
<p>从一个直观的例子理解极大似然估计，比如：在一个未知的袋子里摸球，现有的认知告诉我们是袋子里面的球要么是红色，要么是蓝色。于是我们可以知道从该袋子中摸球颜色的概率服从二项分布如下：</p>
<table>
<thead>
<tr>
<th>$X$</th>
<th>红色</th>
<th>蓝色</th>
</tr>
</thead>
<tbody><tr>
<td>$P$</td>
<td>$θ$</td>
<td>$1-θ$</td>
</tr>
</tbody></table>
<p>由于不知道袋子中究竟有多少个球以及每个颜色的球有多少个，所以无法对参数θ进行计算，也不能计算出摸到哪种颜色的球的概率是多少？于是，假设有一个测试人员对袋内球进行有放回的抽取，进行了100次随机测验之后，统计得出：有30次摸到的是红球，有70次摸到的是蓝球。</p>
<p>从现有的测试结果出发，我们有理由相信袋子中球的比例大概是红色 : 蓝色&#x3D;3 : 7（也就是背后的理论支撑）。所以进而求出概率以及参数 θ&#x3D;0.3 。也就是用抽样时球的颜色出现的频率近似等于概率。<br>注意：<code>极大似然估计中采样需满足一个重要的假设，就是所有的采样都是独立同分布的</code>。</p>
<h3 id="最大释然估计统计方法"><a href="#最大释然估计统计方法" class="headerlink" title="最大释然估计统计方法"></a>最大释然估计统计方法</h3><p>进行$n$次独立随机试验，观测到：</p>
<ul>
<li>“状态1”发生 $n_1$ 次</li>
<li>“状态2”发生 $n_2$ 次<br>（满足 $n_1 + n_2 &#x3D; n$）</li>
</ul>
<p>从频率学派角度，可得经验概率：<br>$$ P_{\text{empirical}}(\text{状态1}) &#x3D; \frac{n_1}{n} $$</p>
<p>为建立公理化描述，定义似然函数：<br>$$ L(\theta) &#x3D; \theta^{n_1}(1-\theta)^{n_2} \quad \text{其中} \quad n_1 + n_2 &#x3D; n $$</p>
<p>首先对似然函数取自然对数（$\ln$）进行化简：</p>
<p>$$ \ln L(\theta) &#x3D; n_1 \ln \theta + n_2 \ln (1 - \theta) $$</p>
<p>对对数似然函数关于$\theta$求导并令导数为零：</p>
<p>$$ \frac{d \ln L(\theta)}{d \theta} &#x3D; \frac{n_1}{\theta} + \frac{n_2}{1 - \theta} &#x3D; 0 $$</p>
<p>求解上述方程得到$\theta$的极大似然估计：</p>
<p>$$ \hat{\theta} &#x3D; \frac{n_1}{n_1 + n_2} &#x3D; \frac{n_1}{n} $$</p>
<p><strong>其中</strong>：$n &#x3D; n_1 + n_2$为总试验次数</p>
<h3 id="做道例题"><a href="#做道例题" class="headerlink" title="做道例题"></a>做道例题</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250609201334679-20250609204542100.png" alt="image-20250609201334679"></p>
<h2 id="泰勒公式及常见泰勒展开式"><a href="#泰勒公式及常见泰勒展开式" class="headerlink" title="泰勒公式及常见泰勒展开式"></a>泰勒公式及常见泰勒展开式</h2><h3 id="带拉格朗日型余项的泰勒公式"><a href="#带拉格朗日型余项的泰勒公式" class="headerlink" title="带拉格朗日型余项的泰勒公式"></a>带拉格朗日型余项的泰勒公式</h3><p>$$ f(x)&#x3D;f(x_{0})+\frac{f^{\prime}(x_{0})}{1 !}(x-x_{0})+\frac{f^{\prime\prime}(x_{0})}{2 !}(x-x_{0})^{2}+\cdots+\frac{f^{(n)}(x_{0})}{n !}(x-x_{0})^{n}+R_{n}(x)$$</p>
<p>在点$x_0$处的泰勒展开式，包含$n$阶导数项和拉格朗日型余项</p>
<h3 id="拉格朗日型余项表达式"><a href="#拉格朗日型余项表达式" class="headerlink" title="拉格朗日型余项表达式"></a>拉格朗日型余项表达式</h3><p>$$ R_{n}(x)&#x3D;o\left[(x-x_{0})^{n}\right] $$</p>
<p>表示余项是$(x-x_0)^n$的高阶无穷小</p>
<h3 id="带佩亚诺型余项的麦克劳林公式"><a href="#带佩亚诺型余项的麦克劳林公式" class="headerlink" title="带佩亚诺型余项的麦克劳林公式"></a>带佩亚诺型余项的麦克劳林公式</h3><p>$$ f(x)&#x3D;f(0)+\frac{f^{\prime}(0)}{1 !}x+\frac{f^{\prime\prime}(0)}{2 !}x^{2}+\cdots+\frac{f^{(n)}(0)}{n !}x^{n}+R_{n}(x)$$</p>
<p>麦克劳林公式（$x_0&#x3D;0$的特殊情况）</p>
<h3 id="佩亚诺型余项表达式"><a href="#佩亚诺型余项表达式" class="headerlink" title="佩亚诺型余项表达式"></a>佩亚诺型余项表达式</h3><p>$$ R_{n}(x)&#x3D;o\left[x^{n}\right]$$</p>
<p>佩亚诺余项表示$x^n$的高阶无穷小</p>
<h3 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h3><ul>
<li>$f^{(n)}(x_0)$：函数在$x_0$处的$n$阶导数</li>
<li>$n!$：$n$的阶乘</li>
<li>$o[\cdot]$：高阶无穷小符号</li>
</ul>
<h3 id="常见函数的泰勒展开式"><a href="#常见函数的泰勒展开式" class="headerlink" title="常见函数的泰勒展开式"></a>常见函数的泰勒展开式</h3><ul>
<li>指数函数</li>
</ul>
<p>$$ e^{x}&#x3D;1+\frac{1}{1 !}x+\frac{1}{2 !}x^{2}+\frac{1}{3 !}x^{3}+o\left(x^{3}\right) $$</p>
<ul>
<li>对数函数</li>
</ul>
<p>$$ \ln (1+x)&#x3D;x-\frac{1}{2} x^{2}+\frac{1}{3} x^{3}+o\left(x^{3}\right) $$</p>
<ul>
<li>正弦函数</li>
</ul>
<p>$$ \sin x&#x3D;x-\frac{1}{3 !}x^{3}+\frac{1}{5 !}x^{5}+o\left(x^{5}\right) $$</p>
<ul>
<li>余弦函数</li>
</ul>
<p>$$ \cos x&#x3D;1-\frac{1}{2 !}x^{2}+\frac{1}{4 !}x^{4}+o\left(x^{4}\right) $$</p>
<ul>
<li>反正弦函数</li>
</ul>
<p>$$ \arcsin x&#x3D;x+\frac{1}{2}\times\frac{x^{3}}{3}+\frac{1\times 3}{2\times 4}\times\frac{x^{5}}{5}+\frac{1\times 3\times 5}{2\times 4\times 6}\times\frac{x^{7}}{7}+o\left(x^{7}\right) $$</p>
<ul>
<li>分式函数</li>
</ul>
<p>$$ \frac{1}{1-x}&#x3D;1+x+x^{2}+x^{3}+o\left(x^{3}\right) $$</p>
<p>$$ \frac{1}{1+x} &#x3D; 1 - x + x^{2} - \cdots + (-1)^{n}x^{n} $$</p>
<ul>
<li>幂函数</li>
</ul>
<p>$$ (1+x)^{a}&#x3D;1+\frac{a}{1 !}x+\frac{a(a-1)}{2 !}x^{2}+\frac{a(a-1)(a-2)}{3 !}x^{3}+o\left(x^{3}\right) $$</p>
<ul>
<li>对数函数展开</li>
</ul>
<p>$$ \ln(1+x) &#x3D; x - \frac{x^{2}}{2} + \cdots + (-1)^{n-1}\frac{x^{n}}{n} + o(x^{n}) $$</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://liamjohnson-w.github.io">李俊泽</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://liamjohnson-w.github.io/2025/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">https://liamjohnson-w.github.io/2025/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/AI/">AI</a></div><div class="post_share"><div class="social-share" data-image="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1d5d1a46-65f8-4544-8195-231e2c2da969.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> Donate</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/06/07/github-push%E5%87%BA%E9%94%99/" title="Github之Push问题解决方案"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250607224051237.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Github之Push问题解决方案</div></div></a></div><div class="next-post pull-right"><a href="/2025/06/03/2025.06.03/" title="OLLAMA"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250603132906789.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">OLLAMA</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2025/06/03/2025.06.03/" title="OLLAMA"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250603132906789.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-03</div><div class="title">OLLAMA</div></div></a></div><div><a href="/2025/10/25/Agent/" title="Agent-Note"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/Pusheen1121.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-25</div><div class="title">Agent-Note</div></div></a></div><div><a href="/2025/10/30/Agent_all/" title="Agent"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/Pusheen12222.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-30</div><div class="title">Agent</div></div></a></div><div><a href="/2025/06/22/NLP_Base/" title="NLP自然语言处理"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/%E7%94%9F%E6%88%90%E7%8C%AB%E5%92%AA%E5%9B%BE%E7%89%87%20(2).png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-22</div><div class="title">NLP自然语言处理</div></div></a></div><div><a href="/2025/11/02/Fine-Tuning/" title="Fine-Tuning"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen11121.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-02</div><div class="title">Fine-Tuning</div></div></a></div><div><a href="/2025/07/18/Project/" title="Jason Project Demo"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="title">Jason Project Demo</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/tx.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">李俊泽</div><div class="author-info__description">机器都在学习,你有什么理由不学习?</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">240</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">58</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/weiswift/"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/weiswift" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:1265019024@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">博客为本人搭建 Github托管 仅记录学习过程 不做引流 不做排名 不打广告！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#AI%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD"><span class="toc-number">1.</span> <span class="toc-text">AI人工智能</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E5%88%86%E7%B1%BB"><span class="toc-number">1.1.</span> <span class="toc-text">人工智能的分类</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="toc-number">2.</span> <span class="toc-text">机器学习基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">2.1.</span> <span class="toc-text">机器学习的概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E2%AD%90%EF%B8%8F"><span class="toc-number">2.2.</span> <span class="toc-text">机器学习工作流程⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-number">2.2.1.</span> <span class="toc-text">获取数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">2.2.2.</span> <span class="toc-text">数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-number">2.2.3.</span> <span class="toc-text">特征工程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%EF%BC%89"><span class="toc-number">2.2.4.</span> <span class="toc-text">机器学习（模型训练）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-number">2.2.5.</span> <span class="toc-text">模型评估</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E8%AF%AF%E5%B7%AE%EF%BC%88MAE%EF%BC%89"><span class="toc-number">2.2.5.1.</span> <span class="toc-text">1. 平均绝对误差（MAE）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE%EF%BC%88MSE%EF%BC%89"><span class="toc-number">2.2.5.2.</span> <span class="toc-text">2. 均方误差（MSE）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%9D%87%E6%96%B9%E6%A0%B9%E8%AF%AF%E5%B7%AE%EF%BC%88RMSE%EF%BC%89"><span class="toc-number">2.2.5.3.</span> <span class="toc-text">3. 均方根误差（RMSE）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%8F%AF%E5%86%B3%E7%B3%BB%E6%95%B0%EF%BC%88R%C2%B2%EF%BC%89"><span class="toc-number">2.2.5.4.</span> <span class="toc-text">4. 可决系数（R²）</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Sklearn%E4%B8%8E%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E2%AD%90%EF%B8%8F"><span class="toc-number">3.</span> <span class="toc-text">Sklearn与特征工程⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#sklearn-%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BB%84%E6%88%90"><span class="toc-number">3.1.</span> <span class="toc-text">sklearn 的核心组成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sklearn%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97-%E9%B8%A2%E5%B0%BE%E8%8A%B1%E6%A1%88%E4%BE%8B%E7%9A%84%E5%85%B8%E5%9E%8B%E4%BB%A3%E7%A0%81%E6%B5%81%E7%A8%8B"><span class="toc-number">3.2.</span> <span class="toc-text">sklearn随机森林-鸢尾花案例的典型代码流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#sklear%E5%85%B8%E5%9E%8B%E6%B5%81%E7%A8%8B-%E5%B8%A6%E6%B3%A8%E9%87%8A%E8%AF%A6%E8%A7%A3"><span class="toc-number">3.2.1.</span> <span class="toc-text">sklear典型流程(带注释详解)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E9%80%89%E6%8B%A9%E4%B8%8E%E8%B0%83%E4%BC%98"><span class="toc-number">3.3.</span> <span class="toc-text">超参数选择与调优</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">3.3.1.</span> <span class="toc-text">交叉验证</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2"><span class="toc-number">3.3.2.</span> <span class="toc-text">网格搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81-%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2%EF%BC%88%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E5%92%8C%E8%B0%83%E4%BC%98%EF%BC%89"><span class="toc-number">3.3.3.</span> <span class="toc-text">交叉验证+网格搜索（模型选择和调优）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%B8%A2%E5%B0%BE%E8%8A%B1%E6%A1%88%E4%BE%8B-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-K%E5%80%BC%E8%B0%83%E4%BC%98"><span class="toc-number">3.4.</span> <span class="toc-text">鸢尾花案例(数据预处理+K值调优)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%AD%97%EF%BC%88%E5%9B%BE%E5%83%8F%EF%BC%89%E8%AF%86%E5%88%AB%E6%A1%88%E4%BE%8B"><span class="toc-number">3.5.</span> <span class="toc-text">数字（图像）识别案例</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#KNN%E7%AE%97%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">KNN算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#KNN%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="toc-number">4.1.</span> <span class="toc-text">KNN算法流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#K%E5%80%BC%E5%A4%A7%E5%B0%8F%E5%AF%B9%E4%BA%8E%E8%AE%AD%E7%BB%83%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">4.1.1.</span> <span class="toc-text">K值大小对于训练的影响</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB"><span class="toc-number">4.2.</span> <span class="toc-text">欧式距离</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E7%BB%B4%E7%A9%BA%E9%97%B4"><span class="toc-number">4.2.1.</span> <span class="toc-text">二维空间</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E7%BB%B4%E7%A9%BA%E9%97%B4"><span class="toc-number">4.2.2.</span> <span class="toc-text">三维空间</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#n%E7%BB%B4%E7%A9%BA%E9%97%B4"><span class="toc-number">4.2.3.</span> <span class="toc-text">n维空间</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sklearn-KNN%E7%AE%97%E6%B3%95"><span class="toc-number">4.3.</span> <span class="toc-text">sklearn-KNN算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">4.3.1.</span> <span class="toc-text">分类问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98"><span class="toc-number">4.3.2.</span> <span class="toc-text">回归问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%84%E7%A7%8D%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F"><span class="toc-number">4.4.</span> <span class="toc-text">各种距离计算方式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB-Euclidean-Distance"><span class="toc-number">4.4.1.</span> <span class="toc-text">欧式距离(Euclidean Distance)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9B%BC%E5%93%88%E9%A1%BF%E8%B7%9D%E7%A6%BB-Manhattan-Distance"><span class="toc-number">4.4.2.</span> <span class="toc-text">曼哈顿距离(Manhattan Distance)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%87%E6%AF%94%E9%9B%AA%E5%A4%AB%E8%B7%9D%E7%A6%BB-Chebyshev-Distance"><span class="toc-number">4.4.3.</span> <span class="toc-text">切比雪夫距离 (Chebyshev Distance)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%87%E5%87%86%E5%8C%96%E6%AC%A7%E6%B0%8F%E8%B7%9D%E7%A6%BB-Standardized-EuclideanDistance"><span class="toc-number">4.4.4.</span> <span class="toc-text">标准化欧氏距离 (Standardized EuclideanDistance)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%99%E5%BC%A6%E8%B7%9D%E7%A6%BB-Cosine-Distance"><span class="toc-number">4.4.5.</span> <span class="toc-text">余弦距离(Cosine Distance)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB-Hamming-Distance"><span class="toc-number">4.4.6.</span> <span class="toc-text">汉明距离(Hamming Distance)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%B0%E5%8D%A1%E5%BE%B7%E8%B7%9D%E7%A6%BB-Jaccard-Distance"><span class="toc-number">4.4.7.</span> <span class="toc-text">杰卡德距离(Jaccard Distance)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%B5%E5%8F%AF%E5%A4%AB%E6%96%AF%E5%9F%BA%E8%B7%9D%E7%A6%BB-Minkowski-Distance"><span class="toc-number">4.4.8.</span> <span class="toc-text">闵可夫斯基距离(Minkowski Distance)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KD%E6%A0%91"><span class="toc-number">4.5.</span> <span class="toc-text">KD树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#KD%E6%A0%91%E7%9A%84%E6%9E%84%E5%BB%BA%EF%BC%88%E4%BA%8C%E7%BB%B4%E5%B9%B3%E9%9D%A2%EF%BC%89"><span class="toc-number">4.5.1.</span> <span class="toc-text">KD树的构建（二维平面）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#KD%E6%A0%91%E7%9A%84%E5%BF%AB%E9%80%9F%E6%9C%80%E8%BF%91%E9%82%BB%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95"><span class="toc-number">4.5.2.</span> <span class="toc-text">KD树的快速最近邻搜索算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#KD%E6%A0%91%E7%9A%84%E6%8F%92%E5%85%A5"><span class="toc-number">4.5.3.</span> <span class="toc-text">KD树的插入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#KD%E6%A0%91%E7%9A%84%E5%88%A0%E9%99%A4"><span class="toc-number">4.5.4.</span> <span class="toc-text">KD树的删除</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">5.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">5.1.</span> <span class="toc-text">一元线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E6%96%B9%E7%A8%8B%E8%A7%A3%E6%B3%95%EF%BC%88%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%EF%BC%89"><span class="toc-number">5.1.1.</span> <span class="toc-text">回归方程解法（最小二乘法）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">5.2.</span> <span class="toc-text">多元线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E6%96%B9%E7%A8%8B%E8%A7%A3%E6%B3%95%EF%BC%88%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B3%95%EF%BC%89"><span class="toc-number">5.2.1.</span> <span class="toc-text">回归方程解法（正规方程法）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E5%BA%94%E7%94%A8"><span class="toc-number">5.2.2.</span> <span class="toc-text">正规方程应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">5.3.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%9D%E5%AF%B9%E5%80%BC%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0-Mean-Absolute-Error-MAE-%EF%BC%9A"><span class="toc-number">5.3.1.</span> <span class="toc-text">绝对值误差函数(Mean Absolute Error, MAE)：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0-Mean-Squared-Error-MSE-%EF%BC%9A"><span class="toc-number">5.3.2.</span> <span class="toc-text">均方误差函数(Mean Squared Error, MSE)：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E6%A0%B9%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0-Root-Mean-Squared-Error-RMSE"><span class="toc-number">5.3.3.</span> <span class="toc-text">均方根误差函数(Root Mean Squared Error ,RMSE)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E2%AD%90%EF%B8%8F%E2%AD%90%EF%B8%8F%E2%AD%90%EF%B8%8F"><span class="toc-number">5.4.</span> <span class="toc-text">梯度下降算法⭐️⭐️⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E5%8F%98%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%A4%BA%E4%BE%8B"><span class="toc-number">5.4.1.</span> <span class="toc-text">单变量梯度下降示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95"><span class="toc-number">5.4.2.</span> <span class="toc-text">多元线性回归梯度算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E8%B0%83%E4%BC%98"><span class="toc-number">5.4.3.</span> <span class="toc-text">梯度下降算法调优</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%B9%E8%BF%9B%E7%9A%84%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95"><span class="toc-number">5.4.4.</span> <span class="toc-text">改进的梯度算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">5.5.</span> <span class="toc-text">正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3"><span class="toc-number">5.5.1.</span> <span class="toc-text">过拟合如何解决</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L2%E6%AD%A3%E5%88%99%E5%8C%96-Ridge%E5%B2%AD%E5%9B%9E%E5%BD%92"><span class="toc-number">5.5.2.</span> <span class="toc-text">L2正则化(Ridge岭回归)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L1%E6%AD%A3%E5%88%99%E5%8C%96-Lasso%E5%9B%9E%E5%BD%92"><span class="toc-number">5.5.3.</span> <span class="toc-text">L1正则化(Lasso回归)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ElasticNet%E5%9B%9E%E5%BD%92"><span class="toc-number">5.5.4.</span> <span class="toc-text">ElasticNet回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E8%A6%81%E6%B1%82%E5%9B%A0%E5%8F%98%E9%87%8F%E6%9C%8D%E4%BB%8E%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%EF%BC%9F"><span class="toc-number">5.5.5.</span> <span class="toc-text">线性回归要求因变量服从正态分布？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%E2%AD%90%EF%B8%8F"><span class="toc-number">5.6.</span> <span class="toc-text">回归模型评估方法⭐️</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E6%A1%88%E4%BE%8B-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%8F%8AAPI"><span class="toc-number">5.7.</span> <span class="toc-text">波士顿房价案例(线性回归及API)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B1%9E%E6%80%A7"><span class="toc-number">5.7.1.</span> <span class="toc-text">数据集属性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90%E6%AD%A5%E9%AA%A4"><span class="toc-number">5.7.2.</span> <span class="toc-text">案例分析步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">5.7.3.</span> <span class="toc-text">梯度下降法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B3%95"><span class="toc-number">5.7.4.</span> <span class="toc-text">正规方程法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">6.</span> <span class="toc-text">逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Sigmoid-%E5%87%BD%E6%95%B0"><span class="toc-number">6.1.</span> <span class="toc-text">Sigmoid 函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-1"><span class="toc-number">6.2.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92API"><span class="toc-number">6.3.</span> <span class="toc-text">逻辑回归API</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%AF%E4%BB%A5%E8%BF%9B%E8%A1%8C%E5%A4%9A%E5%88%86%E7%B1%BB%E5%90%97%EF%BC%9F"><span class="toc-number">6.4.</span> <span class="toc-text">可以进行多分类吗？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%E2%AD%90%EF%B8%8F"><span class="toc-number">6.5.</span> <span class="toc-text">分类评估方法⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5%E5%8F%8A%E5%85%B6%E6%9E%84%E5%BB%BA"><span class="toc-number">6.5.1.</span> <span class="toc-text">混淆矩阵及其构建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E6%AD%A3%E4%BE%8B%E5%81%87%E4%BE%8B%E4%B8%BE%E4%BE%8B%EF%BC%9A"><span class="toc-number">6.5.2.</span> <span class="toc-text">模型预测正例假例举例：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Precision%EF%BC%88%E7%B2%BE%E7%A1%AE%E7%8E%87%EF%BC%89"><span class="toc-number">6.5.3.</span> <span class="toc-text">Precision（精确率）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Recall%EF%BC%88%E5%8F%AC%E5%9B%9E%E7%8E%87%EF%BC%89"><span class="toc-number">6.5.4.</span> <span class="toc-text">Recall（召回率）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#F1-score"><span class="toc-number">6.5.5.</span> <span class="toc-text">F1-score</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-number">6.5.6.</span> <span class="toc-text">代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ROC%E6%9B%B2%E7%BA%BF%E5%92%8CAUC%E6%8C%87%E6%A0%87"><span class="toc-number">6.5.7.</span> <span class="toc-text">ROC曲线和AUC指标</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%B5%E4%BF%A1%E5%AE%A2%E6%88%B7%E6%B5%81%E5%A4%B1%E9%A2%84%E6%B5%8B"><span class="toc-number">6.6.</span> <span class="toc-text">电信客户流失预测</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">7.</span> <span class="toc-text">决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">7.1.</span> <span class="toc-text">构建决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Notice"><span class="toc-number">7.1.1.</span> <span class="toc-text">Notice</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ID3%E7%AE%97%E6%B3%95%E6%9E%84%E5%BB%BA%E5%86%B3%E7%AD%96%E6%A0%91%E2%AD%90%EF%B8%8F"><span class="toc-number">7.2.</span> <span class="toc-text">ID3算法构建决策树⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E7%86%B5"><span class="toc-number">7.2.1.</span> <span class="toc-text">信息熵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A"><span class="toc-number">7.2.2.</span> <span class="toc-text">信息增益</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E8%AE%A1%E7%AE%97%E6%A1%88%E4%BE%8B"><span class="toc-number">7.2.3.</span> <span class="toc-text">信息增益计算案例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ID3%E6%A0%91%E6%9E%84%E5%BB%BA%E6%B5%81%E7%A8%8B"><span class="toc-number">7.2.4.</span> <span class="toc-text">ID3树构建流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#C4-5%E7%AE%97%E6%B3%95%E6%9E%84%E5%BB%BA%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">7.3.</span> <span class="toc-text">C4.5算法构建决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%8E%87"><span class="toc-number">7.3.1.</span> <span class="toc-text">信息增益率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C4-5%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA%E6%B5%81%E7%A8%8B"><span class="toc-number">7.3.2.</span> <span class="toc-text">C4.5决策树构建流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CART%E7%AE%97%E6%B3%95%E6%9E%84%E5%BB%BA%E5%86%B3%E7%AD%96%E6%A0%91%E2%AD%90%EF%B8%8F"><span class="toc-number">7.4.</span> <span class="toc-text">CART算法构建决策树⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Cart%E6%A0%91%E7%AE%80%E4%BB%8B"><span class="toc-number">7.4.1.</span> <span class="toc-text">Cart树简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E5%B0%BC%E6%8C%87%E6%95%B0%E8%AE%A1%E7%AE%97%E6%A1%88%E4%BE%8B"><span class="toc-number">7.4.2.</span> <span class="toc-text">基尼指数计算案例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%98%AF%E5%90%A6%E6%9C%89%E6%88%BF"><span class="toc-number">7.4.2.1.</span> <span class="toc-text">是否有房</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A9%9A%E5%A7%BB%E7%8A%B6%E5%86%B5"><span class="toc-number">7.4.2.2.</span> <span class="toc-text">婚姻状况</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%B4%E6%94%B6%E5%85%A5"><span class="toc-number">7.4.2.3.</span> <span class="toc-text">年收入</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cart%E6%A1%88%E4%BE%8B%EF%BC%88%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E7%94%9F%E5%AD%98%E6%A1%88%E4%BE%8B"><span class="toc-number">7.4.3.</span> <span class="toc-text">Cart案例（泰坦尼克号生存案例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cart%E5%9B%9E%E5%BD%92%E5%86%B3%E7%AD%96%E6%A0%91%E2%AD%90%EF%B8%8F"><span class="toc-number">7.5.</span> <span class="toc-text">Cart回归决策树⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA%E5%8E%9F%E7%90%86"><span class="toc-number">7.5.1.</span> <span class="toc-text">回归决策树构建原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CART-%E5%9B%9E%E5%BD%92%E6%A0%91%E6%9E%84%E5%BB%BA"><span class="toc-number">7.5.2.</span> <span class="toc-text">CART 回归树构建:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%89%AA%E6%9E%9D%E2%AD%90%EF%B8%8F"><span class="toc-number">7.6.</span> <span class="toc-text">决策树剪枝⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%89%AA%E6%9E%9D"><span class="toc-number">7.6.1.</span> <span class="toc-text">什么是剪枝?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E5%87%8F%E6%9E%9D%E6%96%B9%E6%B3%95"><span class="toc-number">7.6.2.</span> <span class="toc-text">常见减枝方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%AA%E6%9E%9D%E6%96%B9%E6%B3%95%E5%AF%B9%E6%AF%94"><span class="toc-number">7.6.3.</span> <span class="toc-text">剪枝方法对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E5%89%AA%E6%9E%9D-pre-pruning%EF%BC%89%E4%B8%BE%E4%BE%8B"><span class="toc-number">7.6.4.</span> <span class="toc-text">预剪枝 (pre-pruning）举例</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0"><span class="toc-number">8.</span> <span class="toc-text">集成学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">8.1.</span> <span class="toc-text">集成学习是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bagging%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95"><span class="toc-number">8.2.</span> <span class="toc-text">Bagging集成算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Boosting-%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95"><span class="toc-number">8.3.</span> <span class="toc-text">Boosting 集成算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bagging-%E4%B8%8E-Boosting%E5%8C%BA%E5%88%AB"><span class="toc-number">8.4.</span> <span class="toc-text">Bagging 与 Boosting区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">8.5.</span> <span class="toc-text">随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E4%B8%8D%E4%BC%9A%E4%BA%A7%E7%94%9F%E8%BF%87%E6%8B%9F%E5%90%88%E5%8E%9F%E5%9B%A0%EF%BC%9A"><span class="toc-number">8.5.1.</span> <span class="toc-text">随机森林不会产生过拟合原因：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97API-%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E6%A1%88%E4%BE%8B"><span class="toc-number">8.5.2.</span> <span class="toc-text">随机森林API(泰坦尼克号案例)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adaboost"><span class="toc-number">8.6.</span> <span class="toc-text">Adaboost</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#AdaBoost%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B%E5%8F%8A%E6%A1%88%E4%BE%8B"><span class="toc-number">8.6.1.</span> <span class="toc-text">AdaBoost推导过程及案例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adaboost%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="toc-number">8.6.2.</span> <span class="toc-text">Adaboost算法流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adaboost%E7%AE%97%E6%B3%95%E6%A1%88%E4%BE%8B"><span class="toc-number">8.6.3.</span> <span class="toc-text">Adaboost算法案例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GBDT"><span class="toc-number">8.7.</span> <span class="toc-text">GBDT</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="toc-number">8.7.1.</span> <span class="toc-text">算法流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GBDT%E7%AE%97%E6%B3%95%E6%A1%88%E4%BE%8B"><span class="toc-number">8.7.2.</span> <span class="toc-text">GBDT算法案例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#XGBoost"><span class="toc-number">8.8.</span> <span class="toc-text">XGBoost</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#XGBoost%E4%B8%8EGBDT%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%8D%E5%90%8C"><span class="toc-number">8.8.1.</span> <span class="toc-text">XGBoost与GBDT有什么不同</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8XGBoost%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80%EF%BC%9A"><span class="toc-number">8.8.2.</span> <span class="toc-text">在XGBoost目标函数泰勒展开：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84%E5%B8%B8%E8%A7%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%BD%A2%E5%BC%8F"><span class="toc-number">8.8.3.</span> <span class="toc-text">不同任务中的常见损失函数形式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E4%BA%8C%E9%98%B6%E5%B1%95%E5%BC%80%EF%BC%9F"><span class="toc-number">8.8.4.</span> <span class="toc-text">为什么需要二阶展开？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#XGBoost%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="toc-number">8.8.5.</span> <span class="toc-text">XGBoost算法流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#XGBoost%E6%A1%88%E4%BE%8B%EF%BC%9A%E4%B9%B3%E8%85%BA%E7%99%8C%E5%88%86%E7%B1%BB"><span class="toc-number">8.8.6.</span> <span class="toc-text">XGBoost案例：乳腺癌分类</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95-Clustering"><span class="toc-number">9.</span> <span class="toc-text">聚类算法(Clustering)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#API"><span class="toc-number">9.1.</span> <span class="toc-text">API</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E9%83%BD%E6%98%AF%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%90%97"><span class="toc-number">9.2.</span> <span class="toc-text">聚类算法都是无监督学习吗?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#k-means-k%E5%9D%87%E5%80%BC-%E7%AE%97%E6%B3%95"><span class="toc-number">9.3.</span> <span class="toc-text">k-means(k均值)算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E8%BF%87%E7%A8%8B"><span class="toc-number">9.3.1.</span> <span class="toc-text">算法过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-2"><span class="toc-number">9.3.2.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#k%E5%80%BC%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">9.3.3.</span> <span class="toc-text">k值的选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#KNN%E4%B8%8EK-means%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">9.3.4.</span> <span class="toc-text">KNN与K-means区别？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-Means%E4%BC%98%E7%BC%BA%E7%82%B9%E5%8F%8A%E6%94%B9%E8%BF%9B"><span class="toc-number">9.3.5.</span> <span class="toc-text">K-Means优缺点及改进</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B-GMM"><span class="toc-number">9.4.</span> <span class="toc-text">高斯混合模型(GMM)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#GMM%E7%9A%84%E6%80%9D%E6%83%B3"><span class="toc-number">9.4.1.</span> <span class="toc-text">GMM的思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GMM%E4%B8%8EK-Means%E7%9B%B8%E6%AF%94"><span class="toc-number">9.4.2.</span> <span class="toc-text">GMM与K-Means相比</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%A6%82%E4%BD%95%E8%AF%84%E4%BC%B0"><span class="toc-number">9.5.</span> <span class="toc-text">聚类算法如何评估</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%99%84%E4%BB%B6"><span class="toc-number">10.</span> <span class="toc-text">附件</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%98%B6%E6%AE%B5%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-number">10.1.</span> <span class="toc-text">机器学习阶段参考链接</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AC%E5%85%B1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86"><span class="toc-number">10.2.</span> <span class="toc-text">公共基础知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%87%E9%87%8F%EF%BC%88Scalar%EF%BC%89"><span class="toc-number">10.2.1.</span> <span class="toc-text">标量（Scalar）:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F%EF%BC%88Vector%EF%BC%89"><span class="toc-number">10.2.2.</span> <span class="toc-text">向量（Vector）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%EF%BC%88Tensor%EF%BC%89"><span class="toc-number">10.2.3.</span> <span class="toc-text">张量（Tensor）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E8%80%85%E5%8C%BA%E5%88%AB"><span class="toc-number">10.2.3.1.</span> <span class="toc-text">三者区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BE%E5%BD%A2%E5%B1%95%E7%A4%BA"><span class="toc-number">10.2.3.2.</span> <span class="toc-text">图形展示</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">10.2.3.3.</span> <span class="toc-text">应用场景</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Latex%E6%95%B0%E5%AD%A6%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81"><span class="toc-number">10.3.</span> <span class="toc-text">Latex数学语法支持</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%BC%E6%95%B0-%E7%9F%A9%E9%98%B5-%E5%90%91%E9%87%8F"><span class="toc-number">10.4.</span> <span class="toc-text">导数&amp;矩阵&amp;向量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0%EF%BC%9A"><span class="toc-number">10.4.1.</span> <span class="toc-text">常见函数的导数：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E6%95%B0%E7%9A%84%E5%9B%9B%E5%88%99%E8%BF%90%E7%AE%97%EF%BC%9A"><span class="toc-number">10.4.2.</span> <span class="toc-text">导数的四则运算：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%8D%E5%90%88%E5%87%BD%E6%95%B0%E6%B1%82%E5%AF%BC%EF%BC%9A"><span class="toc-number">10.4.3.</span> <span class="toc-text">复合函数求导：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E6%95%B0%E5%87%BD%E6%95%B0"><span class="toc-number">10.5.</span> <span class="toc-text">对数函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E5%85%83%E4%B8%80%E6%AC%A1%E6%96%B9%E7%A8%8B%E6%9E%81%E5%80%BC%E9%97%AE%E9%A2%98"><span class="toc-number">10.6.</span> <span class="toc-text">二元一次方程极值问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E7%8E%87"><span class="toc-number">10.7.</span> <span class="toc-text">概率</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E8%A1%A5%E5%85%85"><span class="toc-number">10.7.1.</span> <span class="toc-text">条件概率补充</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E6%A6%82%E7%8E%87%E5%85%AC%E5%BC%8F"><span class="toc-number">10.7.2.</span> <span class="toc-text">全概率公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F"><span class="toc-number">10.7.3.</span> <span class="toc-text">贝叶斯公式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1"><span class="toc-number">10.8.</span> <span class="toc-text">极大似然估计</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A6%BB%E6%95%A3%E5%9E%8B%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%9E%8B"><span class="toc-number">10.8.1.</span> <span class="toc-text">离散型统计模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9E%E7%BB%AD%E5%9E%8B%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%9E%8B"><span class="toc-number">10.8.2.</span> <span class="toc-text">连续型统计模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E9%87%8A%E7%84%B6%E4%BC%B0%E8%AE%A1%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95"><span class="toc-number">10.8.3.</span> <span class="toc-text">最大释然估计统计方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%9A%E9%81%93%E4%BE%8B%E9%A2%98"><span class="toc-number">10.8.4.</span> <span class="toc-text">做道例题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F%E5%8F%8A%E5%B8%B8%E8%A7%81%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80%E5%BC%8F"><span class="toc-number">10.9.</span> <span class="toc-text">泰勒公式及常见泰勒展开式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%A6%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%9E%8B%E4%BD%99%E9%A1%B9%E7%9A%84%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F"><span class="toc-number">10.9.1.</span> <span class="toc-text">带拉格朗日型余项的泰勒公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%9E%8B%E4%BD%99%E9%A1%B9%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="toc-number">10.9.2.</span> <span class="toc-text">拉格朗日型余项表达式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%A6%E4%BD%A9%E4%BA%9A%E8%AF%BA%E5%9E%8B%E4%BD%99%E9%A1%B9%E7%9A%84%E9%BA%A6%E5%85%8B%E5%8A%B3%E6%9E%97%E5%85%AC%E5%BC%8F"><span class="toc-number">10.9.3.</span> <span class="toc-text">带佩亚诺型余项的麦克劳林公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%A9%E4%BA%9A%E8%AF%BA%E5%9E%8B%E4%BD%99%E9%A1%B9%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="toc-number">10.9.4.</span> <span class="toc-text">佩亚诺型余项表达式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A"><span class="toc-number">10.9.5.</span> <span class="toc-text">解释</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E5%87%BD%E6%95%B0%E7%9A%84%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80%E5%BC%8F"><span class="toc-number">10.9.6.</span> <span class="toc-text">常见函数的泰勒展开式</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/11/02/Fine-Tuning/" title="Fine-Tuning"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen11121.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Fine-Tuning"/></a><div class="content"><a class="title" href="/2025/11/02/Fine-Tuning/" title="Fine-Tuning">Fine-Tuning</a><time datetime="2025-11-01T16:00:00.000Z" title="Created 2025-11-02 00:00:00">2025-11-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/30/Agent_all/" title="Agent"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/Pusheen12222.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Agent"/></a><div class="content"><a class="title" href="/2025/10/30/Agent_all/" title="Agent">Agent</a><time datetime="2025-10-29T16:00:00.000Z" title="Created 2025-10-30 00:00:00">2025-10-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/25/Agent/" title="Agent-Note"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/Pusheen1121.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Agent-Note"/></a><div class="content"><a class="title" href="/2025/10/25/Agent/" title="Agent-Note">Agent-Note</a><time datetime="2025-10-24T16:00:00.000Z" title="Created 2025-10-25 00:00:00">2025-10-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/21/RAG/" title="RAG"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="RAG"/></a><div class="content"><a class="title" href="/2025/10/21/RAG/" title="RAG">RAG</a><time datetime="2025-10-20T16:00:00.000Z" title="Created 2025-10-21 00:00:00">2025-10-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/28/LLM_Base/" title="LLM大模型基础"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen112.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM大模型基础"/></a><div class="content"><a class="title" href="/2025/09/28/LLM_Base/" title="LLM大模型基础">LLM大模型基础</a><time datetime="2025-09-27T16:00:00.000Z" title="Created 2025-09-28 00:00:00">2025-09-28</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 李俊泽</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Welcome to 李俊泽 の <a target="_blank" rel="noopener" href="https://www.cnblogs.com/liam-sliversucks/">Blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? '' : ''

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>