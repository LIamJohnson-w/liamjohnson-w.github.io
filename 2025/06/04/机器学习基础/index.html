<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>机器学习 | SilverSucks</title><meta name="author" content="Johnson Liam"><meta name="copyright" content="Johnson Liam"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="AI人工智能 讲到AI人工智能首先得从图灵测试开始说起： 图灵测试就是：测试者与被测试者（一个人和一台机器）喘开的情况下，遍过一些装置（如键盘）向被测试者随意提问。多次测试（一般为5min之内），如果有超过30%的测试者不能确定被测试者是人还是机器，那么这台机器就通过了测试，并被认为具有人类智能。"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://liamjohnson-w.github.io/2025/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-06-10 20:50:19'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css"> <script src="/live2d-widget/autoload.js"></script><script src="/live2d-widget/autoload.js"> </script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/pic.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">222</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">58</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-gamepad"></i><span> Games</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/mikutap/"><i class="fa-fw fa fa-music"></i><span> MikuTap 初音未来</span></a></li><li><a class="site-page child" href="/starbattle/"><i class="fa-fw fa fa-space-shuttle"></i><span> StartBattle 星际大战</span></a></li><li><a class="site-page child" href="/2048/"><i class="fa-fw fa fa-flag"></i><span> 2048 经典游戏</span></a></li><li><a class="site-page child" href="/battlecity/"><i class="fa-fw fa fa-arrow-circle-left"></i><span> BattleCity 坦克大战</span></a></li><li><a class="site-page child" href="/pacman/"><i class="fa-fw fa fa-bolt"></i><span> PacMan  吃豆人</span></a></li><li><a class="site-page child" href="/tetris/"><i class="fa-fw fa fa-arrows-alt"></i><span> Tetris 俄罗斯方块</span></a></li><li><a class="site-page child" href="/smallcat/"><i class="fa-fw fa fa-paw"></i><span> CatchCat 困住小猫</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-leaf"></i><span> Moments</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/diary/"><i class="fa-fw fas fa-bookmark"></i><span> Diary</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fa fa-hourglass-half"></i><span> Gallery</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-podcast"></i><span> More</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags标签</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About关于</span></a></li><li><a class="site-page child" href="/messageboard/"><i class="fa-fw fas fa-bookmark"></i><span> Messageboard留言板</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s1.ax1x.com/2023/04/18/p9i6u5D.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="SilverSucks"><span class="site-name">SilverSucks</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-gamepad"></i><span> Games</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/mikutap/"><i class="fa-fw fa fa-music"></i><span> MikuTap 初音未来</span></a></li><li><a class="site-page child" href="/starbattle/"><i class="fa-fw fa fa-space-shuttle"></i><span> StartBattle 星际大战</span></a></li><li><a class="site-page child" href="/2048/"><i class="fa-fw fa fa-flag"></i><span> 2048 经典游戏</span></a></li><li><a class="site-page child" href="/battlecity/"><i class="fa-fw fa fa-arrow-circle-left"></i><span> BattleCity 坦克大战</span></a></li><li><a class="site-page child" href="/pacman/"><i class="fa-fw fa fa-bolt"></i><span> PacMan  吃豆人</span></a></li><li><a class="site-page child" href="/tetris/"><i class="fa-fw fa fa-arrows-alt"></i><span> Tetris 俄罗斯方块</span></a></li><li><a class="site-page child" href="/smallcat/"><i class="fa-fw fa fa-paw"></i><span> CatchCat 困住小猫</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-leaf"></i><span> Moments</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/diary/"><i class="fa-fw fas fa-bookmark"></i><span> Diary</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fa fa-hourglass-half"></i><span> Gallery</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-podcast"></i><span> More</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags标签</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About关于</span></a></li><li><a class="site-page child" href="/messageboard/"><i class="fa-fw fas fa-bookmark"></i><span> Messageboard留言板</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">机器学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-06-03T16:00:00.000Z" title="Created 2025-06-04 00:00:00">2025-06-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-06-10T12:50:19.119Z" title="Updated 2025-06-10 20:50:19">2025-06-10</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="机器学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="AI人工智能"><a href="#AI人工智能" class="headerlink" title="AI人工智能"></a>AI人工智能</h1><blockquote>
<p>讲到AI人工智能首先得从<code>图灵测试</code>开始说起：</p>
<p>图灵测试就是：测试者与被测试者（一个人和一台机器）喘开的情况下，遍过一些装置（如键盘）向被测试者随意提问。<br>多次测试（一般为5min之内），如果有超过30%的测试者不能确定被测试者是人还是机器，那么这台机器就通过了测试，并被认为具有人类智能。</p>
<p><code>当有人骂你是人机的时候，你不要骂过去，你要说你还没通过图灵测试（这样别人就听不懂了doge）</code>。</p>
</blockquote>
<h2 id="人工智能的分类"><a href="#人工智能的分类" class="headerlink" title="人工智能的分类"></a>人工智能的分类</h2><p>通讯、感知与行动是现代人工智能的三个关键能力</p>
<p>与此对应的三个技术领域分别是</p>
<ul>
<li><code>计算机视觉（CV）</code><ul>
<li>计算机视觉（CV）是指机器感知环境的能力。这一技术类别中的经典任务有图像形成、图像处理、图像提取和图像的三维推理。物体检测和人脸识别是其比较成功的研究领域。</li>
</ul>
</li>
<li><code>自然语言处理（NLP）</code>:在NLP领域中，将覆盖文本挖掘&#x2F;分类、机器翻译和语音识别<ul>
<li>语音识别：是指识别语音（说出的语言）并将其转换成对应文本的技术。相反的任务（文本转语音&#x2F;TTS）也是这一领域内一个类似的研究主题。</li>
<li>文本挖掘：主要是指文本分类，该技术可用于理解、组织和分类结构化或非结构化文本文档。其涵盖的主要任务有句法分析、情绪分析和垃圾信息检测。</li>
<li>机器翻译（MT）：是利用机器的力量自动将一种自然语言（源语言）的文本翻译成另一种语言（目标语言）。</li>
</ul>
</li>
<li><code>机器人</code><ul>
<li>机器人学（Robotics）研究的是机器人的设计、制造、运作和应用，以及控制它们的计算机系统、传感反馈和信息处理。</li>
<li>机器人可以分成两大类：固定机器人和移动机器人。<ul>
<li>固定机器人通常被用于工业生产（比如用于装配线）。</li>
<li>常见的移动机器人应用有货运机器人、空中机器人和自动载具。机器人需要不同部件和系统的协作才能实现最优的作业。其中在硬件上包含传感器、反应器和控制器；另外还有能够实现感知能力的软件，比如定位、地图测绘和目标识别。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a><font size=7 color='orange' face='华文楷体'>机器学习基础</font></h1><h2 id="机器学习的概念"><a href="#机器学习的概念" class="headerlink" title="机器学习的概念"></a>机器学习的概念</h2><p><code>机器学习是从数据中自动分析获得模型，并利用模型对未知数据进行预测</code></p>
<p>如下图所示：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250605141520596.png" alt="image-20250605141520596"></p>
<h2 id="机器学习工作流程⭐️"><a href="#机器学习工作流程⭐️" class="headerlink" title="机器学习工作流程⭐️"></a><font size=5 color='red' face='华文楷体'>机器学习工作流程</font>⭐️</h2><ol>
<li><p>获取数据</p>
<ul>
<li>区分样本和特征</li>
<li>根据数据有无特征值和目标值区分第四步机器学习选择的算法：<ul>
<li>监督学习、无监督学习、强化学习、半监督学习算法</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>学习类型</th>
<th>In</th>
<th>Out</th>
<th>目的</th>
<th>案例</th>
</tr>
</thead>
<tbody><tr>
<td>监督学习（Supervised Learning）</td>
<td>有标签</td>
<td>有反馈</td>
<td>预测结果</td>
<td>猫狗分类、房价预测</td>
</tr>
<tr>
<td>无监督学习（Unsupervised Learning）</td>
<td>无标签</td>
<td>无反馈</td>
<td>发现潜在结构</td>
<td>“物以类聚，人以群分”</td>
</tr>
<tr>
<td>半监督学习（Semi-Supervised Learning）</td>
<td>部分有标签，部分无标签</td>
<td>有反馈</td>
<td>降低数据标记难度</td>
<td></td>
</tr>
<tr>
<td>强化学习（Reinforcement Learning）</td>
<td>决策流程及激励系统</td>
<td>一系列行动</td>
<td>长期利益最大化</td>
<td>学下棋</td>
</tr>
</tbody></table>
</li>
<li><p>数据基本处理</p>
</li>
<li><p>特征工程</p>
<ul>
<li>特征提取</li>
<li>特征预处理</li>
<li>特征降维</li>
</ul>
</li>
<li><p>机器学习（模型训练）</p>
<ul>
<li>选择合适的算法对模型进行训练</li>
</ul>
</li>
<li><p>模型评估</p>
<ul>
<li>结果达到要求，上线服务</li>
<li>没有达到要求，重新上面步骤</li>
</ul>
</li>
</ol>
<h3 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h3><blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250605142251204.png" alt="image-20250605142251204"></p>
<p>对于数据集:</p>
<ul>
<li>一行数据我们称为一个<code>样本</code></li>
<li>一列数据我们成为一个<code>特征</code></li>
<li>有些数据有目标值（标签值），有些数据没有目标值（如上表中，电影类型就是这个数据集的目标值）</li>
</ul>
<p>数据分割：</p>
<ul>
<li>数据类型一：特征值+目标值（目标值是连续的和离散的）</li>
<li>数据类型二：只有特征值，没有目标值</li>
</ul>
<p>针对上述的数据可以分类为<code>有监督学习</code>和<code>无监督学习</code>，可以看看之前的<a href="https://liamjohnson-w.github.io/2023/08/07/2023.08.07/">Note</a>：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/09.png" alt="09"></p>
<p>监督学习典型模型：Linear regression、Logistic regression、SVM、Neural Network等</p>
<p><strong>监督学习</strong>：监督学习指的是人们给机器标记好的数据，有特征值和目标值，比如：一大堆照片，人工先标记出哪些是猫的照片，哪些是狗的照片；训练模型；让模型判断其余照片是什么动物</p>
<p>监督学习一般有两个问题（分类问题、回归问题）</p>
<ul>
<li>分类问题：</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/06.png" alt="img"></p>
<p>​		垃圾邮件识别就是一个分类问题，使用相应的机器学习算法判定邮件属于垃圾邮件还是非垃圾邮件。</p>
<ul>
<li>回归问题：<ul>
<li>数据中会给出大量的自变量和相应的连续因变量（对应输出结果），通过尝试寻找自变量和因变量的关系，就能够预测输出变量。</li>
<li>如房价的预测，根据房屋的面积以及房价的结果来进行后续的预测</li>
<li>还有股票基金的涨跌预测</li>
</ul>
</li>
</ul>
<p><strong>无监督学习</strong>：非监督学习(unsupervised learning)指的是人们给机器一大堆没有分类标记的数据，让机器可以对数据分类、检测异常等。相应的无监督学习只有一个聚类问题</p>
<ul>
<li>聚类问题：聚类是一种探索性数据分析技术，在没有任何相关先验信息的情况下（相当于不清楚数据的信息），它可以帮助我们将数据划分为有意义的小的组别（也叫簇cluster）。其中每个簇内部成员之间有一定的相似度，簇之间有较大的不同。这也正是聚类作为无监督学习的原因。</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/08.png" alt="img"></p>
<ul>
<li><p>聚类的应用场景</p>
<ul>
<li>在对业务不是很熟悉的情况下, 使用聚类可以帮助打开思路</li>
<li>使用聚类算法做聚类分析 是分析过程的开头, 得到聚类结果之后, 再详细的分析每个类别各自的特点</li>
</ul>
</li>
</ul>
<p><strong>强化学习</strong>：实质是make decisions 问题，即自动进行决策，并且可以做连续决策。</p>
<ul>
<li><p>举例：小孩想要走路，但在这之前，他需要先站起来，站起来之后还要保持平衡，接下来还要先迈出一条腿，是左腿还是右腿，迈出一步后还要迈出下一步。</p>
</li>
<li><p>小孩就是 agent，他试图通过采取行动（即行走）来操纵环境（行走的表面），并且从一个状态转变到另一个状态（即他走的每一步），当他</p>
</li>
<li><p>完成任务的子任务（即走了几步）时，孩子得到奖励（给巧克力吃），并且当他不能走路时，就不会给巧克力。</p>
</li>
</ul>
<p><strong>半监督学习</strong>：训练集同时包含有标记样本数据和未标记样本数据。</p>
<p>机器学习一般的数据集会划分为两个部分：</p>
<ul>
<li>训练数据：用于训练，构建模型</li>
<li>测试数据：在模型检验时使用，用于评估模型是否有效</li>
</ul>
<p>划分比例：</p>
<ul>
<li>训练集：70% 80% 75%</li>
<li>测试集：30% 20% 25%</li>
</ul>
</blockquote>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><blockquote>
<p>即对数据进行缺失值、去除异常值等处理</p>
</blockquote>
<h3 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h3><ul>
<li><p>特征提取：将任意数据（如文本或图像）转换为可用于机器学习的数字特征</p>
<ul>
<li>例如将语言、图像等自然语言转为计算机可以识别的字符（base64编码or二进制编码</li>
</ul>
</li>
<li><p>特征预处理：通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程</p>
</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250605160124144.png" alt="image-20250605160124144"></p>
<ul>
<li>特征降维（特征缩放）：指在某些限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量的过程</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250605160141881.png" alt="image-20250605160141881"></p>
<h3 id="机器学习（模型训练）"><a href="#机器学习（模型训练）" class="headerlink" title="机器学习（模型训练）"></a>机器学习（模型训练）</h3><ul>
<li>选择合适的算法对模型进行训练</li>
</ul>
<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><ul>
<li><p>分类模型评估</p>
<ul>
<li>准确率：预测正确的数占样本总数的比例。</li>
<li>其他评价指标：精确率、召回率、<strong>F1-score</strong>、<strong>AUC</strong>指标等</li>
</ul>
</li>
<li><p>回归模型评估</p>
<ul>
<li><p>均方根误差（<strong>Root Mean Squared Error</strong>，<strong>RMSE</strong>）</p>
<ul>
<li><p>RMSE是一个衡量回归模型误差率的常用公式。 不过，它仅能比较误差是相同单位的模型。</p>
<p><span>$ RMSE&#x3D;\sqrt{\frac{\sum_{i&#x3D;1}^{n}\left(p_{i}-a_{i}\right)^{2}}{n}} $</span></p>
</li>
</ul>
</li>
<li><p>相对平方误差（<strong>Relative Squared Error</strong>，<strong>RSE</strong>）、平均绝对误差（<strong>Mean Absolute Error</strong>，<strong>MAE)<strong>、相对绝对误差 （</strong>Relative Absolute Error</strong>，<strong>RAE)</strong></p>
</li>
</ul>
</li>
<li><p>拟合</p>
<ul>
<li>过拟合：机器已经基本能区别天鹅和其他动物了。但很不巧已有的天鹅图片全是白天鹅的，于是机器判断黑的天鹅不是天鹅。</li>
<li>欠拟合：模型学习的太过粗糙，连训练集中的样本数据特征关系都没有学出来。比如判断鹦鹉、鸭子也是天鹅。</li>
</ul>
</li>
</ul>
<h1 id="Sklearn与特征工程⭐️"><a href="#Sklearn与特征工程⭐️" class="headerlink" title="Sklearn与特征工程⭐️"></a>Sklearn与特征工程⭐️</h1><blockquote>
<p><code>scikit-learn</code>（简称 <strong>sklearn</strong>）基于 NumPy、SciPy 和 Matplotlib 构建，提供了简单高效的算法工具，适合从数据预处理到模型训练的全流程。</p>
</blockquote>
<hr>
<h2 id="sklearn-的核心组成"><a href="#sklearn-的核心组成" class="headerlink" title="sklearn 的核心组成"></a><strong>sklearn 的核心组成</strong></h2><blockquote>
<p>sklearn的核心部分主要有</p>
<ul>
<li>数据预处理（Preprocessing）<ul>
<li><code>StandardScaler</code> &#x2F; <code>MinMaxScaler</code>：数据标准化&#x2F;归一化。</li>
<li><code>OneHotEncoder</code>：分类变量独热编码。</li>
<li><code>train_test_split</code>：划分训练集和测试集。</li>
<li><code>SimpleImputer</code>：处理缺失值。</li>
</ul>
</li>
<li>监督学习算法（Supervised Learning）<ul>
<li>分类（Classification）：<ul>
<li><code>LogisticRegression</code>（逻辑回归）</li>
<li><code>SVM</code>（支持向量机）</li>
<li><code>RandomForestClassifier</code>（随机森林）</li>
<li><code>KNeighborsClassifier</code>（K近邻）</li>
</ul>
</li>
<li>回归（Regression）：<ul>
<li><code>LinearRegression</code>（线性回归）</li>
<li><code>DecisionTreeRegressor</code>（决策树回归）</li>
<li><code>GradientBoostingRegressor</code>（梯度提升回归）</li>
</ul>
</li>
</ul>
</li>
<li>无监督学习算法（Unsupervised Learning）<ul>
<li>聚类（Clustering）：<ul>
<li><code>KMeans</code>（K均值聚类）</li>
<li><code>DBSCAN</code>（基于密度的聚类）</li>
</ul>
</li>
<li>降维（Dimensionality Reduction）：<ul>
<li><code>PCA</code>（主成分分析）</li>
<li><code>t-SNE</code>（数据可视化降维）</li>
</ul>
</li>
</ul>
</li>
<li>模型评估与选择（Model Evaluation）<ul>
<li>指标计算：<ul>
<li><code>accuracy_score</code>（分类准确率）</li>
<li><code>mean_squared_error</code>（回归均方误差）</li>
<li><code>confusion_matrix</code>（混淆矩阵）</li>
</ul>
</li>
<li>交叉验证：<ul>
<li><code>cross_val_score</code>（K折交叉验证）</li>
</ul>
</li>
</ul>
</li>
<li>特征工程（Feature Engineering）<ul>
<li><code>SelectKBest</code>：基于统计检验选择特征。</li>
<li><code>RFE</code>（递归特征消除）：自动筛选重要特征。</li>
</ul>
</li>
<li>流水线（Pipeline）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line">pipeline = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;scaler&#x27;</span>, StandardScaler()),</span><br><span class="line">    (<span class="string">&#x27;classifier&#x27;</span>, RandomForestClassifier())</span><br><span class="line">])</span><br></pre></td></tr></table></figure>


</blockquote>
<table>
<thead>
<tr>
<th>库</th>
<th>特点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>sklearn</strong></td>
<td>传统机器学习算法（非深度学习）</td>
<td>中小规模结构化数据</td>
</tr>
<tr>
<td><strong>TensorFlow&#x2F;PyTorch</strong></td>
<td>深度学习框架</td>
<td>图像、文本等复杂数据</td>
</tr>
<tr>
<td><strong>XGBoost</strong></td>
<td>高性能梯度提升树</td>
<td>表格数据竞赛&#x2F;高精度需求</td>
</tr>
</tbody></table>
<p>Iris 鸢尾花数据集内包含 3 种类别，分别为</p>
<ul>
<li>山鸢尾（Iris-setosa）</li>
<li>变色鸢尾（Iris-versicolor）</li>
<li>维吉尼亚鸢尾（Iris-virginica）<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250606102258377.png" alt="image-20250606102258377"></li>
</ul>
<h2 id="sklearn随机森林-鸢尾花案例的典型代码流程"><a href="#sklearn随机森林-鸢尾花案例的典型代码流程" class="headerlink" title="sklearn随机森林-鸢尾花案例的典型代码流程"></a><strong>sklearn随机森林-鸢尾花案例的典型代码流程</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 加载数据</span></span><br><span class="line">data = load_iris()</span><br><span class="line">X, y = data.data, data.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 划分数据集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 训练模型</span></span><br><span class="line">model = RandomForestClassifier()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 预测与评估</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;准确率:&quot;</span>, accuracy_score(y_test, y_pred))</span><br></pre></td></tr></table></figure>

<h3 id="sklear典型流程-带注释详解"><a href="#sklear典型流程-带注释详解" class="headerlink" title="sklear典型流程(带注释详解)"></a>sklear典型流程(带注释详解)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1, 加载数据</span></span><br><span class="line">data = load_iris()</span><br><span class="line">x, y = data.data, data.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据结构</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;特征矩阵 x 的形状:&quot;</span>, x.shape)  <span class="comment"># 输出: (150, 4) → 150个样本，每个样本4个特征</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;标签数组 y 的形状:&quot;</span>, y.shape)  <span class="comment"># 输出: (150,)  → 150个标签</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;前5个样本的特征:\n&quot;</span>, x[:<span class="number">5</span>])</span><br><span class="line"><span class="comment">#  [[5.1 3.5 1.4 0.2]</span></span><br><span class="line"><span class="comment">#   [4.9 3.  1.4 0.2]</span></span><br><span class="line"><span class="comment">#   [4.7 3.2 1.3 0.2]</span></span><br><span class="line"><span class="comment">#   [4.6 3.1 1.5 0.2]</span></span><br><span class="line"><span class="comment">#   [5.  3.6 1.4 0.2]]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;前5个样本的标签:&quot;</span>, y[:<span class="number">5</span>])</span><br><span class="line"><span class="comment"># 前5个样本的标签: [0 0 0 0 0]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2，划分数据集</span></span><br><span class="line"><span class="comment"># 划分训练集和测试集。表示将数据的 20% 划分为测试集，剩余的 80% 自动作为训练集。也可指定绝对数量（如 test_size=200 表示 200 个样本作为测试集）。</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">其余参数：</span></span><br><span class="line"><span class="string">random_state：控制随机划分的种子（如 random_state=42 确保每次运行结果一致）。</span></span><br><span class="line"><span class="string">shuffle：是否打乱数据后再划分（默认为 True）。</span></span><br><span class="line"><span class="string">stratify：按标签分层划分（确保训练集和测试集的类别比例一致，适用于分类任务）。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># x：特征矩阵（输入数据），形状通常为 [样本数, 特征数]（如 (1000, 5) 表示 1000 个样本，每个样本 5 个特征）。</span></span><br><span class="line"><span class="comment"># y：标签数组（输出数据），形状为 [样本数,]（如 (1000,) 表示 1000 个样本对应的类别或回归值）。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># x_train：训练集特征矩阵（80% 的原始 x）。</span></span><br><span class="line"><span class="comment"># x_test：测试集特征矩阵（20% 的原始 x）。</span></span><br><span class="line"><span class="comment"># y_train：训练集标签（与 x_train 对应）。</span></span><br><span class="line"><span class="comment"># y_test：测试集标签（与 x_test 对应）。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3，训练模型</span></span><br><span class="line"><span class="comment"># 创建随机森林模型对象</span></span><br><span class="line">model = RandomForestClassifier()</span><br><span class="line"><span class="comment"># 用训练数据（x_train 和 y_train）训练模型，也就是训练集特征矩阵以及训练集标签进行有监督学习训练</span></span><br><span class="line">model.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4，预测与评估</span></span><br><span class="line"><span class="comment"># 让训练好的模型处理新数据（如预测新样本的类别）</span></span><br><span class="line">y_pred = model.predict(x_test)</span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br><span class="line"><span class="comment"># [0 2 2 1 0 0 1 0 1 0 2 0 1 1 2 1 2 2 2 1 0 1 1 1 0 1 1 1 1 0]</span></span><br></pre></td></tr></table></figure>

<p>注意事项：</p>
<ul>
<li>如果数据未标准化或存在缺失值，可能需要在 <code>fit</code> 之前进行预处理（如使用 <code>StandardScaler</code>）。</li>
<li>训练后可通过 <code>model.score(x_test, y_test)</code> 快速评估模型在测试集上的准确率。</li>
</ul>
<h2 id="Seaborn鸢尾花案例"><a href="#Seaborn鸢尾花案例" class="headerlink" title="Seaborn鸢尾花案例"></a>Seaborn鸢尾花案例</h2><h2 id="模型选择与调优"><a href="#模型选择与调优" class="headerlink" title="模型选择与调优"></a>模型选择与调优</h2><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><h3 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h3><h2 id="鸢尾花案例增加K值调优"><a href="#鸢尾花案例增加K值调优" class="headerlink" title="鸢尾花案例增加K值调优"></a>鸢尾花案例增加K值调优</h2><h1 id="KNN算法"><a href="#KNN算法" class="headerlink" title="KNN算法"></a>KNN算法</h1><blockquote>
<p>K Nearest Neighbor算法又叫KNN算法，这个算法是机器学习里面一个比较经典的算法， 相对比较容易理解。</p>
</blockquote>
<h2 id="KNN算法流程"><a href="#KNN算法流程" class="headerlink" title="KNN算法流程"></a>KNN算法流程</h2><ul>
<li>计算测试样本和训练样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；</li>
<li>对上面所有的距离值进行排序；</li>
<li>选前 k 个最小距离的样本；</li>
<li>根据这 k 个样本的标签进行投票，得到最后的分类类别；</li>
</ul>
<h3 id="K值大小对于训练的影响"><a href="#K值大小对于训练的影响" class="headerlink" title="K值大小对于训练的影响"></a>K值大小对于训练的影响</h3><ul>
<li>K值过小：<ul>
<li>容易受到异常点的影响</li>
<li>容易过拟合</li>
</ul>
</li>
<li>k值过大：<ul>
<li>受到样本均衡的问题</li>
<li>容易欠拟合</li>
</ul>
</li>
</ul>
<h2 id="欧式距离"><a href="#欧式距离" class="headerlink" title="欧式距离"></a>欧式距离</h2><blockquote>
<p>其实就是两点之间的距离计算方式：</p>
</blockquote>
<h3 id="1-二维空间"><a href="#1-二维空间" class="headerlink" title="1. 二维空间"></a>1. 二维空间</h3><p>点 $a(x_{1},y_{1})$ 与 $b(x_{2},y_{2})$ 间的距离：<br>$$ d_{12} &#x3D; \sqrt{(x_{1}-x_{2})^{2} + (y_{1}-y_{2})^{2}} $$</p>
<h3 id="2-三维空间"><a href="#2-三维空间" class="headerlink" title="2. 三维空间"></a>2. 三维空间</h3><p>点 $a(x_{1},y_{1},z_{1})$ 与 $b(x_{2},y_{2},z_{2})$ 间的距离：<br>$$ d_{12} &#x3D; \sqrt{(x_{1}-x_{2})^{2} + (y_{1}-y_{2})^{2} + (z_{1}-z_{2})^{2}} $$</p>
<h3 id="3-n维空间"><a href="#3-n维空间" class="headerlink" title="3. n维空间"></a>3. n维空间</h3><p>点 $a(x_{11},x_{12},\dots,x_{1n})$ 与 $b(x_{21},x_{22},\dots,x_{2n})$ 间的距离：<br>$$ d_{12} &#x3D; \sqrt{\sum_{k&#x3D;1}^{n}(x_{1k} - x_{2k})^{2}} $$</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250606110103367.png" alt="image-20250606110103367"></p>
<p>计算唐探与各个电影的欧式距离，取前5个，统计各个电影类型出现的频率，根据频率判断唐探的目标值为喜剧片</p>
<h2 id="sklearn-KNN算法"><a href="#sklearn-KNN算法" class="headerlink" title="sklearn-KNN算法"></a>sklearn-KNN算法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">x = [[<span class="number">0</span>], [<span class="number">3</span>], [<span class="number">6</span>], [<span class="number">10</span>]]</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">model = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">model.fit(x,y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.predict([[<span class="number">5</span>]]))</span><br></pre></td></tr></table></figure>



<h2 id="各种距离计算方式"><a href="#各种距离计算方式" class="headerlink" title="各种距离计算方式"></a>各种距离计算方式</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250606114135017.png" alt="image-20250606114135017"></p>
<ul>
<li><p>欧式距离(Euclidean Distance)：</p>
<ul>
<li>上面写了</li>
</ul>
</li>
<li><p>曼哈顿距离(Manhattan Distance)：</p>
<ul>
<li><p>二维空间</p>
</li>
<li><p>点 $a(x_{1},y_{1})$ 与 $b(x_{2},y_{2})$ 间的距离：$$ d_{12} &#x3D; |x_{1} - x_{2}| + |y_{1} - y_{2}| $$</p>
</li>
<li><p>n维空间</p>
<ul>
<li>点 $a(x_{11},x_{12},\ldots,x_{1n})$ 与 $b(x_{21},x_{22},\ldots,x_{2n})$ 间的距离：$$ d_{12} &#x3D; \sum_{k&#x3D;1}^{n} |x_{1k} - x_{2k}| $$</li>
</ul>
</li>
</ul>
</li>
<li><p>切比雪夫距离 (Chebyshev Distance)：</p>
</li>
<li><p>二维空间</p>
</li>
<li><p>点 $a(x_{1},y_{1})$ 与 $b(x_{2},y_{2})$ 间的距离：$$ d_{12} &#x3D; \max\left(|x_{1}-x_{2}|,\ |y_{1}-y_{2}|\right) $$</p>
</li>
<li><p>n维空间</p>
<ul>
<li>点 $a(x_{11},x_{12},\ldots,x_{1n})$ 与 $b(x_{21},x_{22},\ldots,x_{2n})$ 间的距离：$$ d_{12} &#x3D; \max_{1 \leq i \leq n} \left( |x_{1i} - x_{2i}| \right) $$</li>
</ul>
</li>
<li><p>标准化欧氏距离 (Standardized EuclideanDistance)：</p>
<ul>
<li>既然数据各维分量的分布不一样，那先将各个分量都”标准化”到均值、方差相等，$$S_k$$表示各个维度的标准差，如果将方差的倒数看成一个权重，也可称之为加权欧氏距离Weiahted Euclidean distance)<ul>
<li>$$ d_{12} &#x3D; \sqrt{\sum_{k&#x3D;1}^{n} \left( \frac{x_{1k} - x_{2k}}{s_{k}} \right)^2 } $$</li>
</ul>
</li>
</ul>
</li>
<li><p>余弦距离(Cosine Distance)：几何中，夹角余弦可用来衡量两个向量方向的差异;机器学习中，借用这一概念来衡量样本向量之间的差异。</p>
<ul>
<li><p>二维空间向量</p>
<ul>
<li>向量 $\vec{a}(x_1,y_1)$ 与 $\vec{b}(x_2,y_2)$ 的夹角余弦：：$$ \cos \theta &#x3D; \frac{x_{1}x_{2} + y_{1}y_{2}}{\sqrt{x_{1}^{2} + y_{1}^{2}} \sqrt{x_{2}^{2} + y_{2}^{2}}} $$</li>
</ul>
</li>
<li><p>n维空间向量</p>
<ul>
<li>对于 $n$ 维样本点：$a(x_{11},x_{12},\ldots,x_{1n})$、$b(x_{21},x_{22},\ldots,x_{2n})$</li>
</ul>
</li>
<li><p>夹角余弦的两种表达形式：</p>
<ul>
<li><strong>点积与模长形式</strong>：<ul>
<li>$$ \cos (\theta) &#x3D; \frac{a \cdot b}{|a| \ |b|} $$</li>
</ul>
</li>
<li><strong>展开形式</strong>：<ul>
<li>$$ \cos (\theta) &#x3D; \frac{\sum\limits_{k&#x3D;1}^{n} x_{1k} x_{2k}}{\sqrt{\sum\limits_{k&#x3D;1}^{n} x_{1k}^{2}} \sqrt{\sum\limits_{k&#x3D;1}^{n} x_{2k}^{2}}} $$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>汉明距离(Hamming Distance)：</p>
<ul>
<li>两个等长字符串s1与s2的汉明距离为: 将其中一个变为另外一个所需要作的<strong>最小字符替换次数</strong>。</li>
</ul>
</li>
<li><p>杰卡德距离(Jaccard Distance)：</p>
<ul>
<li>杰卡德相似系数(Jaccard slmilarity coeficient): 两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示:</li>
</ul>
<p>$$<br>J(A,B) &#x3D; \frac{|A \cap B|}{|A \cup B|}<br>$$</p>
</li>
<li><p>闵可夫斯基距离(Minkowski Distance)，也叫闵式距离：</p>
<ul>
<li><p>$$ d_{12} &#x3D; \sqrt[p]{\sum_{k&#x3D;1}^{n} \left| x_{1k} - x_{2k} \right|^{p}} $$</p>
</li>
<li><p>参数说明</p>
<ul>
<li><p>$p$：距离参数（$p \geq 1$）</p>
</li>
<li><p>$x_{1k}, x_{2k}$：两个$n$维向量的第$k$个分量</p>
</li>
<li><p>$d_{12}$：两点间的闵氏距离</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="KD树"><a href="#KD树" class="headerlink" title="KD树"></a>KD树</h2><blockquote>
<p>根据<strong>KNN</strong>每次需要预测一个点时，我们都需要计算训练数据集里每个点到这个点的距离，然后选出距离最近的k个点进行投票。<strong>当数据集很大时，这个计算成本非常高</strong>。</p>
<p><strong>kd树</strong>：为了避免每次都重新计算一遍距离，算法会把距离信息保存在一棵树里，这样在计算之前从树里查询距离信息，尽量避免重新计算。其基本原理是，<strong>如果A和B距离很远，B和C距离很近，那么A和C的距离也很远</strong>。有了这个信息，就可以在合适的时候跳过距离远的点。</p>
</blockquote>
<h3 id="KD树的构建（二维平面）"><a href="#KD树的构建（二维平面）" class="headerlink" title="KD树的构建（二维平面）"></a>KD树的构建（二维平面）</h3><ol>
<li>确定split域：按照x&#x2F;y轴进行分割，根据x轴以及y轴数据上的方差，方差大的为split域。</li>
<li>确定Node-Data域：按照split值排序，取中间的点作为Node-Data点</li>
<li>确定左子空间和右子空间：按照Node-Data的x&#x2F;y坐标进行点的分割</li>
</ol>
<p>详细步骤：</p>
<blockquote>
<p>给定一个二维空间数据集：T&#x3D;{(2,3),(5,4),(9,6),(4,7),(8,1),(7,2)}，构造一个平衡kd树。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250606154437192.png" alt="image-20250606154437192"></p>
<ol>
<li>确定：split域&#x3D;x。具体是：6个数据点在x，y维度上的数据方差分别为39，28.63，在x轴上方差更大，故split域值为x；</li>
<li>确定：Node-data &#x3D; （7,2）。具体是：根据x维上的值将数据排序，6个数据的中值(所谓中值，即中间大小的值)为5,7的平均値，但是没有6，所以往后进一到7（暂时没搞懂）！！所以Node-data域位数据点（7,2）。</li>
<li>确定：左子空间和右子空间。具体是：分割超平面x&#x3D;7将整个空间分为两部分：x&lt;&#x3D;7的部分为左子空间，包含3个节点&#x3D;{(2,3),(5,4),(4,7)}；另一部分为右子空间，包含2个节点&#x3D;{(9,6)，(8,1)}；</li>
<li>如上算法所述，kd树的构建是一个递归过程，我们对左子空间和右子空间内的数据重复根节点的过程就可以得到一级子节点<strong>（5,4）</strong>和<strong>（9,6）</strong>，同时将空间和数据集进一步细分，如此往复直到空间中只包含一个数据点。</li>
</ol>
<h3 id="KD树的快速最近邻搜索算法"><a href="#KD树的快速最近邻搜索算法" class="headerlink" title="KD树的快速最近邻搜索算法"></a>KD树的<strong>快速最近邻搜索</strong>算法</h3><blockquote>
<p>假设标记为星星的点是 test point， 绿色的点是找到的近似点，在回溯过程中，需要用到一个队列，存储需要回溯的点，在判断其他子节点空间中是否有可能有距离查询点更近的数据点时，做法是以查询点为圆心，以当前的最近距离为半径画圆，这个圆称为候选超球（candidate hypersphere），如果圆与回溯点的轴相交，则需要将轴另一边的节点都放到回溯队列里面来。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250606155816616.png" alt="image-20250606155816616"></p>
<p>样本集{(2,3),(5,4), (9,6), (4,7), (8,1), (7,2)}</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250606160159584.png" alt="image-20250606160159584"></p>
<ul>
<li><p>查找点(2.1,3.1)</p>
</li>
<li><p>确定Search_Path为&lt;(7,2),(5,4), (2,3)&gt;；从search_path中取出(2,3)作为当前最佳结点nearest，dist为0.141</p>
<ul>
<li>然后回溯至(5,4)，以(2.1,3.1)为圆心，以dist&#x3D;0.141为半径画一个圆，并不和超平面y&#x3D;4相交，如上图，所以不必跳到结点(5,4)的右子空间去搜索，因为右子空间中不可能有更近样本点了。</li>
</ul>
</li>
<li><p>于是再回溯至(7,2)，同理，以(2.1,3.1)为圆心，以dist&#x3D;0.141为半径画一个圆并不和超平面x&#x3D;7相交，所以也不用跳到结点(7,2)的右子空间去搜索。</p>
<ul>
<li>至此，search_path为空，结束整个搜索，返回nearest(2,3)作为(2.1,3.1)的最近邻点，最近距离为0.141。</li>
</ul>
</li>
<li><p>查找点(2,4.5)</p>
<ul>
<li>在(7,2)处测试到达(5,4)，在(5,4)处测试到达(4,7)【优先选择在本域搜索】，然后search_path中的结点为&lt;(7,2),(5,4), (4,7)&gt;，从search_path中取出(4,7)作为当前最佳结点nearest, dist为3.202；</li>
<li>然后回溯至(5,4)，以(2,4.5)为圆心，以dist&#x3D;3.202为半径画一个圆与超平面y&#x3D;4相交，所以需要跳到(5,4)的左子空间去搜索。所以要将(2,3)加入到search_path中，现在search_path中的结点为&lt;(7,2),(2, 3)&gt;；另外，(5,4)与(2,4.5)的距离为3.04 &lt; dist &#x3D; 3.202，所以将(5,4)赋给nearest，并且dist&#x3D;3.04。</li>
<li>回溯至(2,3)，(2,3)是叶子节点，直接平判断(2,3)是否离(2,4.5)更近，计算得到距离为1.5，所以nearest更新为(2,3)，dist更新为(1.5)</li>
<li>回溯至(7,2)，同理，以(2,4.5)为圆心，以dist&#x3D;1.5为半径画一个圆并不和超平面x&#x3D;7相交, 所以不用跳到结点(7,2)的右子空间去搜索。</li>
<li>至此，search_path为空，结束整个搜索，返回nearest(2,3)作为(2,4.5)的最近邻点，最近距离为1.5。</li>
</ul>
</li>
</ul>
<h3 id="KD树的插入"><a href="#KD树的插入" class="headerlink" title="KD树的插入"></a>KD树的插入</h3><ul>
<li>在现有KD树中插入点 <code>(3, 6)</code>）：<ul>
<li><strong>现有树结构</strong>（按 <code>x→y→x...</code> 划分）：</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">          (5,4)        ← 根节点（x轴分割）</span><br><span class="line">         /     \</span><br><span class="line">    (2,3)       (8,1)   ← 第2层（y轴分割）</span><br><span class="line">     /   \       /   \</span><br><span class="line">(1,7) (4,2) (7,9) (9,5)</span><br></pre></td></tr></table></figure>

<p><strong>插入步骤</strong>：</p>
<ol>
<li>比较根节点 <code>(5,4)</code>（第1层，x轴）：<ul>
<li>插入点 <code>(3,6)</code> 的x值 <code>3 &lt; 5</code> → 进入左子树。</li>
</ul>
</li>
<li>比较 <code>(2,3)</code>（第2层，y轴）：<ul>
<li>插入点y值 <code>6 &gt; 3</code> → 进入右子树。</li>
</ul>
</li>
<li>到达叶子节点 <code>(4,2)</code>：<ul>
<li>第3层按x轴比较，<code>3 &lt; 4</code> → 作为 <code>(4,2)</code> 的左子节点插入。</li>
</ul>
</li>
</ol>
<p><strong>插入后树结构</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">          (5,4)</span><br><span class="line">         /     \</span><br><span class="line">    (2,3)       (8,1)</span><br><span class="line">     /   \       /   \</span><br><span class="line">(1,7) (4,2) (7,9) (9,5)</span><br><span class="line">     /</span><br><span class="line">  (3,6)       ← 新插入节点</span><br></pre></td></tr></table></figure>

<p>图示：</p>
<pre class="mermaid">graph TD
    A[(5,4)] --> B[(2,3)]
    A --> C[(8,1)]
    B --> D[(1,7)]
    B --> E[(4,2)]
    E --> F[(3,6)]
    C --> G[(7,9)]
    C --> H[(9,5)]</pre>

<h3 id="KD树的删除"><a href="#KD树的删除" class="headerlink" title="KD树的删除"></a>KD树的删除</h3><ul>
<li><p><strong>核心逻辑</strong>：找到待删除节点后，按以下规则处理：</p>
<ul>
<li><p><strong>情况1</strong>：若为叶子节点 → 直接删除。</p>
</li>
<li><p><strong>情况2</strong>：若非叶子节点 → 找到子树中同分割轴的最优替代节点（类似二叉搜索树的中序后继）。</p>
</li>
</ul>
</li>
</ul>
<p>删除节点 <code>(5,4)</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">          (5,4)</span><br><span class="line">         /     \</span><br><span class="line">    (2,3)       (8,1)</span><br><span class="line">     /   \       /   \</span><br><span class="line">(1,7) (4,2) (7,9) (9,5)</span><br><span class="line">     /</span><br><span class="line">  (3,6) </span><br></pre></td></tr></table></figure>

<p><strong>步骤</strong>：</p>
<ol>
<li>定位节点 <code>(5,4)</code>：<ul>
<li>根节点，分割轴为x轴。</li>
</ul>
</li>
<li>寻找替代节点：<ul>
<li>在右子树中找x轴最小的点（即中序后继）→ <code>(7,9)</code>。</li>
</ul>
</li>
<li>替换并递归删除：<ul>
<li>用 <code>(7,9)</code> 替换 <code>(5,4)</code>，再递归删除 <code>(7,9)</code> 的原位置。</li>
</ul>
</li>
</ol>
<p><strong>删除后树结构</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">          (7,9)        ← 原(5,4)被替换</span><br><span class="line">         /     \</span><br><span class="line">    (2,3)       (8,1)</span><br><span class="line">     /   \       /   \</span><br><span class="line">(1,7) (4,2) (9,5)     ← (7,9)从原位置删除</span><br><span class="line">     /</span><br><span class="line">  (3,6)</span><br></pre></td></tr></table></figure>

<p><strong>图示</strong>：</p>
<pre class="mermaid">graph TD
    A[(7,9)] --> B[(2,3)]
    A --> C[(8,1)]
    B --> D[(1,7)]
    B --> E[(4,2)]
    E --> F[(3,6)]
    C --> H[(9,5)]</pre>



<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><blockquote>
<p>线性回归(Linear regression)是利用**回归方程(函数)**对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式。只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归。</p>
</blockquote>
<h2 id="线性回归定义⭐️"><a href="#线性回归定义⭐️" class="headerlink" title="线性回归定义⭐️"></a>线性回归定义⭐️</h2><h3 id="通用公式"><a href="#通用公式" class="headerlink" title="通用公式"></a>通用公式</h3><p>预测函数 $h(w)$ 的向量化表示：<br>$$ h(w) &#x3D; w_1x_1 + w_2x_2 + w_3x_3 \cdots + b &#x3D; w^Tx + b $$</p>
<p>其中参数 $w$ 和特征 $x$ 的矩阵形式：</p>
<p><span>$w &#x3D; \begin{pmatrix}<br>b \<br>{w_{1}}\<br>{w_{2}}<br>\end{pmatrix}$</span></p>
<p><span>$<br>x &#x3D; \begin{pmatrix}<br>1\<br>{x_{1}} \<br>{x_{2}}<br>\end{pmatrix}$</span></p>
<p>给定方程组：</p>
<p><span>$\begin{cases}<br>1 \times x_1 + x_2 &#x3D; 2 \<br>0 \times x_1 + x_2 &#x3D; 2 \<br>2 \times x_1 + x_2 &#x3D; 3<br>\end{cases}$</span></p>
<p><span>$\begin{cases}<br>{1 \times x_1 + x_2 &#x3D; 2} \<br>{0 \times x_1 + x_2 &#x3D; 2 }\<br>{2 \times x_1 + x_2 &#x3D; 3}<br>\end{cases}$</span></p>
<p>其矩阵运算表示为：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250607231936632-20250608222015680.png" alt="image-20250607231936632"></p>
<h2 id="线性回归的分类及应用场景"><a href="#线性回归的分类及应用场景" class="headerlink" title="线性回归的分类及应用场景"></a>线性回归的分类及应用场景</h2><ul>
<li>一元线性回归：$y &#x3D; kx +b $<ul>
<li>目标值只与一个因变量有关系</li>
</ul>
</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20230901102857178-20250608222015711.png" alt="image-20230901102857178"></p>
<ul>
<li>多元线性回归：$$ h(w) &#x3D; w_1x_1 + w_2x_2 + w_3x_3 \cdots + b &#x3D; w^Tx + b $$<ul>
<li>目标值只与多个因变量有关系</li>
</ul>
</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20230901103000204-20250608222015734.png" alt="image-20230901103000204"></p>
<p>应用场景：</p>
<ul>
<li>国内GDP与双11销售额的关系</li>
<li>贷款额度与工作情况家庭情况的关系</li>
</ul>
<h2 id="损失函数⭐️"><a href="#损失函数⭐️" class="headerlink" title="损失函数⭐️"></a>损失函数⭐️</h2><ul>
<li><strong>误差</strong>：用预测值$y$ – 真实值$y$就是误差</li>
<li><strong>损失函数</strong>：衡量每个样本预测值与真实值效果的函数</li>
<li>其实损失函数有好几种写法，没有什么本质上的区别，具体写哪种要看具体的问题如何方便：<ul>
<li><p><strong>归一化形式（带样本数归一）</strong><br>$$ J(\theta)&#x3D;\frac{1}{2N}\sum_{i&#x3D;1}^{N}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2} $$</p>
</li>
<li><p><strong>半归一化形式（去样本数保留1&#x2F;2）</strong><br>$$ J(\theta)&#x3D;\frac{1}{2}\sum_{i&#x3D;1}^{N}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2} $$</p>
</li>
<li><p><strong>原始求和形式（无归一化）</strong><br>$$ J(\theta)&#x3D;\sum_{i&#x3D;1}^{N}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2} $$</p>
</li>
</ul>
</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250607112225011-20250608222015776.png" alt="image-20250607112225011"></p>
<p>假设线性方程式为：$$ y &#x3D; kx + b $$</p>
<p>每个样本的<strong>预测值与真实值的误差</strong>来构成损失函数：$$ L(k,b) &#x3D; \sum_{i&#x3D;1}^{n} (y_i - \hat{y}_i)^2 $$</p>
<p>给定数据后，损失函数表达式：<br>$$<br>\begin{aligned}<br>L(k,b) &#x3D; &amp;(160k + b - 56.3)^2 + (166k + b - 60.6)^2 \<br>         + &amp;(172k + b - 65.1)^2 + (174k + b - 58.5)^2 \<br>         + &amp;(180k + b - 56.3)^2<br>\end{aligned}<br>$$</p>
<p>求参数 $(k, b)$ 使得损失函数 $L(k,b)$ 最小化</p>
<p>于是损失函数的求解就变成了数学问题：（为简化计算，先固定截距b，当k为0时，b可以设置成一个负值，b固定成-100）</p>
<p>由此损失函数就成了</p>
<p><span>$\begin{aligned}<br>L(k,b) &#x3D; &amp;(160k - 156.3)^2 + (166k - 160.6)^2 +(172k - 165.1)^2 + (174k - 158.5)^2 + (180k - 156.3)^2<br>\end{aligned}$</span></p>
<p>展开成二元一次方程组</p>
<p>$$\begin{aligned}<br>L(k,b) &#x3D; 145416k^2 - 281671.6k + 136496.32<br>\end{aligned}$$</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250607143836920-20250608222015992.png" alt="image-20250607143836920"></p>
<p>根据图像得知，损失函数的最小值为图像的切线斜率为0的时候，也就是抛物线的低点。</p>
<p>由此可以通过求一元二次方程组的导，然后令导数为0即可求出解。</p>
<h2 id="线性回归求解方法"><a href="#线性回归求解方法" class="headerlink" title="线性回归求解方法"></a>线性回归求解方法</h2><h3 id="一元线性回归"><a href="#一元线性回归" class="headerlink" title="一元线性回归"></a>一元线性回归</h3><blockquote>
<p>通过对两个变量分别求偏导数，然后联立二元一次方程组求解</p>
</blockquote>
<h3 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h3><blockquote>
<p>通过多元线性回归方程求解：</p>
<ul>
<li>公式$w&#x3D;(X^{T}X)^{-1}X^{T}y$<ul>
<li>X为特征值矩阵，y为目标值向量，w为模型参数的向量形式</li>
</ul>
</li>
</ul>
<p>例如根据房子的空间、卧室数量、层数以及房子年龄预测房子的价格：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250608161611645.png" alt="image-20250608161611645"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250608192832398.png" alt="image-20250608192832398"></p>
</blockquote>
<h2 id="梯度下降算法⭐️"><a href="#梯度下降算法⭐️" class="headerlink" title="梯度下降算法⭐️"></a>梯度下降算法⭐️</h2><p>算法思想：<code>沿着梯度下降的方向求解极小值</code></p>
<ul>
<li>输入:初始化位置S;每步距离为a 。输出:从位置S到达山底</li>
<li>步骤1:令初始化位置为山的任意位置S</li>
<li>步骤2:在当前位置环顾四周，如果四周都比S高返回S;否则执行步骤3</li>
<li>步骤3: 在当前位置环顾四周，寻找坡度最陡的方向，令其为x方向</li>
<li>步骤4:沿着x方向往下走，长度为a，到达新的位置$S’$</li>
<li>步骤5:在$S’$位置环顾四周，如果四周都比$S’$高，则返回$S’$。否则转到步骤3</li>
</ul>
<p>梯度是什么？简单理解记忆成:梯度就是导数+方向</p>
<ul>
<li>单变量函数中，梯度就是某一点切线斜率(某一点的导数)</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250608163506950.png" alt="image-20250608163506950"></p>
<ul>
<li>多变量函数中，梯度就是某一个点的偏导数<ul>
<li>$L(a,b,c)$的梯度为 $\left(\frac{\partial L}{\partial a}, \frac{\partial L}{\partial b}, \frac{\partial L}{\partial c}\right)$</li>
</ul>
</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250608163451996.png" alt="image-20250608163451996"></p>
<p>梯度下降公式：$$ \theta_{i+1} &#x3D; \theta_{i} - \alpha \frac{\partial}{\partial \theta_{i}} J(\theta) $$</p>
<p>或简写为：$$ W &#x3D; W - \eta \Delta $$</p>
<p>参数说明</p>
<ul>
<li>$\alpha$（或$\eta$）：学习率（步长）<ul>
<li>机器学习常用范围：$0.001 \sim 0.01$</li>
<li>深度学习常用范围：$10^{-6}$量级</li>
</ul>
</li>
<li>负号的意义：梯度是上升最快的方向，我们需要下降最快的方向，因此需要加负号</li>
<li>$\alpha$后面的是损失函数对某个特征求偏导</li>
</ul>
<h3 id="单变量梯度下降示例"><a href="#单变量梯度下降示例" class="headerlink" title="单变量梯度下降示例"></a>单变量梯度下降示例</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250608183909988.png" alt="image-20250608183909988"></p>
<p>求函数 $J(\theta) &#x3D; \theta^{2}$ 的最小值，即确定 $\theta$ 为何值时 $J(\theta)$ 最小。</p>
<ul>
<li><p>函数导数：$\frac{\partial J(\theta)}{\partial \theta} &#x3D; 2\theta$于是</p>
<ul>
<li>梯度下降更新公式：$\theta_{new} &#x3D; \theta - \alpha \cdot 2\theta$</li>
</ul>
</li>
<li><p>令：初始值：$\theta_0 &#x3D; 1$、学习率：$\alpha &#x3D; 0.4$</p>
</li>
</ul>
<p>迭代过程</p>
<table>
<thead>
<tr>
<th>步骤</th>
<th>计算公式</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td>第一步</td>
<td>$\theta &#x3D; 1$</td>
<td>1.0</td>
</tr>
<tr>
<td>第二步</td>
<td>$1 - 0.4 \times (2 \times 1)$</td>
<td>0.2</td>
</tr>
<tr>
<td>第三步</td>
<td>$0.2 - 0.4 \times (2 \times 0.2)$</td>
<td>0.04</td>
</tr>
<tr>
<td>第四步</td>
<td>$0.04 - 0.4 \times (2 \times 0.04)$</td>
<td>0.008</td>
</tr>
<tr>
<td>第五步</td>
<td>$0.008 - 0.4 \times (2 \times 0.008)$</td>
<td>0.0016</td>
</tr>
</tbody></table>
<p>收敛分析</p>
<ul>
<li>经过5次迭代后，$\theta$ 值从1收敛到0.0016</li>
<li>当 $N \to \infty$ 时，$\theta \to 0$，$J(\theta) \to 0$</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250608192602478.png" alt="image-20250608192602478"></p>
<h2 id="回归模型评估方法⭐️"><a href="#回归模型评估方法⭐️" class="headerlink" title="回归模型评估方法⭐️"></a>回归模型评估方法⭐️</h2><h3 id="绝对值误差函数-Mean-Absolute-Error-MAE-："><a href="#绝对值误差函数-Mean-Absolute-Error-MAE-：" class="headerlink" title="绝对值误差函数(Mean Absolute Error, MAE)："></a>绝对值误差函数(Mean Absolute Error, MAE)：</h3><p>$$ \text{MAE} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} \left| Y_i - \hat{Y}_i \right| $$</p>
<ul>
<li><p>上面的公式中：n 为样本数量, y 为实际值, $\hat{y}$ 为预测值</p>
</li>
<li><p>MAE 越小模型预测约准确</p>
</li>
</ul>
<p>Sklearn 中MAE的API</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line">mean_absolute_error(y_test,y_predict)</span><br></pre></td></tr></table></figure>



<h3 id="均方误差函数-Mean-Squared-Error-MSE-："><a href="#均方误差函数-Mean-Squared-Error-MSE-：" class="headerlink" title="均方误差函数(Mean Squared Error, MSE)："></a>均方误差函数(Mean Squared Error, MSE)：</h3><p>$$ \text{MSE} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} \left(Y_i - \hat{Y}_i\right)^2 $$</p>
<ul>
<li>上面的公式中：n 为样本数量, y 为实际值, $\hat{y}$ 为预测值</li>
<li>MSE 越小模型预测约准确</li>
</ul>
<p>Sklearn 中MSE的API</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line">mean_squared_error(y_test,y_predict)</span><br></pre></td></tr></table></figure>



<h3 id="均方误差函数-Root-Mean-Squared-Error-RMSE"><a href="#均方误差函数-Root-Mean-Squared-Error-RMSE" class="headerlink" title="均方误差函数(Root Mean Squared Error ,RMSE)"></a>均方误差函数(Root Mean Squared Error ,RMSE)</h3><p>$$<br>RMSE &#x3D; \sqrt{\frac{1}{n}\sum_{i&#x3D;1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}}<br>$$</p>
<ul>
<li>上面的公式中：n 为样本数量, y 为实际值, $\hat{y}$ 为预测值</li>
<li>RMSE 越小模型预测约准确</li>
</ul>
<h3 id="MSE与MAE与RMSE对比⭐️"><a href="#MSE与MAE与RMSE对比⭐️" class="headerlink" title="MSE与MAE与RMSE对比⭐️"></a>MSE与MAE与RMSE对比⭐️</h3><table>
<thead>
<tr>
<th>指标</th>
<th>公式</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td><strong>MAE</strong><br>(Mean Absolute Error)</td>
<td>$\frac{1}{n}\sum_{i&#x3D;1}^{n}|y_i-\hat{y}_i|$</td>
<td>绝对误差的平均值</td>
</tr>
<tr>
<td><strong>MSE</strong><br>(Mean Squared Error)</td>
<td>$\frac{1}{n}\sum_{i&#x3D;1}^{n}(y_i-\hat{y}_i)^2$</td>
<td>平方误差的平均值</td>
</tr>
<tr>
<td><strong>RMSE</strong><br>(Root Mean Squared Error)</td>
<td>$\sqrt{\frac{1}{n}\sum_{i&#x3D;1}^{n}(y_i-\hat{y}_i)^2}$</td>
<td>MSE的平方根</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>特性</th>
<th>MAE</th>
<th>MSE</th>
<th>RMSE</th>
</tr>
</thead>
<tbody><tr>
<td><strong>误差敏感性</strong></td>
<td>对异常值不敏感</td>
<td>对异常值敏感</td>
<td>对异常值敏感</td>
</tr>
<tr>
<td><strong>量纲</strong></td>
<td>与原数据一致</td>
<td>原数据平方量纲</td>
<td>与原数据一致</td>
</tr>
<tr>
<td><strong>数学性质</strong></td>
<td>不可导</td>
<td>处处可导</td>
<td>处处可导</td>
</tr>
<tr>
<td><strong>优化方向</strong></td>
<td>中位数</td>
<td>平均值</td>
<td>平均值</td>
</tr>
<tr>
<td><strong>计算复杂度</strong></td>
<td>低</td>
<td>中等</td>
<td>中等（需开方）</td>
</tr>
</tbody></table>
<ol>
<li><strong>MAE适用场景</strong>：<ul>
<li>需要解释性强的场景</li>
<li>数据存在少量异常值时</li>
<li>例如：房价预测、零售销量预测</li>
</ul>
</li>
<li><strong>MSE&#x2F;RMSE适用场景</strong>：<ul>
<li>需要惩罚大误差的场景</li>
<li>数据质量较高且分布均匀时</li>
<li>例如：金融风险评估、科学实验分析</li>
</ul>
</li>
<li><strong>特殊注意事项</strong>：<ul>
<li>MSE值通常比MAE大1-2个数量级</li>
<li>RMSE在量纲上与MAE可比，但数值通常更大</li>
<li>当误差分布呈高斯分布时，MSE是最优指标</li>
</ul>
</li>
</ol>
<h2 id="梯度下降算法调优及改进的梯度算法"><a href="#梯度下降算法调优及改进的梯度算法" class="headerlink" title="梯度下降算法调优及改进的梯度算法"></a>梯度下降算法调优及改进的梯度算法</h2><h3 id="梯度下降算法调优"><a href="#梯度下降算法调优" class="headerlink" title="梯度下降算法调优"></a>梯度下降算法调优</h3><p>梯度下降公式：$$ \theta_{i+1} &#x3D; \theta_{i} - \alpha \frac{\partial}{\partial \theta_{i}} J(\theta) $$</p>
<ul>
<li><p>其中</p>
<ul>
<li>步长：$\theta_0$</li>
<li>学习率：$\alpha$</li>
</ul>
</li>
</ul>
<p>在使用梯度下降时，需要进行调优。哪些地方需要调优呢？</p>
<p>  　　1. 算法的步长选择。在前面取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从小到大，分别运行算法，看看迭代效果，<code>如果损失函数在变小，说明取值有效，否则要增大步长</code>。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。</p>
<p>  　　2. 算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</p>
<p>　　3.归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对于每个特征x，求出它的期望x¯和标准差std(x)，然后转化为：$$ \frac{x - \overline{x}}{\operatorname{std}(x)} $$</p>
<p>这样特征的新期望为0，新方差为1，迭代速度可以大大加快。</p>
<h3 id="改进的梯度算法"><a href="#改进的梯度算法" class="headerlink" title="改进的梯度算法"></a>改进的梯度算法</h3><ul>
<li>批量梯度下降法BGD（Batch Gradient Descent）</li>
</ul>
<p>$$<br>\theta_{j} &#x3D; \theta_{j} - \alpha \left(<br>\frac{1}{N} \sum_{i&#x3D;1}^{N} \left(<br>h_{\theta}\left(x_{0}^{(i)}, x_{1}^{(i)}, \cdots, x_{n}^{(i)}\right) - y^{(i)}<br>\right) x_{j}^{(i)}<br>\right)<br>$$</p>
<p>批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时<code>使用所有的样</code>本来进行更新，这个方法就是之前的线性回归的梯度下降算法。</p>
<ul>
<li>随机梯度下降法SGD（Stochastic Gradient Descent）</li>
</ul>
<p>$$<br>\theta_{j} :&#x3D; \theta_{j} - \alpha \left(<br>h_{\theta}\left(x_{0}^{(i)}, x_{1}^{(i)}, \cdots, x_{n}^{(i)}\right) - y^{(i)}<br>\right) x_{j}^{(i)}<br>$$</p>
<p>随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的N个样本的数据，而是仅仅选取一个样本j来求梯度。</p>
<p>随机梯度下降法，和批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。<br>那么，有没有一个中庸的办法能够结合两种方法的优点呢？有！这小批量梯度下降法MBGD。</p>
<ul>
<li>小批量梯度下降法MBGD（Mini-batch Gradient Descent）</li>
</ul>
<p>$$<br>\theta_{j} :&#x3D; \theta_{j} - \alpha \left(<br>\frac{1}{x} \sum_{i&#x3D;t}^{t+x-1} \left(<br>h_{\theta}\left(x_{0}^{(i)}, x_{1}^{(i)}, \cdots, x_{n}^{(i)}\right) - y^{(i)}<br>\right) x_{j}^{(i)}<br>\right)<br>$$</p>
<p>小批量梯度下降法是批量梯度下降法和随机梯度下降法的折中，也就是对于N个样本，我们采用x个样本来迭代，1&lt;x&lt;N。一般可以取x&#x3D;10，当然根据样本的数据，可以调整这个x的值。</p>
<p>借鉴别的资料上的一些叫法，反正最后都是使用mini-batch（小批量梯度下降算法</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250608215326966.png" alt="image-20250608215326966"></p>
<h2 id="过拟合和欠拟合出现原因及解决方案⭐️"><a href="#过拟合和欠拟合出现原因及解决方案⭐️" class="headerlink" title="过拟合和欠拟合出现原因及解决方案⭐️"></a>过拟合和欠拟合出现原因及解决方案⭐️</h2><p>过拟合：一个假设 <strong>在训练数据上能够获得比其他假设更好的拟合， 但是在测试数据集上却不能很好地拟合数据</strong> (体现在准确率下降)，此时认为这个假设出现了过拟合的现象。(模型过于复杂)</p>
<p>欠拟合：一个假设 <strong>在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据</strong> ，此时认为这个假设出现了欠拟合的现象。(模型过于简单)</p>
<p>欠拟合在训练集和测试集上的误差都较大</p>
<p>过拟合在训练集上误差较小，而测试集上误差较大欠拟合在训练集和测试集上的误差都较大</p>
<p>过拟合在训练集上误差较小，而测试集上误差较大</p>
<p><strong>欠拟合产生原因：</strong> 学习到数据的特征过少</p>
<p>解决办法：</p>
<p><strong>1）添加其他特征项</strong>，有时出现欠拟合是因为特征项不够导致的，可以添加其他特征项来解决</p>
<p><strong>2）添加多项式特征</strong>，模型过于简单时的常用套路，例如将线性模型通过添加二次项或三次项使模型泛化能力更强</p>
<p><strong>过拟合产生原因：</strong> 原始特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾所有测试样本</p>
<p>解决办法：</p>
<p>1）重新清洗数据，导致过拟合的一个原因有可能是数据不纯，如果出现了过拟合就需要重新清洗数据。</p>
<p>2）增大数据的训练量，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。</p>
<p><strong>3）正则化</strong></p>
<p>4）减少特征维度</p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><h2 id="案例：银行信贷案例-多元线性回归"><a href="#案例：银行信贷案例-多元线性回归" class="headerlink" title="案例：银行信贷案例(多元线性回归)"></a>案例：银行信贷案例(多元线性回归)</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250608220958009.png" alt="image-20250608220958009"></p>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><h3 id="正规方程法"><a href="#正规方程法" class="headerlink" title="正规方程法"></a>正规方程法</h3><h2 id="案例：波士顿房价案例-线性回归及API"><a href="#案例：波士顿房价案例-线性回归及API" class="headerlink" title="案例：波士顿房价案例(线性回归及API)"></a>案例：波士顿房价案例(线性回归及API)</h2><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li>详解梯度下降：<a target="_blank" rel="noopener" href="https://blog.csdn.net/zhouaho2010/article/details/102756411">https://blog.csdn.net/zhouaho2010/article/details/102756411</a></li>
<li>过拟合正则化解决：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/641618528">https://zhuanlan.zhihu.com/p/641618528</a></li>
</ul>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><h2 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h2><blockquote>
<p>Logistic Regression 虽然被称为回归，但其实际上是分类模型，并常用于二分类。其本质是：假设数据服从这个分布，然后使用极大似然估计做参数的估计。</p>
</blockquote>
<p>其分布函数和密度函数分别为：</p>
<p>$F(x) &#x3D; P(X\leq{x})&#x3D;\frac{1}{1 + e^{-(x-u)&#x2F;\gamma}}$</p>
<p>$f(x) &#x3D; F’(X\leq{x})&#x3D;\frac{e^{-(x-u)&#x2F;\gamma}}{\gamma(1 + e^{-(x-u)&#x2F;\gamma})^{2}}$</p>
<p>其中， $\mu$ 表示<strong>位置参数</strong>， $\gamma&gt;0$ 为<strong>形状参数</strong>，可以看下其图像特征：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250609160632664.png" alt="image-20250609160632664"></p>
<p>Logistic 分布是由其位置和尺度参数定义的连续分布。Logistic 分布的形状与正态分布的形状相似，但是 Logistic 分布的尾部更长，所以我们可以使用 Logistic 分布来建模比正态分布具有更长尾部和更高波峰的数据分布。在深度学习中常用到的 Sigmoid 函数就是 Logistic 的分布函数在 $\mu&#x3D;0, \gamma&#x3D;1$ 的特殊形式。</p>
<h2 id="Sigmoid-函数"><a href="#Sigmoid-函数" class="headerlink" title="Sigmoid 函数"></a>Sigmoid 函数</h2><ul>
<li><p>一种分类模型，把线性回归的输出，作为逻辑回归的输入。 </p>
</li>
<li><p>输出是(0, 1)之间的值</p>
</li>
<li><p>基本思想</p>
<ul>
<li><p>利用线性模型 $f(x) &#x3D; wx + b $根据特征的重要性计算出一个值</p>
</li>
<li><p>再使用 sigmoid 函数将 $f(x) $的输出值映射为概率值</p>
<ul>
<li>设置阈值(eg:0.5)，输出概率值大于 0.5，则将未知样本输出为 1 类</li>
<li>否则输出为 0 类</li>
</ul>
</li>
<li><p>逻辑回归的假设函数<br>$h(w) &#x3D; sigmoid(wx + b)$</p>
</li>
</ul>
</li>
</ul>
<p>线性回归的输出，作为逻辑回归的输入</p>
<p>那么如何去衡量逻辑回归的预测结果与真实结果的差异？</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250609191457815-20250609204421917.png" alt="image-20250609191457815"></p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>公式：</p>
<p>$$ Loss(L) &#x3D; -\sum_{i&#x3D;1}^{m} \left( y_{i}\log(p_{i}) + (1 - y_{i})\log(1 - p_{i}) \right) $$</p>
<p>其中$$ p_{i} &#x3D; \operatorname{sigmoid}(w^{T}x + b) $$是逻辑回归的输出结果</p>
<ul>
<li>损失函数的工作原理：每个样本预测值有A、B两个类别，真实类别对应的位置，概率值域越大越好</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250609185456031.png" alt="image-20250609185456031"></p>
<p>一个样本的</p>
<h2 id="逻辑回归API"><a href="#逻辑回归API" class="headerlink" title="逻辑回归API"></a>逻辑回归API</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.linear_model.LogisticRegression(solver=<span class="string">&#x27;liblinear&#x27;</span>, penalty=‘l2’, C = <span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>solver</strong> ：<strong>损失函数优化方法</strong>:</p>
<ul>
<li><p>训练速度:liblinear 对小数据集场景训练速度更快，sag 和 saga 对大数据集更快一些。 </p>
</li>
<li><p>正则化:</p>
<ul>
<li>newton-cg、lbfgs、sag、saga 支持 L2 正则化或者没有正则化</li>
<li>2liblinear 和 saga 支持 L1 正则化</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>penalty</strong>：正则化的种类</p>
<ul>
<li>L1 正则化或者  L2 正则化</li>
</ul>
</li>
<li><p><strong>C</strong>：正则化力度</p>
<ul>
<li>默认将类别数量少的当做正例</li>
</ul>
</li>
</ul>
<h2 id="分类评估方法⭐️"><a href="#分类评估方法⭐️" class="headerlink" title="分类评估方法⭐️"></a>分类评估方法⭐️</h2><h3 id="混淆矩阵及其构建"><a href="#混淆矩阵及其构建" class="headerlink" title="混淆矩阵及其构建"></a>混淆矩阵及其构建</h3><blockquote>
<p>简单根据象限来记忆：TP、FN、FP、TN（从上到下、从左到右都是正、假）</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250609202123066.png" alt="image-20250609202123066"></p>
<p>混淆矩阵作用在测试集样本集中：</p>
<ol>
<li>真实值是 <strong>正例</strong> 的样本中，被分类为 <strong>正例</strong> 的样本数量有多少，这部分样本叫做真正例（TP，True Positive）</li>
<li>真实值是 <strong>正例</strong> 的样本中，被分类为 <strong>假例</strong> 的样本数量有多少，这部分样本叫做伪反例（FN，False Negative）</li>
<li>真实值是 <strong>假例</strong> 的样本中，被分类为 <strong>正例</strong> 的样本数量有多少，这部分样本叫做伪正例（FP，False Positive）</li>
<li>真实值是 <strong>假例</strong> 的样本中，被分类为 <strong>假例</strong> 的样本数量有多少，这部分样本叫做真反例（TN，True Negative）</li>
</ol>
<h3 id="模型预测正例假例举例："><a href="#模型预测正例假例举例：" class="headerlink" title="模型预测正例假例举例："></a><strong>模型预测正例假例举例：</strong></h3><p>样本集中有 6 个恶性肿瘤样本，4 个良性肿瘤样本，假设恶性肿瘤为正例，则：</p>
<p><strong>模型 A：</strong> 预测对了 3 个恶性肿瘤样本，4 个良性肿瘤样本</p>
<ol>
<li>真正例 TP 为：3 </li>
<li>伪反例 FN 为：3</li>
<li>假正例 FP 为：0</li>
<li>真反例 TN：4</li>
<li><strong>精准率：3&#x2F;(3+0) &#x3D; 100%</strong></li>
<li><strong>召回率：3&#x2F;(3+3)&#x3D;50%</strong></li>
<li><strong>F1-score：(2*3)&#x2F;(2*3+3+0)&#x3D;67%</strong></li>
</ol>
<p><strong>模型 B：</strong> 预测对了 6 个恶性肿瘤样本，1个良性肿瘤样本</p>
<ol>
<li>真正例 TP 为：6</li>
<li>伪反例 FN 为：0</li>
<li>假正例 FP 为：3</li>
<li>真反例 TN：1</li>
<li><strong>精准率：6&#x2F;(6+3) &#x3D; 67%</strong></li>
<li><strong>召回率：6&#x2F;(6+0)&#x3D; 100%</strong> </li>
<li><strong>F1-score：(2*6)&#x2F;(2*6+0+3)&#x3D;80%</strong></li>
</ol>
<p><code>TP+FN+FP+TN = 总样本数量</code></p>
<h3 id="Precision（精确率）"><a href="#Precision（精确率）" class="headerlink" title="Precision（精确率）"></a>Precision（精确率）</h3><blockquote>
<p>精确率也叫做查准率，指的是对正例样本的预测准确率。</p>
<p>比如：我们把恶性肿瘤当做正例样本，则我们就需要知道<code>模型对恶性肿瘤的预测准确率</code>。</p>
</blockquote>
<p>公式：$$ P &#x3D; \frac{TP}{TP + FP} &#x3D; \frac{第二象限}{第二象限 + 第三象限}$$</p>
<h3 id="Recall（召回率）"><a href="#Recall（召回率）" class="headerlink" title="Recall（召回率）"></a>Recall（召回率）</h3><blockquote>
<p>召回率也叫做查全率，指的是预测为真正例样本占所有真实正例样本的比重。</p>
<p>例如：我们把恶性肿瘤当做正例样本，则我们想知道<code>模型是否能把所有的恶性肿瘤患者都预测出来</code>。</p>
</blockquote>
<p>公式：$$ P &#x3D; \frac{TP}{TP + FN} $$</p>
<h3 id="F1-score"><a href="#F1-score" class="headerlink" title="F1-score"></a>F1-score</h3><blockquote>
<p>如果我们对模型的精度、召回率都有要求，希望知道模型在这两个评估方向的综合预测能力如何？则可以使用 F1-score 指标。</p>
</blockquote>
<p>$$ F1 &#x3D; \frac{2TP}{2TP + FN + FP} &#x3D; \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} $$</p>
<h3 id="ROC曲线和AUC指标"><a href="#ROC曲线和AUC指标" class="headerlink" title="ROC曲线和AUC指标"></a>ROC曲线和AUC指标</h3><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><p>参考链接：<a href="https://liamjohnson-w.github.io/2023/08/09/2023.08.09/">https://liamjohnson-w.github.io/2023/08/09/2023.08.09/</a></p>
<p><code>决策树</code>算法是一种监督学习算法，英文是Decision tree。</p>
<p><code>决策树</code>是一个类似于流程图的树结构：其中，每个内部结点表示一个特征或属性，而每个树叶结点代表一个分类。树的最顶层是根结点。使用决策树分类时就是将实例分配到叶节点的类中。该叶节点所属的类就是该节点的分类。</p>
<p><code>决策树</code>思想的来源非常朴素，试想每个人的大脑都有类似于if-else这样的逻辑判断，这其中的if表示的是条件，if之后的then就是一种选择或决策。程序设计中的条件分支结构就是if-then结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法。</p>
<h2 id="构建决策树"><a href="#构建决策树" class="headerlink" title="构建决策树"></a>构建决策树</h2><blockquote>
<p>构建决策树包括三个步骤：</p>
<ul>
<li>特征选择：选取有较强分类能力的特征。</li>
<li>决策树生成：根据选择的特征生成决策树。典型的算法有ID3、C4.5、CART，它们生成决策树过程相似，ID3是采用<code>信息增益</code>作为特征选择度量，而C4.5采用<code>信息增益率</code>、CART<code>基尼指数</code>。</li>
<li>决策树剪枝：决策树也易过拟合，采用剪枝的方法缓解过拟合。剪枝原因是决策树生成算法生成的树对训练数据的预测很准确，但是对于未知数据分类很差，这就产生了<code>过拟合</code>的现象。</li>
</ul>
</blockquote>
<table>
<thead>
<tr>
<th><strong>名称</strong></th>
<th><strong>提出时间</strong></th>
<th><strong>分支方式</strong></th>
<th><strong>特点</strong></th>
</tr>
</thead>
<tbody><tr>
<td>ID3</td>
<td>1975</td>
<td>信息增益</td>
<td>1.ID3只能对离散属性的数据集构成决策树  2.倾向于选择取值较多的属性</td>
</tr>
<tr>
<td>C4.5</td>
<td>1993</td>
<td>信息增益率</td>
<td>1.缓解了ID3分支过程中总喜欢偏向选择值较多的属性  2.可处理连续数值型属性，也增加了对缺失值的处理方法  3.只适合于能够驻留于内存的数据集,大数据集无能为力</td>
</tr>
<tr>
<td>CART</td>
<td>1984</td>
<td>基尼指数</td>
<td>1.可以进行分类和回归，可处理离散属性，也可以处理连续属性  2.采用基尼指数，计算量减小  3.一定是二叉树</td>
</tr>
</tbody></table>
<h3 id="Notice"><a href="#Notice" class="headerlink" title="Notice"></a>Notice</h3><ol>
<li><p>信息增益（ID3）、信息增益率值越大（C4.5），则说明优先选择该特征。</p>
</li>
<li><p>基尼指数值越小（cart），则说明优先选择该特征。</p>
</li>
</ol>
<h2 id="ID3算法构建决策树⭐️"><a href="#ID3算法构建决策树⭐️" class="headerlink" title="ID3算法构建决策树⭐️"></a>ID3算法构建决策树⭐️</h2><blockquote>
<p>ID3 树是基于<code>信息增益</code>构建的决策树.</p>
<p><code>使用信息增益最大的特征作为决策树的一个分裂节点。</code></p>
</blockquote>
<h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><p>ID3 树是基于信息增益构建的决策树.</p>
<p>定义</p>
<ul>
<li>熵在信息论中代表随机变量不确定度的度量。</li>
<li>熵越大，数据的不确定性度越高 </li>
<li>熵越小，数据的不确定性越低</li>
</ul>
<p>公式</p>
<p>$$<br>\large<br>H &#x3D; -\sum_{i&#x3D;1}^{k}p_i\log_{b}(p_i)<br>$$</p>
<p><strong>参数说明</strong></p>
<ul>
<li>$b&#x3D;2$（默认）：单位为 比特（bits）（信息论常用）。</li>
<li>$b&#x3D;e$（自然对数）：单位为 纳特（nats）。</li>
<li>$b&#x3D;10$：单位为 哈特莱（hartleys）。</li>
</ul>
<p>例子1：假如有三个类别，分别占比为：{1&#x2F;3,1&#x2F;3,1&#x2F;3}，信息熵计算结果为：</p>
<p>$H&#x3D;-\frac{1}{3}\log_{2}(\frac{1}{3})-\frac{1}{3}\log_{2}(\frac{1}{3})-\frac{1}{3}\log_{2}(\frac{1}{3})&#x3D;1.0986$</p>
<p>例子2：假如有三个类别，分别占比为：{1&#x2F;10,2&#x2F;10,7&#x2F;10}，信息熵计算结果为：</p>
<p>$H&#x3D;-\frac{1}{10}\log_{2}(\frac{1}{10})-\frac{2}{10}\log_{2}(\frac{2}{10})-\frac{7}{10}\log_{2}(\frac{7}{10})&#x3D;0.8018$</p>
<p>熵越大，表示整个系统不确定性越大，越随机，反之确定性越强。</p>
<p>例子3：假如有三个类别，分别占比为：{1,0,0}，信息熵计算结果为：</p>
<p>$H&#x3D;-1\log_{2}(1)&#x3D;0$</p>
<h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>特征$A$对 数据集D的信息增益$g(D,A)$，定义为<code>集合$D$的熵$H(D)$与特征A给定条件下D的熵$H(D|A)$之差</code>。即</p>
<p>$$<br>\large<br>g(D,A)&#x3D;H(D)-H(D|A)<br>$$</p>
<p>根据信息增益选择特征方法是：对训练数据集D，计算其每个特征的信息增益，并比较它们的大小，并选择信息增益最大的特征进行划分。<strong>表示由于特征$A$而使得对数据D的分类不确定性减少的程度。</strong></p>
<p>算法：</p>
<p>设训练数据集为D，$\mid D\mid$表示其样本个数。设有$K$个类$C_k$，$k&#x3D;1,2,\cdots,K$，$\mid C_k\mid$为属于类$C_k$的样本个数，$\sum\limits_{k&#x3D;1}^{K}&#x3D;\mid{D}\mid$。设特征A有$n$个不同取值${a_1, a_2, \cdots,a_n}$，根据特征A的取值将D划分为$n$个子集$D_1, D_2, \cdots,D_n$，$\mid D_i\mid$为$D_i$样本个数，$\sum\limits_{i&#x3D;1}^n\mid D_i\mid&#x3D;\mid D\mid$。子集中属于类$C_k$的样本集合为$D_{ik}$，即$D_{ik}&#x3D;D_i\bigcap C_k$，$\mid D_{ik}\mid$为$D_{ik}$的样本个数。信息增益算法如下：</p>
<ul>
<li><p>输入：训练数据集$D$和特征$A$；</p>
</li>
<li><p>输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$</p>
</li>
<li><p>(1) 计算数据集$D$的经验熵$H(D)$</p>
<p>$H(D)&#x3D;-\sum\limits_{k&#x3D;1}^{K}\frac{\mid C_k\mid}{\mid D\mid}\log_2\frac{\mid C_k\mid}{\mid D\mid}$</p>
</li>
<li><p>(2) 计算特征$A$对数据集$D$的经验条件熵$H(D\mid A)$</p>
<p>$H(D\mid A)&#x3D;\sum\limits_{i&#x3D;1}^{n}\frac{\mid D_i\mid}{\mid D\mid}H(D_i)&#x3D;-\sum\limits_{i&#x3D;1}^{n}\frac{\mid D_i\mid}{\mid D\mid}\sum_\limits{k&#x3D;1}^{K}\frac{\mid D_{ik}\mid}{\mid D_i\mid}\log_2\frac{\mid D_{ik}\mid}{\mid D_i\mid}$</p>
</li>
<li><p>(3) 计算信息增益</p>
<p>$g(D,A)&#x3D;H(D)-H(D|A)$</p>
</li>
</ul>
<h3 id="信息增益计算案例"><a href="#信息增益计算案例" class="headerlink" title="信息增益计算案例"></a>信息增益计算案例</h3><p>例子：已知6个样本，根据特征a：</p>
<table>
<thead>
<tr>
<th>特征a</th>
<th>目标值</th>
</tr>
</thead>
<tbody><tr>
<td>α</td>
<td>A</td>
</tr>
<tr>
<td>α</td>
<td>A</td>
</tr>
<tr>
<td>β</td>
<td>B</td>
</tr>
<tr>
<td>α</td>
<td>A</td>
</tr>
<tr>
<td>β</td>
<td>B</td>
</tr>
<tr>
<td>α</td>
<td>B</td>
</tr>
</tbody></table>
<p>$α$ 部分对应的目标值为： $AAAB$</p>
<p>$β $部分对应的目标值为：$BB$</p>
<ul>
<li>条件为$α$熵：$-\frac{3}{4} * log_{2}(\frac{3}{4}) - \frac{1}{4} * log_{2}(\frac{1}{4}) &#x3D; 0.81 $</li>
<li>条件为$β$熵：$ -(\frac{2}{2} * log_{2}(\frac{2}{2})) &#x3D; 0$</li>
<li>条件熵：$α $部分占了 $\frac{4}{6}$，$β$ 部分 占了 $\frac{2}{6}$<ul>
<li>$(\frac{4}{6}) * 0.81 + (\frac{2}{6}）* 0 &#x3D; 0.54$</li>
</ul>
</li>
<li>熵：$-\frac{3}{6} * log_{2}(\frac{3}{6}) – \frac{3}{6} * log_{2}(\frac{3}{6}) &#x3D; 1$</li>
<li>信息增益：熵 – 条件熵： $1.0 – 0.54 &#x3D; 0.46$</li>
</ul>
<h3 id="ID3树构建流程"><a href="#ID3树构建流程" class="headerlink" title="ID3树构建流程"></a>ID3树构建流程</h3><p><strong>构建流程：</strong></p>
<ol>
<li>计算每个特征的信息增益</li>
<li>使用信息增益最大的特征将数据集 $S $拆分为子集</li>
<li>使用该特征（信息增益最大的特征）作为决策树的一个节点</li>
<li>使用剩余特征对子集重复上述过程</li>
</ol>
<p><strong>案例：</strong></p>
<p>已知：某一个论坛客户流失率数据</p>
<p>需求：考察性别、活跃度特征哪一个特征对流失率的影响更大</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610200825130.png" alt="image-20250610200825130"></p>
<p>分析：</p>
<p>15条样本：5正样本、10个负样本</p>
<ul>
<li>计算熵</li>
</ul>
<p>$$ H(D) &#x3D; \left(-\frac{5}{15}\log_{2}\frac{5}{15}\right) + \left(-\frac{10}{15}\log_{2}\frac{10}{15}\right) &#x3D; 0.9812 $$</p>
<ul>
<li>计算性别条件熵(a&#x3D;”性别”)：</li>
</ul>
<p>$$<br>\begin{aligned}<br>H(D, \text{性别}) &amp;&#x3D; \sum_{v&#x3D;1}^{n} \frac{D^{v}}{D} H(D^{v})<br>\end{aligned}<br>$$</p>
<p>$$&#x3D; \left(\frac{8}{15}\right)\left(-\frac{3}{8} \log_{2} \frac{3}{8} - \frac{5}{8} \log_{2} \frac{5}{8}\right) + \left(\frac{7}{15}\right)\left(-\frac{2}{7} \log_{2} \frac{2}{7} - \frac{5}{7} \log_{2} \frac{5}{7}\right)$$</p>
<ul>
<li>计算性别信息增益(a&#x3D;”性别”)</li>
</ul>
<p>$$<br>\begin{aligned}<br>g(D, \alpha) &amp;&#x3D; H(D) - H(D \mid \alpha) \<br>&amp;&#x3D; 0.9812 - \left[<br>    \left(\frac{8}{15}\right)\left(-\frac{3}{8}\log_{2}\frac{3}{8} - \frac{5}{8}\log_{2}\frac{5}{8}\right)<br>    + \left(\frac{7}{15}\right)\left(-\frac{2}{7}\log_{2}\frac{2}{7} - \frac{5}{7}\log_{2}\frac{5}{7}\right)<br>\right] \<br>&amp;&#x3D; 0.0064<br>\end{aligned}<br>$$</p>
<ul>
<li>计算活跃度条件熵(a&#x3D;“活跃度”)</li>
</ul>
<p>$$<br>\begin{aligned}<br>H(D, \text{活跃度}) &amp;&#x3D; \sum_{v&#x3D;1}^{n} \frac{D^{v}}{D} H(D^{v}) \<br>&amp;&#x3D; \left(\frac{6}{15}\right)(0) + \left(\frac{5}{15}\right)\left(-\frac{1}{5}\log_{2}\frac{1}{5} - \frac{4}{5}\log_{2}\frac{4}{5}\right) + \left(\frac{4}{15}\right)(0)<br>\end{aligned}<br>$$</p>
<ul>
<li>计算活跃度信息增益(a&#x3D;活跃度”)</li>
</ul>
<p>$$<br>\begin{aligned}<br>g(D, \alpha) &amp;&#x3D; H(D) - H(D \mid \alpha) \<br>&amp;&#x3D; 0.9812 - \left[<br>    \left(\frac{6}{15}\right)(0) +<br>    \left(\frac{5}{15}\right)\left(-\frac{1}{5}\log_{2}\frac{1}{5} - \frac{4}{5}\log_{2}\frac{4}{5}\right) +<br>    \left(\frac{4}{15}\right)(0)<br>\right] \<br>&amp;&#x3D; 0.6776<br>\end{aligned}<br>$$</p>
<p><strong>结论</strong>：活跃度的信息增益比性别的信息增益大，对用户流失的影响比性别大。</p>
<pre class="mermaid">graph TD
    A[活跃度] --> B[高]
    A --> C[低]
    A --> D[中]
    B --> E[未流失]
    C --> F[流失]
    D --> G[性别]
    G --> H[女]
    G --> I[男]
    H --> J[流失]
    I --> K[未流失]</pre>



<h2 id="C4-5算法构建决策树"><a href="#C4-5算法构建决策树" class="headerlink" title="C4.5算法构建决策树"></a>C4.5算法构建决策树</h2><h3 id="信息增益率"><a href="#信息增益率" class="headerlink" title="信息增益率"></a>信息增益率</h3><blockquote>
<p><code>根据信息增益率，选择特征的信息增益率大作为分裂特征。</code></p>
</blockquote>
<p>$$<br>\begin{aligned}<br>\text{Gain_Ratio}(D, a) &amp;&#x3D; \frac{\text{Gain}(D, a)}{IV(a)} \<br>IV(a) &amp;&#x3D; -\sum_{v&#x3D;1}^{V} \frac{D^{v}}{D} \log \frac{D^{v}}{D}<br>\end{aligned}<br>$$</p>
<ol>
<li>Gain_Ratio 表示信息增益率</li>
<li>IV 表示分裂信息、内在信息</li>
<li>信息增益率&#x3D;特征的信息增益 ➗ 内在信息<ul>
<li>如果某个特征的特征值种类较多，则其内在信息值就越大。即：特征值种类越多，除以的系数就越大。</li>
<li>如果某个特征的特征值种类较小，则其内在信息值就越小。即：特征值种类越小，除以的系数就越小。</li>
</ul>
</li>
</ol>
<p><strong>信息增益比本质： 是在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。惩罚参数：数据集D以特征A作为随机变量的熵的倒数。</strong></p>
<h3 id="C4-5决策树构建流程"><a href="#C4-5决策树构建流程" class="headerlink" title="C4.5决策树构建流程"></a>C4.5决策树构建流程</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610202930336.png" alt="image-20250610202930336"></p>
<p>特征a的信息增益率：</p>
<ol>
<li><strong>信息增益</strong>：</li>
</ol>
<p>$$<br>\begin{aligned}<br>&amp;\left(-\frac{3}{6}\log_{2}\frac{3}{6} - \frac{3}{6}\log_{2}\frac{3}{6}\right) \<br>&amp;- \left[ \frac{4}{6}\left(-\frac{3}{4}\log_{2}\frac{3}{4} - \frac{1}{4}\log_{2}\frac{1}{4}\right) + \frac{2}{6}(0) \right] \<br>&amp;&#x3D; 1 - 0.54 &#x3D; 0.46<br>\end{aligned}<br>$$</p>
<ol start="2">
<li><strong>IV信息熵</strong>：</li>
</ol>
<p>$$<br>-\frac{4}{6}\log_{2}\frac{4}{6} - \frac{2}{6}\log_{2}\frac{2}{6} &#x3D; 0.92<br>$$</p>
<ol start="3">
<li><strong>信息增益率</strong>：</li>
</ol>
<p>$$<br>\frac{0.46}{0.92} &#x3D; 0.5<br>$$</p>
<p>特征b的信息增益率：</p>
<ol>
<li><strong>信息增益</strong>：</li>
</ol>
<p>$$<br>-\frac{3}{6}\log_{2}\frac{3}{6} - \frac{3}{6}\log_{2}\frac{3}{6} - 6 \times 0 &#x3D; 1<br>$$</p>
<ol start="2">
<li><strong>IV信息熵</strong>：</li>
</ol>
<p>$$<br>-\frac{1}{6}\log_{2}\frac{1}{6} \times 6 &#x3D; 2.58<br>$$</p>
<ol start="3">
<li><strong>信息增益率</strong>：</li>
</ol>
<p>$$<br>\frac{1}{2.58} &#x3D; 0.39<br>$$</p>
<p>结论：特征a的信息增益率大于特征b的信息增益率，根据信息增益率，应该选择特征a作为分裂特征</p>
<h2 id="CART算法构建决策树⭐️"><a href="#CART算法构建决策树⭐️" class="headerlink" title="CART算法构建决策树⭐️"></a>CART算法构建决策树⭐️</h2><h3 id="Cart树简介"><a href="#Cart树简介" class="headerlink" title="Cart树简介"></a>Cart树简介</h3><blockquote>
<p>Cart模型是一种决策树模型，它即可以用于分类，也可以用于回归。</p>
<p>分类和回归树模型采用不同的最优化策略。Cart回归树使用平方误差最小化策略，<code>Cart分类生成树采用的基尼指数最小化策略。</code></p>
</blockquote>
<h3 id="基尼指数计算案例"><a href="#基尼指数计算案例" class="headerlink" title="基尼指数计算案例"></a>基尼指数计算案例</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610203514367.png" alt="image-20250610203514367"></p>
<h4 id="是否有房"><a href="#是否有房" class="headerlink" title="是否有房"></a>是否有房</h4><p>计算过程如下：根据是否有房将目标值划分为两部分：</p>
<ol>
<li><p>计算有房子的基尼值： <strong>有房子有 1、4、7 共计三个样本，对应：3个no、0个yes</strong></p>
<p>$G i n i(\text {是否有房，yes })&#x3D;1-\left(\frac{0}{3}\right)^{2}-\left(\frac{3}{3}\right)^{2}&#x3D;0$</p>
</li>
<li><p>计算无房子的基尼值：<strong>无房子有 2、3、5、6、8、9、10 共七个样本，对应：4个no、3个yes</strong></p>
<p>$\operatorname{Gini}(\text {是否有房，no })&#x3D;1-\left(\frac{3}{7}\right)^{2}-\left(\frac{4}{7}\right)^{2}&#x3D;0.4898$</p>
</li>
<li><p>计算基尼指数：<strong>第一部分样本数量占了总样本的 3&#x2F;10、第二部分样本数量占了总样本的 7&#x2F;10：</strong></p>
<p>$\operatorname{Gini_{-}} i n \operatorname{dex}(D, \text { 是否有房 })&#x3D;\frac{7}{10} * 0.4898+\frac{3}{10} * 0&#x3D;0.343$</p>
</li>
</ol>
<pre class="mermaid">graph TD
    A[7yes 3no] -->|有房| B[2no 0yes]
    A -->|无房| C[4no 3yes]
    
    style A fill:#ADD8E6,stroke:#333
    style B fill:#ADD8E6,stroke:#333
    style C fill:#ADD8E6,stroke:#333
    linkStyle 0 stroke:#FF0000,stroke-width:2px
    linkStyle 1 stroke:#FF0000,stroke-width:2px</pre>

<h4 id="婚姻状况"><a href="#婚姻状况" class="headerlink" title="婚姻状况"></a>婚姻状况</h4><ol>
<li><p>计算  <strong>{married} 和 {single,divorced}</strong> 情况下的基尼指数：</p>
<p>结婚的基尼值，有 2、4、6、9 共 4 个样本，并且对应目标值全部为 no：</p>
<p>$\operatorname{Gini_index}(D,\text)&#x3D;0$</p>
<p>不结婚的基尼值，有 1、3、5、7、8、10 共 6 个样本，并且对应 3 个 no，3 个 yes：</p>
<p>$\operatorname{Gini_index}(D, \text { {single,divorced} })&#x3D;1-\left(\frac{3}{6}\right)^{2}-\left(\frac{3}{6}\right)^{2}&#x3D;0.5$</p>
<p>以 married 作为分裂点的基尼指数：</p>
<p>$\operatorname{Gini_index}(D, \text { married })&#x3D;\frac{4}{10} * 0+\frac{6}{10} *\left[1-\left(\frac{3}{6}\right)^{2}-\left(\frac{3}{6}\right)^{2}\right]&#x3D;0.3$</p>
</li>
<li><p>计算  <strong>{single} | {married,divorced}</strong> 情况下的基尼指数</p>
<p>$\operatorname{Gini_index}(D,\text{婚姻状况})&#x3D;\frac{4}{10} * 0.5+\frac{6}{10} *\left[1-\left(\frac{1}{6}\right)^{2}-\left(\frac{5}{6}\right)^{2}\right]&#x3D;0.367$</p>
</li>
<li><p>计算  <strong>{divorced} | {single,married}</strong> 情况下基尼指数</p>
<p>$\operatorname{Gini_index}(D, \text { 婚姻状况 })&#x3D;\frac{2}{10} * 0.5+\frac{8}{10} *\left[1-\left(\frac{2}{8}\right)^{2}-\left(\frac{6}{8}\right)^{2}\right]&#x3D;0.4$</p>
</li>
<li><p>最终：该特征的基尼值为 0.3，并且预选分裂点为：**{married} 和 {single,divorced}**</p>
</li>
</ol>
<pre class="mermaid">graph TD
    A[7yes 3no] -->|结婚| B[4no 0yes]
    A -->|非结婚| C[3no 3yes]
    
    %% 样式设置
    style A fill:#ADD8E6,stroke:#333,stroke-width:2px
    style B fill:#ADD8E6,stroke:#333,stroke-width:2px
    style C fill:#ADD8E6,stroke:#333,stroke-width:2px
    
    %% 分支线样式
    linkStyle 0 stroke:#FF6B6B,stroke-width:2px
    linkStyle 1 stroke:#4ECDC4,stroke-width:2px</pre>

<h4 id="年收入"><a href="#年收入" class="headerlink" title="年收入"></a>年收入</h4><p>先将数值型属性升序排列，以相邻中间值作为待确定分裂点：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20230905153041427.png" alt="image-20230905153041427"></p>
<p>以年收入 65 将样本分为两部分，计算基尼指数:</p>
<p>$节点为65时:{年收入}&#x3D;\frac{1}{10} * 0 + \frac{9}{10} *\left[1-\left(\frac{6}{9}\right)^{2}-\left(\frac{3}{9}\right)^{2}\right]&#x3D;0.4$</p>
<p>以此类推计算所有分割点的基尼指数，我们发现最小的基尼指数为 0.3。</p>
<p>此时，我们发现：</p>
<ol>
<li>以是否有房作为分裂点的基尼指数为：0.343</li>
<li>以婚姻状况为分裂特征、以 married 作为分裂点的基尼指数为：0.3</li>
<li>以年收入作为分裂特征、以 97.5 作为分裂点的的基尼指数为：0.3</li>
</ol>
<p>最小基尼指数有两个分裂点，我们随机选择一个即可，假设婚姻状况，则可确定决策树如下：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610203859367.png" alt="image-20250610203859367"></p>
<h3 id="Cart案例（泰坦尼克号生存案例"><a href="#Cart案例（泰坦尼克号生存案例" class="headerlink" title="Cart案例（泰坦尼克号生存案例"></a>Cart案例（泰坦尼克号生存案例</h3><p>API</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">sklearn</span>.tree.DecisionTreeClassifier(criterion=’gini’,max_depth=<span class="literal">None</span>, random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>



<ul>
<li>criterion<ul>
<li>特征选择标准</li>
<li>“gini”或者”entropy”，前者代表基尼系数，后者代表信息增益。一默认”gini”，即CART算法。</li>
</ul>
</li>
<li>min_samples_split<ul>
<li>内部节点再划分所需最小样本数</li>
<li>这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。我之前的一个项目例子，有大概10万样本，建立决策树时，我选择了min_samples_split&#x3D;10。可以作为参考。</li>
</ul>
</li>
<li>min_samples_leaf<ul>
<li>叶子节点最少样本数</li>
<li>这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。之前的10万样本项目使用min_samples_leaf的值为5，仅供参考。</li>
</ul>
</li>
<li>max_depth<ul>
<li>决策树最大深度</li>
<li>决策树的最大深度，默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间</li>
</ul>
</li>
<li>random_state<ul>
<li>随机数种子</li>
</ul>
</li>
</ul>
<h2 id="Cart回归决策树⭐️"><a href="#Cart回归决策树⭐️" class="headerlink" title="Cart回归决策树⭐️"></a>Cart回归决策树⭐️</h2><h3 id="回归决策树构建原理"><a href="#回归决策树构建原理" class="headerlink" title="回归决策树构建原理"></a>回归决策树构建原理</h3><p>CART 回归树和 CART 分类树的不同之处在于:</p>
<ol>
<li>CART 分类树预测输出的是一个离散值，CART 回归树预测输出的是一个连续值。</li>
<li>CART 分类树使用基尼指数作为划分、构建树的依据，CART 回归树使用平方损失。</li>
<li>分类树使用叶子节点里出现更多次数的类别作为预测类别，回归树则采用叶子节点里均值作为预测输出</li>
</ol>
<h3 id="CART-回归树构建"><a href="#CART-回归树构建" class="headerlink" title="CART 回归树构建:"></a><strong>CART 回归树构建:</strong></h3><p>$$<br>\operatorname{Loss}(y, f(x))&#x3D;(f(x)-y)^{2}<br>$$</p>
<p><strong>例子：</strong></p>
<p>假设：数据集只有 1 个特征 x, 目标值值为 y，如下图所示：</p>
<table>
<thead>
<tr>
<th>x</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody><tr>
<td>y</td>
<td>5.56</td>
<td>5.7</td>
<td>5.91</td>
<td>6.4</td>
<td>6.8</td>
<td>7.05</td>
<td>8.9</td>
<td>8.7</td>
<td>9</td>
<td>9.05</td>
</tr>
</tbody></table>
<p>由于只有 1 个特征，所以只需要选择该特征的最优划分点，并不需要计算其他特征。</p>
<ol>
<li><p><strong>先将特征 x 的值排序，并取相邻元素均值作为待划分点，如下图所示：</strong></p>
<table>
<thead>
<tr>
<th>s</th>
<th>1.5</th>
<th>2.5</th>
<th>3.5</th>
<th>4.5</th>
<th>5.5</th>
<th>6.5</th>
<th>7.5</th>
<th>8.5</th>
<th>9.5</th>
</tr>
</thead>
</table>
</li>
<li><p><strong>计算每一个划分点的平方损失，例如：1.5 的平方损失计算过程为：</strong></p>
<p>R1 为 小于 1.5 的样本个数，样本数量为：1，其输出值为：5.56</p>
<p>$R_1 &#x3D;5.56$</p>
<p>R2 为 大于 1.5 的样本个数，样本数量为：9 ，其输出值为：</p>
<p>$R_2&#x3D;(5.7+5.91+6.4+6.8+7.05+8.9+8.7+9+9.05) &#x2F; 9&#x3D;7.50$</p>
<p>该划分点的平方损失：</p>
<p>$L(1.5)&#x3D;(5.56-5.56)^{2}+\left[(5.7-7.5)^{2}+(5.91-7.5)^{2}+\ldots+(9.05-7.5)^{2}\right]&#x3D;0+15.72&#x3D;15.72$</p>
</li>
<li><p><strong>以此方式计算 2.5、3.5… 等划分点的平方损失，结果如下所示：</strong></p>
<table>
<thead>
<tr>
<th>s</th>
<th>1.5</th>
<th>2.5</th>
<th>3.5</th>
<th>4.5</th>
<th>5.5</th>
<th>6.5</th>
<th>7.5</th>
<th>8.5</th>
<th>9.5</th>
</tr>
</thead>
<tbody><tr>
<td>m(s)</td>
<td>15.72</td>
<td>12.07</td>
<td>8.36</td>
<td>5.78</td>
<td>3.91</td>
<td><strong>1.93</strong></td>
<td>8.01</td>
<td>11.73</td>
<td>15.74</td>
</tr>
</tbody></table>
</li>
<li><p><strong>当划分点 s&#x3D;6.5 时，m(s) 最小。因此，第一个划分变量：特征为 X, 切分点为 6.5，即：j&#x3D;x,  s&#x3D;6.5</strong></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/cart1.png" alt="image-20220305183857165"></p>
</li>
<li><p><strong>对左子树的 6 个结点计算每个划分点的平方式损失，找出最优划分点：</strong></p>
<table>
<thead>
<tr>
<th>x</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody><tr>
<td>y</td>
<td>5.56</td>
<td>5.7</td>
<td>5.91</td>
<td>6.4</td>
<td>6.8</td>
<td>7.05</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>s</th>
<th>1.5</th>
<th>2.5</th>
<th>3.5</th>
<th>4.5</th>
<th>5.5</th>
</tr>
</thead>
<tbody><tr>
<td>c1</td>
<td>5.56</td>
<td>5.63</td>
<td>5.72</td>
<td>5.89</td>
<td>6.07</td>
</tr>
<tr>
<td>c2</td>
<td>6.37</td>
<td>6.54</td>
<td>6.75</td>
<td>6.93</td>
<td>7.05</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>s</th>
<th>1.5</th>
<th>2.5</th>
<th>3.5</th>
<th>4.5</th>
<th>5.5</th>
</tr>
</thead>
<tbody><tr>
<td>m(s)</td>
<td>1.3087</td>
<td>0.754</td>
<td>0.2771</td>
<td>0.4368</td>
<td>1.0644</td>
</tr>
</tbody></table>
</li>
<li><p><strong>s&#x3D;3.5时，m(s) 最小，所以左子树继续以 3.5 进行分裂:</strong></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/cart2.png"></p>
</li>
<li><p><strong>假设在生成3个区域</strong> 之后停止划分，以上就是回归树。每一个叶子结点的输出为：挂在该结点上的所有样本均值。</p>
</li>
</ol>
<table>
<thead>
<tr>
<th>x</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody><tr>
<td>y</td>
<td>5.56</td>
<td>5.7</td>
<td>5.91</td>
<td>6.4</td>
<td>6.8</td>
<td>7.05</td>
<td>8.9</td>
<td>8.7</td>
<td>9</td>
<td>9.05</td>
</tr>
</tbody></table>
<p>1号样本真实值  5.56 预测结果：5.72</p>
<p>2号样本真实值是 5.7 预测结果：5.72</p>
<p>3 号样本真实值是 5.91 预测结果 5.72</p>
<p>CART 回归树构建过程如下：</p>
<ol>
<li>选择第一个特征，将该特征的值进行排序，取相邻点计算均值作为待划分点</li>
<li>根据所有划分点，将数据集分成两部分：R1、R2</li>
<li>R1 和 R2 两部分的平方损失相加作为该切分点平方损失</li>
<li>取最小的平方损失的划分点，作为当前特征的划分点</li>
<li>以此计算其他特征的最优划分点、以及该划分点对应的损失值</li>
<li>在所有的特征的划分点中，选择出最小平方损失的划分点，作为当前树的分裂点</li>
</ol>
<h2 id="决策树剪枝⭐️"><a href="#决策树剪枝⭐️" class="headerlink" title="决策树剪枝⭐️"></a>决策树剪枝⭐️</h2><h3 id="什么是剪枝"><a href="#什么是剪枝" class="headerlink" title="什么是剪枝?"></a>什么是剪枝?</h3><blockquote>
<p>剪枝 (pruning)是决策树学习算法对付 <strong>过拟合</strong> 的主要手段。</p>
<p>在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，这时就可能因训练样本学得”太好”了，以致于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合。因此，可通过主动去掉一些分支来降低过拟合的风险。</p>
</blockquote>
<p>剪枝是指将一颗子树的子节点全部删掉，利用叶子节点替换子树(实质上是后剪枝技术)，也可以（假定当前对以root为根的子树进行剪枝）只保留根节点本身而删除所有的叶子，以下图为例：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610170109157.png" alt="image-20250610170109157"></p>
<h3 id="常见减枝方法"><a href="#常见减枝方法" class="headerlink" title="常见减枝方法"></a>常见减枝方法</h3><p>决策树剪枝的基本策略有”预剪枝” (pre-pruning）和”后剪枝”（post- pruning) 。</p>
<ol>
<li>预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点;</li>
<li>后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。</li>
</ol>
<h3 id="剪枝方法对比"><a href="#剪枝方法对比" class="headerlink" title="剪枝方法对比"></a>剪枝方法对比</h3><p>预剪枝优点：</p>
<ul>
<li>预剪枝使决策树的很多分支没有展开，不单降低了过拟合风险，还显著减少了决策树的训练、测试时间开销</li>
</ul>
<p>预剪枝缺点：</p>
<ul>
<li>有些分支的当前划分虽不能提升泛化性能，甚至会导致泛化性能降低，但在其基础上进行的后续划分却有可能导致性能的显著提高</li>
<li>预剪枝决策树也带来了欠拟合的风险</li>
</ul>
<p>后剪枝优点：</p>
<ul>
<li>比预剪枝保留了更多的分支。一般情况下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝</li>
</ul>
<p>后剪枝缺点：</p>
<ul>
<li>但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中所有非叶子节点进行逐一考察，因此在训练时间开销比未剪枝的决策树和预剪枝的决策树都要大得多。</li>
</ul>
<h3 id="预剪枝-pre-pruning）举例"><a href="#预剪枝-pre-pruning）举例" class="headerlink" title="预剪枝 (pre-pruning）举例"></a>预剪枝 (pre-pruning）举例</h3><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/17.png" />

<ol>
<li><p>假设: 当前树只有一个结点, 即编号为1的结点. 此时, 所有的样本预测类别为: 其类别标记为训练样例数最多的类别，假设我们将这个叶结点标记为 “好瓜”。此时, 在验证集上所有的样本都会被预测为 “好瓜”, 此时的准确率为: 3&#x2F;7</p>
</li>
<li><p>如果进行此次分裂, 则树的深度为 2, 有三个分支. 在用属性”脐部”划分之后，上图中的结点2、3、4分别包含编号为 {1，2，3， 14}、 {6，7， 15， 17}、 {10， 16} 的训练样例，因此这 3 个结点分别被标记为叶结点”好瓜”、 “好瓜”、 “坏瓜”。此时, 在验证集上 4、5、8、11、12 样本预测正确，准确率为: 5&#x2F;7。很显然, 通过此次分裂准确率有所提升, 值得分裂.</p>
</li>
<li><p>接下来，对结点2进行划分，基于信息增益准则将挑选出划分属性”色泽”。然而，在使用”色泽”划分后，编号为 {5} 的验证集样本分类结果会由正确转为错误，使得验证集精度下降为 57.1%。于是，预剪枝策略将禁止结点2被划分。</p>
</li>
<li><p>对结点3，最优划分属性为”根蒂”，划分后验证集精度仍为 5&#x2F;7. 这个 划分不能提升验证集精度，于是，预剪枝策略禁止结点3被划分。</p>
</li>
<li><p>对结点4，其所含训练样例己属于同一类，不再进行划分.</p>
</li>
</ol>
<p>于是，基于预剪枝策略从上表数据所生成的决策树如上图所示，其验证集精度为 71.4%. 这是一棵仅有一层划分的决策树。</p>
<h3 id="后剪枝-post-pruning-举例"><a href="#后剪枝-post-pruning-举例" class="headerlink" title="后剪枝(post- pruning)举例"></a><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610204200236.png" alt="image-20250610204200236">后剪枝(post- pruning)举例</h3><p> 后剪枝先从训练集生成一棵完整决策树，继续使用上面的案例，从前面计算，我们知前面构造的决策树的验证集精度为42.9%。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20230905171420720.png" alt="image-20230905171420720"></p>
<ol>
<li>首先考察结点6，若将其领衔的分支剪除则相当于把6替换为叶结点。替换后的叶结点包含编号为 {7， 15} 的训练样本，于是该叶结点的类别标记为”好瓜”, 此时决策树的验证集精度提高至 57.1%。</li>
</ol>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250610204247674.png" alt="image-20250610204247674"></p>
<ol start="2">
<li>然后考察结点5，若将其领衔的子树替换为叶结点，则替换后的叶结点包含编号为 {6，7，15}的训练样例，叶结点类别标记为”好瓜’；此时决策树验证集精度仍为 57.1%. 于是，可以不进行剪枝.</li>
<li>对结点2，若将其领衔的子树替换为叶结点，则替换后的叶结点包含编号 为 {1， 2， 3， 14} 的训练样例，叶结点标记为”好瓜”此时决策树的验证集精度提高至 71.4%. 于是，后剪枝策略决定剪枝.</li>
<li>对结点3和1，若将其领衔的子树替换为叶结点，则所得决策树的验证集 精度分别为 71.4% 与 42.9%，均未得到提高，于是它们被保留。</li>
<li>最终, 基于后剪枝策略生成的决策树如上图所示, 其验证集精度为 71.4%。</li>
</ol>
<h1 id="附件"><a href="#附件" class="headerlink" title="附件"></a>附件</h1><h2 id="公共基础知识"><a href="#公共基础知识" class="headerlink" title="公共基础知识"></a>公共基础知识</h2><h3 id="标量（Scalar）"><a href="#标量（Scalar）" class="headerlink" title="标量（Scalar）:"></a>标量（Scalar）:</h3><ul>
<li><p>零阶张量，仅包含大小（Magnitude）​ 没有方向（<code>就是单个数值</code>）</p>
</li>
<li><p>数学表示：<em>s</em>∈R（单个实数或复数）</p>
<ul>
<li>例如：质量：5kg、损失函数值：L&#x3D;0.32</li>
</ul>
</li>
</ul>
<h3 id="向量（Vector）"><a href="#向量（Vector）" class="headerlink" title="向量（Vector）"></a><strong>向量（Vector）</strong></h3><ul>
<li>一阶张量，是标量的有序组合，具有 ​<strong>​大小和方向​</strong>​。（<code>就是一个向量，线性代数里的列向量以及行向量</code>）</li>
<li>可表示空间中的点或方向，例如：<ul>
<li>坐标点：(2,3,5)</li>
<li>速度向量：<strong>v</strong>&#x3D;3<strong>i</strong>+4<strong>j</strong></li>
</ul>
</li>
</ul>
<h3 id="张量（Tensor）"><a href="#张量（Tensor）" class="headerlink" title="张量（Tensor）"></a><strong>张量（Tensor）</strong></h3><ul>
<li><strong>定义</strong>：高阶广义数组（标量是0阶，向量是1阶），可表示 ​<strong>​多维数据关系​</strong>​</li>
<li><strong>特性</strong>：<ul>
<li>阶数（Rank）：索引的自由度（如矩阵是2阶）</li>
<li>支持张量积（⊗）、收缩（Contraction）等运算</li>
<li>内存占用随维度指数增长</li>
</ul>
</li>
<li><strong>示例</strong>：<ul>
<li>矩阵（2阶张量）：$A∈R^{3×3}$(三阶方阵)</li>
<li>RGB图像（3阶张量）：$I∈R^{H×W×3}$</li>
<li>时间序列视频（4阶张量）：$V∈R^{T×H×W×3}$</li>
</ul>
</li>
</ul>
<h4 id="三者区别"><a href="#三者区别" class="headerlink" title="三者区别"></a><strong>三者区别</strong></h4><table>
<thead>
<tr>
<th align="center">特性</th>
<th align="center">标量</th>
<th align="center">向量</th>
<th align="center">张量</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>阶数</strong></td>
<td align="center">0阶</td>
<td align="center">1阶</td>
<td align="center"><em>k</em>阶（<em>k</em>≥2）</td>
</tr>
<tr>
<td align="center"><strong>方向性</strong></td>
<td align="center">无</td>
<td align="center">有</td>
<td align="center">多维方向</td>
</tr>
<tr>
<td align="center"><strong>存储结构</strong></td>
<td align="center">单个数值</td>
<td align="center">一维数组</td>
<td align="center">多维数组</td>
</tr>
<tr>
<td align="center"><strong>运算</strong></td>
<td align="center">算术运算</td>
<td align="center">线性代数运算</td>
<td align="center">张量分解&#x2F;收缩</td>
</tr>
<tr>
<td align="center"><strong>PyTorch表示</strong></td>
<td align="center"><code>torch.tensor(3)</code></td>
<td align="center"><code>torch.tensor([1,2])</code></td>
<td align="center"><code>torch.rand(2,3,4)</code></td>
</tr>
</tbody></table>
<h4 id="图形展示"><a href="#图形展示" class="headerlink" title="图形展示"></a><strong>图形展示</strong></h4><pre class="mermaid">graph LR
    A[标量] -->|升维| B[向量]
    B -->|升维| C[矩阵]
    C -->|升维| D[3阶张量]</pre>

<h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a><strong>应用场景</strong></h4><ul>
<li><strong>标量</strong>：损失值、阈值参数</li>
<li><strong>向量</strong>：特征表示、词嵌入（Word2Vec）</li>
<li>张量：<ul>
<li>计算机视觉：卷积核（4阶：<code>[out_channels, in_channels, h, w]</code>）</li>
<li>自然语言处理：注意力权重矩阵（<code>[batch_size, seq_len, seq_len]</code>）</li>
</ul>
</li>
</ul>
<h2 id="Latex数学语法支持"><a href="#Latex数学语法支持" class="headerlink" title="Latex数学语法支持"></a>Latex数学语法支持</h2><ul>
<li><a target="_blank" rel="noopener" href="https://blog.kevinchu.top/2023/09/12/hexo-supports-latex/">https://blog.kevinchu.top/2023/09/12/hexo-supports-latex/</a></li>
<li>矩阵在写MarkDown的时候可能会出现异常，用&lt;span&gt;&lt;span&gt;包裹起来就没事了</li>
</ul>
<h2 id="导数-amp-矩阵-amp-向量"><a href="#导数-amp-矩阵-amp-向量" class="headerlink" title="导数&amp;矩阵&amp;向量"></a>导数&amp;矩阵&amp;向量</h2><h3 id="常见函数的导数："><a href="#常见函数的导数：" class="headerlink" title="常见函数的导数："></a>常见函数的导数：</h3><table>
<thead>
<tr>
<th align="center">公式</th>
<th align="center">例子</th>
</tr>
</thead>
<tbody><tr>
<td align="center">$(C)^\prime&#x3D;0$</td>
<td align="center">$\left(5\right)^\prime&#x3D;0$         $\left(10\right)^\prime&#x3D;0$</td>
</tr>
<tr>
<td align="center">$\left(x^\alpha\right)^\prime&#x3D;\alpha x^{\alpha-1}$</td>
<td align="center">$\left(x^3\right)^\prime&#x3D;3 x^{2}$      $\left(x^5\right)^\prime&#x3D;5 x^{4}$</td>
</tr>
<tr>
<td align="center">$\left(a^x\right)^\prime&#x3D;a^{x}\ln{a}$</td>
<td align="center">$\left(2^x\right)^\prime&#x3D;2^x\ln{2}$      $\left(7^x\right)^\prime&#x3D;7^x\ln{7}$</td>
</tr>
<tr>
<td align="center">$\left(e^x\right)^\prime&#x3D;e^{x}$</td>
<td align="center">$\left(e^x\right)^\prime&#x3D;e^{x}$</td>
</tr>
<tr>
<td align="center">$\left(\log{_a}x\right)^\prime&#x3D;\frac{1}{x\ln{a}}$</td>
<td align="center">$\left(\log{_{10}}x\right)^\prime&#x3D;\frac{1}{x\ln{10}}$</td>
</tr>
<tr>
<td align="center">$\left(\ln{x}\right)^\prime&#x3D;\frac{1}{x}$</td>
<td align="center">$\left(\ln{x}\right)^\prime&#x3D;\frac{1}{x}$</td>
</tr>
<tr>
<td align="center">$\left(\sin{x}\right)^\prime&#x3D;\cos{x}$</td>
<td align="center">$\left(\sin{x}\right)^\prime&#x3D;\cos{x}$</td>
</tr>
<tr>
<td align="center">$\left(\cos{x}\right)^\prime&#x3D;-\sin{x}$</td>
<td align="center">$\left(\cos{x}\right)^\prime&#x3D;-\sin{x}$</td>
</tr>
</tbody></table>
<h3 id="导数的四则运算："><a href="#导数的四则运算：" class="headerlink" title="导数的四则运算："></a>导数的四则运算：</h3><table>
<thead>
<tr>
<th align="center">公式</th>
<th align="center">例子</th>
</tr>
</thead>
<tbody><tr>
<td align="center">$\left[u(x)\pm v(x)\right]^\prime&#x3D;u^\prime(x) \pm v^\prime(x)$</td>
<td align="center">$(e^x+4\ln{x})^\prime&#x3D;(e^x)^\prime+(4\ln{x})^\prime&#x3D;e^x+\frac{4}{x}$</td>
</tr>
<tr>
<td align="center">$\left[u(x)\cdot v(x)\right]^\prime&#x3D;u^\prime(x) \cdot v(x) + u(x) \cdot v^\prime(x)$</td>
<td align="center">$(\sin{x}\cdot\ln{x})^\prime&#x3D;\cos{x}\cdot\ln{x}+\sin{x}\cdot\frac{1}{x}$</td>
</tr>
<tr>
<td align="center">$\left[\frac{u(x)}{v(x)}\right]^\prime&#x3D;\frac{u^\prime(x) \cdot v(x) - u(x) \cdot v^\prime(x)}{v^2(x)}$</td>
<td align="center">$\left(\frac{e^x}{\cos{x}}\right)^\prime&#x3D;\frac{e^x\cdot\cos{x}-e^x\cdot(-\sin{x})}{cos^2(x)}$</td>
</tr>
<tr>
<td align="center">${g[h(x)]}^\prime&#x3D;g^\prime(h)*h^\prime(x)$</td>
<td align="center">$(\sin{2x})^\prime&#x3D;\cos{2x}\cdot(2x)^\prime&#x3D;2\cos(2x)$</td>
</tr>
</tbody></table>
<h3 id="复合函数求导："><a href="#复合函数求导：" class="headerlink" title="复合函数求导："></a>复合函数求导：</h3><blockquote>
<p>链式法则：先对外函数求导，再对内函数求导</p>
</blockquote>
<p>例如：计算函数 $y &#x3D; (x^{2} + 2x)^{2}$ 的导函数：<br>$$<br>\begin{aligned}<br>y’ &amp;&#x3D; 2(x^{2} + 2x)^{(2-1)} \cdot (x^{2} + 2x)’ \<br>   &amp;&#x3D; 2(x^{2} + 2x)(2x + 2) \<br>   &amp;&#x3D; 4(x^{3} + 3x^{2} + 2x) \<br>   &amp;&#x3D; 4x^{3} + 12x^{2} + 8x<br>\end{aligned}<br>$$</p>
<h2 id="对数函数"><a href="#对数函数" class="headerlink" title="对数函数"></a>对数函数</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250609111130085-20250609204542024.png" alt="image-20250609111130085"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250609110804572-20250609204542047.png" alt="image-20250609110804572"></p>
<h2 id="概率"><a href="#概率" class="headerlink" title="概率"></a>概率</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250609190507660-20250609204542069.png" alt="image-20250609190507660"></p>
<h3 id="条件概率补充"><a href="#条件概率补充" class="headerlink" title="条件概率补充"></a>条件概率补充</h3><p><strong>公式：</strong>$$ P(B \mid A) &#x3D; \frac{P(AB)}{P(A)} $$</p>
<ul>
<li><p>$P(B \mid A)$：在事件A发生的条件下，事件B发生的概率 </p>
</li>
<li><p>$P(AB)$：事件A和事件B同时发生的联合概率 </p>
</li>
<li><p>$P(A)$：事件A发生的边际概率</p>
</li>
</ul>
<h3 id="全概率公式"><a href="#全概率公式" class="headerlink" title="全概率公式"></a>全概率公式</h3><blockquote>
<p>如果事件$B_{1}、B_{2}、B_{3}…B_{i}$构成一个完备事件组，即它们两两互不相容，其和为全集；并且$P(B_{i})$大于0，则对任一事件A有</p>
<p>$$ P(A) &#x3D; P(A \mid B_{1})P(B_{1}) + P(A \mid B_{2})P(B_{2}) + \cdots + P(A \mid B_{i})P(B_{i}) $$</p>
</blockquote>
<p>公式：$$ P(A) &#x3D; \sum_{i&#x3D;1}^{n} P(A|B_i) \cdot P(B_i) $$</p>
<h3 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h3><blockquote>
<p>也就是在已知结果的情况下，倒推某个事件发生的概率</p>
</blockquote>
<p>$$ P(A|B) &#x3D; \frac{P(B|A) \cdot P(A)}{P(B)} $$</p>
<p>使用全概率公式展开</p>
<p>$$ P(A|B) &#x3D; \frac{P(B|A) \cdot P(A)}{\sum_{i&#x3D;1}^{n} P(B|A_i) \cdot P(A_i)} $$</p>
<h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>参考链接：</p>
<p>极大释然估计：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41775769/article/details/113514294">https://blog.csdn.net/qq_41775769/article/details/113514294</a></p>
<h3 id="离散型统计模型"><a href="#离散型统计模型" class="headerlink" title="离散型统计模型"></a>离散型统计模型</h3><p>$$ L(\theta) &#x3D; \prod_{i&#x3D;1}^{n} P_{\theta}(X_{i}&#x3D;x_{i}) \quad i&#x3D;1,2,3,\cdots,n $$</p>
<ul>
<li>$L(\theta)$：似然函数</li>
<li>$\prod$：连乘符号</li>
<li>$P_{\theta}(X_{i}&#x3D;x_{i})$：在参数$\theta$下随机变量$X_i$取$x_i$的概率</li>
</ul>
<h3 id="连续型统计模型"><a href="#连续型统计模型" class="headerlink" title="连续型统计模型"></a>连续型统计模型</h3><p>$$ L(\theta) &#x3D; \prod_{i&#x3D;1}^{n} f(x_{i};\theta) \quad i&#x3D;1,2,3,\cdots,n $$</p>
<p><strong>参数说明</strong>：</p>
<ul>
<li>$f(x_{i};\theta)$：概率密度函数</li>
<li>其他符号含义与离散型相同</li>
</ul>
<p>从一个直观的例子理解极大似然估计，比如：在一个未知的袋子里摸球，现有的认知告诉我们是袋子里面的球要么是红色，要么是蓝色。于是我们可以知道从该袋子中摸球颜色的概率服从二项分布如下：</p>
<table>
<thead>
<tr>
<th>$X$</th>
<th>红色</th>
<th>蓝色</th>
</tr>
</thead>
<tbody><tr>
<td>$P$</td>
<td>$θ$</td>
<td>$1-θ$</td>
</tr>
</tbody></table>
<p>由于不知道袋子中究竟有多少个球以及每个颜色的球有多少个，所以无法对参数θ进行计算，也不能计算出摸到哪种颜色的球的概率是多少？于是，假设有一个测试人员对袋内球进行有放回的抽取，进行了100次随机测验之后，统计得出：有30次摸到的是红球，有70次摸到的是蓝球。</p>
<p>从现有的测试结果出发，我们有理由相信袋子中球的比例大概是红色 : 蓝色&#x3D;3 : 7（也就是背后的理论支撑）。所以进而求出概率以及参数 θ&#x3D;0.3 。也就是用抽样时球的颜色出现的频率近似等于概率。<br>注意：<code>极大似然估计中采样需满足一个重要的假设，就是所有的采样都是独立同分布的</code>。</p>
<h3 id="最大释然估计统计方法"><a href="#最大释然估计统计方法" class="headerlink" title="最大释然估计统计方法"></a>最大释然估计统计方法</h3><p>进行$n$次独立随机试验，观测到：</p>
<ul>
<li>“状态1”发生 $n_1$ 次</li>
<li>“状态2”发生 $n_2$ 次<br>（满足 $n_1 + n_2 &#x3D; n$）</li>
</ul>
<p>从频率学派角度，可得经验概率：<br>$$ P_{\text{empirical}}(\text{状态1}) &#x3D; \frac{n_1}{n} $$</p>
<p>为建立公理化描述，定义似然函数：<br>$$ L(\theta) &#x3D; \theta^{n_1}(1-\theta)^{n_2} \quad \text{其中} \quad n_1 + n_2 &#x3D; n $$</p>
<p>首先对似然函数取自然对数（$\ln$）进行化简：</p>
<p>$$ \ln L(\theta) &#x3D; n_1 \ln \theta + n_2 \ln (1 - \theta) $$</p>
<p>对对数似然函数关于$\theta$求导并令导数为零：</p>
<p>$$ \frac{d \ln L(\theta)}{d \theta} &#x3D; \frac{n_1}{\theta} + \frac{n_2}{1 - \theta} &#x3D; 0 $$</p>
<p>求解上述方程得到$\theta$的极大似然估计：</p>
<p>$$ \hat{\theta} &#x3D; \frac{n_1}{n_1 + n_2} &#x3D; \frac{n_1}{n} $$</p>
<p><strong>其中</strong>：$n &#x3D; n_1 + n_2$为总试验次数</p>
<h3 id="做道例题"><a href="#做道例题" class="headerlink" title="做道例题"></a>做道例题</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250609201334679-20250609204542100.png" alt="image-20250609201334679"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://liamjohnson-w.github.io">Johnson Liam</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://liamjohnson-w.github.io/2025/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">https://liamjohnson-w.github.io/2025/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/AI/">AI</a></div><div class="post_share"><div class="social-share" data-image="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250605114200229.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> Donate</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/06/07/github-push%E5%87%BA%E9%94%99/" title="Github之Push问题解决方案"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250607224051237.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Github之Push问题解决方案</div></div></a></div><div class="next-post pull-right"><a href="/2025/06/03/2025.06.03/" title="OLLAMA"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250603132906789.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">OLLAMA</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2025/06/03/2025.06.03/" title="OLLAMA"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250603132906789.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-03</div><div class="title">OLLAMA</div></div></a></div><div><a href="/2020/06/07/2020-06-07/" title="基于Python的Socket服务器和客户端通信（Pycharm）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-06-07</div><div class="title">基于Python的Socket服务器和客户端通信（Pycharm）</div></div></a></div><div><a href="/2020/06/15/2020.06.15/" title="Python面向Socket编程Demo"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-06-15</div><div class="title">Python面向Socket编程Demo</div></div></a></div><div><a href="/2020/06/08/2020.06.08/" title="Python爬虫前置配置及Demo"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-06-08</div><div class="title">Python爬虫前置配置及Demo</div></div></a></div><div><a href="/2023/04/12/2023.04.12/" title="Python爬虫回顾(爬报价网)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-12</div><div class="title">Python爬虫回顾(爬报价网)</div></div></a></div><div><a href="/2023/05/03/2023.04.18/" title="Python初识函数"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-03</div><div class="title">Python初识函数</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/pic.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Johnson Liam</div><div class="author-info__description">机器都在学习,你有什么理由不学习?</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">222</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">58</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/weiswift/"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/weiswift" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:1265019024@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">网站由Github服务器托管,感谢支持！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#AI%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD"><span class="toc-number">1.</span> <span class="toc-text">AI人工智能</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E5%88%86%E7%B1%BB"><span class="toc-number">1.1.</span> <span class="toc-text">人工智能的分类</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="toc-number">2.</span> <span class="toc-text">机器学习基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">2.1.</span> <span class="toc-text">机器学习的概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E2%AD%90%EF%B8%8F"><span class="toc-number">2.2.</span> <span class="toc-text">机器学习工作流程⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-number">2.2.1.</span> <span class="toc-text">获取数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">2.2.2.</span> <span class="toc-text">数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-number">2.2.3.</span> <span class="toc-text">特征工程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%EF%BC%89"><span class="toc-number">2.2.4.</span> <span class="toc-text">机器学习（模型训练）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-number">2.2.5.</span> <span class="toc-text">模型评估</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Sklearn%E4%B8%8E%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E2%AD%90%EF%B8%8F"><span class="toc-number">3.</span> <span class="toc-text">Sklearn与特征工程⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#sklearn-%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BB%84%E6%88%90"><span class="toc-number">3.1.</span> <span class="toc-text">sklearn 的核心组成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sklearn%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97-%E9%B8%A2%E5%B0%BE%E8%8A%B1%E6%A1%88%E4%BE%8B%E7%9A%84%E5%85%B8%E5%9E%8B%E4%BB%A3%E7%A0%81%E6%B5%81%E7%A8%8B"><span class="toc-number">3.2.</span> <span class="toc-text">sklearn随机森林-鸢尾花案例的典型代码流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#sklear%E5%85%B8%E5%9E%8B%E6%B5%81%E7%A8%8B-%E5%B8%A6%E6%B3%A8%E9%87%8A%E8%AF%A6%E8%A7%A3"><span class="toc-number">3.2.1.</span> <span class="toc-text">sklear典型流程(带注释详解)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Seaborn%E9%B8%A2%E5%B0%BE%E8%8A%B1%E6%A1%88%E4%BE%8B"><span class="toc-number">3.3.</span> <span class="toc-text">Seaborn鸢尾花案例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%B0%83%E4%BC%98"><span class="toc-number">3.4.</span> <span class="toc-text">模型选择与调优</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">3.4.1.</span> <span class="toc-text">交叉验证</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2"><span class="toc-number">3.4.2.</span> <span class="toc-text">网格搜索</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%B8%A2%E5%B0%BE%E8%8A%B1%E6%A1%88%E4%BE%8B%E5%A2%9E%E5%8A%A0K%E5%80%BC%E8%B0%83%E4%BC%98"><span class="toc-number">3.5.</span> <span class="toc-text">鸢尾花案例增加K值调优</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#KNN%E7%AE%97%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">KNN算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#KNN%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="toc-number">4.1.</span> <span class="toc-text">KNN算法流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#K%E5%80%BC%E5%A4%A7%E5%B0%8F%E5%AF%B9%E4%BA%8E%E8%AE%AD%E7%BB%83%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">4.1.1.</span> <span class="toc-text">K值大小对于训练的影响</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB"><span class="toc-number">4.2.</span> <span class="toc-text">欧式距离</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%BA%8C%E7%BB%B4%E7%A9%BA%E9%97%B4"><span class="toc-number">4.2.1.</span> <span class="toc-text">1. 二维空间</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E4%B8%89%E7%BB%B4%E7%A9%BA%E9%97%B4"><span class="toc-number">4.2.2.</span> <span class="toc-text">2. 三维空间</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-n%E7%BB%B4%E7%A9%BA%E9%97%B4"><span class="toc-number">4.2.3.</span> <span class="toc-text">3. n维空间</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sklearn-KNN%E7%AE%97%E6%B3%95"><span class="toc-number">4.3.</span> <span class="toc-text">sklearn-KNN算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%84%E7%A7%8D%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F"><span class="toc-number">4.4.</span> <span class="toc-text">各种距离计算方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KD%E6%A0%91"><span class="toc-number">4.5.</span> <span class="toc-text">KD树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#KD%E6%A0%91%E7%9A%84%E6%9E%84%E5%BB%BA%EF%BC%88%E4%BA%8C%E7%BB%B4%E5%B9%B3%E9%9D%A2%EF%BC%89"><span class="toc-number">4.5.1.</span> <span class="toc-text">KD树的构建（二维平面）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#KD%E6%A0%91%E7%9A%84%E5%BF%AB%E9%80%9F%E6%9C%80%E8%BF%91%E9%82%BB%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95"><span class="toc-number">4.5.2.</span> <span class="toc-text">KD树的快速最近邻搜索算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#KD%E6%A0%91%E7%9A%84%E6%8F%92%E5%85%A5"><span class="toc-number">4.5.3.</span> <span class="toc-text">KD树的插入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#KD%E6%A0%91%E7%9A%84%E5%88%A0%E9%99%A4"><span class="toc-number">4.5.4.</span> <span class="toc-text">KD树的删除</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">5.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9A%E4%B9%89%E2%AD%90%EF%B8%8F"><span class="toc-number">5.1.</span> <span class="toc-text">线性回归定义⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E7%94%A8%E5%85%AC%E5%BC%8F"><span class="toc-number">5.1.1.</span> <span class="toc-text">通用公式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%88%86%E7%B1%BB%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">5.2.</span> <span class="toc-text">线性回归的分类及应用场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E2%AD%90%EF%B8%8F"><span class="toc-number">5.3.</span> <span class="toc-text">损失函数⭐️</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%B1%82%E8%A7%A3%E6%96%B9%E6%B3%95"><span class="toc-number">5.4.</span> <span class="toc-text">线性回归求解方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">5.4.1.</span> <span class="toc-text">一元线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">5.4.2.</span> <span class="toc-text">多元线性回归</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E2%AD%90%EF%B8%8F"><span class="toc-number">5.5.</span> <span class="toc-text">梯度下降算法⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E5%8F%98%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%A4%BA%E4%BE%8B"><span class="toc-number">5.5.1.</span> <span class="toc-text">单变量梯度下降示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%E2%AD%90%EF%B8%8F"><span class="toc-number">5.6.</span> <span class="toc-text">回归模型评估方法⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%9D%E5%AF%B9%E5%80%BC%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0-Mean-Absolute-Error-MAE-%EF%BC%9A"><span class="toc-number">5.6.1.</span> <span class="toc-text">绝对值误差函数(Mean Absolute Error, MAE)：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0-Mean-Squared-Error-MSE-%EF%BC%9A"><span class="toc-number">5.6.2.</span> <span class="toc-text">均方误差函数(Mean Squared Error, MSE)：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0-Root-Mean-Squared-Error-RMSE"><span class="toc-number">5.6.3.</span> <span class="toc-text">均方误差函数(Root Mean Squared Error ,RMSE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MSE%E4%B8%8EMAE%E4%B8%8ERMSE%E5%AF%B9%E6%AF%94%E2%AD%90%EF%B8%8F"><span class="toc-number">5.6.4.</span> <span class="toc-text">MSE与MAE与RMSE对比⭐️</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E8%B0%83%E4%BC%98%E5%8F%8A%E6%94%B9%E8%BF%9B%E7%9A%84%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95"><span class="toc-number">5.7.</span> <span class="toc-text">梯度下降算法调优及改进的梯度算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E8%B0%83%E4%BC%98"><span class="toc-number">5.7.1.</span> <span class="toc-text">梯度下降算法调优</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%B9%E8%BF%9B%E7%9A%84%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95"><span class="toc-number">5.7.2.</span> <span class="toc-text">改进的梯度算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88%E5%87%BA%E7%8E%B0%E5%8E%9F%E5%9B%A0%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E2%AD%90%EF%B8%8F"><span class="toc-number">5.8.</span> <span class="toc-text">过拟合和欠拟合出现原因及解决方案⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">5.8.1.</span> <span class="toc-text">正则化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%EF%BC%9A%E9%93%B6%E8%A1%8C%E4%BF%A1%E8%B4%B7%E6%A1%88%E4%BE%8B-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">5.9.</span> <span class="toc-text">案例：银行信贷案例(多元线性回归)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">5.9.1.</span> <span class="toc-text">梯度下降法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B3%95"><span class="toc-number">5.9.2.</span> <span class="toc-text">正规方程法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%EF%BC%9A%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E6%A1%88%E4%BE%8B-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%8F%8AAPI"><span class="toc-number">5.10.</span> <span class="toc-text">案例：波士顿房价案例(线性回归及API)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-number">5.11.</span> <span class="toc-text">参考链接</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">6.</span> <span class="toc-text">逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D"><span class="toc-number">6.1.</span> <span class="toc-text">模型介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Sigmoid-%E5%87%BD%E6%95%B0"><span class="toc-number">6.2.</span> <span class="toc-text">Sigmoid 函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">6.3.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92API"><span class="toc-number">6.4.</span> <span class="toc-text">逻辑回归API</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%E2%AD%90%EF%B8%8F"><span class="toc-number">6.5.</span> <span class="toc-text">分类评估方法⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5%E5%8F%8A%E5%85%B6%E6%9E%84%E5%BB%BA"><span class="toc-number">6.5.1.</span> <span class="toc-text">混淆矩阵及其构建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E6%AD%A3%E4%BE%8B%E5%81%87%E4%BE%8B%E4%B8%BE%E4%BE%8B%EF%BC%9A"><span class="toc-number">6.5.2.</span> <span class="toc-text">模型预测正例假例举例：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Precision%EF%BC%88%E7%B2%BE%E7%A1%AE%E7%8E%87%EF%BC%89"><span class="toc-number">6.5.3.</span> <span class="toc-text">Precision（精确率）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Recall%EF%BC%88%E5%8F%AC%E5%9B%9E%E7%8E%87%EF%BC%89"><span class="toc-number">6.5.4.</span> <span class="toc-text">Recall（召回率）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#F1-score"><span class="toc-number">6.5.5.</span> <span class="toc-text">F1-score</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ROC%E6%9B%B2%E7%BA%BF%E5%92%8CAUC%E6%8C%87%E6%A0%87"><span class="toc-number">6.5.6.</span> <span class="toc-text">ROC曲线和AUC指标</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B"><span class="toc-number">6.6.</span> <span class="toc-text">案例</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">7.</span> <span class="toc-text">决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">7.1.</span> <span class="toc-text">构建决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Notice"><span class="toc-number">7.1.1.</span> <span class="toc-text">Notice</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ID3%E7%AE%97%E6%B3%95%E6%9E%84%E5%BB%BA%E5%86%B3%E7%AD%96%E6%A0%91%E2%AD%90%EF%B8%8F"><span class="toc-number">7.2.</span> <span class="toc-text">ID3算法构建决策树⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E7%86%B5"><span class="toc-number">7.2.1.</span> <span class="toc-text">信息熵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A"><span class="toc-number">7.2.2.</span> <span class="toc-text">信息增益</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E8%AE%A1%E7%AE%97%E6%A1%88%E4%BE%8B"><span class="toc-number">7.2.3.</span> <span class="toc-text">信息增益计算案例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ID3%E6%A0%91%E6%9E%84%E5%BB%BA%E6%B5%81%E7%A8%8B"><span class="toc-number">7.2.4.</span> <span class="toc-text">ID3树构建流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#C4-5%E7%AE%97%E6%B3%95%E6%9E%84%E5%BB%BA%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">7.3.</span> <span class="toc-text">C4.5算法构建决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%8E%87"><span class="toc-number">7.3.1.</span> <span class="toc-text">信息增益率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C4-5%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA%E6%B5%81%E7%A8%8B"><span class="toc-number">7.3.2.</span> <span class="toc-text">C4.5决策树构建流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CART%E7%AE%97%E6%B3%95%E6%9E%84%E5%BB%BA%E5%86%B3%E7%AD%96%E6%A0%91%E2%AD%90%EF%B8%8F"><span class="toc-number">7.4.</span> <span class="toc-text">CART算法构建决策树⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Cart%E6%A0%91%E7%AE%80%E4%BB%8B"><span class="toc-number">7.4.1.</span> <span class="toc-text">Cart树简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E5%B0%BC%E6%8C%87%E6%95%B0%E8%AE%A1%E7%AE%97%E6%A1%88%E4%BE%8B"><span class="toc-number">7.4.2.</span> <span class="toc-text">基尼指数计算案例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%98%AF%E5%90%A6%E6%9C%89%E6%88%BF"><span class="toc-number">7.4.2.1.</span> <span class="toc-text">是否有房</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A9%9A%E5%A7%BB%E7%8A%B6%E5%86%B5"><span class="toc-number">7.4.2.2.</span> <span class="toc-text">婚姻状况</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%B4%E6%94%B6%E5%85%A5"><span class="toc-number">7.4.2.3.</span> <span class="toc-text">年收入</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cart%E6%A1%88%E4%BE%8B%EF%BC%88%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E7%94%9F%E5%AD%98%E6%A1%88%E4%BE%8B"><span class="toc-number">7.4.3.</span> <span class="toc-text">Cart案例（泰坦尼克号生存案例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cart%E5%9B%9E%E5%BD%92%E5%86%B3%E7%AD%96%E6%A0%91%E2%AD%90%EF%B8%8F"><span class="toc-number">7.5.</span> <span class="toc-text">Cart回归决策树⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA%E5%8E%9F%E7%90%86"><span class="toc-number">7.5.1.</span> <span class="toc-text">回归决策树构建原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CART-%E5%9B%9E%E5%BD%92%E6%A0%91%E6%9E%84%E5%BB%BA"><span class="toc-number">7.5.2.</span> <span class="toc-text">CART 回归树构建:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%89%AA%E6%9E%9D%E2%AD%90%EF%B8%8F"><span class="toc-number">7.6.</span> <span class="toc-text">决策树剪枝⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%89%AA%E6%9E%9D"><span class="toc-number">7.6.1.</span> <span class="toc-text">什么是剪枝?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E5%87%8F%E6%9E%9D%E6%96%B9%E6%B3%95"><span class="toc-number">7.6.2.</span> <span class="toc-text">常见减枝方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%AA%E6%9E%9D%E6%96%B9%E6%B3%95%E5%AF%B9%E6%AF%94"><span class="toc-number">7.6.3.</span> <span class="toc-text">剪枝方法对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E5%89%AA%E6%9E%9D-pre-pruning%EF%BC%89%E4%B8%BE%E4%BE%8B"><span class="toc-number">7.6.4.</span> <span class="toc-text">预剪枝 (pre-pruning）举例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%8E%E5%89%AA%E6%9E%9D-post-pruning-%E4%B8%BE%E4%BE%8B"><span class="toc-number">7.6.5.</span> <span class="toc-text">后剪枝(post- pruning)举例</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%99%84%E4%BB%B6"><span class="toc-number">8.</span> <span class="toc-text">附件</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AC%E5%85%B1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86"><span class="toc-number">8.1.</span> <span class="toc-text">公共基础知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%87%E9%87%8F%EF%BC%88Scalar%EF%BC%89"><span class="toc-number">8.1.1.</span> <span class="toc-text">标量（Scalar）:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F%EF%BC%88Vector%EF%BC%89"><span class="toc-number">8.1.2.</span> <span class="toc-text">向量（Vector）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%EF%BC%88Tensor%EF%BC%89"><span class="toc-number">8.1.3.</span> <span class="toc-text">张量（Tensor）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E8%80%85%E5%8C%BA%E5%88%AB"><span class="toc-number">8.1.3.1.</span> <span class="toc-text">三者区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BE%E5%BD%A2%E5%B1%95%E7%A4%BA"><span class="toc-number">8.1.3.2.</span> <span class="toc-text">图形展示</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">8.1.3.3.</span> <span class="toc-text">应用场景</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Latex%E6%95%B0%E5%AD%A6%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81"><span class="toc-number">8.2.</span> <span class="toc-text">Latex数学语法支持</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%BC%E6%95%B0-amp-%E7%9F%A9%E9%98%B5-amp-%E5%90%91%E9%87%8F"><span class="toc-number">8.3.</span> <span class="toc-text">导数&amp;矩阵&amp;向量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0%EF%BC%9A"><span class="toc-number">8.3.1.</span> <span class="toc-text">常见函数的导数：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E6%95%B0%E7%9A%84%E5%9B%9B%E5%88%99%E8%BF%90%E7%AE%97%EF%BC%9A"><span class="toc-number">8.3.2.</span> <span class="toc-text">导数的四则运算：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%8D%E5%90%88%E5%87%BD%E6%95%B0%E6%B1%82%E5%AF%BC%EF%BC%9A"><span class="toc-number">8.3.3.</span> <span class="toc-text">复合函数求导：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E6%95%B0%E5%87%BD%E6%95%B0"><span class="toc-number">8.4.</span> <span class="toc-text">对数函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E7%8E%87"><span class="toc-number">8.5.</span> <span class="toc-text">概率</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E8%A1%A5%E5%85%85"><span class="toc-number">8.5.1.</span> <span class="toc-text">条件概率补充</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E6%A6%82%E7%8E%87%E5%85%AC%E5%BC%8F"><span class="toc-number">8.5.2.</span> <span class="toc-text">全概率公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F"><span class="toc-number">8.5.3.</span> <span class="toc-text">贝叶斯公式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1"><span class="toc-number">8.6.</span> <span class="toc-text">极大似然估计</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A6%BB%E6%95%A3%E5%9E%8B%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%9E%8B"><span class="toc-number">8.6.1.</span> <span class="toc-text">离散型统计模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9E%E7%BB%AD%E5%9E%8B%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%9E%8B"><span class="toc-number">8.6.2.</span> <span class="toc-text">连续型统计模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E9%87%8A%E7%84%B6%E4%BC%B0%E8%AE%A1%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95"><span class="toc-number">8.6.3.</span> <span class="toc-text">最大释然估计统计方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%9A%E9%81%93%E4%BE%8B%E9%A2%98"><span class="toc-number">8.6.4.</span> <span class="toc-text">做道例题</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/06/07/github-push%E5%87%BA%E9%94%99/" title="Github之Push问题解决方案"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250607224051237.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Github之Push问题解决方案"/></a><div class="content"><a class="title" href="/2025/06/07/github-push%E5%87%BA%E9%94%99/" title="Github之Push问题解决方案">Github之Push问题解决方案</a><time datetime="2025-06-06T16:00:00.000Z" title="Created 2025-06-07 00:00:00">2025-06-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" title="机器学习"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250605114200229.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习"/></a><div class="content"><a class="title" href="/2025/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" title="机器学习">机器学习</a><time datetime="2025-06-03T16:00:00.000Z" title="Created 2025-06-04 00:00:00">2025-06-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/03/2025.06.03/" title="OLLAMA"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250603132906789.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="OLLAMA"/></a><div class="content"><a class="title" href="/2025/06/03/2025.06.03/" title="OLLAMA">OLLAMA</a><time datetime="2025-06-02T16:00:00.000Z" title="Created 2025-06-03 00:00:00">2025-06-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/10/2025.05.10/" title="Mozi病毒样本分析"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250604210840482.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Mozi病毒样本分析"/></a><div class="content"><a class="title" href="/2025/05/10/2025.05.10/" title="Mozi病毒样本分析">Mozi病毒样本分析</a><time datetime="2025-05-09T16:00:00.000Z" title="Created 2025-05-10 00:00:00">2025-05-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/28/2025.04.28/" title="P2P通信"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250604211011599.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="P2P通信"/></a><div class="content"><a class="title" href="/2025/04/28/2025.04.28/" title="P2P通信">P2P通信</a><time datetime="2025-04-27T16:00:00.000Z" title="Created 2025-04-28 00:00:00">2025-04-28</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Johnson Liam</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Welcome to 小威の <a target="_blank" rel="noopener" href="https://www.cnblogs.com/liam-sliversucks/">Blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? '' : ''

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>