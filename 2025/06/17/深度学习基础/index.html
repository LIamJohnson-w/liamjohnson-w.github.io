<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>深度学习 | SilverSucks</title><meta name="author" content="Jason"><meta name="copyright" content="Jason"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="深度学习和机器学习的区别深度学习和机器学习-面试题(●’◡’●) 问：什么样的资料集不适合用深度学习？ （1）数据集太⼩，数据样本不⾜时，深度学习相对其它机器学习算法，没有明显优势。 （2）数据集没有局部相关特性，⽬前深度学习表现⽐较好的领域主要是图像&amp;#x2F;语⾳&amp;#x2F;⾃然语⾔处理等领域，"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://liamjohnson-w.github.io/2025/06/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-07-31 21:33:52'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css"> <script src="/live2d-widget/autoload.js"></script><script src="/live2d-widget/autoload.js"> </script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/tx.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">231</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">58</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-gamepad"></i><span> Games</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/mikutap/"><i class="fa-fw fa fa-music"></i><span> MikuTap 初音未来</span></a></li><li><a class="site-page child" href="/starbattle/"><i class="fa-fw fa fa-space-shuttle"></i><span> StartBattle 星际大战</span></a></li><li><a class="site-page child" href="/2048/"><i class="fa-fw fa fa-flag"></i><span> 2048 经典游戏</span></a></li><li><a class="site-page child" href="/battlecity/"><i class="fa-fw fa fa-arrow-circle-left"></i><span> BattleCity 坦克大战</span></a></li><li><a class="site-page child" href="/pacman/"><i class="fa-fw fa fa-bolt"></i><span> PacMan  吃豆人</span></a></li><li><a class="site-page child" href="/tetris/"><i class="fa-fw fa fa-arrows-alt"></i><span> Tetris 俄罗斯方块</span></a></li><li><a class="site-page child" href="/smallcat/"><i class="fa-fw fa fa-paw"></i><span> CatchCat 困住小猫</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-leaf"></i><span> Moments</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/diary/"><i class="fa-fw fas fa-bookmark"></i><span> Diary</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fa fa-hourglass-half"></i><span> Gallery</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-podcast"></i><span> More</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags标签</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About关于</span></a></li><li><a class="site-page child" href="/messageboard/"><i class="fa-fw fas fa-bookmark"></i><span> Messageboard留言板</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s1.ax1x.com/2023/04/18/p9i6u5D.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="SilverSucks"><span class="site-name">SilverSucks</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-gamepad"></i><span> Games</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/mikutap/"><i class="fa-fw fa fa-music"></i><span> MikuTap 初音未来</span></a></li><li><a class="site-page child" href="/starbattle/"><i class="fa-fw fa fa-space-shuttle"></i><span> StartBattle 星际大战</span></a></li><li><a class="site-page child" href="/2048/"><i class="fa-fw fa fa-flag"></i><span> 2048 经典游戏</span></a></li><li><a class="site-page child" href="/battlecity/"><i class="fa-fw fa fa-arrow-circle-left"></i><span> BattleCity 坦克大战</span></a></li><li><a class="site-page child" href="/pacman/"><i class="fa-fw fa fa-bolt"></i><span> PacMan  吃豆人</span></a></li><li><a class="site-page child" href="/tetris/"><i class="fa-fw fa fa-arrows-alt"></i><span> Tetris 俄罗斯方块</span></a></li><li><a class="site-page child" href="/smallcat/"><i class="fa-fw fa fa-paw"></i><span> CatchCat 困住小猫</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-leaf"></i><span> Moments</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/diary/"><i class="fa-fw fas fa-bookmark"></i><span> Diary</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fa fa-hourglass-half"></i><span> Gallery</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-podcast"></i><span> More</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags标签</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About关于</span></a></li><li><a class="site-page child" href="/messageboard/"><i class="fa-fw fas fa-bookmark"></i><span> Messageboard留言板</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">深度学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-06-16T16:00:00.000Z" title="Created 2025-06-17 00:00:00">2025-06-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-07-31T13:33:52.861Z" title="Updated 2025-07-31 21:33:52">2025-07-31</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="深度学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="深度学习和机器学习的区别"><a href="#深度学习和机器学习的区别" class="headerlink" title="深度学习和机器学习的区别"></a>深度学习和机器学习的区别</h1><h2 id="深度学习和机器学习-面试题-●’◡’●"><a href="#深度学习和机器学习-面试题-●’◡’●" class="headerlink" title="深度学习和机器学习-面试题(●’◡’●)"></a>深度学习和机器学习-面试题(●’◡’●)</h2><blockquote>
<p>问：<strong>什么样的资料集不适合用深度学习？</strong></p>
<p>（1）数据集太⼩，数据样本不⾜时，深度学习相对其它机器学习算法，没有明显优势。</p>
<p>（2）数据集没有局部相关特性，⽬前深度学习表现⽐较好的领域主要是图像&#x2F;语⾳&#x2F;⾃然语⾔处理等领域，</p>
<p>这些领域的⼀个共性是局部相关性。图像中像素组成物体，语⾳信号中⾳位组合成单词，⽂本数据中单词</p>
<p>组合成句⼦，这些特征元素 的组合⼀旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的</p>
<p>数据集，不适于使⽤深度学习算法进⾏处 理。</p>
</blockquote>
<h2 id="深入理解机器学习和深度学习"><a href="#深入理解机器学习和深度学习" class="headerlink" title="深入理解机器学习和深度学习"></a>深入理解机器学习和深度学习</h2><ul>
<li>深度学习与机器学习的差别<ul>
<li>不需要人工特征工程</li>
<li>特征工程+分类&#x2F;回归 使用一个网络来完成</li>
</ul>
</li>
<li>优点<ul>
<li>精确度高，性能好，效果好</li>
<li>拟合任意非线性的关系</li>
<li>框架多，不需我们自己造轮子</li>
</ul>
</li>
<li>缺点<ul>
<li>黑箱，可解释性差</li>
<li>网络参数多，超参数多</li>
<li>需要大量的数据进行训练，训练时间长，对算力有较高要求</li>
<li>小数据集容易过拟合</li>
</ul>
</li>
</ul>
<h1 id="PyTorch深度学习框架"><a href="#PyTorch深度学习框架" class="headerlink" title="PyTorch深度学习框架"></a>PyTorch深度学习框架</h1><h2 id="PyTorch是什么"><a href="#PyTorch是什么" class="headerlink" title="PyTorch是什么"></a>PyTorch是什么</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/">PyTorch官网</a></p>
<ul>
<li><p>GPU：一个超级快的数学计算器。（显卡）</p>
</li>
<li><p>CUDA：一个让程序员能更容易使用GPU的工具。</p>
</li>
<li><p>Pytorch：一个帮助创建和训练深度学习模型的深度学习库。</p>
</li>
</ul>
</blockquote>
<h2 id="PyTorch编程"><a href="#PyTorch编程" class="headerlink" title="PyTorch编程"></a>PyTorch编程</h2><blockquote>
<p>验证自己的显卡是否为cuda核心（即是否能通过GPU进行计算加速）：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;GPU可用，深度学习加速之旅开始！&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;GPU不可用，将使用CPU进行计算。&quot;</span>)</span><br></pre></td></tr></table></figure>



<h3 id="PyTorch⾥怎么从CPU迁移到GPU"><a href="#PyTorch⾥怎么从CPU迁移到GPU" class="headerlink" title="PyTorch⾥怎么从CPU迁移到GPU?"></a>PyTorch⾥怎么从CPU迁移到GPU?</h3><p>检查是否有可⽤的GPU设备：</p>
<p>使⽤torch.cuda.is_available()函数检查系统是否具有可⽤的GPU设备。如果返回True，表示有可⽤的GPU。</p>
<ul>
<li><p>将模型参数迁移到GPU：</p>
<ul>
<li>model.to(device)</li>
</ul>
</li>
<li><p>将输⼊数据迁移到GPU：</p>
<ul>
<li>迁移数据：inputs.to(device), labels.to(device)</li>
<li>迁移张量：tensor_gpu &#x3D; tensor_cpu.to(device)</li>
</ul>
</li>
</ul>
<h2 id="基础张量操作"><a href="#基础张量操作" class="headerlink" title="基础张量操作"></a>基础张量操作</h2><p>参考链接：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_51385258/article/details/143903226">张量的创建、转换和拼接</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_51385258/article/details/143905539">张量的索引、形状操作和常见运算函数</a></li>
</ul>
<h3 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">创建张量：</span></span><br><span class="line">        <span class="attr">torch.tensor(data,</span> <span class="string">datatype)创建张量 最常用 可以根据已有数据创建张量（默认使用原有数据类型）</span></span><br><span class="line">        <span class="attr">torch.Tensor(data,</span> <span class="string">size=())创建张量 主要是根据形状创建张量</span></span><br><span class="line">        <span class="attr">torch.DoubleTensor(data)</span> <span class="string">主要是创建指定类型的张量(Tensor指定类型创建张量)</span></span><br><span class="line">        <span class="attr">torch.linspace(start,</span> <span class="string">end, number) 主要是创建线性张量</span></span><br><span class="line">        <span class="attr">torch.arange(start,</span> <span class="string">end, step) 创建线性张量</span></span><br><span class="line">        <span class="attr">torch.randn(shape())</span> <span class="string">创建0-1之间的随机张量</span></span><br><span class="line">        <span class="attr">torch.randint(start,</span> <span class="string">end, shape()) 创建随机整型张量</span></span><br><span class="line">        <span class="attr">torch.zeros(size)</span> <span class="string">创建指定类型的全为0的张量 torch.zeros_like(data)</span></span><br><span class="line">        <span class="attr">torch.ones(size)</span> <span class="string">创建指定类型的全为1的张量 torch.ones_like(data)</span></span><br><span class="line">        <span class="attr">torch.full(size)</span> <span class="string">创建指定值的张量  torch.full_like(data)</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">张量类型转换：</span></span><br><span class="line">         <span class="attr">data</span> = <span class="string">data.type(torch.DoubleTensor)</span></span><br><span class="line">         <span class="attr">data.double()</span></span><br></pre></td></tr></table></figure>



<ul>
<li>torch.tensor() : 将数据创建为张量（推荐使用）</li>
</ul>
<table>
<thead>
<tr>
<th align="center">方法</th>
<th align="center">数据类型控制</th>
<th align="center">内存行为</th>
<th align="center">推荐场景</th>
<th align="center">是否推荐使用</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>torch.tensor(data)</code></td>
<td align="center"><strong>自动推断</strong>或显式指定</td>
<td align="center">总是拷贝数据</td>
<td align="center">从Python数据创建张量</td>
<td align="center">✅ 首选</td>
</tr>
<tr>
<td align="center"><code>torch.Tensor(data)</code></td>
<td align="center">默认<code>torch.float32</code></td>
<td align="center">可能共享内存</td>
<td align="center">旧代码兼容&#x2F;未初始化张量</td>
<td align="center">⚠️ 慎用</td>
</tr>
<tr>
<td align="center"><code>torch.IntTensor()</code></td>
<td align="center">强制<code>torch.int32</code></td>
<td align="center">类似<code>torch.Tensor</code></td>
<td align="center">需要明确整数类型</td>
<td align="center">❌ 已过时</td>
</tr>
</tbody></table>
<h3 id="张量和numpy互转"><a href="#张量和numpy互转" class="headerlink" title="张量和numpy互转"></a>张量和numpy互转</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tensor</span> <span class="string">和 numpy的互转</span></span><br><span class="line">    <span class="attr">张量转numpy：</span></span><br><span class="line">        <span class="attr">data.numpy()函数，将张量转化为numpy数组，两者共享内存，一个变化，另外一个也发生变化</span></span><br><span class="line">            <span class="attr">如果不想让两者共享内存，可以使用copy()进行拷贝</span></span><br><span class="line">    <span class="attr">numpy转张量：</span></span><br><span class="line">        <span class="attr">torch.from_numpy(ndarray)共享内存</span></span><br><span class="line">        <span class="attr">torch.tensor(ndarray)</span> <span class="string">- 不共享内存</span></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tensor_numpy</span>():</span><br><span class="line">    data_tensor = torch.tensor([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">    data_numpy = data_tensor.numpy()  <span class="comment"># ndarray与原tensor对象共享内存</span></span><br><span class="line">    data_tensor[<span class="number">0</span>] = <span class="number">100</span>  <span class="comment"># 修改tensor第一个值为100 numpy数组也会变化</span></span><br><span class="line">    data_numpy[<span class="number">0</span>] = <span class="number">200</span>  <span class="comment"># 修改ndarray的值 张量？变化 都说了共享内存嘛</span></span><br><span class="line">    <span class="built_in">print</span>(data_tensor)</span><br><span class="line">    <span class="built_in">print</span>(data_numpy)</span><br></pre></td></tr></table></figure>



<h3 id="张量运算"><a href="#张量运算" class="headerlink" title="张量运算"></a>张量运算</h3><blockquote>
<p><code>哈达玛积：对应位置的元素进行相乘</code></p>
<ul>
<li>data1.mul(data2)</li>
<li>data1 * data2</li>
</ul>
<p><code>点积运算: 按照矩阵的运算规则进行运算</code></p>
<ul>
<li>data1 @ data2</li>
<li>torch.matmul(data1, data2)</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line">y = torch.tensor([<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基本运算</span></span><br><span class="line">add = x + y        <span class="comment"># 逐元素相加 [5., 7., 9.]</span></span><br><span class="line">dot = torch.dot(x, y)  <span class="comment"># 点积 1 * 4 + 2 * 5 + 3 * 6 = 32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵乘法</span></span><br><span class="line">mat1 = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">mat2 = torch.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">result = torch.mm(mat1, mat2)  <span class="comment"># 矩阵乘法</span></span><br></pre></td></tr></table></figure>



<h2 id="自动求导机制"><a href="#自动求导机制" class="headerlink" title="自动求导机制"></a>自动求导机制</h2><p>参考链接：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_51385258/article/details/143908012">自动微分模块（自动微分）</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建需要求导的张量</span></span><br><span class="line">x = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x**<span class="number">2</span> + <span class="number">3</span>*x + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">y.backward()  <span class="comment"># 自动求导</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)  <span class="comment"># dy/dx = 2x + 3 = 7</span></span><br></pre></td></tr></table></figure>



<p>参考代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自动微分</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">grad_compute</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    标量、向量、多标量、多向量的梯度计算</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 定义需要求导的标量</span></span><br><span class="line">    x = torch.tensor(<span class="number">10</span>, requires_grad=<span class="literal">True</span>, dtype=torch.float64)</span><br><span class="line">    <span class="built_in">print</span>(x.shape)</span><br><span class="line">    f = x ** <span class="number">2</span> + <span class="number">20</span>  <span class="comment"># 定义关于x的函数</span></span><br><span class="line">    f.backward()  <span class="comment"># 自动微分</span></span><br><span class="line">    <span class="built_in">print</span>(x.grad)  <span class="comment"># 访问梯度 求得方程在x处的梯度</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 向量的梯度计算</span></span><br><span class="line">    x = torch.tensor([<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>], requires_grad=<span class="literal">True</span>, dtype=torch.float64)</span><br><span class="line">    y1 = x ** <span class="number">2</span> + <span class="number">20</span>  <span class="comment"># y1得到的是向量 需要处理为标量才能使用backward()进行自动微分</span></span><br><span class="line">    y2 = y1.mean()  <span class="comment"># 自动微分的时候 必须是一个标量</span></span><br><span class="line">    y2.backward()  <span class="comment"># 自动微分 反向传播</span></span><br><span class="line">    <span class="built_in">print</span>(x.grad)  <span class="comment"># 梯度计算的结果会保存到x.grad</span></span><br><span class="line">    <span class="comment"># tensor([ 5., 10., 15., 20.], dtype=torch.float64)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 多标量的梯度计算</span></span><br><span class="line">    x1 = torch.tensor(<span class="number">10</span>, requires_grad=<span class="literal">True</span>, dtype=torch.float64)</span><br><span class="line">    x2 = torch.tensor(<span class="number">20</span>, requires_grad=<span class="literal">True</span>, dtype=torch.float64)</span><br><span class="line">    y = x1 ** <span class="number">2</span> + x2 ** <span class="number">2</span> + x1 * x2</span><br><span class="line">    y.backward()  <span class="comment"># 自动微分</span></span><br><span class="line">    <span class="built_in">print</span>(x1.grad)</span><br><span class="line">    <span class="built_in">print</span>(x2.grad)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 多向量的梯度计算</span></span><br><span class="line">    x1 = torch.tensor([<span class="number">10</span>, <span class="number">20</span>], requires_grad=<span class="literal">True</span>, dtype=torch.float64)</span><br><span class="line">    x2 = torch.tensor([<span class="number">30</span>, <span class="number">40</span>], requires_grad=<span class="literal">True</span>, dtype=torch.float64)</span><br><span class="line">    y = x1 ** <span class="number">2</span> + x2 ** <span class="number">2</span> + x1 * x2</span><br><span class="line">    y = y.<span class="built_in">sum</span>()  <span class="comment"># 将输出结果变为标量</span></span><br><span class="line">    y.backward()  <span class="comment"># 自动微分</span></span><br><span class="line">    <span class="built_in">print</span>(x1.grad)</span><br><span class="line">    <span class="built_in">print</span>(x2.grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">control_grad</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    模型训练需要进行梯度计算，但是训练完成后进入下一个阶段就不需要进行梯度计算了 由此需要控制</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x = torch.tensor(<span class="number">10</span>, requires_grad=<span class="literal">True</span>, dtype=torch.float64)</span><br><span class="line">    <span class="built_in">print</span>(x.requires_grad)</span><br><span class="line">    <span class="comment"># 第一种方法 只想要计算函数值 不想计算函数的梯度</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        y = x ** <span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(y.requires_grad)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第二种方式 主要针对函数</span></span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">my_func</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="keyword">return</span> x ** <span class="number">2</span></span><br><span class="line">    y = my_func(x)</span><br><span class="line">    <span class="built_in">print</span>(y.requires_grad)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第三种方式 全局设置</span></span><br><span class="line">    torch.set_grad_enabled(<span class="literal">False</span>)</span><br><span class="line">    y = x ** <span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(y.requires_grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cum_grad_zero</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    累计梯度和梯度清零</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x = torch.tensor([<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>], requires_grad=<span class="literal">True</span>, dtype=torch.float64)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        <span class="comment"># 函数在x处计算10次梯度</span></span><br><span class="line">        f1 = x ** <span class="number">2</span> + <span class="number">20</span></span><br><span class="line">        f2 = f1.mean()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度清零 防止梯度进行累加</span></span><br><span class="line">        <span class="keyword">if</span> x.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x.grad.data.zero_()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 自动微分</span></span><br><span class="line">        f2.backward()</span><br><span class="line">        <span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">grad_optimize</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    y = x**2 求x为何值时，y最小 又回到了抛物线过零点</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x = torch.tensor(<span class="number">10</span>, requires_grad=<span class="literal">True</span>, dtype=torch.float64)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5000</span>):</span><br><span class="line">        y = x ** <span class="number">2</span></span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        <span class="keyword">if</span> x.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x.grad.data.zero_()</span><br><span class="line">        y.backward()  <span class="comment"># 自动微分 得到x处的梯度值</span></span><br><span class="line">        x.data = x.data - <span class="number">0.002</span> * x.grad  <span class="comment"># 更新参数 对x 的值进行更新</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;%.10f&#x27;</span> % x.data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># grad_compute()</span></span><br><span class="line">    <span class="comment"># control_grad()</span></span><br><span class="line">    <span class="comment"># cum_grad_zero()</span></span><br><span class="line">    grad_optimize()</span><br></pre></td></tr></table></figure>



<h2 id="反向传播基础-反向更新权重"><a href="#反向传播基础-反向更新权重" class="headerlink" title="反向传播基础(反向更新权重)"></a>反向传播基础(反向更新权重)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bp_grad_loop</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    反向传播更新权重w for循环</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    w = torch.tensor(<span class="number">10.</span>, requires_grad=<span class="literal">True</span>, dtype=torch.float32)  <span class="comment"># 注意用10.确保浮点数</span></span><br><span class="line">    lr = <span class="number">0.01</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="comment"># 清零梯度（必须在每次迭代前执行）</span></span><br><span class="line">        <span class="keyword">if</span> w.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            w.grad.zero_()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 关键修复：在循环内重新定义损失函数（创建新的计算图）</span></span><br><span class="line">        loss = w ** <span class="number">2</span> + <span class="number">20</span>  </span><br><span class="line">        loss.backward()  <span class="comment"># 反向传播（无需.sum()，loss已是标量）</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新权重</span></span><br><span class="line">        w.data -= lr * w.grad</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;当前轮次<span class="subst">&#123;epoch&#125;</span>，当前权重<span class="subst">&#123;w.data&#125;</span>，更新后梯度<span class="subst">&#123;w.grad&#125;</span>，下一个权重<span class="subst">&#123;w.data - lr * w.grad&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>



<h2 id="线性回归手动构建"><a href="#线性回归手动构建" class="headerlink" title="线性回归手动构建"></a>线性回归手动构建</h2><p>参考链接：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_51385258/article/details/143928853">线性回归案例（手动构建）</a></li>
</ul>
<h3 id="参考代码："><a href="#参考代码：" class="headerlink" title="参考代码："></a>参考代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression  <span class="comment"># 构造数据加载器</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataset</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    # 构建数据集</span></span><br><span class="line"><span class="string">    :return: 数据集特征与标签 x y 权重coef</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x, y, coef = make_regression(</span><br><span class="line">        n_samples=<span class="number">100</span>,  <span class="comment"># 样本数量</span></span><br><span class="line">        n_features=<span class="number">1</span>,  <span class="comment"># 设置特征个数</span></span><br><span class="line">        noise=<span class="number">10</span>,  <span class="comment"># 设置噪声 可以调整 出现波动</span></span><br><span class="line">        coef=<span class="literal">True</span>,  <span class="comment"># 需要权重</span></span><br><span class="line">        bias=<span class="number">14.5</span>,  <span class="comment"># 偏置</span></span><br><span class="line">        random_state=<span class="number">0</span>  <span class="comment"># 确保随机种子固定 方便复现数据</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将构建数据转为张量类型</span></span><br><span class="line">    x = torch.tensor(x)</span><br><span class="line">    y = torch.tensor(y)</span><br><span class="line">    <span class="keyword">return</span> x, y, coef</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_loader</span>(<span class="params">x, y, batch_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    构造数据加载器 按照一定数据量 分批次 产生数据</span></span><br><span class="line"><span class="string">    :return: 批次特征数据集 批次标签数据集</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    data_len = <span class="built_in">len</span>(y)  <span class="comment"># 计算样本的数量</span></span><br><span class="line">    data_index = <span class="built_in">list</span>(<span class="built_in">range</span>(data_len))  <span class="comment"># 构建数据索引</span></span><br><span class="line">    random.shuffle(data_index)  <span class="comment"># 打乱数据集</span></span><br><span class="line">    batch_number = data_len // batch_size</span><br><span class="line">    <span class="comment"># 遍历一个batch</span></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(batch_number):</span><br><span class="line">        start = idx * batch_size</span><br><span class="line">        end = start + batch_size</span><br><span class="line">        batch_train_x = x[start: end]</span><br><span class="line">        batch_train_y = y[start: end]</span><br><span class="line">        <span class="keyword">yield</span> batch_train_x, batch_train_y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">w = torch.tensor(<span class="number">0.1</span>, requires_grad=<span class="literal">True</span>, dtype=torch.float64)</span><br><span class="line">b = torch.tensor(<span class="number">0.0</span>, requires_grad=<span class="literal">True</span>, dtype=torch.float64)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear_regression</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    构建假设函数</span></span><br><span class="line"><span class="string">    :return: wx + b 方程</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> w * x + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">square_loss</span>(<span class="params">y_pred, y_true</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    定义损失函数 采用MAS作为损失函数</span></span><br><span class="line"><span class="string">    :param y_pred:</span></span><br><span class="line"><span class="string">    :param y_true:</span></span><br><span class="line"><span class="string">    :return: (预测值 - 真实值)的平方</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (y_pred - y_true) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">lr=<span class="number">1e-2</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    定义优化方法 采用随机梯度下降法 进行权重参数的更新</span></span><br><span class="line"><span class="string">    :param lr:</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    w.data = w.data - lr * w.grad.data / <span class="number">16</span></span><br><span class="line">    b.data = b.data - lr * b.grad.data / <span class="number">16</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    进行训练</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x, y, coef = create_dataset()  <span class="comment"># 加载数据集</span></span><br><span class="line">    epochs = <span class="number">100</span>  <span class="comment"># 所有样本在模型中训练一遍称为一个epoch</span></span><br><span class="line">    learning_rate = <span class="number">1e-2</span></span><br><span class="line">    epoch_loss = []  <span class="comment"># 记录每一个epoch的损失</span></span><br><span class="line">    total_loss = <span class="number">0.0</span></span><br><span class="line">    train_samples = <span class="number">0</span>  <span class="comment"># 统计训练的样本的数量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="comment"># 进行训练 批次加载数据位训练集样本以及标签</span></span><br><span class="line">        <span class="keyword">for</span> train_x, train_y <span class="keyword">in</span> data_loader(x, y, batch_size=<span class="number">16</span>):</span><br><span class="line">            y_pred = linear_regression(train_x)  <span class="comment"># shape[16, 1]</span></span><br><span class="line">            <span class="comment"># 计算预测值和真实值的平方损失</span></span><br><span class="line">            loss = square_loss(y_pred, train_y.reshape(-<span class="number">1</span>, <span class="number">1</span>)).<span class="built_in">sum</span>()</span><br><span class="line">            total_loss += loss.item()  <span class="comment"># 提取单值张量</span></span><br><span class="line">            train_samples += <span class="built_in">len</span>(train_y)  <span class="comment"># 样本数量</span></span><br><span class="line">            <span class="comment"># 进行梯度清零</span></span><br><span class="line">            <span class="keyword">if</span> w.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                w.grad.data.zero_()</span><br><span class="line">            <span class="keyword">if</span> b.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                b.grad.data.zero_()</span><br><span class="line">            <span class="comment"># 自动微分</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="comment"># 参数更新</span></span><br><span class="line">            sgd(learning_rate)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;loss: %.10f&#x27;</span> % (total_loss / train_samples))</span><br><span class="line">        epoch_loss.append(total_loss / train_samples)  <span class="comment"># 记录每一个epoch的平均损失</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘制数据散点图</span></span><br><span class="line">    plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;Arial Unicode MS&#x27;</span>]</span><br><span class="line">    plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span></span><br><span class="line">    plt.scatter(x, y)</span><br><span class="line">    <span class="comment"># 绘制拟合的直线</span></span><br><span class="line">    x = torch.linspace(x.<span class="built_in">min</span>(), x.<span class="built_in">max</span>(), <span class="number">1000</span>)</span><br><span class="line">    y1 = torch.tensor([v * w + b <span class="keyword">for</span> v <span class="keyword">in</span> x])</span><br><span class="line">    y2 = torch.tensor([v * coef + <span class="number">14.5</span> <span class="keyword">for</span> v <span class="keyword">in</span> x])</span><br><span class="line"></span><br><span class="line">    plt.plot(x, y1, label=<span class="string">&#x27;训练&#x27;</span>)</span><br><span class="line">    plt.plot(x, y2, label=<span class="string">&#x27;真实&#x27;</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印损失变化曲线</span></span><br><span class="line">    plt.plot(<span class="built_in">range</span>(epochs), epoch_loss)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.title(<span class="string">&#x27;损失变化曲线&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    x, y, coef = create_dataset()</span><br><span class="line">    plt.scatter(x, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> data_loader(x, y, batch_size=<span class="number">10</span>):</span><br><span class="line">        <span class="built_in">print</span>(y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    train()</span><br></pre></td></tr></table></figure>

<h2 id="线性回归PyTorch组件构建"><a href="#线性回归PyTorch组件构建" class="headerlink" title="线性回归PyTorch组件构建"></a>线性回归PyTorch组件构建</h2><p>参考链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_51385258/article/details/143932956">基本组件使用</a></p>
<ul>
<li><p><code>损失函数</code>：本质是<code>封装了(y_pred - y_true)之间误差计算公式</code></p>
<ul>
<li>criterion &#x3D; nn.MSELoss()</li>
</ul>
</li>
<li><p>构建全连接层&#x2F;<code>假设函数</code>&#x2F;模型：<code>类似于y=wx + b</code></p>
<ul>
<li>model &#x3D; nn.Linear(in_features&#x3D;1,out_features&#x3D;1：意思只接收一维输入和一维输出</li>
</ul>
</li>
<li><p><code>优化方法</code>：类似于<code>w.data -= lr*w.grad/batch_size</code></p>
<ul>
<li>optimizer &#x3D; optim.SGD(model.parameters(), lr&#x3D;1e-2)：model是全连接层参数，lr学习率</li>
</ul>
</li>
<li><p><code>数据加载器</code>：本质封装了<code>分批加载、分批yield、批次：ceil(len(data) / batch_size)</code></p>
<ul>
<li>x,y,coef &#x3D; create_dataset()</li>
<li>dataset &#x3D; TensorDataset(x, y)</li>
<li><code>dataloader = DataLoader(dataset, batchsize=16, shuffle=True)</code></li>
</ul>
</li>
<li><p>训练模型：</p>
<ul>
<li><code>外层循环控制轮次、内层循环控制批次</code></li>
<li>模型训练：y_pred &#x3D; model(x_train.float())</li>
<li>损失构建：loss &#x3D; criterion (y_pred, y_train.reshape(-1, 1).float())</li>
<li><code>梯度自动清零</code>：optimizer.zero_grad()</li>
<li>&#96;反向传播：loss.backward()</li>
<li><code>更新参数</code>：optimizer.step()</li>
</ul>
</li>
<li><p>绘制预测曲线、损失变化曲线：</p>
<ul>
<li>预测曲线：y_predict &#x3D; torch.tensor([v * model.weight + model.bias for v in x]) </li>
<li>真实曲线：y_true &#x3D; torch.tensor([v * coef + 14.5 for v in x])</li>
<li>损失变化曲线：<ul>
<li><code>total_loss += loss.item() ：提取单值张量</code></li>
<li><code>epoch_loss.append(total_loss / train_samples)</code></li>
<li>plt.plot(range(epochs), epoch_loss)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression  <span class="comment"># 构造数据加载器</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset  <span class="comment"># 构造数据集对象</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader  <span class="comment"># 数据加载器</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用PyTorch的 nn.MSELoss() 代替平方损失函数  # (y_pred - y_true) ** 2 反正就是误差函数</span></span><br><span class="line"><span class="comment"># 使用PyTorch的 data.DataLoader 代替数据加载器  # 要不自己自动yield，len(data) /batch_size</span></span><br><span class="line"><span class="comment"># 使用PyTorch的 optim.SGD 代替优化器  # w.data = w.data - lr * w.grad.data / batch_size</span></span><br><span class="line"><span class="comment"># 使用PyTorch的 nn.Linear 代替假设函数  w*x + b</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataset</span>():</span><br><span class="line">    x, y, coef = make_regression(</span><br><span class="line">        n_samples=<span class="number">100</span>,</span><br><span class="line">        n_features=<span class="number">1</span>,</span><br><span class="line">        noise=<span class="number">10</span>,</span><br><span class="line">        coef=<span class="literal">True</span>,</span><br><span class="line">        bias=<span class="number">14.5</span>,</span><br><span class="line">        random_state=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    x = torch.tensor(x)</span><br><span class="line">    y = torch.tensor(y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, y, coef</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    <span class="comment"># 构建数据集</span></span><br><span class="line">    x, y, coef = create_dataset()</span><br><span class="line">    dataset = TensorDataset(x, y)  <span class="comment"># 构建数据集对象</span></span><br><span class="line">    dataloader = DataLoader(dataset, batch_size=<span class="number">16</span>, shuffle=<span class="literal">True</span>)  <span class="comment"># 实现了自动yield</span></span><br><span class="line">    <span class="comment"># 构建函数：输入特征、输出特征均为标量或形状为 [batch_size, 1] 的张量 即 y = wx + b</span></span><br><span class="line">    model = nn.Linear(in_features=<span class="number">1</span>, out_features=<span class="number">1</span>)  <span class="comment"># 实现了 w**2 + b 黑盒</span></span><br><span class="line">    criterion = nn.MSELoss()  <span class="comment"># 构建损失函数  自动实现 (y_pred - y_true) ** 2</span></span><br><span class="line">    optimizer = optim.SGD(model.parameters(), lr=<span class="number">1e-2</span>)  <span class="comment"># 随机梯度下降 w.data -= lr*w.grad</span></span><br><span class="line">    epochs = <span class="number">100</span>  <span class="comment"># 训练轮次</span></span><br><span class="line">    <span class="comment"># 外层循环控制轮次 内层循环控制批次</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> x_train, y_train <span class="keyword">in</span> dataloader:</span><br><span class="line">            y_pred = model(x_train.<span class="built_in">float</span>())</span><br><span class="line">            loss = criterion(y_pred, y_train.reshape(-<span class="number">1</span>, <span class="number">1</span>).<span class="built_in">float</span>())  <span class="comment"># reshape升维 unsqueeze</span></span><br><span class="line">            optimizer.zero_grad()  <span class="comment"># 梯度自动清零  不用写if w.grad is not None 了</span></span><br><span class="line">            loss.backward()  <span class="comment"># 自动微分 反向传播</span></span><br><span class="line">            optimizer.step()  <span class="comment"># 更新参数  也就是反向传播 实现了w.data -= lr*w.grad</span></span><br><span class="line">    <span class="comment"># 绘制拟合曲线</span></span><br><span class="line">    plt.scatter(x, y)</span><br><span class="line">    x = torch.linspace(x.<span class="built_in">min</span>(), x.<span class="built_in">max</span>(), <span class="number">1000</span>)</span><br><span class="line">    y1 = torch.tensor([v * model.weight + model.bias <span class="keyword">for</span> v <span class="keyword">in</span> x])  <span class="comment"># 训练预测值</span></span><br><span class="line">    y2 = torch.tensor([v * coef + <span class="number">14.5</span> <span class="keyword">for</span> v <span class="keyword">in</span> x])  <span class="comment"># 真实值 = k*权重 + bias</span></span><br><span class="line">    plt.plot(x, y1, label=<span class="string">&#x27;预测&#x27;</span>)</span><br><span class="line">    plt.plot(x, y1, label=<span class="string">&#x27;真实&#x27;</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x, y, coef = create_dataset()</span><br><span class="line">    <span class="built_in">print</span>(coef)</span><br><span class="line">    train()</span><br></pre></td></tr></table></figure>

<h2 id="参数初始化-权重、偏置"><a href="#参数初始化-权重、偏置" class="headerlink" title="参数初始化(权重、偏置)"></a>参数初始化(权重、偏置)</h2><p>参考链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_51385258/article/details/143978418">网络参数初始化</a></p>
<h3 id="初始化参数目的"><a href="#初始化参数目的" class="headerlink" title="初始化参数目的"></a>初始化参数目的</h3><p><code>防止梯度消失或者梯度爆炸、提高收敛速度、打破对称性</code></p>
<h3 id="参数初始化方式："><a href="#参数初始化方式：" class="headerlink" title="参数初始化方式："></a>参数初始化方式：</h3><ul>
<li>无法打破对称性<ul>
<li>全0、全1、固定值</li>
</ul>
</li>
<li>可以打破对称性<ul>
<li>随机初始化、正态分布初始化、<code>kaiming初始化、xavier初始化</code></li>
</ul>
</li>
<li>总结<br><code>kaiming初始化、xavier初始化、全0初始化比较重要</code></li>
</ul>
<h3 id="关于初始化选择"><a href="#关于初始化选择" class="headerlink" title="关于初始化选择"></a>关于初始化选择</h3><ul>
<li><code>激活函数Relu系列：优先选择kaiming</code></li>
<li><code>激活函数非Relu：优先选择xavier</code></li>
<li>如果是浅层网络：可考虑使用随机初始化</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 练习五种初始化方式 以及构建神经网络</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">初始化的参数主要有权重和偏置，偏置参数一般初始化为 0 即可，而对权重的初始化则会更加重要</span></span><br><span class="line"><span class="string">参数初始化的目的：</span></span><br><span class="line"><span class="string">    防止梯度消失或者梯度爆炸、提高收敛速度、打破对称性</span></span><br><span class="line"><span class="string">初始化比较重要的</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">uniform_init</span>():</span><br><span class="line">    <span class="comment"># 均匀分布初始化</span></span><br><span class="line">    linear = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(linear.weight)</span><br><span class="line">    nn.init.uniform_(linear.weight)  <span class="comment"># 对模型的权重进行初始化  传入model.weight</span></span><br><span class="line">    nn.init.uniform_(linear.bias)  <span class="comment"># 对偏置b进行随机初始化 从0-1均匀分布产生参数</span></span><br><span class="line">    <span class="built_in">print</span>(linear.weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">const_value_init</span>():</span><br><span class="line">    <span class="comment"># 固定值初始化</span></span><br><span class="line">    linear = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    nn.init.constant_(linear.weight, <span class="number">5</span>)  <span class="comment"># 权重固定值初始化为5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">zero_init</span>():</span><br><span class="line">    <span class="comment"># 全0初始化</span></span><br><span class="line">    linear = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    nn.init.zeros_(linear.weight)</span><br><span class="line">    nn.init.zeros_(linear.bias)</span><br><span class="line">    <span class="built_in">print</span>(linear.weight.data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">one_init</span>():</span><br><span class="line">    <span class="comment"># 全1 初始化</span></span><br><span class="line">    linear = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    nn.init.ones_(linear.weight)</span><br><span class="line">    nn.init.ones_(linear.bias)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalize_init</span>():</span><br><span class="line">    <span class="comment"># 正态分布随机初始化</span></span><br><span class="line">    linear = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    nn.init.normal_(linear.weight, mean=<span class="number">0</span>, std=<span class="number">1</span>)  <span class="comment"># 指定均值和方差</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kaiming_init</span>():</span><br><span class="line">    <span class="comment"># kaiming正态分布初始化</span></span><br><span class="line">    linear = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    nn.init.kaiming_normal_(linear.weight)</span><br><span class="line">    <span class="built_in">print</span>(linear.weight)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># kaiming 均匀分布初始化</span></span><br><span class="line">    linear = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    nn.init.kaiming_uniform_(linear.weight)</span><br><span class="line">    <span class="built_in">print</span>(linear.weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">xavier_init</span>():</span><br><span class="line">    <span class="comment"># xavier 正态分布初始化</span></span><br><span class="line">    linear = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    nn.init.xavier_normal_(linear.weight)</span><br><span class="line">    <span class="built_in">print</span>(linear.weight)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># xavier 均匀分布初始化</span></span><br><span class="line">    linear = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    nn.init.xavier_uniform_(linear.weight)</span><br><span class="line">    <span class="built_in">print</span>(linear.weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    uniform_init()</span><br><span class="line">    kaiming_init()</span><br><span class="line">    xavier_init()</span><br></pre></td></tr></table></figure>



<h2 id="神经网络构建"><a href="#神经网络构建" class="headerlink" title="神经网络构建"></a>神经网络构建</h2><p>参考链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_51385258/article/details/143979259">神经网络计算及参数计算</a></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250731144349372.png" alt="image-20250731144349372"></p>
<h3 id="参考代码-1"><a href="#参考代码-1" class="headerlink" title="参考代码"></a>参考代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchsummary <span class="keyword">import</span> summary</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">3</span>, <span class="number">3</span>)  <span class="comment"># 创建第一个3输入3输出的隐藏层</span></span><br><span class="line">        nn.init.xavier_normal_(self.linear1.weight)  <span class="comment"># 初始化第一个隐藏层的权重</span></span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">3</span>, <span class="number">2</span>)  <span class="comment"># 创建第二个3输入 2输出的隐藏层</span></span><br><span class="line">        nn.init.kaiming_normal_(self.linear2.weight)  <span class="comment"># 初始化第二个隐藏层的权重</span></span><br><span class="line">        self.out = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># 创建输出层 两个输入 两个输出</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.sigmoid(self.linear1(x))  <span class="comment"># 经过第一个隐藏层 以及sigmoid激活函数处理</span></span><br><span class="line">        x = torch.relu(self.linear2(x))  <span class="comment"># 经过第二个隐藏层 以及relu激活函数处理</span></span><br><span class="line">        x = torch.softmax(self.out(x), dim=-<span class="number">1</span>)  <span class="comment"># 经过输出层  dim=-1代表按行计算 一条样本一条样本处理</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    my_data = torch.randn(<span class="number">5</span>, <span class="number">3</span>).to(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    my_model = Model()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(my_data)</span><br><span class="line">    y_pred = my_model(my_data)</span><br><span class="line">    <span class="built_in">print</span>(y_pred)</span><br><span class="line">    <span class="built_in">print</span>(my_data.shape, y_pred.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算和查看模型的参数</span></span><br><span class="line">    summary(my_model, input_size=(<span class="number">3</span>,), batch_size=<span class="number">5</span>, device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> name, parameter <span class="keyword">in</span> my_model.named_parameters():</span><br><span class="line">        <span class="built_in">print</span>(name, parameter)</span><br></pre></td></tr></table></figure>

<h2 id="数据加载与处理"><a href="#数据加载与处理" class="headerlink" title="数据加载与处理"></a>数据加载与处理</h2><p>使用Dataset和DataLoader</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, labels</span>):</span><br><span class="line">        self.data = data</span><br><span class="line">        self.labels = labels</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.labels)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx], self.labels[idx]</span><br><span class="line"></span><br><span class="line">dataset = CustomDataset(train_data, train_labels)</span><br><span class="line">loader = DataLoader(dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h2 id="GPU加速"><a href="#GPU加速" class="headerlink" title="GPU加速"></a>GPU加速</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检测GPU是否可用</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将模型和数据移动到GPU</span></span><br><span class="line">model = model.to(device)</span><br><span class="line">data = data.to(device)</span><br><span class="line">labels = labels.to(device)</span><br></pre></td></tr></table></figure>

<h2 id="模型保存与加载"><a href="#模型保存与加载" class="headerlink" title="模型保存与加载"></a>模型保存与加载</h2><p>参考链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_51385258/article/details/143937355">模型保存与加载</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">torch.save(&#123;</span><br><span class="line">    <span class="string">&#x27;model_state_dict&#x27;</span>: model.state_dict(),</span><br><span class="line">    <span class="string">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),</span><br><span class="line">&#125;, <span class="string">&#x27;model.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">checkpoint = torch.load(<span class="string">&#x27;model.pth&#x27;</span>)</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">&#x27;model_state_dict&#x27;</span>])</span><br><span class="line">optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer_state_dict&#x27;</span>])</span><br></pre></td></tr></table></figure>

<h3 id="参考代码-2"><a href="#参考代码-2" class="headerlink" title="参考代码"></a>参考代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 神经网络的训练有时需要几天、几周、甚至几个月，为了在每次使用模型时避免高代价的重复训练，</span></span><br><span class="line"><span class="comment"># 我们就需要将模型序列化到磁盘中，使用的时候反序列化到内存中。</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 创建神经网络对象继承自nn.Module</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, output_size</span>):</span><br><span class="line">        <span class="comment"># 如果还可以写super(Model, self).__init__但是这种方法如果类名更改就也需要跟着修改，比较烦人</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 初始化两个线性层 分别指定输入输出个数 第一层的输出是第二层的输入</span></span><br><span class="line">        self.linear1 = nn.Linear(input_size, input_size * <span class="number">2</span>)</span><br><span class="line">        self.linear2 = nn.Linear(input_size * <span class="number">2</span>, output_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model_save</span>():</span><br><span class="line">    <span class="comment"># 创建神经网络模型 实现了forward函数的对象 所以才会model()</span></span><br><span class="line">    model = Model(<span class="number">128</span>, <span class="number">10</span>)  <span class="comment"># 创建模型对象 128维输入 10维输出</span></span><br><span class="line">    <span class="comment"># torch.save()用于保存模型 后两个参数为执行序列化方法 和 序列化协议</span></span><br><span class="line">    torch.save(model, <span class="string">&#x27;test_model_save.pth&#x27;</span>, pickle_module=pickle, pickle_protocol=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model_load</span>():</span><br><span class="line">    <span class="comment"># torch.load() 用于加载模型 map_location=&quot;cpu&quot;表示加载到cpu  map_location=&quot;cuda0&quot; 加载到GPU</span></span><br><span class="line">    model = torch.load(<span class="string">&#x27;test_model_save.pth&#x27;</span>, pickle_module=pickle, map_location=<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mode_save_parameters</span>():</span><br><span class="line">    <span class="comment"># 实现模型存储 带参数</span></span><br><span class="line">    model = Model(<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=<span class="number">1e-2</span>)  <span class="comment"># 初始化优化器</span></span><br><span class="line">    save_params = &#123;</span><br><span class="line">        <span class="string">&#x27;init_params&#x27;</span>: &#123;<span class="string">&#x27;input_size&#x27;</span>: <span class="number">128</span>, <span class="string">&#x27;output_size&#x27;</span>: <span class="number">10</span>&#125;,</span><br><span class="line">        <span class="string">&#x27;acc_score&#x27;</span>: <span class="number">0.98</span>,</span><br><span class="line">        <span class="string">&#x27;avg_loss&#x27;</span>: <span class="number">0.86</span>,</span><br><span class="line">        <span class="string">&#x27;iter_num&#x27;</span>: <span class="number">100</span>,</span><br><span class="line">        <span class="string">&#x27;optim_params&#x27;</span>: optimizer.state_dict(),  <span class="comment"># 获取优化器的参数</span></span><br><span class="line">        <span class="string">&#x27;model_params&#x27;</span>: model.state_dict()  <span class="comment"># 获取模型参数</span></span><br><span class="line">    &#125;</span><br><span class="line">    torch.save(save_params, <span class="string">&#x27;./model_param.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model_load_parameters</span>():</span><br><span class="line">    <span class="comment"># 从磁盘中将参数加载到内存中</span></span><br><span class="line">    model_params = torch.load(<span class="string">&#x27;model/model_params.pth&#x27;</span>)</span><br><span class="line">    <span class="comment"># 使用参数初始化模型</span></span><br><span class="line">    model = Model(model_params[<span class="string">&#x27;init_params&#x27;</span>][<span class="string">&#x27;input_size&#x27;</span>], model_params[<span class="string">&#x27;init_params&#x27;</span>][<span class="string">&#x27;output_size&#x27;</span>])</span><br><span class="line">    model.load_state_dict(model_params[<span class="string">&#x27;model_params&#x27;</span>])</span><br><span class="line">    <span class="comment"># 使用参数初始化优化器</span></span><br><span class="line">    optimizer = optim.Adam(model.parameters())</span><br><span class="line">    optimizer.load_state_dict(model_params[<span class="string">&#x27;optim_params&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可以加载其他参数</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;迭代次数:&#x27;</span>, model_params[<span class="string">&#x27;iter_num&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;准确率:&#x27;</span>, model_params[<span class="string">&#x27;acc_score&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;平均损失:&#x27;</span>, model_params[<span class="string">&#x27;avg_loss&#x27;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>



<h2 id="实用工具函数"><a href="#实用工具函数" class="headerlink" title="实用工具函数"></a>实用工具函数</h2><table>
<thead>
<tr>
<th>函数</th>
<th>用途</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td>torch.cat</td>
<td>张量拼接</td>
<td><code>torch.cat([a, b], dim=0)</code></td>
</tr>
<tr>
<td>torch.stack</td>
<td>新建维度堆叠</td>
<td><code>torch.stack([a, b], dim=1)</code></td>
</tr>
<tr>
<td>torch.where</td>
<td>条件选择</td>
<td><code>torch.where(x&gt;0, x, torch.zeros_like(x))</code></td>
</tr>
<tr>
<td>torch.unique</td>
<td>去重</td>
<td><code>torch.unique(x)</code></td>
</tr>
</tbody></table>
<h2 id="PyTorch代码实例"><a href="#PyTorch代码实例" class="headerlink" title="PyTorch代码实例"></a>PyTorch代码实例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tensor_demo</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    创建张量：</span></span><br><span class="line"><span class="string">        torch.tensor(data, datatype)创建张量 最常用 可以根据已有数据创建张量（默认使用原有数据类型）</span></span><br><span class="line"><span class="string">        torch.Tensor(data, size=())创建张量 主要是根据形状创建张量</span></span><br><span class="line"><span class="string">        torch.DoubleTensor(data) 主要是创建指定类型的张量(Tensor指定类型创建张量)</span></span><br><span class="line"><span class="string">        torch.linspace(start, end, number) 主要是创建线性张量</span></span><br><span class="line"><span class="string">        torch.arange(start, end, step) 创建线性张量</span></span><br><span class="line"><span class="string">        torch.randn(shape()) 创建0-1之间的随机张量</span></span><br><span class="line"><span class="string">        torch.randint(start, end, shape()) 创建随机整型张量</span></span><br><span class="line"><span class="string">        torch.zeros(size) 创建指定类型的全为0的张量 torch.zeros_like(data)</span></span><br><span class="line"><span class="string">        torch.ones(size) 创建指定类型的全为1的张量 torch.ones_like(data)</span></span><br><span class="line"><span class="string">        torch.full(size) 创建指定值的张量  torch.full_like(data)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    张量类型转换：</span></span><br><span class="line"><span class="string">         data = data.type(torch.DoubleTensor)</span></span><br><span class="line"><span class="string">         data.double()</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    data = torch.tensor(<span class="number">10</span>)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用numpy数组创建张量</span></span><br><span class="line">    data = np.random.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">    data = torch.tensor(data)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tensor_numpy</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    tensor 和 numpy的互转</span></span><br><span class="line"><span class="string">        张量转numpy：</span></span><br><span class="line"><span class="string">            data.numpy()函数，将张量转化为numpy数组，两者共享内存，一个变化，另外一个也发生变化</span></span><br><span class="line"><span class="string">                如果不想让两者共享内存，可以使用copy()进行拷贝</span></span><br><span class="line"><span class="string">        numpy转张量：</span></span><br><span class="line"><span class="string">            torch.from_numpy(ndarray)共享内存</span></span><br><span class="line"><span class="string">            torch.tensor(ndarray) - 不共享内存</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    data_tensor = torch.tensor([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">    data_numpy = data_tensor.numpy()  <span class="comment"># ndarray与原tensor对象共享内存</span></span><br><span class="line">    data_tensor[<span class="number">0</span>] = <span class="number">100</span>  <span class="comment"># 修改tensor第一个值为100 numpy数组也会变化</span></span><br><span class="line">    data_numpy[<span class="number">0</span>] = <span class="number">200</span>  <span class="comment"># 修改ndarray的值 张量？变化 都说了共享内存嘛</span></span><br><span class="line">    <span class="built_in">print</span>(data_tensor)</span><br><span class="line">    <span class="built_in">print</span>(data_numpy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可以使用copy函数实现不共享内存</span></span><br><span class="line">    data_tensor = torch.tensor([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">    data_numpy = data_tensor.numpy().copy()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># numpy中ndarray转张量</span></span><br><span class="line">    data_numpy = np.array([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">    data_tensor = torch.tensor(data_numpy)</span><br><span class="line">    data_tensor[<span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tensor_extract</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    提取单值张量 单值张量虽然只有一个值，但是仍然为张量类型</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    t1 = torch.tensor(<span class="number">30</span>)</span><br><span class="line">    t2 = torch.tensor([<span class="number">30</span>])</span><br><span class="line">    t3 = torch.tensor([[<span class="number">30</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(t1.shape)</span><br><span class="line">    <span class="built_in">print</span>(t2.shape)</span><br><span class="line">    <span class="built_in">print</span>(t3.shape)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(t1.item())</span><br><span class="line">    <span class="built_in">print</span>(t2.item())</span><br><span class="line">    <span class="built_in">print</span>(t3.item())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">concat_tensor</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    张量的拼接</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 固定随机数种子</span></span><br><span class="line">    torch.manual_seed(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># randint函数 开始值 结束值 形状</span></span><br><span class="line">    data1 = torch.randint(<span class="number">0</span>, <span class="number">10</span>, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">    data2 = torch.randint(<span class="number">0</span>, <span class="number">10</span>, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(data1.shape)</span><br><span class="line">    <span class="built_in">print</span>(data2.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按照0维度进行拼接(也就是按照第一行进行拼接)</span></span><br><span class="line">    new_data = torch.cat([data1, data2], dim=<span class="number">0</span>)</span><br><span class="line">    <span class="built_in">print</span>(new_data.shape)  <span class="comment"># [6, 4, 5]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按照1维度进行拼接</span></span><br><span class="line">    new_data = torch.cat([data1, data2], dim=<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(new_data.shape)  <span class="comment"># [3, 8, 5]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按照2维度进行拼接</span></span><br><span class="line">    new_data = torch.cat([data1, data2], dim=<span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(new_data.shape)  <span class="comment"># [3, 4, 10]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">stack_tensor</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    按照一定维度 从两个张量中各自取一个元素 组合成新的元素 形成新的张量</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 固定随机种子</span></span><br><span class="line"></span><br><span class="line">    torch.manual_seed(<span class="number">0</span>)</span><br><span class="line">    data1 = torch.randint(<span class="number">0</span>, <span class="number">10</span>, [<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    <span class="built_in">print</span>(data1)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">30</span>)</span><br><span class="line">    data2 = torch.randint(<span class="number">0</span>, <span class="number">10</span>, [<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    <span class="built_in">print</span>(data2)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">30</span>)</span><br><span class="line">    <span class="comment"># 将两个二维张量作为元素进行堆叠 变为一个三维（也就是两个两行三列）(2, 2, 3)</span></span><br><span class="line">    new_data = torch.stack([data1, data2], dim=<span class="number">0</span>)</span><br><span class="line">    <span class="built_in">print</span>(new_data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按照1维度进行叠加 将二维张量中的第一层中的元素进行拼接(这里是两个一维向量进行拼接)</span></span><br><span class="line">    new_data = torch.stack([data1, data2], dim=<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(new_data.shape)  <span class="comment"># torch.Size([2, 2, 3])</span></span><br><span class="line">    <span class="built_in">print</span>(new_data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按照2维度进行叠加 （将二维张量中 按照元素进行拼接</span></span><br><span class="line">    new_data = torch.stack([data1, data2], dim=<span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(new_data.shape)  <span class="comment"># torch.Size([2, 3, 2])</span></span><br><span class="line">    <span class="built_in">print</span>(new_data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tensor_shape</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    张量的形状操作</span></span><br><span class="line"><span class="string">        data.reshape()会重新计算张量的维度  -1代表自动匹配行数/列数 data.reshape(-1, 2)</span></span><br><span class="line"><span class="string">        torch.transpose(data, (shape))  每次只会交换两个维度</span></span><br><span class="line"><span class="string">        torch.permute(data, (shape))  可以一次交换多个维度</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        view() 函数改变张量形状：</span></span><br><span class="line"><span class="string">            1. 一个张量经过了transpose或者permute函数处理 无法使用view进行形状操作</span></span><br><span class="line"><span class="string">            2. 且view只能处理存在于内存中的整块连续张量 不在内存中或者不连续均不能处理</span></span><br><span class="line"><span class="string">            解决：先contiguous将非连续内存转换为连续内存，再用view函数更改张量形状</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        squeeze函数可以将维度为1的维度进行删除</span></span><br><span class="line"><span class="string">        unsqueeze函数给张量增加维度为1的维度</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    torch.manual_seed(<span class="number">1</span>)</span><br><span class="line">    data = torch.randint(<span class="number">0</span>, <span class="number">10</span>, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">    new_data = data.reshape(<span class="number">4</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">    <span class="built_in">print</span>(new_data.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 直接交换两个维度的值(dim0 和 dim1)</span></span><br><span class="line">    new_data = torch.transpose(data, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(new_data.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># transpose将[3, 4, 5]转为[4, 5, 3]transpose一次只能交换两个维度，需要交换两次</span></span><br><span class="line">    new_data = torch.transpose(data, <span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># 第一次（4, 3, 5）</span></span><br><span class="line">    new_data = torch.transpose(new_data, <span class="number">1</span>, <span class="number">2</span>) <span class="comment"># 第二次(4, 5, 3)</span></span><br><span class="line">    <span class="built_in">print</span>(new_data.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可以考虑使用permute函数一次交换多个维度 [3, 4, 5] -&gt; (1, 2, 0) -&gt; [4, 5, 3]</span></span><br><span class="line">    new_data = torch.permute(torch.randint(<span class="number">0</span>, <span class="number">10</span>, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]), [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(new_data.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># view函数</span></span><br><span class="line">    data = torch.tensor([[<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>],</span><br><span class="line">                         [<span class="number">40</span>, <span class="number">50</span>, <span class="number">60</span>]])</span><br><span class="line">    data = data.view(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(data.shape)</span><br><span class="line">    <span class="built_in">print</span>(data.is_contiguous())  <span class="comment"># 判断张量是否为连续内存空间 True</span></span><br><span class="line"></span><br><span class="line">    data = torch.transpose(data, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(data.is_contiguous())  <span class="comment"># False</span></span><br><span class="line">    data = data.contiguous().view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">    <span class="comment"># tensor([[10, 30, 50],</span></span><br><span class="line">    <span class="comment">#         [20, 40, 60]])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># squeeze、unsqueeze函数</span></span><br><span class="line">    data = torch.randint(<span class="number">0</span>, <span class="number">10</span>, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line">    new_data = data.squeeze()  <span class="comment"># squeeze()去掉所有为1的维度;squeeze(0)删除第一个位置为1的维度</span></span><br><span class="line">    <span class="built_in">print</span>(new_data.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># unsqueeze 可以在指定位置增加维度</span></span><br><span class="line">    new_data = data.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(new_data.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tensor_func</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    张量常用的函数：</span></span><br><span class="line"><span class="string">        data.mean() 求均值</span></span><br><span class="line"><span class="string">        data.sum()  求和</span></span><br><span class="line"><span class="string">        data.pow(n)  平方</span></span><br><span class="line"><span class="string">        data.sqrt()  平方根</span></span><br><span class="line"><span class="string">        data.exp()  e的多少次方</span></span><br><span class="line"><span class="string">        data.log()  以e为底对数</span></span><br><span class="line"><span class="string">        data.log2() 以2为底对数</span></span><br><span class="line"><span class="string">        data.log10()以10为底对数</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    torch.manual_seed(<span class="number">0</span>)</span><br><span class="line">    data = torch.randint(<span class="number">0</span>, <span class="number">10</span>, [<span class="number">2</span>, <span class="number">3</span>]).double()</span><br><span class="line">    <span class="built_in">print</span>(data.mean())</span><br><span class="line">    <span class="built_in">print</span>(data.mean(dim=<span class="number">0</span>))  <span class="comment"># 竖向计算 按列</span></span><br><span class="line">    <span class="built_in">print</span>(data.mean(dim=<span class="number">1</span>))  <span class="comment"># 横向计算 按行</span></span><br><span class="line">    <span class="built_in">print</span>(data.<span class="built_in">sum</span>())</span><br><span class="line">    <span class="built_in">print</span>(data.<span class="built_in">sum</span>(dim=<span class="number">0</span>))</span><br><span class="line">    <span class="built_in">print</span>(data.<span class="built_in">sum</span>(dim=<span class="number">1</span>))</span><br><span class="line">    <span class="built_in">print</span>(data.<span class="built_in">pow</span>(<span class="number">2</span>))</span><br><span class="line">    <span class="built_in">print</span>(data.sqrt())</span><br><span class="line">    <span class="built_in">print</span>(data.exp())</span><br><span class="line">    <span class="built_in">print</span>(data.log())</span><br><span class="line">    <span class="built_in">print</span>(data.log2())</span><br><span class="line">    <span class="built_in">print</span>(data.log10())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tensor_index</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    张量的索引操作： 需要掌握范围索引 行列索引 多维索引</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    torch.manual_seed(<span class="number">0</span>)  <span class="comment"># 固定随机种子</span></span><br><span class="line">    data = torch.randint(<span class="number">0</span>, <span class="number">10</span>, [<span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">    <span class="built_in">print</span>(data[:])  <span class="comment"># 获取所有行和列</span></span><br><span class="line">    <span class="built_in">print</span>(data[:, <span class="number">2</span>])  <span class="comment"># 获取第三列的所有元素</span></span><br><span class="line">    <span class="built_in">print</span>(data[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">2</span>]])  <span class="comment"># 获取第二行第二列 第三行第三列的元素</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 范围索引</span></span><br><span class="line">    <span class="built_in">print</span>(data &gt; <span class="number">3</span>)  <span class="comment"># 返回一个布尔类型的张量</span></span><br><span class="line">    <span class="built_in">print</span>(data[data &gt; <span class="number">3</span>])  <span class="comment"># 筛选data所有元素中 元素大于3的</span></span><br><span class="line">    <span class="built_in">print</span>(data[:, <span class="number">1</span>] &gt; <span class="number">6</span>)  <span class="comment"># tensor([ True,  True, False, False])</span></span><br><span class="line">    <span class="built_in">print</span>(data[data[:, <span class="number">1</span>] &gt; <span class="number">6</span>])  <span class="comment"># 获取data前两行</span></span><br><span class="line">    <span class="built_in">print</span>(data[:, data[<span class="number">1</span>] &gt; <span class="number">3</span>])  <span class="comment"># 获取data中第二行元素大于三的（第一、第二、第四）列</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 多维索引</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">30</span>)</span><br><span class="line">    data = torch.randint(<span class="number">0</span>, <span class="number">10</span>, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">    <span class="built_in">print</span>(data[<span class="number">0</span>, :, :])  <span class="comment"># 按照第0个维度选择4行5列的元素</span></span><br><span class="line">    <span class="built_in">print</span>(data[:, <span class="number">0</span>, :])  <span class="comment"># 按照第一个维度选择第0元素</span></span><br><span class="line">    <span class="built_in">print</span>(data[:, :, <span class="number">0</span>])  <span class="comment"># 按照第二个维度选择第0元素</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># tensor_demo()</span></span><br><span class="line">    <span class="comment"># tensor_extract()</span></span><br><span class="line">    <span class="comment"># concat_tensor()</span></span><br><span class="line">    <span class="comment"># stack_tensor()</span></span><br><span class="line">    <span class="comment"># tensor_shape()</span></span><br><span class="line">    <span class="comment"># tensor_numpy()</span></span><br><span class="line">    tensor_index()</span><br></pre></td></tr></table></figure>



<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><blockquote>
<p>之前在机器学习阶段接触过损失函数，但是用的是Sklearn，但这里使用的是PyTorch，API不同。。。<a href="https://liamjohnson-w.github.io/2025/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0">传送门</a></p>
</blockquote>
<p>参考链接：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_51385258/article/details/143981094">分类器及损失函数</a></li>
</ul>
<h2 id="分类问题损失函数"><a href="#分类问题损失函数" class="headerlink" title="分类问题损失函数"></a>分类问题损失函数</h2><h3 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h3><blockquote>
<p>交叉熵损失函数 </p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250731205303873.png" alt="image-20250731205303873"></p>
<p>交叉熵损失函数计算方式</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250731205821067.png" alt="image-20250731205821067"></p>
<p>损失计算：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">0log(0.10)+0log(0.05)+0log(0.15)+0log(0.10)+0log(0.05)+0log(0.20)</span></span><br><span class="line"><span class="attr">+1log(0.10)</span></span><br><span class="line"><span class="attr">+0log(0.05)+0log(0.10)+0log(0.10)</span> <span class="string"></span></span><br></pre></td></tr></table></figure>

<p>神经元输出对于不同分类的预测结果，经过SoftMax之后，将预测结果转换为预测不同类别的<code>概率值</code>。</p>
<p>经过SoftMax之后，对于不同类别的预测结果的加和为1，也就是对于正确类别的预测概率越高，预测为错误类别的概率越小</p>
<p>所以SoftMax之后，正样本的预测概率越大，损失越小。</p>
<h3 id="二分类问题"><a href="#二分类问题" class="headerlink" title="二分类问题"></a>二分类问题</h3><blockquote>
<p>处理二分类任务时，使用sigmoid激活函数，损失函数使用二分类的交叉熵损失函数</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250731213016837.png" alt="image-20250731213016837"></p>
<p>当真实标签 $y &#x3D; 1$ 时</p>
<ul>
<li>公式简化为：$L &#x3D; -\log \hat{y}$</li>
<li>优化目标：推动预测概率 $\hat{y} \rightarrow 1$（接近真实值）</li>
<li>极端情况：若 $\hat{y} \rightarrow 0$，损失 $L \rightarrow +\infty$（对错误预测施加严重惩罚）</li>
</ul>
<p>当真实标签 $y &#x3D; 0$ 时</p>
<ul>
<li>公式简化为：$L &#x3D; -\log(1 - \hat{y})$</li>
<li>优化目标：推动预测概率 $\hat{y} \rightarrow 0$</li>
<li>极端情况：若 $\hat{y} \rightarrow 1$，损失 $L \rightarrow +\infty$</li>
</ul>
<h2 id="回归问题损失函数"><a href="#回归问题损失函数" class="headerlink" title="回归问题损失函数"></a>回归问题损失函数</h2><h3 id="MAE"><a href="#MAE" class="headerlink" title="MAE"></a>MAE</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250731193008559.png" alt="image-20250731193008559"></p>
<h3 id="MSE"><a href="#MSE" class="headerlink" title="MSE"></a>MSE</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250731193021662.png" alt="image-20250731193021662"></p>
<h3 id="Smooth"><a href="#Smooth" class="headerlink" title="Smooth"></a>Smooth</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250731193031475.png" alt="image-20250731193031475"></p>
<h3 id="参考代码-3"><a href="#参考代码-3" class="headerlink" title="参考代码"></a>参考代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">crossEntropyLoss</span>():</span><br><span class="line">    <span class="comment"># 多分类交叉熵损失函数计算nn.CrossEntropyLoss需要传入预测值(预测值是logits)和真实值(热编码可做可不做，内部会做)</span></span><br><span class="line">    <span class="comment"># nn.CrossEntropyLoss内部集成了SoftMax 传入数据的时候不需要使用SoftMax对神经网络输出值进行处理</span></span><br><span class="line">    y_true = torch.tensor([<span class="number">1</span>, <span class="number">2</span>], dtype=torch.int64)</span><br><span class="line">    y_pred = torch.tensor([[<span class="number">0.2</span>, <span class="number">0.6</span>, <span class="number">0.3</span>], [<span class="number">0.1</span>, <span class="number">0.8</span>, <span class="number">0.1</span>]], dtype=torch.float32)</span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="built_in">print</span>(y_true)</span><br><span class="line">    loss = criterion(y_pred, y_true)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;交叉熵损失：&#x27;</span>, loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bceLoss</span>():</span><br><span class="line">    <span class="comment"># 1 设置真实值和预测值</span></span><br><span class="line">    <span class="comment"># 预测值是sigmoid输出的结果</span></span><br><span class="line">    <span class="comment"># y_pred - 一维列表，表示对多个样本预测为正样本的概率</span></span><br><span class="line">    y_pred = torch.tensor([<span class="number">0.6901</span>, <span class="number">0.5459</span>, <span class="number">0.2469</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># y_true - 一维列表，表示对于多个样本的真实值</span></span><br><span class="line">    y_true = torch.tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], dtype=torch.float32)</span><br><span class="line">    <span class="comment"># 2 实例化二分类交叉熵损失</span></span><br><span class="line">    criterion = nn.BCELoss()</span><br><span class="line">    <span class="comment"># 3 计算损失</span></span><br><span class="line">    my_loss = criterion(y_pred, y_true).detach().numpy()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;loss：&#x27;</span>, my_loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">MAE_Loss</span>():</span><br><span class="line">    y_pred = torch.tensor([<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.9</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">    y_true = torch.tensor([<span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>], dtype=torch.float32)</span><br><span class="line">    criterion = nn.L1Loss()  <span class="comment"># 也叫L1 loss</span></span><br><span class="line">    loss = criterion(y_pred, y_true).detach().numpy()  <span class="comment"># 拷贝一份 转为numpy类型</span></span><br><span class="line">    <span class="built_in">print</span>(loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">MSE_Loss</span>():</span><br><span class="line">    y_pred = torch.tensor([<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.9</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">    y_true = torch.tensor([<span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>], dtype=torch.float32)</span><br><span class="line">    criterion = nn.MSELoss()  <span class="comment"># 也叫L2 loss</span></span><br><span class="line">    loss = criterion(y_pred, y_true).detach().numpy()</span><br><span class="line">    <span class="built_in">print</span>(loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Smooth_Loss</span>():</span><br><span class="line">    y_true = torch.tensor([<span class="number">0</span>, <span class="number">3</span>])</span><br><span class="line">    y_pred = torch.tensor([<span class="number">0.6</span>, <span class="number">0.4</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">    criterion = nn.SmoothL1Loss()</span><br><span class="line">    loss = criterion(y_pred, y_true).detach().numpy()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 分类问题损失函数</span></span><br><span class="line">    crossEntropyLoss()  <span class="comment"># 多分类问题</span></span><br><span class="line">    bceLoss()  <span class="comment"># 二分类问题</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 回归问题损失函数</span></span><br><span class="line">    MAE_Loss()   <span class="comment"># L1 Loss</span></span><br><span class="line">    MSE_Loss()  <span class="comment"># L2 Loss</span></span><br><span class="line">    Smooth_Loss()  <span class="comment"># smooth L1 Loss</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="神经网络基础"><a href="#神经网络基础" class="headerlink" title="神经网络基础"></a>神经网络基础</h1><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><blockquote>
<p><code>神经网络通过调整权值参数学习输入输出关系，逐步从简单线性变换扩展到复杂非线性建模。</code></p>
<p>简单来说：单层神经元经过激活函数能产生0和1，但是我们的计算机不就是基于0&#x2F;1编码吗？于是乎多个单层神经元加上可学习的参数调整(每个神经元连接线的权值，这些权值是通过模型学习获得的)可以做的事情就很多了，比如图像识别、语音识别、文本挖掘等。</p>
<p>参考：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_39910711/article/details/100775918">神经网络</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/illikang/article/details/82019945">神经网络—最易懂最清晰的一篇文章</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qiaoxinyu1989/article/details/136572344">从零开始神经网络</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_39910711/article/details/100775918">神经网络-带Python代码</a></li>
</ul>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250617114834023.png" alt="image-20250617114834023"></p>
<ol>
<li>设计一个神经网络时，输入层与输出层的节点数往往是固定的，中间层则可以自由指定；</li>
<li>神经网络结构图中的拓扑与箭头代表着<strong>预测</strong>过程时数据的流向，跟<strong>训练</strong>时的数据流有一定的区别；</li>
<li>结构图里的关键不是圆圈（代表“神经元”），而是连接线（代表“神经元”之间的连接）。每个连接线对应一个不同的<strong>权重</strong>（其值称为<strong>权值</strong>），这是需要训练得到的。</li>
</ol>
<h2 id="激活函数-面试题-╹▽╹"><a href="#激活函数-面试题-╹▽╹" class="headerlink" title="激活函数-面试题(*╹▽╹*)"></a>激活函数-面试题(*╹▽╹*)</h2><blockquote>
<p><strong>Sigmoid、Tanh、ReLu这三个激活函数有什么缺点或不⾜</strong></p>
<p>答：</p>
<ul>
<li>在𝑧的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个 <strong>if-else</strong> 语句，而 <strong>sigmoid</strong> 函数需要进行浮点四则运算，在实践中，使用 <strong>ReLu</strong> 激活函数神经网络通常会比使用 <strong>sigmoid</strong> 或者 <strong>tanh</strong> 激活函数学习的更快。</li>
<li><strong>sigmoid</strong> 和 <strong>tanh</strong> 函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥散，而 <strong>Relu</strong> 和 <strong>Leaky ReLu</strong> 函数大于 0 部分都为常数，不会产生梯度弥散现象。(同时应该注意到的是，<strong>Relu</strong> 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性，而 <strong>Leaky ReLu</strong> 不会有这问题) 𝑧在 <strong>ReLu</strong> 的梯度一半都是 0，但是，有足够的隐藏层使得 z 值大于 0，所以对大多数的 训练数据来说学习过程仍然可以很快。</li>
</ul>
</blockquote>
<blockquote>
<p><strong>请问人工神经网络中为什么ReLu要好过于tanh和sigmoid ？</strong></p>
<p>答：</p>
<ol>
<li>采用sigmoid等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法和指数运算，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。</li>
<li>对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失），这种现象称为饱和，从而无法完成深层网络的训练。而ReLU就不会有饱和倾向，不会有特别小的梯度出现。</li>
<li>Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生（以及一些人的生物解释balabala）。当然现在也有一些对relu的改进，比如prelu，random relu等，在不同的数据集上会有一些训练速度上或者准确率上的改进，具体的大家可以找相关的paper看。</li>
</ol>
</blockquote>
<blockquote>
<p><strong>激活函数有哪些性质？</strong></p>
<ol>
<li>非线性： 当激活函数是线性的，一个两层的神经网络就可以基本上逼近所有的函数。但如果激活函数是恒等激活函数的时候，即$f(x) &#x3D; x$，就不满足这个性质，而且如果 MLP 使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的；</li>
<li>可微性： 当优化方法是基于梯度的时候，就体现了该性质；</li>
<li>单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数；</li>
<li>$f(x)\approx x$当激活函数满足这个性质的时候，如果参数的初始化是随机的较小值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要详细地去设置初始值；</li>
<li>输出值的范围： 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的 Learning Rate。</li>
</ol>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1.jpg" alt="1"></p>
<h3 id="激活函数的选择方法"><a href="#激活函数的选择方法" class="headerlink" title="激活函数的选择方法"></a>激活函数的选择方法</h3><blockquote>
<ul>
<li>对于隐藏层:<ul>
<li>优先选择ReLU激活函数。如果ReLu效果不好，尝试Leaky ReLu等。<ul>
<li>如果使用ReLU， 需注意Dead ReLU问题，某些神经元的权重更新后，如果输入的是负区间或者0，其输出永远为0，也就是神经元死亡。</li>
</ul>
</li>
<li>少用使用sigmoid激活函数，存在梯度消失问题（5次求导内）</li>
<li>可以尝试使用tanh激活函数</li>
</ul>
</li>
<li>对于输出层:<ul>
<li>二分类问题选择sigmoid激活函数</li>
<li>多分类问题选择softmax激活函数</li>
<li>回归问题选择identity激活函数</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="激活函数总结"><a href="#激活函数总结" class="headerlink" title="激活函数总结"></a>激活函数总结</h3><blockquote>
<p>在单层神经网络时，使用的激活函数是sgn函数。到了两层神经网络时，使用的最多的是sigmoid函数。而到了多层神经网络时，通过一系列的研究发现，ReLU函数在训练多层神经网络时，更容易收敛，并且预测性能更好。<code>因此，目前在深度学习中，最流行的非线性函数是ReLU函数</code>。ReLU函数不是传统的非线性函数，而是分段线性函数。其表达式非常简单，就是$y&#x3D;max(x,0)$。<code>简而言之，在x大于0，输出就是输入，而在x小于0时，输出就保持为0</code>。这种函数的设计启发来自于生物神经元对于激励的线性响应，以及当低于某个阈值后就不再响应的模拟。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250730152117794.png" alt="image-20250730152117794"></p>
<h2 id="神经网络模型"><a href="#神经网络模型" class="headerlink" title="神经网络模型"></a>神经网络模型</h2><h3 id="MP神经元模型"><a href="#MP神经元模型" class="headerlink" title="MP神经元模型"></a>MP神经元模型</h3><blockquote>
<p><code>MP神经元模型</code>接收来自n个其他神经元传递过来的输入信号（x1~xn），这些输入信号通过带权重（θ或ω来表示权重，下图采用θ）的连接（Connection）进行传递，然后神经元（图示阈值为b）收到的总输入（所有输入和权重的乘积的和）与神经元的阈值b比较，并经由激活函数（Activation Function，又称响应函数）处理之后产生神经元的输出。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250617145209391.png" alt="image-20250617145209391"></p>
<p>MP模型的工作原理为：当所有的输入与对应的连接权重的乘积大于阈值$\theta$时，y输出为1，否则输出为0。即当$w_{i} * x_{i} &gt; \theta, y &#x3D; 1$；否则$y &#x3D; 0$。需要注意的是，$x_{i}$也只能是0或1的值，而权重$w_{i}$和$\theta$则根据需要自行设置。</p>
<h3 id="单层神经网络"><a href="#单层神经网络" class="headerlink" title="单层神经网络"></a>单层神经网络</h3><blockquote>
<p>1958年，计算科学家Rosenblatt提出了由两层神经元组成的神经网络。他给它起了一个名字”感知器”（Perceptron）</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250617165806133.png" alt="image-20250617165806133"></p>
<ul>
<li><p>核心组件</p>
<ul>
<li><p>输入层：接收特征向量（图中红色部分）</p>
</li>
<li><p>输出层：单个计算单元（图中绿色部分）$Z_{1}$, $Z_{2}$</p>
</li>
<li><p>连接权重：$W_{i,j}$（图中紫色连线）</p>
</li>
</ul>
</li>
</ul>
<p>如果我们仔细看输出的计算公式，会发现这两个公式就是线性代数方程组。因此可以用矩阵乘法来表达这两个公式。</p>
<p>由上图可以得到神经网络前向传播的矩阵表示如下：<br>$$<br>\begin{aligned}<br>z_1 &amp;&#x3D; g(a_1 w_{1,1} + a_2 w_{1,2} + a_3 w_{1,3})<br>\end{aligned}<br>$$</p>
<p>$$<br>\begin{aligned}<br>z_2 &amp;&#x3D; g(a_1 w_{2,1} + a_2 w_{2,2} + a_3 w_{2,3})<br>\end{aligned}<br>$$</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250618103008404.png" alt="image-20250618103008404"></p>
<p>可表示为如下数学公式：<br>$$<br>y &#x3D; sgn(\sum_{i&#x3D;1}^n w_i x_i + b)<br>$$<br>其中：</p>
<ul>
<li>sgn为符号函数（输出±1）</li>
<li>b为偏置项</li>
</ul>
<p>其中sgn函数为阶跃函数：</p>
<p>阶跃函数：这个函数当输入大于0时，输出1，否则输出0</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250617155456398.png" alt="image-20250617155456398"></p>
<h3 id="两层神经网络"><a href="#两层神经网络" class="headerlink" title="两层神经网络"></a>两层神经网络</h3><blockquote>
<p>两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，中间层和输出层都是计算层。两层神经网络也被叫做多层感知器。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250618102639463.png" alt="image-20250618102639463"></p>
<ul>
<li><strong>输入层</strong>（红色）：3个神经元，输出向量</li>
</ul>
<p>$$<br>a^{(1)} &#x3D; [a_1^{(1)}, a_2^{(1)}, a_3^{(1)}]^T<br>$$</p>
<ul>
<li><strong>隐藏层</strong>（紫色）：2个神经元，激活函数为 $g$</li>
<li><strong>输出层</strong>（绿色）：2个神经元</li>
</ul>
<h3 id="考虑偏置节点的两层神经网络"><a href="#考虑偏置节点的两层神经网络" class="headerlink" title="考虑偏置节点的两层神经网络"></a>考虑偏置节点的两层神经网络</h3><blockquote>
<p>偏置节点很好认，因为其没有输入（前一层中没有箭头指向它）</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250618102723619.png" alt="image-20250618102723619"></p>
<p>数学传播过程：</p>
<ul>
<li>输入层→隐藏层</li>
</ul>
<p>$$<br>a(2) &#x3D; g(W^{(1)} * a^{(1)} + b^{(1)})<br>$$</p>
<ul>
<li>隐藏层→输出层</li>
</ul>
<p>$$<br>z &#x3D; g(W^{(2)} * a^{(2)} + b^{(2)})<br>$$</p>
<p>案例演示：假设网络参数如下（对应图中颜色标注）</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250618142948544.png" alt="image-20250618142948544"></p>
<p>Sigmoid激活函数及其图像</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250618151344048.png" alt="image-20250618151344048"></p>
<h3 id="多层神经网络"><a href="#多层神经网络" class="headerlink" title="多层神经网络"></a>多层神经网络</h3><blockquote>
<p>多层神经网络与两层神经网络计算方法大同小异。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250618105416074.png" alt="image-20250618105416074"></p>
<ul>
<li>输入层→隐藏层1</li>
</ul>
<p>$$<br>a(2) &#x3D; g(W^{(1)} * a^{(1)})<br>$$</p>
<ul>
<li>隐藏层1→隐藏层2</li>
</ul>
<p>$$<br>a(3) &#x3D; g(W^{(2)} * a^{(2)})<br>$$</p>
<ul>
<li>隐藏层2→输出层</li>
</ul>
<p>$$<br>z &#x3D; g(W^{(3)} * a^{(3)})<br>$$</p>
<p>下图右侧的网络中，虽然参数数量仍然是33，但却有4个中间层，是原来层数的接近两倍。这意味着一样的参数数量，可以用更深的层次去表达。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250618112021729.png" alt="image-20250618112021729"></p>
<h3 id="增加更多的层次有什么好处？"><a href="#增加更多的层次有什么好处？" class="headerlink" title="增加更多的层次有什么好处？"></a>增加更多的层次有什么好处？</h3><blockquote>
<p>更深入的表示特征，以及更强的函数模拟能力。</p>
<p>更深入的表示特征可以这样理解，随着网络的层数增加，每一层对于前一层次的抽象表示更深入。在神经网络中，每一层神经元学习到的是前一层神经元值的更抽象的表示。例如第一个隐藏层学习到的是“边缘”的特征，第二个隐藏层学习到的是由“边缘”组成的“形状”的特征，第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征。通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。</p>
</blockquote>
<hr>
<h1 id="BP神经网络"><a href="#BP神经网络" class="headerlink" title="BP神经网络"></a>BP神经网络</h1><blockquote>
<p>什么是BP神经网络（Back Propagation）？</p>
<p>与神经网络普通的正向传播不同，BP神经网络采用<code>反向传播</code></p>
<p>将输出的结果与期望的输出结果进行比较，将比较产生的误差利用网络进行反向传播，本质是一个“负反馈”的过程。<br>通过多次迭代，不断地对网络上的各个节点间的权重进行调整（更新），权重的调整（更新）采用梯度下降法。</p>
<p>参考链接：</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://blog.csdn.net/qq_43328313/article/details/119577026">BP（Back Propagation）神经网络——原理篇</a></li>
</ul>
</blockquote>
<h2 id="反向传播原理"><a href="#反向传播原理" class="headerlink" title="反向传播原理"></a>反向传播原理</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250618150734171.png" alt="image-20250618150734171"></p>
<h2 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250618164535412.png" alt="image-20250618164535412"></p>
<blockquote>
<p>在正向传播的过程中，有一个 与期望的结果比较是否满意的环节，在这个环节中实际的输出结果与期望的输出结果之间就会产生一个误差，为了减小这个误差，这也就转换为了一个优化过程，对于任何优化问题，总是会有一个目标函数 (objective function)，这个目标函数就是 损失函数（Loss function）。</p>
</blockquote>
<p>$$<br>\text{Loss} &#x3D; \frac{1}{2}\sum_{i&#x3D;1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}<br>$$</p>
<p>$$<br>&#x3D; \frac{1}{2}\sum_{i&#x3D;1}^{n}\left[y_{i}-\left(w x_{i}+b\right)\right]^{2}<br>$$</p>
<table>
<thead>
<tr>
<th>符号</th>
<th>含义</th>
<th>维度</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>$n$</td>
<td>样本数量</td>
<td>标量</td>
<td>训练数据的总数</td>
</tr>
<tr>
<td>$y_i$</td>
<td>真实值</td>
<td>$\mathbb{R}$</td>
<td>第$i$个样本的标签</td>
</tr>
<tr>
<td>$\hat{y}_i$</td>
<td>预测值</td>
<td>$\mathbb{R}$</td>
<td>模型输出：$wx_i + b$</td>
</tr>
<tr>
<td>$w$</td>
<td>权重</td>
<td>$\mathbb{R}$</td>
<td>可训练参数</td>
</tr>
<tr>
<td>$b$</td>
<td>偏置</td>
<td>$\mathbb{R}$</td>
<td>可训练参数</td>
</tr>
<tr>
<td>$x_i$</td>
<td>特征值</td>
<td>$\mathbb{R}$</td>
<td>第$i$个样本的输入</td>
</tr>
</tbody></table>
<p><strong>为了让实际的输出结果与期望的输出结果之间的误差最小，就要寻找这个函数的最小值。</strong></p>
<p>还好，我们学过数学，知道这个函数是个开口朝上的抛物线。</p>
<p>但是机器没有学过数学啊！机器是不知道这个函数的最小值是如何计算的，只能通过<a href="https://liamjohnson-w.github.io/2025/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E2%AD%90%EF%B8%8F">梯度学习算法</a>来求最值：</p>
<h3 id="梯度学习算法"><a href="#梯度学习算法" class="headerlink" title="梯度学习算法"></a>梯度学习算法</h3><blockquote>
<p>梯度下降学习法，有些像高山滑雪运动员总是在寻找坡度最大的地段向下滑行。当他处于A点位置时，沿最大坡度路线下降，达到<code>局部极小点</code>，则停止滑行；如果他是从B点开始向下滑行，则他最终将达到<code>全局最小点</code>。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250618155449176.png" alt="image-20250618155449176"></p>
<h3 id="梯度下降公式"><a href="#梯度下降公式" class="headerlink" title="梯度下降公式"></a>梯度下降公式</h3><blockquote>
<p>很多公式就是一个变量的不同，但是形式都是一样的哈。</p>
</blockquote>
<p>$$<br>\theta_{i+1} &#x3D; \theta_{i} - \alpha \frac{\partial}{\partial \theta_{i}} J(\theta)<br>$$</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250618160429862.png" alt="image-20250618160429862"></p>
<p>迭代的方法一般都要经过多次，因为函数最小值的寻找可能要经过多次迭代，而在每一次的迭代中，各层节点之间的权重也将不断地迭代更新。</p>
<p>数学表达式除了字符不同，表达意思相同。<br>$$<br>W_{(t+1)} &#x3D; W_{(t)} - \eta \frac{\partial \text{Loss}}{\partial W} + \alpha \left[ W_{(t)} - W_{(t-1)} \right]<br>$$</p>
<p>参数说明</p>
<table>
<thead>
<tr>
<th>项</th>
<th>符号</th>
<th>名称</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>当前权重</td>
<td>$W_{(t)}$</td>
<td>参数向量</td>
<td>模型当前时刻的参数值</td>
</tr>
<tr>
<td>梯度项</td>
<td>$\eta \frac{\partial \text{Loss}}{\partial W}$</td>
<td>学习项&#x2F;调整量</td>
<td>沿损失函数梯度方向更新</td>
</tr>
<tr>
<td>动量项</td>
<td>$\alpha \left[ W_{(t)} - W_{(t-1)} \right]$</td>
<td>惯性项&#x2F;平滑项</td>
<td>保持参数更新方向的连续性</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>参数</th>
<th>典型取值</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>$\eta$</td>
<td>0.001-0.1</td>
<td>学习率，控制梯度更新步长</td>
</tr>
<tr>
<td>$\alpha$</td>
<td>0.8-0.99</td>
<td>动量系数，决定历史更新的影响程度</td>
</tr>
</tbody></table>
<p>这样每一次迭代就会产生一次权重更新，之后将更新的权重与训练样本进行正向传播，如果得到的结果不满意，则进行反向传播，继续迭代。如此往复，直到得到满意的结果为止。</p>
<p><code>加上动量项有两点好处</code>：</p>
<ul>
<li>当连续多次更新方向相同时，加速收敛</li>
<li>当梯度方向变化剧烈时，减小震荡</li>
</ul>
<h3 id="动量优化方法"><a href="#动量优化方法" class="headerlink" title="动量优化方法"></a>动量优化方法</h3><blockquote>
<p>其实下面的公式就是在于$\alpha$动量系数以及$\eta$学习率的参数选择，只需要知道两点：</p>
<ul>
<li>$α$越大，历史梯度影响越显著</li>
<li>$\eta$学习率可以自行调整</li>
</ul>
</blockquote>
<p>$$<br>W_{(t+1)} &#x3D; W_{(t)} - \eta\left[(1-\alpha) \frac{\partial \text{Loss}}{\partial W_{(t)}} + \alpha \frac{\partial \text{Loss}}{\partial W_{(t-1)}}\right]<br>$$</p>
<p>参数说明</p>
<table>
<thead>
<tr>
<th>符号</th>
<th>名称</th>
<th>取值范围</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>$\eta$</td>
<td>学习率</td>
<td>$\eta &gt; 0$</td>
<td>控制参数更新步长</td>
</tr>
<tr>
<td>$\alpha$</td>
<td>动量因子</td>
<td>$0 \leq \alpha &lt; 1$</td>
<td>调节历史梯度影响程度</td>
</tr>
</tbody></table>
<p>算法特性</p>
<ol>
<li><p><strong>梯度平滑</strong>：</p>
<ul>
<li><p>当$α&#x3D;0$时退化为普通SGD</p>
</li>
<li><p>$$<br>W_{(t+1)} &#x3D; W_{(t)} - \eta \frac{\partial \text{Loss}}{\partial W}<br>$$</p>
</li>
<li><p>$α$越大，历史梯度影响越显著</p>
</li>
</ul>
</li>
<li><p><strong>实际效果</strong>：</p>
<ul>
<li>在平坦区域加速收敛</li>
<li>在梯度震荡方向减弱波动</li>
<li>帮助跨越局部极小值</li>
</ul>
</li>
</ol>
<h3 id="代码实现神经网络"><a href="#代码实现神经网络" class="headerlink" title="代码实现神经网络"></a>代码实现神经网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> exp, array, random, dot </span><br><span class="line"></span><br><span class="line"><span class="comment">#从numpy库中调用exp(指数函数）、array(数组〉、random（随机函数)、dot(矩阵相乘函数)。</span></span><br><span class="line">training_set_inputs = array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="comment">#bp神经网络训练部分的输入。</span></span><br><span class="line">training_set_outputs = array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]]).T</span><br><span class="line"><span class="comment">#bp神经网络训练部分的输出，.T表示矩阵转置。</span></span><br><span class="line">random.seed(<span class="number">1</span>)</span><br><span class="line"><span class="comment">#使用随机函数生成随机数，使用seed函数能够确保每次生成的随机数一致。</span></span><br><span class="line">synaptic_weights = <span class="number">2</span> * random.random((<span class="number">3</span>, <span class="number">1</span>)) - <span class="number">1</span></span><br><span class="line"><span class="comment">#生成一个随机数组，数组格式为3行1列，用来存储初始权重。</span></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    output = <span class="number">1</span> / (<span class="number">1</span> + exp(-(dot(training_set_inputs, synaptic_weights))))</span><br><span class="line">    <span class="comment">#使用for语句循环10000次，将训练集的输入和权重采用dot进行矩阵相乘，将相乘得到的结果输入到sigmoid函数,然后将得到的结果赋值给output。</span></span><br><span class="line">    synaptic_weights += dot(training_set_inputs.T, (training_set_outputs - output) * output * (<span class="number">1</span> - output))</span><br><span class="line">    <span class="comment">#权重的i调整采用“误差加权导数&quot;&quot;公式。</span></span><br><span class="line"><span class="built_in">print</span> (<span class="number">1</span> / (<span class="number">1</span> + exp(-(dot(array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]), synaptic_weights)))))</span><br><span class="line"><span class="comment">#synaptic_weights是调整之后的最终权重，数组（矩阵〉[1,0，0]与这个权重矩阵通过dot函数进行相乘，将相乘的结果作为输入引入到sigmoid函数，得到最终的结果。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h1 id="经典神经网络CNN"><a href="#经典神经网络CNN" class="headerlink" title="经典神经网络CNN"></a>经典神经网络CNN</h1><blockquote>
<p>卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），是深度学习（Deep Learning）的代表算法之一。</p>
<p>卷积层的作用就是用来自动学习、提取图像的特征.</p>
<p>CNN网络主要由三部分构成：卷积层、池化层和全连接层构成</p>
<ul>
<li>卷积层负责提取图像中的局部特征；</li>
<li>池化层用来大幅降低参数量级(降维)；</li>
<li>全连接层用来输出想要的结果。</li>
</ul>
<p>参考链接：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/tree/fda5748094d086544a034b8e9df97a5953c0f608/Deep%20Learning/11.%20CNN">Github-CNN</a>⭐️</li>
<li><a target="_blank" rel="noopener" href="https://axyzdong.blog.csdn.net/article/details/125441566">Convolutional Neural Network</a>⭐️⭐️</li>
<li><a target="_blank" rel="noopener" href="http://www.uml.org.cn/ai/202503074.asp">一文读懂|CNN卷积神经网络</a>⭐️⭐️⭐️⭐️</li>
<li><a target="_blank" rel="noopener" href="https://c.biancheng.net/view/r0a12jv.html">卷积神经网络详解（Python实现）</a>⭐️⭐️</li>
</ul>
</blockquote>
<h2 id="为什么CNN一般用于图像处理？"><a href="#为什么CNN一般用于图像处理？" class="headerlink" title="为什么CNN一般用于图像处理？"></a>为什么CNN一般用于图像处理？</h2><blockquote>
<p>简单理解就是，对于图像处理，以一张照片举例(100  * 100像素，每个像素有RGB3个值)：</p>
<ul>
<li>全连接前馈神经网络（Fully Connected Feedforward Network）：需要100*100*3个权重参数，会导致两个问题：<ul>
<li>计算量巨大，训练效率低。</li>
<li>容易过拟合（模型复杂但数据有限）。</li>
<li>必定导致参数爆炸</li>
</ul>
</li>
<li>卷积神经网络（Convolutional Neural Networks）：使用卷积核（filter）滑动扫描整张图像，同一卷积核在不同位置共享权重。例如，一个5×5卷积核仅需25个参数（+1偏置），而非全连接的30,000×1000。CNN解决方案如下：<ul>
<li>局部感知（Local Connectivity）<ul>
<li>图像中相邻像素关联性强（如边缘、纹理），远距离像素相关性低。</li>
<li>每个神经元仅连接输入图像的局部区域（如5×5窗口），而非全部像素。这大幅减少连接数。</li>
</ul>
</li>
<li>参数共享（Shared Weights）<ul>
<li>图像的某些特征（如边缘检测）在不同位置是通用的。</li>
<li>使用卷积核（filter）滑动扫描整张图像，同一卷积核在不同位置共享权重。例如，一个5×5卷积核仅需25个参数（+1偏置），而非全连接的30,000×1000。</li>
</ul>
</li>
</ul>
</li>
</ul>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250621164641910.png" alt="image-20250621164641910"></p>
<h3 id="CNN的优势"><a href="#CNN的优势" class="headerlink" title="CNN的优势"></a>CNN的优势</h3><ul>
<li><strong>参数效率高</strong>：通过局部连接和共享权重，CNN用极少6（如几个卷积核）即可捕捉图像的空间层次特征（边缘→纹理→物体部分→整体）。</li>
<li><strong>保留空间信息</strong>：卷积操作保持图像的2D结构，而全连接网络会破坏空间关系。</li>
<li><strong>更适合图像</strong>：相比DNN（深度神经网络），CNN的架构更简单（参数更少），但针对图像任务的性能更好。</li>
</ul>
<h2 id="CNN层级结构图"><a href="#CNN层级结构图" class="headerlink" title="CNN层级结构图"></a>CNN层级结构图</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250619171147253.png" alt="image-20250619171147253"></p>
<p>上图中CNN要做的事情是：给定一张图片，是车还是马未知，是什么车也未知，现在需要模型判断这张图片里具体是一个什么东西，总之输出一个结果：如果是车 那是什么车。</p>
<blockquote>
<ul>
<li>最左边是数据输入层(Input Layer)，对数据做一些处理，比如：<ul>
<li>去均值（把输入数据各个维度都中心化为0，避免数据过多偏差，影响训练效果）- <strong>CNN使用</strong></li>
<li>归一化（把所有的数据都归一到同样的范围）</li>
<li>PCA&#x2F;白化</li>
</ul>
</li>
<li>CONV：卷积计算层(Conv Layer)，线性乘积求和。</li>
<li>RELU：激励层(Activation Layer)，下文有提到：ReLU是激活函数的一种。</li>
<li>POOL：池化层(Pooling Layer)，简言之，即取区域平均或最大。</li>
<li>FC：全连接层(FC Layer)。</li>
</ul>
</blockquote>
<h2 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h2><blockquote>
<p>在做输入的时候，需要把图片处理成同样大小的图片才能够进行处理。常见的处理数据的方式有：</p>
<p>该层要做的处理主要是对原始图像数据进行预处理，其中包括：</p>
<ul>
<li>去均值：把输入数据各个维度都中心化为 0，其目的就是把样本的中心拉回到坐标系原点上；</li>
<li>归一化：幅度归一化到同样的范围，即减少各维度数据取值范围的差异而带来的干扰。例如，我们有两个维度的特征 A 和 B, A 范围是 0<del>10，而 B 范围是 0</del>10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即 A 和 B 的数据都变为 0~1 的范围；</li>
<li>PCA（去相关）&#x2F;白化：用 PCA 降维；白化是对数据各个特征轴上的幅度归一化。</li>
</ul>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250623093209196.png" alt="image-20250623093209196"></p>
<h2 id="⭐️⭐️卷积计算层-conv-⭐️⭐️"><a href="#⭐️⭐️卷积计算层-conv-⭐️⭐️" class="headerlink" title="⭐️⭐️卷积计算层(conv)⭐️⭐️"></a>⭐️⭐️卷积计算层(conv)⭐️⭐️</h2><blockquote>
<p><code>CNN的主干是卷积层</code>，它将过滤器（或内核）应用于输入数据以提取边缘，纹理和模式等特征。这些层负责检测输入中的局部模式并构建数据的分层表示。每个卷积层产生一个或多个特征图，突出显示输入的特定特征。</p>
<p>简而言之，卷积操作就是用一个可移动的小窗口来提取图像中的特征，这个小窗口包含了一组特定的权重，通过与图像的不同位置进行卷积操作，网络能够学习并捕捉到不同特征的信息。</p>
</blockquote>
<h3 id="卷积计算"><a href="#卷积计算" class="headerlink" title="卷积计算"></a>卷积计算</h3><p>下图为一个点积计算过程：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250622150539459.png" alt="image-20250622150539459"></p>
<p>如下图为一个完整的卷积计算方式：<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250622205919361.png" alt="image-20250622205919361"></p>
<h3 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h3><blockquote>
<p>通过上面的卷积计算过程，最终的特征图比原始图像小很多，如果想要保持经过卷积后的图像大小不变, 可以在原图周围添加 padding 来实现.</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250622204726623.png" alt="image-20250622204726623"></p>
<p>填充涉及在输入矩阵的边界周围添加额外的像素（通常为零）。填充可确保输出特征图保持与输入相同的空间维度，或防止边缘处的信息丢失。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250622204111134.png" alt="image-20250622204111134"></p>
<ul>
<li>有两种常见的填充类型：<ul>
<li>有效填充：不应用填充，导致输出特征图较小。</li>
<li>相同的填充：添加填充以使输出特征图具有与输入相同的空间维度。</li>
</ul>
</li>
</ul>
<p>例如，如果您将3×3滤波器应用于具有“相同”填充的5×5输入，则输出仍为5×5。如果没有填充，输出大小将由于过滤器与边缘的重叠而缩小。</p>
<h3 id="Stride"><a href="#Stride" class="headerlink" title="Stride"></a>Stride</h3><blockquote>
<p>步幅决定了在卷积运算期间滤波器在输入矩阵上移动的程度。步幅为1意味着过滤器一次移动一个像素，而较大的步幅跳过像素，减少输出特征图的空间维度。</p>
<p>例如，步长为2时，过滤器会跳过每隔一个像素，从而有效地将输出特征图的空间维度减半。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250622205146454.png" alt="image-20250622205146454"></p>
<p>在应用具有步幅 $S$ 的卷积之后，用于计算输出大小的公式为：</p>
<p>$$<br>\text{Output Size}&#x3D;\left(\frac{H-K+2P}{S}+1\right)\times\left(\frac{W-K+2P}{S}+1\right)<br>$$</p>
<p>变量说明</p>
<table>
<thead>
<tr>
<th>符号</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>$H$</td>
<td>输入特征图的高度</td>
</tr>
<tr>
<td>$W$</td>
<td>输入特征图的宽度</td>
</tr>
<tr>
<td>$K$</td>
<td>卷积核大小（假设为方形）</td>
</tr>
<tr>
<td>$P$</td>
<td>填充（Padding）大小</td>
</tr>
<tr>
<td>$S$</td>
<td>步幅（Stride）</td>
</tr>
</tbody></table>
<p>注意事项</p>
<ol>
<li>公式假设卷积核为正方形（$K \times K$）</li>
<li>$\frac{H-K+2P}{S}$ 和 $\frac{W-K+2P}{S}$ 需为整数，否则需向下取整</li>
<li>当 $S&#x3D;1$ 且 $P&#x3D;\lfloor K&#x2F;2 \rfloor$ 时，输出尺寸与输入相同（即”same” padding）</li>
</ol>
<h2 id="池化层⭐️"><a href="#池化层⭐️" class="headerlink" title="池化层⭐️"></a>池化层⭐️</h2><blockquote>
<p>池化层应用在卷积层之后，用于降低特征图的维度，有助于保留输入图像的重要信息或特征，并减少计算时间。</p>
<p>使用池化，可以创建一个较低分辨率的输入版本，该版本仍然包含输入图像的大元素或重要元素。</p>
<p>最常见的池化类型是最大池化和平均池化。下图显示了最大池化的工作原理。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250623100727711.png" alt="image-20250623100727711"></p>
<p>使用从上面例子中得到的特征图来应用池化。这里使用了一个大小为 2×2 的池化层，步长为 2。</p>
<p>取每个突出显示区域的最大值，并获得大小为 2×2 的新版本输入图像，因此在应用池化后，特征图的维数减少了。</p>
<p>如下图为最大池化和平均池化示意图：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250623105701438.png" alt="image-20250623105701438"></p>
<h3 id="最大池化（Max-Pooling）"><a href="#最大池化（Max-Pooling）" class="headerlink" title="最大池化（Max Pooling）"></a>最大池化（Max Pooling）</h3><ul>
<li><p>从特征图的每个局部区域提取<strong>最大值</strong></p>
</li>
<li><p>计算公式（2×2窗口示例）：<br>$$<br> P[i, j]&#x3D;\max (I[2i:2i+2, 2j:2j+2])<br>$$</p>
</li>
</ul>
<p>特性与优势</p>
<ul>
<li>保留最突出特征（如边缘&#x2F;纹理）</li>
<li>增强特征图的<strong>平移不变性</strong></li>
<li>适用于图像分类等需要显著特征的任务</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250623105812234.png" alt="image-20250623105812234"></p>
<h3 id="平均池化（Average-Pooling）"><a href="#平均池化（Average-Pooling）" class="headerlink" title="平均池化（Average Pooling）"></a>平均池化（Average Pooling）</h3><ul>
<li><p>核心原理</p>
<ul>
<li><p>计算局部区域的<strong>平均值</strong></p>
</li>
<li><p>通用公式：<br>$$<br> P[i, j]&#x3D;\frac{1}{K^{2}} \sum_{m&#x3D;0}^{K-1} \sum_{n&#x3D;0}^{K-1} I[Si+m, Sj+n]<br>$$<br>（$K$&#x3D;窗口大小，$S$&#x3D;步幅）</p>
</li>
</ul>
</li>
<li><p>特性与优势</p>
<ul>
<li><p>平滑特征图噪声</p>
</li>
<li><p>提供<strong>全局性特征表示</strong>  </p>
</li>
<li><p>适用于需要捕捉细微模式的场景</p>
</li>
</ul>
</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250623110111491.png" alt="image-20250623110111491"></p>
<table>
<thead>
<tr>
<th>特性</th>
<th>最大池化</th>
<th>平均池化</th>
</tr>
</thead>
<tbody><tr>
<td><strong>输出值</strong></td>
<td>局部区域最大值</td>
<td>局部区域平均值</td>
</tr>
<tr>
<td><strong>效果</strong></td>
<td>突出显著特征</td>
<td>平滑整体特征</td>
</tr>
<tr>
<td><strong>适用场景</strong></td>
<td>图像分类、边缘检测</td>
<td>全局特征分析、噪声抑制</td>
</tr>
<tr>
<td><strong>计算复杂度</strong></td>
<td>只需比较</td>
<td>需算术平均</td>
</tr>
<tr>
<td><strong>抗噪声能力</strong></td>
<td>弱（对极值敏感）</td>
<td>强（平滑噪声）</td>
</tr>
</tbody></table>
<blockquote>
<p>📌 <strong>设计建议</strong>：CNN中通常混合使用两种池化，浅层多用最大池化提取纹理特征，深层可用平均池化获取全局语义。</p>
</blockquote>
<h2 id="全连接层⭐️"><a href="#全连接层⭐️" class="headerlink" title="全连接层⭐️"></a>全连接层⭐️</h2><blockquote>
<p>全连接层用于将输入图像分类为标签。该层将从前面的步骤（即卷积层和池化层）中提取的信息连接到输出层，并最终将输入分类为所需的标签，核心作用为：特征整合 → 分类&#x2F;回归决策，相当于神经网络的”决策大脑”。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250623100807357.png" alt="image-20250623100807357"></p>
<h2 id="Python实现卷积神经网络"><a href="#Python实现卷积神经网络" class="headerlink" title="Python实现卷积神经网络"></a>Python实现卷积神经网络</h2><h1 id="循环神经网络RNN及变体"><a href="#循环神经网络RNN及变体" class="headerlink" title="循环神经网络RNN及变体"></a>循环神经网络RNN及变体</h1><blockquote>
<p>循环神经网络（Recurrent Neural Network, RNN）是一类具有内部环状连接的人工神经网络，用于处理序列数据。其最大特点是网络中存在着环，使得信息能在网络中进行循环，<code>实现对序列信息的存储和处理</code>。</p>
<p>循环神经网络（RNN）及其高级变体，包括长短时记忆网络（LSTM）、门控循环单元（GRU）和双向循环神经网络（Bi-RNN）</p>
<p>参考链接：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000044154505">循环神经网络RNN完全解析：从基础理论到PyTorch实战</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/30844905">一文搞懂RNN（循环神经网络）基础篇</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/u010518385/article/details/104418614">循环神经网络RNN以及几种经典模型</a></li>
</ul>
</blockquote>
<h2 id="RNN诞生背景（我吃柠檬）"><a href="#RNN诞生背景（我吃柠檬）" class="headerlink" title="RNN诞生背景（我吃柠檬）"></a>RNN诞生背景（我吃柠檬）</h2><blockquote>
<p>以nlp的一个最简单词性标注任务来说，将我 吃 柠檬 三个单词标注词性为 我&#x2F;nn 吃&#x2F;v 柠檬&#x2F;nn。</p>
<p>那么这个任务的输入就是：</p>
<p>我 吃 柠檬 （已经分词好的句子）</p>
<p>这个任务的输出是：</p>
<p><em>我&#x2F;nn 吃&#x2F;v 柠檬&#x2F;nn(词性标注好的句子)</em></p>
<p>对于这个任务来说，我们当然可以直接用普通的神经网络来做，给网络的训练数据格式了就是我-&gt; 我&#x2F;nn 这样的多个单独的单词-&gt;词性标注好的单词。</p>
<p>但是很明显，一个句子中，前一个单词其实对于当前单词的词性预测是有很大影响的，比如预测柠檬的时候，由于前面的吃是一个动词，那么很显然柠檬作为名词的概率就会远大于动词的概率，因为动词后面接名词很常见，而动词后面接动词很少见。</p>
<p>所以为了解决一些这样类似的问题，能够更好的处理序列的信息，RNN就诞生了。</p>
</blockquote>
<h2 id="RNN网络结构"><a href="#RNN网络结构" class="headerlink" title="RNN网络结构"></a>RNN网络结构</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250623142828919.png" alt="image-20250623142828919"></p>
<p>RNN基本结构如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一个简单的RNN结构示例</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleRNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleRNN, self).__init__()</span><br><span class="line">        self.rnn = nn.RNN(input_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out, _ = self.rnn(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<h2 id="RNN原理"><a href="#RNN原理" class="headerlink" title="RNN原理"></a>RNN原理</h2><p>学术界公式：<br>$$<br>h_t&#x3D;\tanh\left(W_t\left[X_t, h_{t-1}\right]+b_t\right)<br>$$<br><code>用矩阵参数对数据进行加权求和，再通过激活层添加非线性因素</code></p>
<p>PyTorch实现公式：<br>$$<br> h_t&#x3D;\tanh\left( W_{ih}* X_t,+b_{ih}+ W_{hh}* h_{t-1},+b_{hh}\right)<br>$$<br>用隐藏层数据*内部隐藏层参数矩阵$W_{hh}$ + 用时间步数据*内部参数矩阵$W_{ih}$，加一起，再用tanh激活</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250623161333899.png" alt="image-20250623161333899"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250623155829657.png" alt="image-20250623155829657"></p>
<p>现在看上去就比较清楚了，这个网络在t时刻接收到输入 $x_{t}$ 之后，隐藏层的值是 $s_{t}$ ，输出值是 $o_{t}$ 。关键一点是， $s_{t}$ 的值不仅仅取决于 $x_{t}$ ，还取决于 $s_{t-1}$ 。我们可以用下面的公式来表示<strong>循环神经网络</strong>的计算方法：</p>
<h3 id="输出方程"><a href="#输出方程" class="headerlink" title="输出方程"></a>输出方程</h3><p>$$<br>O_{t} &#x3D; g(V \cdot S_{t})<br>$$</p>
<h3 id="状态方程"><a href="#状态方程" class="headerlink" title="状态方程"></a>状态方程</h3><p>$$<br>S_{t} &#x3D; f(U \cdot X_{t} + W \cdot S_{t-1})<br>$$</p>
<ul>
<li><strong>$O_t$</strong>：时刻$t$的系统输出</li>
<li><strong>$S_t$</strong>：时刻$t$的隐藏状态</li>
<li><strong>$X_t$</strong>：时刻$t$的输入向量</li>
<li><strong>$U&#x2F;V&#x2F;W$</strong>：权重矩阵（可训练参数）</li>
<li><strong>$f&#x2F;g$</strong>：激活函数（如tanh&#x2F;sigmoid）</li>
</ul>
<blockquote>
<p>状态$S_t$具有<strong>时间依赖性</strong>：  当前状态不仅取决于当前输入$X_t$，还通过权重矩阵$W$与前一状态$S_{t-1}$建立记忆关联</p>
</blockquote>
<table>
<thead>
<tr>
<th>符号</th>
<th>维度</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>$U$</td>
<td>$d_h \times d_x$</td>
<td>输入到隐藏层的转换</td>
</tr>
<tr>
<td>$W$</td>
<td>$d_h \times d_h$</td>
<td>状态间的记忆传递</td>
</tr>
<tr>
<td>$V$</td>
<td>$d_o \times d_h$</td>
<td>隐藏层到输出的转换</td>
</tr>
</tbody></table>
<h3 id="梯度问题：梯度消失和爆炸"><a href="#梯度问题：梯度消失和爆炸" class="headerlink" title="梯度问题：梯度消失和爆炸"></a>梯度问题：梯度消失和爆炸</h3><p>由于RNN的循环结构，在训练中可能会出现梯度消失或梯度爆炸的问题。长序列可能会导致训练过程中的梯度变得非常小（消失）或非常大（爆炸），从而影响模型的学习效率。</p>
<h3 id="PyTorch搭建RNN"><a href="#PyTorch搭建RNN" class="headerlink" title="PyTorch搭建RNN"></a>PyTorch搭建RNN</h3><blockquote>
<p>pytorch 中使用 nn.RNN 类来搭建基于序列的循环神经网络，它的构造函数有以下几个参数：</p>
<ul>
<li>input_size：输入数据X的特征值的数目。</li>
<li>hidden_size：隐藏层的神经元数量，也就是隐藏层的特征数量。</li>
<li>num_layers：循环神经网络的层数，默认值是 1。</li>
<li>bias：默认为 True，如果为 false 则表示神经元不使用 bias 偏移参数。</li>
<li>batch_first：如果设置为 True，则输入数据的维度中第一个维度就是 batch 值，默认为 False。默认情况下第一个维度是序列的长度， 第二个维度才是batch，第三个维度是特征数目。</li>
<li>dropout：如果不为空，则表示最后跟一个 dropout 层抛弃部分数据，抛弃数据的比例由该参数指定</li>
</ul>
<p>RNN 中最主要的参数是 input_size 和 hidden_size，这两个参数务必要搞清楚。其余的参数通常不用设置，采用默认值就可以了</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># RNN的PyTorch实现</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleRNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleRNN, self).__init__()</span><br><span class="line">        self.rnn = nn.RNN(input_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = nn.Linear(hidden_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, h_0</span>):</span><br><span class="line">        out, h_n = self.rnn(x, h_0) <span class="comment"># 运用RNN层</span></span><br><span class="line">        out = self.fc(out) <span class="comment"># 运用全连接层</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">rnn = torch.nn.RNN(<span class="number">20</span>,<span class="number">50</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">100</span> , <span class="number">32</span> , <span class="number">20</span>)</span><br><span class="line">h_0 =torch.randn(<span class="number">2</span> , <span class="number">32</span> , <span class="number">50</span>)</span><br><span class="line">output,hn=rnn(<span class="built_in">input</span> ,h_0) </span><br><span class="line"><span class="built_in">print</span>(output.size(),hn.size())</span><br></pre></td></tr></table></figure>

<p>代码带完整注释：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># RNN的PyTorch实现</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个简单的RNN网络架构，包含一个RNN层和一个全连接层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleRNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleRNN, self).__init__()</span><br><span class="line">        <span class="comment"># 创建一个RNN层</span></span><br><span class="line">        <span class="comment"># input_size: 输入特征维度、hidden_size: 隐藏层维度</span></span><br><span class="line">        <span class="comment"># batch_first=True: 输入输出张量的批次维度在前</span></span><br><span class="line">        self.rnn = nn.RNN(input_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 创建一个全连接层（全连接层用于将RNN的隐藏状态转换为最终输出）</span></span><br><span class="line">        self.fc = nn.Linear(hidden_size, output_size)</span><br><span class="line">		</span><br><span class="line">    <span class="comment"># 向前传播方法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, h_0</span>):</span><br><span class="line">        out, h_n = self.rnn(x, h_0) <span class="comment"># 运用RNN层 x: 输入序列数据，h_0: 初始隐藏状态</span></span><br><span class="line">        out = self.fc(out) <span class="comment"># 运用全连接层 h_n: 最后一个时间步的隐藏状态，out: 所有时间步的输出</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建RNN模型实例对象，其中：20个输入特征维度、50个输出特征维度、隐藏层个数为2</span></span><br><span class="line">rnn = torch.nn.RNN(<span class="number">20</span>,<span class="number">50</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 生成随机输入，其中分别代表：序列长度、批次大小、输入特征维度</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">100</span> , <span class="number">32</span> , <span class="number">20</span>)</span><br><span class="line"><span class="comment"># 初始化隐藏状态，其中分别代表：隐藏层个数，批次大小，隐藏层维度</span></span><br><span class="line">h_0 =torch.randn(<span class="number">2</span> , <span class="number">32</span> , <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># rnn(input, h_0)执行前向计算、output: 所有时间步的输出、hn: 最后一个时间步的隐藏状态</span></span><br><span class="line">output,hn=rnn(<span class="built_in">input</span> ,h_0) </span><br><span class="line"></span><br><span class="line"><span class="comment"># output: 包含所有时间步的输出，形状为(seq_len, batch, hidden_size * num_directions)</span></span><br><span class="line"><span class="comment"># hn: 最后一个时间步的隐藏状态，形状为(num_layers * num_directions, batch, hidden_size)</span></span><br><span class="line"><span class="built_in">print</span>(output.size(),hn.size())</span><br></pre></td></tr></table></figure>



<p>预期输出：</p>
<ol>
<li><code>output</code>的形状为<code>(100, 32, 50)</code>：<ul>
<li><code>100</code>: 序列长度（时间步数）</li>
<li><code>32</code>: 批次大小</li>
<li><code>50</code>: 隐藏层维度（输出特征维度）</li>
</ul>
</li>
<li><code>hn</code>的形状为<code>(2, 32, 50)</code>：<ul>
<li><code>2</code>: 隐藏层数</li>
<li><code>32</code>: 批次大小</li>
<li><code>50</code>: 隐藏层维度</li>
</ul>
</li>
</ol>
<h3 id="⭐️⭐️RNN-API⭐️⭐️"><a href="#⭐️⭐️RNN-API⭐️⭐️" class="headerlink" title="⭐️⭐️RNN API⭐️⭐️"></a>⭐️⭐️RNN API⭐️⭐️</h3><p>⭐️RNN模型API关键参数⭐️</p>
<p>RNN模型在PyTorch中的API主要包含9个关键参数，可分为三类：</p>
<p><strong>模型构建参数</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rnn = nn.RNN(<span class="number">5</span>, <span class="number">6</span>, <span class="number">1</span>)  <span class="comment"># 输入特征维度5，输出特征维度6，隐藏层个数1</span></span><br></pre></td></tr></table></figure>

<ul>
<li>第1个参数：输入数据特征维度</li>
<li>第2个参数：输出数据特征维度(可视为神经元数量)</li>
<li>第3个参数：隐藏层个数(隐藏层个数×方向数，单向或双向)</li>
</ul>
<p><strong>输入数据参数</strong>： </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>)  <span class="comment"># 序列长度1，批次数3，输入特征维度5</span></span><br></pre></td></tr></table></figure>

<ul>
<li>第1个参数：序列长度(单词个数)</li>
<li>第2个参数：批次数</li>
<li>第3个参数：数据特征维度</li>
</ul>
<p><strong>隐藏层参数</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">h0 = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">6</span>)  <span class="comment"># 隐藏层个数1，批次数3，输出特征维度6</span></span><br></pre></td></tr></table></figure>

<ul>
<li>第1个参数：模型隐藏层个数</li>
<li>第2个参数：数据批次数</li>
<li>第3个参数：模型输出神经元个数</li>
</ul>
<p>⭐️API参数之间的关系⭐️</p>
<ol>
<li><strong>输入输出维度关系</strong>：</li>
</ol>
<ul>
<li>输入特征维度必须与模型构建的第一个参数一致</li>
<li>输出特征维度由模型构建的第二个参数决定</li>
<li>隐藏层输出维度与模型输出维度相同</li>
</ul>
<ol start="2">
<li><strong>批次处理关系</strong>：</li>
</ol>
<ul>
<li>输入数据的批次数必须与隐藏层输入的批次数一致</li>
<li>当<code>batch_first=True</code>时，输入输出数据的批次维度在前，但不影响h0和hn的形状</li>
</ul>
<ol start="3">
<li><strong>隐藏层特殊关系</strong>：</li>
</ol>
<ul>
<li>当隐藏层个数配置为n时，output的结果和最后一个隐藏层输出一致</li>
<li>隐藏层个数增加会提高模型复杂度但不会改变输出维度</li>
</ul>
<ol start="4">
<li><strong>序列长度影响</strong>：</li>
</ol>
<ul>
<li>输入序列长度可以变化，不影响模型参数，只影响输出序列长度</li>
<li>输出序列长度与输入序列长度一致(对于N vs N结构)</li>
</ul>
<h3 id="RNN模型的优缺点"><a href="#RNN模型的优缺点" class="headerlink" title="RNN模型的优缺点"></a>RNN模型的优缺点</h3><p>RNN网络优点</p>
<ol>
<li><strong>结构简单</strong>：内部结构相对简单，对计算资源要求低</li>
<li><strong>参数效率</strong>：相比LSTM和GRU等变体，参数总量少很多</li>
<li><strong>短序列优势</strong>：在短序列任务上性能和效果表现优异</li>
<li><strong>序列处理</strong>：能够连续性地输入序列数据，进行特征提取</li>
</ol>
<p>RNN网络缺点</p>
<ol>
<li><strong>长序列问题</strong>：长序列文本特征提取效果差</li>
<li><strong>梯度问题</strong>：过长的序列导致梯度计算异常，容易发生梯度消失或爆炸</li>
<li><strong>并行限制</strong>：由于时间步间的依赖关系，难以进行并行计算</li>
<li><strong>记忆有限</strong>：对长期依赖关系的捕捉能力有限</li>
</ol>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><blockquote>
<p>长短期记忆网络(Long Short Term Memory Networks)是一种改进之后的循环神经网络，可以解决RNN无法处理长距离的依赖的问题，目前比较流行。原始 RNN 的隐藏层只有一个状态，即h，它对于短期的输入非常敏感。</p>
<p>参考链接：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/chihaoyuIsnotHere/p/10604085.html">详解 LSTM</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/144132609?utm_medium=social&utm_id=0">使用PyTorch手写代码从头构建LSTM，更深入的理解其工作原理</a></li>
</ul>
</blockquote>
<p>原始 RNN 的隐藏层只有一个状态，即h，它对于短期的输入非常敏感。再增加一个状态，即c，让它来保存长期的状态，称为单元状态(cell state)。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250623165030826.png" alt="image-20250623165030826"></p>
<p>把上图按照时间维度展开：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250623165100481.png" alt="image-20250623165100481"></p>
<blockquote>
<p>在 t 时刻，LSTM 的输入有三个：当前时刻网络的输入值 $x_t$、上一时刻 LSTM 的输出值 $h_t-1$、以及上一时刻的单元状态 $c_t-1$；<br>LSTM 的输出有两个：当前时刻 LSTM 输出值 $h_t$、和当前时刻的单元状态 $c_t$.</p>
</blockquote>
<h3 id="怎样控制长期状态"><a href="#怎样控制长期状态" class="headerlink" title="怎样控制长期状态"></a>怎样控制长期状态</h3><blockquote>
<p>方法是：使用三个控制开关</p>
<p>第一个开关，负责控制继续保存长期状态c；<br>第二个开关，负责控制把即时状态输入到长期状态c；<br>第三个开关，负责控制是否把长期状态c作为当前的LSTM的输出</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250623165411839.png" alt="image-20250623165411839"></p>
<p>如何在算法中实现这三个开关？</p>
<p>方法：用门（gate）</p>
<p>定义：gate 实际上就是一层全连接层，输入是一个向量，输出是一个 0到1 之间的实数向量。</p>
<p>公式为：<br>$$<br>g(x) &#x3D; \sigma(Wx + b)<br>$$</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250623194026745.png" alt="image-20250623194026745"></p>
<h3 id="如何进行控制？"><a href="#如何进行控制？" class="headerlink" title="如何进行控制？"></a>如何进行控制？</h3><blockquote>
<p>方法：用门的输出向量按元素乘以我们需要控制的那个向量<br>原理：门的输出是 0到1 之间的实数向量，<br>当门输出为 0 时，任何向量与之相乘都会得到 0 向量，这就相当于什么都不能通过；<br>输出为 1 时，任何向量与之相乘都不会有任何改变，这就相当于什么都可以通过。</p>
</blockquote>
<h3 id="LSTM-的前向计算"><a href="#LSTM-的前向计算" class="headerlink" title="LSTM 的前向计算"></a>LSTM 的前向计算</h3><blockquote>
<p><strong>遗忘门（forget gate）</strong><br>它决定了上一时刻的单元状态 $c_t-1$ 有多少保留到当前时刻 $c_t$</p>
<p><strong>输入门（input gate）</strong><br>它决定了当前时刻网络的输入 $x_t$ 有多少保存到单元状态 $c_t$</p>
<p><strong>输出门（output gate）</strong><br>控制单元状态 $c_t$ 有多少输出到 LSTM 的当前输出值 $h_t$</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250623194554613.png" alt="image-20250623194554613"></p>
<h3 id="遗忘门（Forget-Gate）"><a href="#遗忘门（Forget-Gate）" class="headerlink" title="遗忘门（Forget Gate）"></a><strong>遗忘门（Forget Gate）</strong></h3><blockquote>
<p>它决定了上一时刻的单元状态 $c_t-1$ 有多少保留到当前时刻 $c_t$</p>
<p><code>特性以及核心作用</code></p>
<ol>
<li><strong>门控机制</strong><ul>
<li>通过$f_t$值动态控制$C_{t-1}$的保留量</li>
<li>示例：当$f_t$&#x3D;0.6时，保留60%历史记忆</li>
</ul>
</li>
<li><strong>梯度保护</strong><ul>
<li>相比普通RNN，有效缓解梯度消失问题</li>
<li>实验数据：在100步序列中梯度保留率提升83%</li>
</ul>
</li>
<li><strong>参数学习</strong><ul>
<li>$W_f$和$b_f$通过BPTT算法更新</li>
<li>学习目标：优化长期依赖关系的捕捉能力</li>
</ul>
</li>
</ol>
<p>遗忘门（forget gate）是输入信息与候选者一起操作的门，作为长期记忆。请注意，在输入、隐藏状态和偏差的第一个线性组合上，应用一个sigmoid函数：</p>
</blockquote>
<p>$$<br>f_t &#x3D; \sigma(W_f \cdot [\hat{y}_{t-1}, x_t] + b_f)<br>$$</p>
<p>参数说明：</p>
<table>
<thead>
<tr>
<th>符号</th>
<th>维度</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>$W_f$</td>
<td>$n \times (k+m)$</td>
<td>遗忘权重矩阵</td>
</tr>
<tr>
<td>$b_f$</td>
<td>$n \times 1$</td>
<td>遗忘偏置项</td>
</tr>
<tr>
<td>$[\hat{y}_{t-1}, x_t]$</td>
<td>$(k+m) \times 1$</td>
<td>增广输入向量</td>
</tr>
</tbody></table>
<ol>
<li><p><strong>输入处理</strong></p>
<ul>
<li>拼接前一时刻输出$\hat{y}_{t-1}$与当前输入$x_t$形成增广向量</li>
<li>矩阵乘法：$W_f \cdot [\hat{y}_{t-1}, x_t]$</li>
</ul>
</li>
<li><p><strong>非线性变换</strong></p>
<ul>
<li>通过sigmoid函数$\sigma$将结果压缩到[0,1]区间</li>
<li>输出值$f_t$表示记忆保留比例</li>
</ul>
</li>
<li><p><strong>状态更新</strong><br>$$<br>C_t &#x3D; f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t<br>$$</p>
<ul>
<li>实现历史记忆的选择性遗忘</li>
</ul>
</li>
</ol>
<h3 id="输入门（Input-Gate）"><a href="#输入门（Input-Gate）" class="headerlink" title="输入门（Input Gate）"></a><strong>输入门（Input Gate）</strong></h3><blockquote>
<p>它决定了当前时刻网络的输入 $x_t$ 有多少保存到单元状态 $c_t$（简单来说就是：控制新信息进入神经网络状态的程度）</p>
</blockquote>
<p>$$<br> i_t &#x3D; \sigma(W_i \cdot [\hat{y}_{t-1}, x_t] + b_i)<br>$$</p>
<ul>
<li><p>使用$i_t$作为输出变量</p>
</li>
<li><p>包含输入门特有的权重矩阵$W_i$和偏置$b_i$</p>
</li>
<li><p>接收两个标准输入：$\hat{y}_{t-1}$（前一时刻输出）和$x_t$（当前输入）</p>
</li>
<li><p>通过sigmoid激活函数$\sigma$输出</p>
</li>
</ul>
<p>参数说明表</p>
<table>
<thead>
<tr>
<th>符号</th>
<th>维度</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>$W_i$</td>
<td>$n \times (k+m)$</td>
<td>输入门权重矩阵</td>
</tr>
<tr>
<td>$b_i$</td>
<td>$n \times 1$</td>
<td>输入门偏置项</td>
</tr>
<tr>
<td>$i_t$</td>
<td>$n \times 1$</td>
<td>输入门输出值</td>
</tr>
</tbody></table>
<p>与其他门的区别</p>
<table>
<thead>
<tr>
<th>门类型</th>
<th>输出变量</th>
<th>核心功能</th>
<th>激活值范围</th>
</tr>
</thead>
<tbody><tr>
<td>输入门</td>
<td>$i_t$</td>
<td>控制新信息流入</td>
<td>[0,1]</td>
</tr>
<tr>
<td>遗忘门</td>
<td>$f_t$</td>
<td>控制历史记忆保留</td>
<td>[0,1]</td>
</tr>
<tr>
<td>输出门</td>
<td>$o_t$</td>
<td>控制当前输出</td>
<td>[0,1]</td>
</tr>
</tbody></table>
<h3 id="输出门（Output-Gate）"><a href="#输出门（Output-Gate）" class="headerlink" title="输出门（Output Gate）"></a><strong>输出门（Output Gate）</strong></h3><blockquote>
<p>控制单元状态 $c_t$ 有多少输出到 LSTM 的当前输出值 $h_t$（简单来说就是：控制<strong>当前时刻信息</strong>的输出强度）</p>
</blockquote>
<p>$$<br>o_t &#x3D; \sigma(W_o \cdot [\hat{y}_{t-1}, x_t] + b_o)<br>$$</p>
<ul>
<li><p>使用$o_t$作为输出变量</p>
</li>
<li><p>包含输出门特有的权重矩阵$W_o$和偏置$b_o$</p>
</li>
<li><p>$\sigma$为激活函数</p>
</li>
<li><p>输出直接连接$\hat{y}_t$（候选输出）</p>
</li>
</ul>
<p>参数说明</p>
<table>
<thead>
<tr>
<th>符号</th>
<th>维度</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>$W_o$</td>
<td>$n \times (k+m)$</td>
<td>输出门权重矩阵</td>
</tr>
<tr>
<td>$b_o$</td>
<td>$n \times 1$</td>
<td>输出门偏置项</td>
</tr>
<tr>
<td>$o_t$</td>
<td>$n \times 1$</td>
<td>门控输出值</td>
</tr>
</tbody></table>
<h3 id="状态更新"><a href="#状态更新" class="headerlink" title="状态更新"></a>状态更新</h3><blockquote>
<p>之前的三个门在未组合进行状态更新之前，本质上都只是一个简单映射。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250623202019516.png" alt="image-20250623202019516"></p>
<h3 id="⭐️⭐️LSTM-API⭐️⭐️"><a href="#⭐️⭐️LSTM-API⭐️⭐️" class="headerlink" title="⭐️⭐️LSTM API⭐️⭐️"></a>⭐️⭐️LSTM API⭐️⭐️</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LSTM(input_size, hidden_size, num_layers=<span class="number">1</span>, </span><br><span class="line">             bias=<span class="literal">True</span>, batch_first=<span class="literal">False</span>, dropout=<span class="number">0</span>, </span><br><span class="line">             bidirectional=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>关键参数说明：</p>
<table>
<thead>
<tr>
<th align="center">参数</th>
<th align="center">类型</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">input_size</td>
<td align="center">int</td>
<td align="center">输入特征维度</td>
</tr>
<tr>
<td align="center">hidden_size</td>
<td align="center">int</td>
<td align="center">隐藏状态维度</td>
</tr>
<tr>
<td align="center">num_layers</td>
<td align="center">int</td>
<td align="center">堆叠的LSTM层数（默认1）</td>
</tr>
<tr>
<td align="center">batch_first</td>
<td align="center">bool</td>
<td align="center">输入&#x2F;输出形状是否为(batch, seq, feature)</td>
</tr>
<tr>
<td align="center">dropout</td>
<td align="center">float</td>
<td align="center">层间dropout概率（0表示不使用）</td>
</tr>
<tr>
<td align="center">bidirectional</td>
<td align="center">bool</td>
<td align="center">是否使用双向LSTM</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建LSTM网络</span></span><br><span class="line">lstm = nn.LSTM(input_size=<span class="number">10</span>, hidden_size=<span class="number">20</span>, num_layers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备输入数据</span></span><br><span class="line"><span class="comment"># 5: 序列长度（时间步数）</span></span><br><span class="line"><span class="comment"># 3: 批次大小（同时处理3个独立序列）</span></span><br><span class="line"><span class="comment"># 10: 每个时间步的特征维度（必须与input_size一致）</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">10</span>)  <span class="comment"># (seq_len, batch, input_size)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># h0: 初始隐藏状态</span></span><br><span class="line"><span class="comment"># 2: 对应num_layers（层数）</span></span><br><span class="line"><span class="comment"># 3: 批次大小（与输入一致）</span></span><br><span class="line"><span class="comment"># 20: 隐藏状态维度（与hidden_size一致）</span></span><br><span class="line">h0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)     <span class="comment"># (num_layers, batch, hidden_size)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># c0: 初始细胞状态（LSTM特有）</span></span><br><span class="line"><span class="comment"># 形状与h0相同 存储LSTM的长期记忆</span></span><br><span class="line">c0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)     <span class="comment"># 初始细胞状态</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">output, (hn, cn) = lstm(<span class="built_in">input</span>, (h0, c0))</span><br><span class="line"><span class="comment"># 输出:output: [5, 3, 20] (seq_len, batch, hidden_size)</span></span><br><span class="line"><span class="comment"># hn/cn: [2, 3, 20] (num_layers, batch, hidden_size)</span></span><br></pre></td></tr></table></figure>





<h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><blockquote>
<p> GRU（Gate Recurrent Unit）是循环神经网络（RNN）的一种，<code>可以解决RNN中不能长期记忆和反向传播中的梯度等问题</code>，与LSTM的作用类似，不过比LSTM简单，容易进行训练。<code>GRU模型中有两个门，重置门和更新门</code>。</p>
<p> 参考链接：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/Michale_L/article/details/122778270">GRU(门控循环单元)，易懂</a></li>
</ul>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250626202802664.png" alt="image-20250626202802664"></p>
<ul>
<li>符号定义</li>
</ul>
<table>
<thead>
<tr>
<th>符号</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>$x_{t}$</td>
<td>当前时刻输入信息</td>
</tr>
<tr>
<td>$h_{t-1}$</td>
<td>上一时刻的隐藏状态（神经网络记忆，包含历史数据信息）</td>
</tr>
<tr>
<td>$h_{t}$</td>
<td>传递到下一时刻的隐藏状态</td>
</tr>
<tr>
<td>$\tilde{h}_{t}$</td>
<td>候选隐藏状态</td>
</tr>
<tr>
<td>$r_{t}$</td>
<td>重置门</td>
</tr>
<tr>
<td>$z_{t}$</td>
<td>更新门</td>
</tr>
</tbody></table>
<ul>
<li><p>激活函数说明</p>
<ul>
<li><p><strong>Sigmoid函数（σ）</strong></p>
<ul>
<li><p>作用：将数据压缩到[0,1]范围</p>
</li>
<li><p>典型应用：门控机制（重置门&#x2F;更新门）</p>
</li>
</ul>
</li>
<li><p><strong>Tanh函数</strong></p>
<ul>
<li><p>作用：将数据压缩到[-1,1]范围  </p>
</li>
<li><p>典型应用：候选状态计算</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="⭐️重置门⭐️"><a href="#⭐️重置门⭐️" class="headerlink" title="⭐️重置门⭐️"></a>⭐️重置门⭐️</h3><blockquote>
<p>下面会清晰的讲解：<strong>重置门决定了如何将新的输入信息与前面的记忆相结合</strong>。</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250626203904519.png"></p>
<p>重置门公式<br>$$<br>r_t &#x3D; \sigma(W_r \cdot [h_{t-1}, x_t])<br>$$</p>
<p>其中$W_r$是权重矩阵，用这个权重矩阵对$x^t$和$h^{t-1}$拼接而成的矩阵进行线性变换(两个矩阵相乘)。然后将两个矩阵相乘得到的值投入$sigmod$函数，会得到$r^t$的值，比如:0.6。这个值会用到候选隐藏状态的公式中，即下面这个公式:<br>$$<br>\tilde{h}<em>t &#x3D; \tanh(W \cdot [r_t * h</em>{t-1}, x_t])<br>$$</p>
<p>对上述公式展开：<br>$$<br>\tilde{h}<em>t &#x3D; \tanh(x_tW</em>{xh}+(r_t * h_{t-1})W_{hh}+b_h)<br>$$</p>
<p>可以观察到：</p>
<p>$r_t$越小，$r_t * h_{t-1}$越小，$(r_t * h_{t-1})W_{hh}$越小，因此说明上一刻需要遗忘的越多，丢弃的越多。</p>
<p>$r_t$越大，$r_t * h_{t-1}$越大，$(r_t * h_{t-1})W_{hh}$越大，说明上一刻需要记住的越多，新的输入信息也就是$x_t$与前面的记忆相结合的越多</p>
<p>当$r_t$越接近1，$(r_t * h_{t-1})W_{hh}$值也接近1，表示保留上一时刻的隐藏状态。</p>
<h3 id="⭐️更新门⭐️"><a href="#⭐️更新门⭐️" class="headerlink" title="⭐️更新门⭐️"></a>⭐️更新门⭐️</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250626210549294.png" alt="image-20250626210549294"></p>
<p>更新门公式：<br>$$<br>z_t &#x3D; \sigma(W_z \cdot [h_{t-1}, x_t])<br>$$<br>更新记忆表达式:</p>
<p>$$<br>h_t &#x3D; (1-z_t) * h_{t-1} + z_t * \tilde{h}_t<br>$$</p>
<p>$z_t$越接近1，记忆下来的数据越多；$z_t$越接近0则代表遗忘的越多。</p>
<p>$(1-z_t)*h_{t-1}$：表示对上一时刻隐藏状态进行选择性“遗忘”。忘记$h_{t-1}$中一些不重要的信息，把不相关的丢弃</p>
<p>$z_t*\tilde{h}_t$：表示对候选隐藏状态的进一步选择性”记忆”。会忘记$\tilde{h}_t$,中的一些不重要的信息。</p>
<p>综上</p>
<p>$$<br>h_t &#x3D; (1-z_t) * h_{t-1} + z_t * \tilde{h}_t<br>$$</p>
<p>$h_t$忘记传递下来的 $h_{t-1}$中的某些信息，并加入当前节点输入的某些信息。这就是最终的记忆。</p>
<p>门控循环单元GRU不会随时间而清除以前的信息，它会保留相关的信息并传递到下一个单元。</p>
<h3 id="所使用的公式-整合"><a href="#所使用的公式-整合" class="headerlink" title="所使用的公式-整合"></a>所使用的公式-整合</h3><p>GRU通过以下门控机制计算当前时刻的隐藏状态：</p>
<ol>
<li><p><strong>更新门（Update Gate）</strong><br>$$<br>z_t &#x3D; \sigma(W_z \cdot [h_{t-1}, x_t])<br>$$</p>
<ul>
<li>控制历史记忆的保留比例</li>
<li>使用Sigmoid激活输出[0,1]区间值</li>
</ul>
</li>
<li><p><strong>重置门（Reset Gate）</strong><br>$$<br>r_t &#x3D; \sigma(W_r \cdot [h_{t-1}, x_t])<br>$$</p>
<ul>
<li>决定历史记忆对候选状态的影响程度</li>
<li>同样使用Sigmoid激活</li>
</ul>
</li>
<li><p><strong>候选隐藏状态</strong><br>$$<br>\tilde{h}<em>t &#x3D; \tanh(W \cdot [r_t * h</em>{t-1}, x_t])<br>$$</p>
<ul>
<li>融合当前输入与筛选后的历史记忆</li>
<li>使用Tanh激活输出[-1,1]区间值</li>
</ul>
</li>
<li><p><strong>最终隐藏状态</strong><br>$$<br>h_t &#x3D; (1-z_t) * h_{t-1} + z_t * \tilde{h}_t<br>$$</p>
<ul>
<li>通过更新门平衡历史记忆与当前信息</li>
<li>形成新的记忆状态传递给下一时间步</li>
</ul>
</li>
</ol>
<h3 id="GRU-API"><a href="#GRU-API" class="headerlink" title="GRU API"></a>GRU API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRU模型初始化</span></span><br><span class="line">mygru = nn.GRU(<span class="number">5</span>, <span class="number">6</span>, <span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 输入数据</span></span><br><span class="line"><span class="comment"># 第1个参数:单词个数1</span></span><br><span class="line"><span class="comment"># 第2个参数:批次数3</span></span><br><span class="line"><span class="comment"># 第3个参数:数据的尺寸-数据的特征维度5</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>)  <span class="comment"># B</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 隐藏层数据</span></span><br><span class="line"><span class="comment"># 第1个参数:模型的隐藏层个数2</span></span><br><span class="line"><span class="comment"># 第2个参数:数据的批次数3</span></span><br><span class="line"><span class="comment"># 第3个参数:模型的输出神经元的个数-6</span></span><br><span class="line">h0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>)  <span class="comment"># C</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 给模型送数据 input[1,3,5] h0[2,3,6] -&gt; output[1,3,6],hn[2,3,6]</span></span><br><span class="line">output, hn = mygru(<span class="built_in">input</span>, h0)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印输出</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output--&gt;&#x27;</span>, output.shape, output)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;hn---&gt;&#x27;</span>, hn.shape, hn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="comment"># output--&gt; torch.Size([1, 3, 6])  # 输出序列的形状</span></span><br><span class="line"><span class="comment"># hn---&gt; torch.Size([2, 3, 6])     # 最终隐藏状态的形状</span></span><br></pre></td></tr></table></figure>

<h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ol>
<li><p><strong>随机性说明</strong>：</p>
<ul>
<li>由于使用<code>torch.randn()</code>，每次运行具体数值不同，但维度结构保持稳定</li>
</ul>
</li>
<li><p><strong>批处理优势</strong>：</p>
<ul>
<li>同时处理3个样本，效率高于串行处理</li>
<li>每个样本的隐藏状态独立计算</li>
</ul>
</li>
<li><p><strong>多层GRU特性</strong>：</p>
<ul>
<li>第1层输出作为第2层输入</li>
<li>最终<code>hn</code>包含各层的最终状态</li>
</ul>
</li>
<li><p><strong>实际应用扩展</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 若要获取具体数值（示例）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;样本1的输出:&quot;</span>, output[<span class="number">0</span>, <span class="number">0</span>, :].detach().numpy()) </span><br><span class="line"><span class="comment"># 输出示例：[-0.12  0.45 -0.33 ...]</span></span><br></pre></td></tr></table></figure></li>
</ol>
<p>该结果展示了GRU处理序列数据的基础能力，后续通常需要添加全连接层等结构完成具体任务。</p>
<h1 id="从0-1实现RNN人名分类器案例"><a href="#从0-1实现RNN人名分类器案例" class="headerlink" title="从0-1实现RNN人名分类器案例"></a>从0-1实现RNN人名分类器案例</h1><blockquote>
<p>该案例旨在实现RNN人名分类器案例，即输入一个人名，使用模型判断该人名可能来自哪个国家。</p>
<p>业务应用场景包括：</p>
<ul>
<li>用户注册过程中，根据填写的姓名自动分配国家&#x2F;地区信息</li>
<li>限制手机号码位数等表单验证</li>
<li>提升用户体验和注册效率</li>
</ul>
</blockquote>
<h3 id="数据获取"><a href="#数据获取" class="headerlink" title="数据获取"></a>数据获取</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://download.pytorch.org/tutorial/data.zip">https://download.pytorch.org/tutorial/data.zip</a></p>
</blockquote>
<h3 id="数据特点"><a href="#数据特点" class="headerlink" title="数据特点"></a>数据特点</h3><ul>
<li><p>数据格式：每行第一个单词为人名，第二个单词为国家名，中间用制表符(tab)分隔</p>
</li>
<li><p>数据规模：共20074条数据，涵盖18个国家，其中n_letters也就是字母的个数（包括字符）有57个</p>
</li>
<li><p>示例数据：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Huffmann</span> <span class="string">German</span></span><br><span class="line"><span class="attr">Hummel</span> <span class="string">German</span></span><br><span class="line"><span class="attr">Deng</span> <span class="string">Chinese</span></span><br><span class="line"><span class="attr">Ding</span> <span class="string">Chinese</span></span><br><span class="line"><span class="attr">Abaimov</span> <span class="string">Russian</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="从0到1实现人名分类器"><a href="#从0到1实现人名分类器" class="headerlink" title="从0到1实现人名分类器"></a>从0到1实现人名分类器</h2><ul>
<li>技术路线1：以单词为单位，word2id后送给RNN模型抽取事物特征进行分类。这样每个姓氏只能整体送1次。word2id，找到姓名对应的张量，送入到RNN中。</li>
<li>技术路线2：比如：z-h-a-n-g以字母为单位，送给RNN模型抽取事物特征进行分类。</li>
</ul>
<h3 id="数据处理流程"><a href="#数据处理流程" class="headerlink" title="数据处理流程"></a>数据处理流程</h3><h3 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取常用字符包括字母和常用标点</span></span><br><span class="line">all_letters = string.ascii_letters + <span class="string">&quot;.,;‘&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取常用字符数量</span></span><br><span class="line">n_letters = <span class="built_in">len</span>(all_letters)</span><br><span class="line"><span class="comment"># all_letters =》abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,;‘</span></span><br><span class="line"><span class="comment"># n_letters = 57</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 国家名 种类数</span></span><br><span class="line">categorys = [<span class="string">&quot;Italian&quot;</span>,<span class="string">&quot;English&quot;</span>,<span class="string">&quot;Arabic&quot;</span>,<span class="string">&quot;Spanish&quot;</span>,<span class="string">&quot;Scottish&quot;</span>,<span class="string">&quot;Irish&quot;</span>,<span class="string">&quot;Chinese&quot;</span>,<span class="string">&quot;Vietnamese&quot;</span>,<span class="string">&quot;Japanese&quot;</span>,<span class="string">&quot;French&quot;</span>,<span class="string">&quot;Greek&quot;</span>,<span class="string">&quot;Dutch&quot;</span>,<span class="string">&quot;Korean&quot;</span>,<span class="string">&quot;Polish&quot;</span>,<span class="string">&quot;Portuguese&quot;</span>,<span class="string">&quot;Russian&quot;</span>,<span class="string">&quot;Czech&quot;</span>,<span class="string">&quot;German&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 国家名个数</span></span><br><span class="line">categorynum = <span class="built_in">len</span>(categorys)  <span class="comment"># 18</span></span><br></pre></td></tr></table></figure>



<h4 id="读取数据到内存"><a href="#读取数据到内存" class="headerlink" title="读取数据到内存"></a>读取数据到内存</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">read_data</span>(<span class="params">filename</span>):</span><br><span class="line">    <span class="comment"># 该函数返回两个列表，x包括所有人名，y包括所有的国家</span></span><br><span class="line">    my_list_x, my_list_y = [], []</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filename, mode=<span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            x, y = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">            my_list_x.append(x)</span><br><span class="line">            my_list_y.append(y)</span><br><span class="line">    <span class="keyword">return</span> my_list_x, my_list_y</span><br></pre></td></tr></table></figure>

<h4 id="构建Dataset类"><a href="#构建Dataset类" class="headerlink" title="构建Dataset类"></a>构建Dataset类</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NameClassDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, my_list_x, my_list_y</span>):</span><br><span class="line">        self.my_list_x = my_list_x</span><br><span class="line">        self.my_list_y = my_list_y</span><br><span class="line">        self.sample_len = <span class="built_in">len</span>(my_list_x)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.sample_len</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        x = self.my_list_x[index]  <span class="comment"># 如&#x27;zhang&#x27;</span></span><br><span class="line">        y = self.my_list_y[index]  <span class="comment"># 如&#x27;china&#x27;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># One-hot编码</span></span><br><span class="line">        <span class="comment"># len(x)为单个词的长度(如zhang 5)，n_letters = 57 创建5行57列全0矩阵</span></span><br><span class="line">        tensor_x = torch.zeros(<span class="built_in">len</span>(x), n_letters)</span><br><span class="line">        <span class="keyword">for</span> line, letter <span class="keyword">in</span> <span class="built_in">enumerate</span>(x):</span><br><span class="line">            <span class="comment"># 0 z（line  letter）</span></span><br><span class="line">            <span class="comment"># 1 h</span></span><br><span class="line">            <span class="comment"># 2 a</span></span><br><span class="line">            <span class="comment"># 3 n</span></span><br><span class="line">            <span class="comment"># 4 g</span></span><br><span class="line">            tensor_x[line][all_letters.find(letter)] = <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">        tensor_y = torch.tensor(categorys.index(y), dtype=torch.long)</span><br><span class="line">        <span class="keyword">return</span> tensor_x, tensor_y</span><br></pre></td></tr></table></figure>

<p>NOTE：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250627152109683.png" alt="image-20250627152109683"></p>
<h4 id="构建DataLoader"><a href="#构建DataLoader" class="headerlink" title="构建DataLoader"></a>构建DataLoader</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">my_list_x, my_list_y = read_data(<span class="string">&#x27;./data/name_classfication.txt&#x27;</span>)</span><br><span class="line">my_nameclassdataset = NameClassDataset(my_list_x, my_list_y)</span><br><span class="line">mydataloader = DataLoader(dataset=my_nameclassdataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<ul>
<li>DataLoader类参数详解：<ul>
<li><code>dataset</code>：绑定前面创建的数据集对象</li>
<li><code>batch_size=1</code>：每批返回1个样本<ul>
<li>人名长度不固定，批处理需要padding</li>
</ul>
</li>
<li><code>shuffle=True</code>：每个epoch打乱数据顺序<ul>
<li>防止训练过程中出现顺序偏差</li>
<li>提升模型泛化能力</li>
</ul>
</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h3><h4 id="RNN模型实现"><a href="#RNN模型实现" class="headerlink" title="RNN模型实现"></a>RNN模型实现</h4><blockquote>
<ul>
<li>创建RNN类，继承自nn.Module</li>
<li>初始化方法：<ul>
<li><code>input_size</code>：输入特征维度（本案例中为57，对应字母表大小）</li>
<li><code>hidden_size</code>：隐藏层维度（文档中设为128）</li>
<li><code>output_size</code>：输出类别数（本案例为18个国家）</li>
<li><code>num_layers</code>：RNN层数（默认为1）</li>
<li>self.linear：全连接层<ul>
<li>将RNN最后时间步的输出映射到类别空间</li>
</ul>
</li>
<li>nn.LogSoftmax：对数softmax<ul>
<li>配合NLLLoss使用更数值稳定</li>
<li><code>dim=-1</code>表示在最后一个维度计算</li>
</ul>
</li>
</ul>
</li>
<li>向前传播方法：<ul>
<li><p>input &#x3D; input.unsqueeze(1)：在指定维度插入一个大小为1的新维度</p>
<ul>
<li><strong>原始形状</strong>：<code>[seq_len, input_size]</code> （如 <code>[5, 57]</code>，表示5个字符，每个字符57维特征）</li>
<li><strong>变换后形状</strong>：<code>[seq_len, 1, input_size]</code> （如 <code>[5, 1, 57]</code>）</li>
</ul>
</li>
<li><p>self.rnn(input, hidden)进行向前计算，看到这里还懵了，翻了源码想着源码可能有nn.RNN对象的rnn()方法，后来才发现没有，初步猜测这是递归，但大模型说这是正常调用：</p>
<ul>
<li><code>rr</code>：所有时间步的输出 [seq_len,1,hidden_size]</li>
<li><code>hn</code>：最后时间步的隐藏状态 [num_layers,1,hidden_size]</li>
</ul>
</li>
<li><p>tmprr &#x3D; rr[-1]  取最后时间步 [1,hidden_size]</p>
<ul>
<li>使用最后时间步作为整个序列的表示</li>
</ul>
</li>
<li><p>tmprr &#x3D; self.linear(tmprr)</p>
<ul>
<li>执行全连接层的线性变换</li>
</ul>
</li>
<li><p>return self.softmax(tmprr), hn</p>
<ul>
<li>将线性输出转换为对数概率分布</li>
</ul>
</li>
</ul>
</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size, num_layers=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(RNN, self).__init__()</span><br><span class="line">        self.rnn = nn.RNN(input_size, hidden_size, num_layers)</span><br><span class="line">        self.linear = nn.Linear(hidden_size, output_size)</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, hidden</span>):</span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        rr, hn = self.rnn(<span class="built_in">input</span>, hidden)</span><br><span class="line">        tmprr = rr[-<span class="number">1</span>]</span><br><span class="line">        tmprr = self.linear(tmprr)</span><br><span class="line">        <span class="keyword">return</span> self.softmax(tmprr), hn</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inithidden</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.zeros(self.num_layers, <span class="number">1</span>, self.hidden_size)</span><br></pre></td></tr></table></figure>



<h4 id="LSTM模型实现"><a href="#LSTM模型实现" class="headerlink" title="LSTM模型实现"></a>LSTM模型实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LSTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size, num_layers=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LSTM, self).__init__()</span><br><span class="line">        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)</span><br><span class="line">        self.linear = nn.Linear(hidden_size, output_size)</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, hidden, c0</span>):</span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        rr, (hn, cn) = self.lstm(<span class="built_in">input</span>, (hidden, c0))</span><br><span class="line">        tmprr = rr[-<span class="number">1</span>]</span><br><span class="line">        tmprr = self.linear(tmprr)</span><br><span class="line">        <span class="keyword">return</span> self.softmax(tmprr), hn, cn</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inithidden</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.zeros(self.num_layers, <span class="number">1</span>, self.hidden_size)</span><br></pre></td></tr></table></figure>

<h4 id="GRU模型实现"><a href="#GRU模型实现" class="headerlink" title="GRU模型实现"></a>GRU模型实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GRU</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size, num_layers=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GRU, self).__init__()</span><br><span class="line">        self.gru = nn.GRU(input_size, hidden_size, num_layers)</span><br><span class="line">        self.linear = nn.Linear(hidden_size, output_size)</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, hidden</span>):</span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        rr, hn = self.gru(<span class="built_in">input</span>, hidden)</span><br><span class="line">        tmprr = rr[-<span class="number">1</span>]</span><br><span class="line">        tmprr = self.linear(tmprr)</span><br><span class="line">        <span class="keyword">return</span> self.softmax(tmprr), hn</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inithidden</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.zeros(self.num_layers, <span class="number">1</span>, self.hidden_size)</span><br></pre></td></tr></table></figure>

<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><h4 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h4><blockquote>
<ul>
<li>myadam &#x3D; optim.Adam(myrnn.parameters(), lr&#x3D;1e-3)<ul>
<li><code>lr=1e-3</code>：初始学习率（常用值0.001）</li>
</ul>
</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_train_rnn</span>():</span><br><span class="line">    <span class="comment"># 1. 准备数据</span></span><br><span class="line">    my_list_x, my_list_y = read_data(<span class="string">&#x27;./data/name_classfication.txt&#x27;</span>)</span><br><span class="line">    my_nameclassdataset = NameClassDataset(my_list_x, my_list_y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. 初始化模型、损失函数和优化器</span></span><br><span class="line">    myrnn = RNN(<span class="number">57</span>, <span class="number">128</span>, <span class="number">18</span>)</span><br><span class="line">    <span class="comment"># 定义负对数似然损失，用于衡量模型预测与真实标签的差异</span></span><br><span class="line">    mycrossentropyloss = nn.NLLLoss()</span><br><span class="line">    <span class="comment"># Adam优化算法，用于更新模型参数以最小化损失</span></span><br><span class="line">    myadam = optim.Adam(myrnn.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. 训练循环</span></span><br><span class="line">    <span class="keyword">for</span> epoch_idx <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> i, (x, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(mydataloader):</span><br><span class="line">            <span class="comment"># 前向传播</span></span><br><span class="line">            output_y, hidden = myrnn(x[<span class="number">0</span>], myrnn.inithidden())</span><br><span class="line">            <span class="comment"># 计算损失</span></span><br><span class="line">            myloss = mycrossentropyloss(output_y, y)</span><br><span class="line">            <span class="comment"># 反向传播</span></span><br><span class="line">            myadam.zero_grad()</span><br><span class="line">            myloss.backward()</span><br><span class="line">            myadam.step()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 记录训练信息</span></span><br><span class="line">            total_iter_num += <span class="number">1</span></span><br><span class="line">            total_loss += myloss.item()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 每100次迭代记录一次平均损失和准确率</span></span><br><span class="line">            <span class="keyword">if</span> total_iter_num % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                tmploss = total_loss / total_iter_num</span><br><span class="line">                total_loss_list.append(tmploss)</span><br><span class="line">                </span><br><span class="line">        <span class="comment"># 保存模型</span></span><br><span class="line">        torch.save(myrnn.state_dict(), <span class="string">f&#x27;./my_rnn_model_<span class="subst">&#123;epoch_idx+<span class="number">1</span>&#125;</span>.bin&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> total_loss_list, total_time, total_acc_list</span><br></pre></td></tr></table></figure>

<h4 id="模型效果对比"><a href="#模型效果对比" class="headerlink" title="模型效果对比"></a>模型效果对比</h4><p>从实验结果来看，三种模型各有优劣：</p>
<ol>
<li>损失对比：<ul>
<li>第一个轮次RNN收敛最快</li>
<li>随着训练数据增加，GRU效果最好，LSTM次之，RNN最后</li>
</ul>
</li>
</ol>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/74ab81b10cf7acc9c5036cc44d80bff6-image.png" alt="img"></p>
<ol>
<li>训练时间对比：<ul>
<li>RNN复杂度最低，耗时最短</li>
<li>GRU次之，LSTM最耗时</li>
</ul>
</li>
</ol>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/0d63307b86786219c4fa6e604b02a126-image.png" alt="img"></p>
<ol start="2">
<li><p>准确率对比：</p>
<ul>
<li><p>GRU效果最好</p>
</li>
<li><p>LSTM次之</p>
</li>
<li><p>RNN相对较差</p>
</li>
</ul>
</li>
</ol>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/a23c88131a7aaea554c8643ce290b0ab-image.png" alt="img"></p>
<h3 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h3><h4 id="预测函数实现"><a href="#预测函数实现" class="headerlink" title="预测函数实现"></a>预测函数实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_predict_rnn</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># 1. 文本张量化</span></span><br><span class="line">    tensor_x = lineToTensor(x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. 加载训练好的模型</span></span><br><span class="line">    myrnn = RNN(<span class="number">57</span>, <span class="number">128</span>, <span class="number">18</span>)</span><br><span class="line">    myrnn.load_state_dict(torch.load(my_path_rnn))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. 预测</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        output, hidden = myrnn(tensor_x, myrnn.inithidden())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 4. 输出前3个预测结果</span></span><br><span class="line">    topv, topi = output.topk(<span class="number">3</span>, <span class="number">1</span>, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;x===&gt;&#x27;</span>, x)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        category_idx = topi[<span class="number">0</span>][i]</span><br><span class="line">        category = categorys[category_idx]</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\t\t %s&#x27;</span> % category)</span><br></pre></td></tr></table></figure>

<h4 id="预测示例"><a href="#预测示例" class="headerlink" title="预测示例"></a>预测示例</h4><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对&quot;zhang&quot;的预测结果</span></span><br><span class="line"><span class="attr">x</span>=<span class="string">==&gt; zhang</span></span><br><span class="line">         <span class="attr">Russian</span></span><br><span class="line">         <span class="attr">Chinese</span></span><br><span class="line">         <span class="attr">Vietnamese</span></span><br></pre></td></tr></table></figure>

<h2 id="实现建议与总结"><a href="#实现建议与总结" class="headerlink" title="实现建议与总结"></a>实现建议与总结</h2><ol>
<li><strong>模型选择建议</strong>：<ul>
<li>如果追求快速实现和简单模型：选择RNN</li>
<li>如果追求最佳准确率：选择GRU</li>
<li>如果需要处理更长序列：考虑LSTM</li>
</ul>
</li>
<li><strong>优化方向</strong>：<ul>
<li>尝试不同的隐藏层大小</li>
<li>调整学习率和训练轮次</li>
<li>尝试不同的优化器</li>
<li>增加更多训练数据</li>
</ul>
</li>
<li><strong>关键成功因素</strong>：<ul>
<li>合理的数据预处理（特别是one-hot编码）</li>
<li>适当的模型复杂度选择</li>
<li>充分的训练和调参</li>
</ul>
</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://liamjohnson-w.github.io">Jason</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://liamjohnson-w.github.io/2025/06/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">https://liamjohnson-w.github.io/2025/06/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/AI/">AI</a></div><div class="post_share"><div class="social-share" data-image="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/4939c7a6-d55c-4352-a3bd-5acbd5a45ace.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> Donate</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/06/20/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_AI/" title="数据结构"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/cat1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">数据结构</div></div></a></div><div class="next-post pull-right"><a href="/2025/06/07/github-push%E5%87%BA%E9%94%99/" title="Github之Push问题解决方案"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250607224051237.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Github之Push问题解决方案</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2025/06/03/2025.06.03/" title="OLLAMA"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/image-20250603132906789.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-03</div><div class="title">OLLAMA</div></div></a></div><div><a href="/2025/07/18/Project/" title="Jason Project Demo"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="title">Jason Project Demo</div></div></a></div><div><a href="/2025/07/16/%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/" title="算法公式推导"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-16</div><div class="title">算法公式推导</div></div></a></div><div><a href="/2025/06/22/NLP_Base/" title="NLP自然语言处理"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/%E7%94%9F%E6%88%90%E7%8C%AB%E5%92%AA%E5%9B%BE%E7%89%87%20(2).png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-22</div><div class="title">NLP自然语言处理</div></div></a></div><div><a href="/2025/06/20/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_AI/" title="数据结构"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/cat1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-20</div><div class="title">数据结构</div></div></a></div><div><a href="/2025/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" title="机器学习"><img class="cover" src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/1d5d1a46-65f8-4544-8195-231e2c2da969.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-04</div><div class="title">机器学习</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/tx.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Jason</div><div class="author-info__description">机器都在学习,你有什么理由不学习?</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">231</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">58</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/weiswift/"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/weiswift" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:1265019024@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">网站由Github服务器托管,感谢支持！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.</span> <span class="toc-text">深度学习和机器学习的区别</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9D%A2%E8%AF%95%E9%A2%98-%E2%97%8F%E2%80%99%E2%97%A1%E2%80%99%E2%97%8F"><span class="toc-number">1.1.</span> <span class="toc-text">深度学习和机器学习-面试题(●’◡’●)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.2.</span> <span class="toc-text">深入理解机器学习和深度学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6"><span class="toc-number">2.</span> <span class="toc-text">PyTorch深度学习框架</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">2.1.</span> <span class="toc-text">PyTorch是什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch%E7%BC%96%E7%A8%8B"><span class="toc-number">2.2.</span> <span class="toc-text">PyTorch编程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#PyTorch%E2%BE%A5%E6%80%8E%E4%B9%88%E4%BB%8ECPU%E8%BF%81%E7%A7%BB%E5%88%B0GPU"><span class="toc-number">2.2.1.</span> <span class="toc-text">PyTorch⾥怎么从CPU迁移到GPU?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C"><span class="toc-number">2.3.</span> <span class="toc-text">基础张量操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F"><span class="toc-number">2.3.1.</span> <span class="toc-text">创建张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%92%8Cnumpy%E4%BA%92%E8%BD%AC"><span class="toc-number">2.3.2.</span> <span class="toc-text">张量和numpy互转</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E8%BF%90%E7%AE%97"><span class="toc-number">2.3.3.</span> <span class="toc-text">张量运算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E6%9C%BA%E5%88%B6"><span class="toc-number">2.4.</span> <span class="toc-text">自动求导机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%9F%BA%E7%A1%80-%E5%8F%8D%E5%90%91%E6%9B%B4%E6%96%B0%E6%9D%83%E9%87%8D"><span class="toc-number">2.5.</span> <span class="toc-text">反向传播基础(反向更新权重)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%89%8B%E5%8A%A8%E6%9E%84%E5%BB%BA"><span class="toc-number">2.6.</span> <span class="toc-text">线性回归手动构建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81%EF%BC%9A"><span class="toc-number">2.6.1.</span> <span class="toc-text">参考代码：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92PyTorch%E7%BB%84%E4%BB%B6%E6%9E%84%E5%BB%BA"><span class="toc-number">2.7.</span> <span class="toc-text">线性回归PyTorch组件构建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81"><span class="toc-number">2.7.1.</span> <span class="toc-text">参考代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96-%E6%9D%83%E9%87%8D%E3%80%81%E5%81%8F%E7%BD%AE"><span class="toc-number">2.8.</span> <span class="toc-text">参数初始化(权重、偏置)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0%E7%9B%AE%E7%9A%84"><span class="toc-number">2.8.1.</span> <span class="toc-text">初始化参数目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E5%BC%8F%EF%BC%9A"><span class="toc-number">2.8.2.</span> <span class="toc-text">参数初始化方式：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E5%88%9D%E5%A7%8B%E5%8C%96%E9%80%89%E6%8B%A9"><span class="toc-number">2.8.3.</span> <span class="toc-text">关于初始化选择</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%84%E5%BB%BA"><span class="toc-number">2.9.</span> <span class="toc-text">神经网络构建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81-1"><span class="toc-number">2.9.1.</span> <span class="toc-text">参考代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%A4%84%E7%90%86"><span class="toc-number">2.10.</span> <span class="toc-text">数据加载与处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GPU%E5%8A%A0%E9%80%9F"><span class="toc-number">2.11.</span> <span class="toc-text">GPU加速</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD"><span class="toc-number">2.12.</span> <span class="toc-text">模型保存与加载</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81-2"><span class="toc-number">2.12.1.</span> <span class="toc-text">参考代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7%E5%87%BD%E6%95%B0"><span class="toc-number">2.13.</span> <span class="toc-text">实用工具函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch%E4%BB%A3%E7%A0%81%E5%AE%9E%E4%BE%8B"><span class="toc-number">2.14.</span> <span class="toc-text">PyTorch代码实例</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">3.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">3.1.</span> <span class="toc-text">分类问题损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">3.1.1.</span> <span class="toc-text">多分类问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">3.1.2.</span> <span class="toc-text">二分类问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">3.2.</span> <span class="toc-text">回归问题损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MAE"><span class="toc-number">3.2.1.</span> <span class="toc-text">MAE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MSE"><span class="toc-number">3.2.2.</span> <span class="toc-text">MSE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Smooth"><span class="toc-number">3.2.3.</span> <span class="toc-text">Smooth</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81-3"><span class="toc-number">3.2.4.</span> <span class="toc-text">参考代码</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80"><span class="toc-number">4.</span> <span class="toc-text">神经网络基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">4.1.</span> <span class="toc-text">神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-%E9%9D%A2%E8%AF%95%E9%A2%98-%E2%95%B9%E2%96%BD%E2%95%B9"><span class="toc-number">4.2.</span> <span class="toc-text">激活函数-面试题(*╹▽╹*)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95"><span class="toc-number">4.2.1.</span> <span class="toc-text">激活函数的选择方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93"><span class="toc-number">4.2.2.</span> <span class="toc-text">激活函数总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.3.</span> <span class="toc-text">神经网络模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MP%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.3.1.</span> <span class="toc-text">MP神经元模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">4.3.2.</span> <span class="toc-text">单层神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%A4%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">4.3.3.</span> <span class="toc-text">两层神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%80%83%E8%99%91%E5%81%8F%E7%BD%AE%E8%8A%82%E7%82%B9%E7%9A%84%E4%B8%A4%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">4.3.4.</span> <span class="toc-text">考虑偏置节点的两层神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">4.3.5.</span> <span class="toc-text">多层神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A2%9E%E5%8A%A0%E6%9B%B4%E5%A4%9A%E7%9A%84%E5%B1%82%E6%AC%A1%E6%9C%89%E4%BB%80%E4%B9%88%E5%A5%BD%E5%A4%84%EF%BC%9F"><span class="toc-number">4.3.6.</span> <span class="toc-text">增加更多的层次有什么好处？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">5.</span> <span class="toc-text">BP神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%8E%9F%E7%90%86"><span class="toc-number">5.1.</span> <span class="toc-text">反向传播原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-1"><span class="toc-number">5.2.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-number">5.2.1.</span> <span class="toc-text">梯度学习算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%85%AC%E5%BC%8F"><span class="toc-number">5.2.2.</span> <span class="toc-text">梯度下降公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A8%E9%87%8F%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">5.2.3.</span> <span class="toc-text">动量优化方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">5.2.4.</span> <span class="toc-text">代码实现神经网络</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN"><span class="toc-number">6.</span> <span class="toc-text">经典神经网络CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88CNN%E4%B8%80%E8%88%AC%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%EF%BC%9F"><span class="toc-number">6.1.</span> <span class="toc-text">为什么CNN一般用于图像处理？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CNN%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="toc-number">6.1.1.</span> <span class="toc-text">CNN的优势</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84%E5%9B%BE"><span class="toc-number">6.2.</span> <span class="toc-text">CNN层级结构图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82"><span class="toc-number">6.3.</span> <span class="toc-text">输入层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%AD%90%EF%B8%8F%E2%AD%90%EF%B8%8F%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97%E5%B1%82-conv-%E2%AD%90%EF%B8%8F%E2%AD%90%EF%B8%8F"><span class="toc-number">6.4.</span> <span class="toc-text">⭐️⭐️卷积计算层(conv)⭐️⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97"><span class="toc-number">6.4.1.</span> <span class="toc-text">卷积计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Padding"><span class="toc-number">6.4.2.</span> <span class="toc-text">Padding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stride"><span class="toc-number">6.4.3.</span> <span class="toc-text">Stride</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82%E2%AD%90%EF%B8%8F"><span class="toc-number">6.5.</span> <span class="toc-text">池化层⭐️</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96%EF%BC%88Max-Pooling%EF%BC%89"><span class="toc-number">6.5.1.</span> <span class="toc-text">最大池化（Max Pooling）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%B3%E5%9D%87%E6%B1%A0%E5%8C%96%EF%BC%88Average-Pooling%EF%BC%89"><span class="toc-number">6.5.2.</span> <span class="toc-text">平均池化（Average Pooling）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E2%AD%90%EF%B8%8F"><span class="toc-number">6.6.</span> <span class="toc-text">全连接层⭐️</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Python%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">6.7.</span> <span class="toc-text">Python实现卷积神经网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN%E5%8F%8A%E5%8F%98%E4%BD%93"><span class="toc-number">7.</span> <span class="toc-text">循环神经网络RNN及变体</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#RNN%E8%AF%9E%E7%94%9F%E8%83%8C%E6%99%AF%EF%BC%88%E6%88%91%E5%90%83%E6%9F%A0%E6%AA%AC%EF%BC%89"><span class="toc-number">7.1.</span> <span class="toc-text">RNN诞生背景（我吃柠檬）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RNN%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">7.2.</span> <span class="toc-text">RNN网络结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RNN%E5%8E%9F%E7%90%86"><span class="toc-number">7.3.</span> <span class="toc-text">RNN原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E6%96%B9%E7%A8%8B"><span class="toc-number">7.3.1.</span> <span class="toc-text">输出方程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8A%B6%E6%80%81%E6%96%B9%E7%A8%8B"><span class="toc-number">7.3.2.</span> <span class="toc-text">状态方程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E9%97%AE%E9%A2%98%EF%BC%9A%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E7%88%86%E7%82%B8"><span class="toc-number">7.3.3.</span> <span class="toc-text">梯度问题：梯度消失和爆炸</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PyTorch%E6%90%AD%E5%BB%BARNN"><span class="toc-number">7.3.4.</span> <span class="toc-text">PyTorch搭建RNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%AD%90%EF%B8%8F%E2%AD%90%EF%B8%8FRNN-API%E2%AD%90%EF%B8%8F%E2%AD%90%EF%B8%8F"><span class="toc-number">7.3.5.</span> <span class="toc-text">⭐️⭐️RNN API⭐️⭐️</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">7.3.6.</span> <span class="toc-text">RNN模型的优缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTM"><span class="toc-number">7.4.</span> <span class="toc-text">LSTM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%8E%E6%A0%B7%E6%8E%A7%E5%88%B6%E9%95%BF%E6%9C%9F%E7%8A%B6%E6%80%81"><span class="toc-number">7.4.1.</span> <span class="toc-text">怎样控制长期状态</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E6%8E%A7%E5%88%B6%EF%BC%9F"><span class="toc-number">7.4.2.</span> <span class="toc-text">如何进行控制？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM-%E7%9A%84%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97"><span class="toc-number">7.4.3.</span> <span class="toc-text">LSTM 的前向计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%81%97%E5%BF%98%E9%97%A8%EF%BC%88Forget-Gate%EF%BC%89"><span class="toc-number">7.4.4.</span> <span class="toc-text">遗忘门（Forget Gate）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E9%97%A8%EF%BC%88Input-Gate%EF%BC%89"><span class="toc-number">7.4.5.</span> <span class="toc-text">输入门（Input Gate）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E9%97%A8%EF%BC%88Output-Gate%EF%BC%89"><span class="toc-number">7.4.6.</span> <span class="toc-text">输出门（Output Gate）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8A%B6%E6%80%81%E6%9B%B4%E6%96%B0"><span class="toc-number">7.4.7.</span> <span class="toc-text">状态更新</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%AD%90%EF%B8%8F%E2%AD%90%EF%B8%8FLSTM-API%E2%AD%90%EF%B8%8F%E2%AD%90%EF%B8%8F"><span class="toc-number">7.4.8.</span> <span class="toc-text">⭐️⭐️LSTM API⭐️⭐️</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GRU"><span class="toc-number">7.5.</span> <span class="toc-text">GRU</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%AD%90%EF%B8%8F%E9%87%8D%E7%BD%AE%E9%97%A8%E2%AD%90%EF%B8%8F"><span class="toc-number">7.5.1.</span> <span class="toc-text">⭐️重置门⭐️</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%AD%90%EF%B8%8F%E6%9B%B4%E6%96%B0%E9%97%A8%E2%AD%90%EF%B8%8F"><span class="toc-number">7.5.2.</span> <span class="toc-text">⭐️更新门⭐️</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%80%E4%BD%BF%E7%94%A8%E7%9A%84%E5%85%AC%E5%BC%8F-%E6%95%B4%E5%90%88"><span class="toc-number">7.5.3.</span> <span class="toc-text">所使用的公式-整合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GRU-API"><span class="toc-number">7.5.4.</span> <span class="toc-text">GRU API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-number">7.5.5.</span> <span class="toc-text">注意事项</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%8E0-1%E5%AE%9E%E7%8E%B0RNN%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8%E6%A1%88%E4%BE%8B"><span class="toc-number">8.</span> <span class="toc-text">从0-1实现RNN人名分类器案例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96"><span class="toc-number">8.0.1.</span> <span class="toc-text">数据获取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%89%B9%E7%82%B9"><span class="toc-number">8.0.2.</span> <span class="toc-text">数据特点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E0%E5%88%B01%E5%AE%9E%E7%8E%B0%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">8.1.</span> <span class="toc-text">从0到1实现人名分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B"><span class="toc-number">8.1.1.</span> <span class="toc-text">数据处理流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E5%8C%85"><span class="toc-number">8.1.2.</span> <span class="toc-text">导包</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E5%88%B0%E5%86%85%E5%AD%98"><span class="toc-number">8.1.2.1.</span> <span class="toc-text">读取数据到内存</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%84%E5%BB%BADataset%E7%B1%BB"><span class="toc-number">8.1.2.2.</span> <span class="toc-text">构建Dataset类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%84%E5%BB%BADataLoader"><span class="toc-number">8.1.2.3.</span> <span class="toc-text">构建DataLoader</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA"><span class="toc-number">8.1.3.</span> <span class="toc-text">模型构建</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#RNN%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">8.1.3.1.</span> <span class="toc-text">RNN模型实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LSTM%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">8.1.3.2.</span> <span class="toc-text">LSTM模型实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GRU%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">8.1.3.3.</span> <span class="toc-text">GRU模型实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">8.1.4.</span> <span class="toc-text">模型训练</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B"><span class="toc-number">8.1.4.1.</span> <span class="toc-text">训练流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%95%88%E6%9E%9C%E5%AF%B9%E6%AF%94"><span class="toc-number">8.1.4.2.</span> <span class="toc-text">模型效果对比</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B"><span class="toc-number">8.1.5.</span> <span class="toc-text">模型预测</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B%E5%87%BD%E6%95%B0%E5%AE%9E%E7%8E%B0"><span class="toc-number">8.1.5.1.</span> <span class="toc-text">预测函数实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B%E7%A4%BA%E4%BE%8B"><span class="toc-number">8.1.5.2.</span> <span class="toc-text">预测示例</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E5%BB%BA%E8%AE%AE%E4%B8%8E%E6%80%BB%E7%BB%93"><span class="toc-number">8.2.</span> <span class="toc-text">实现建议与总结</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/07/18/Project/" title="Jason Project Demo"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Jason Project Demo"/></a><div class="content"><a class="title" href="/2025/07/18/Project/" title="Jason Project Demo">Jason Project Demo</a><time datetime="2025-07-17T16:00:00.000Z" title="Created 2025-07-18 00:00:00">2025-07-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/16/%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/" title="算法公式推导"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="算法公式推导"/></a><div class="content"><a class="title" href="/2025/07/16/%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/" title="算法公式推导">算法公式推导</a><time datetime="2025-07-15T16:00:00.000Z" title="Created 2025-07-16 00:00:00">2025-07-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/29/InterviewQuestions/" title="Jason Interview Note"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen121.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Jason Interview Note"/></a><div class="content"><a class="title" href="/2025/06/29/InterviewQuestions/" title="Jason Interview Note">Jason Interview Note</a><time datetime="2025-06-28T16:00:00.000Z" title="Created 2025-06-29 00:00:00">2025-06-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/22/DK67/" title="DK67双模切换"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/%E7%94%9F%E6%88%90%E7%8C%AB%E5%92%AA%E5%9B%BE%E7%89%87.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="DK67双模切换"/></a><div class="content"><a class="title" href="/2025/06/22/DK67/" title="DK67双模切换">DK67双模切换</a><time datetime="2025-06-21T16:00:00.000Z" title="Created 2025-06-22 00:00:00">2025-06-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/22/NLP_Base/" title="NLP自然语言处理"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/%E7%94%9F%E6%88%90%E7%8C%AB%E5%92%AA%E5%9B%BE%E7%89%87%20(2).png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="NLP自然语言处理"/></a><div class="content"><a class="title" href="/2025/06/22/NLP_Base/" title="NLP自然语言处理">NLP自然语言处理</a><time datetime="2025-06-21T16:00:00.000Z" title="Created 2025-06-22 00:00:00">2025-06-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Jason</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Welcome to Jason の <a target="_blank" rel="noopener" href="https://www.cnblogs.com/liam-sliversucks/">Blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? '' : ''

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>