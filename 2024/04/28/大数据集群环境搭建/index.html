<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大数据集群环境搭建 | SilverSucks</title><meta name="author" content="Luck威"><meta name="copyright" content="Luck威"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一、VMware的安装 注意:本文档演示的是12版本的,所有版本都是一样的! First step： 运行“VMware_workstation_full_12.5.2.exe”（或者其他版本）   Second step：引导页面，直接点击下一步  Third step： 同意许可，然后继续点击下"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://liamjohnson-w.github.io/2024/04/28/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大数据集群环境搭建',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-28 21:49:58'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css"> <script src="/live2d-widget/autoload.js"></script><script src="/live2d-widget/autoload.js"> </script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/tx.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">230</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">58</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-gamepad"></i><span> Games</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/mikutap/"><i class="fa-fw fa fa-music"></i><span> MikuTap 初音未来</span></a></li><li><a class="site-page child" href="/starbattle/"><i class="fa-fw fa fa-space-shuttle"></i><span> StartBattle 星际大战</span></a></li><li><a class="site-page child" href="/2048/"><i class="fa-fw fa fa-flag"></i><span> 2048 经典游戏</span></a></li><li><a class="site-page child" href="/battlecity/"><i class="fa-fw fa fa-arrow-circle-left"></i><span> BattleCity 坦克大战</span></a></li><li><a class="site-page child" href="/pacman/"><i class="fa-fw fa fa-bolt"></i><span> PacMan  吃豆人</span></a></li><li><a class="site-page child" href="/tetris/"><i class="fa-fw fa fa-arrows-alt"></i><span> Tetris 俄罗斯方块</span></a></li><li><a class="site-page child" href="/smallcat/"><i class="fa-fw fa fa-paw"></i><span> CatchCat 困住小猫</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-leaf"></i><span> Moments</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/diary/"><i class="fa-fw fas fa-bookmark"></i><span> Diary</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fa fa-hourglass-half"></i><span> Gallery</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-podcast"></i><span> More</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags标签</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About关于</span></a></li><li><a class="site-page child" href="/messageboard/"><i class="fa-fw fas fa-bookmark"></i><span> Messageboard留言板</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://wei-blog.oss-cn-beijing.aliyuncs.com/img/224634-171232839454f0.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="SilverSucks"><span class="site-name">SilverSucks</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-gamepad"></i><span> Games</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/mikutap/"><i class="fa-fw fa fa-music"></i><span> MikuTap 初音未来</span></a></li><li><a class="site-page child" href="/starbattle/"><i class="fa-fw fa fa-space-shuttle"></i><span> StartBattle 星际大战</span></a></li><li><a class="site-page child" href="/2048/"><i class="fa-fw fa fa-flag"></i><span> 2048 经典游戏</span></a></li><li><a class="site-page child" href="/battlecity/"><i class="fa-fw fa fa-arrow-circle-left"></i><span> BattleCity 坦克大战</span></a></li><li><a class="site-page child" href="/pacman/"><i class="fa-fw fa fa-bolt"></i><span> PacMan  吃豆人</span></a></li><li><a class="site-page child" href="/tetris/"><i class="fa-fw fa fa-arrows-alt"></i><span> Tetris 俄罗斯方块</span></a></li><li><a class="site-page child" href="/smallcat/"><i class="fa-fw fa fa-paw"></i><span> CatchCat 困住小猫</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-leaf"></i><span> Moments</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/diary/"><i class="fa-fw fas fa-bookmark"></i><span> Diary</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fa fa-hourglass-half"></i><span> Gallery</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-podcast"></i><span> More</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags标签</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About关于</span></a></li><li><a class="site-page child" href="/messageboard/"><i class="fa-fw fas fa-bookmark"></i><span> Messageboard留言板</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">大数据集群环境搭建</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-04-27T16:00:00.000Z" title="Created 2024-04-28 00:00:00">2024-04-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-04-28T13:49:58.000Z" title="Updated 2024-04-28 21:49:58">2024-04-28</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="大数据集群环境搭建"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="一、VMware的安装"><a href="#一、VMware的安装" class="headerlink" title="一、VMware的安装"></a>一、VMware的安装</h1><blockquote>
<p>注意:本文档演示的是12版本的,所有版本都是一样的!</p>
<p>First step： 运行“VMware_workstation_full_12.5.2.exe”（或者其他版本）</p>
</blockquote>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/feca80d9409d561aee392460a77cd466.png"></p>
<p>Second step：引导页面，直接点击下一步</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/fa410417355a8bc47ef05b11f80fb01b.png"></p>
<p>Third step： 同意许可，然后继续点击下一步</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/c7cae8e92f3f9955fd85f136cc970c25.png"></p>
<p>Forth step： 选择VMware安装位置，然后点击下一步</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/b1cc5eb5b98ab42df7622808a9073bfd.png"></p>
<p>Sixth step： 用户体验设置，建议全部取消勾选，然后点击下一步</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/4324203f637e13a29890c21714c14ada.png"></p>
<p>Fifth step： 根据个人喜好选择，然后点击下一步</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/d2b7564c9ed0e85dd0eb2d614c413633.png"></p>
<p>Seventh step： 点击安装</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/ac45e5ae230fdec4406caeb80f793f5e.png"></p>
<p>Eighth step： 等待安装完成，然后点击许可证</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/d6ce0dce696839ed93155abdbb9b93e3.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/c85b03140603e58930f138ed340d823b.png"></p>
<p>Ninth step： 自己在百度搜索一个vmware12密匙，粘贴复制，然后点击输入</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/ffb8eeda711076fbac59b9f8bd3cc074.png"></p>
<p>Last step： 安装完成</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/image-20240428213935417.png" alt="image-20240428213935417"></p>
<h1 id="二、虚拟机安装操作"><a href="#二、虚拟机安装操作" class="headerlink" title="二、虚拟机安装操作"></a>二、虚拟机安装操作</h1><h2 id="创建虚拟机"><a href="#创建虚拟机" class="headerlink" title="创建虚拟机"></a>创建虚拟机</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/0a416ca4db0271715153f48595aab911.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/7ebef69f0360043862c43bb58157cf05.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/e13d36a819f25e151d46384ca0ba1d4e.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/975b48a227c05524c24bbfc0a3b5fa93.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/2c8a5f0c6d0eca862ab201f7d92ae990.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/0e15c3e6b3e4af4ca6eead15cb01acff.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/a626ab58c90a2d4038c391d54dc6f565.png"></p>
<h2 id="添加linux的iso镜像文件"><a href="#添加linux的iso镜像文件" class="headerlink" title="添加linux的iso镜像文件"></a>添加linux的iso镜像文件</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/ca741e7c6dd6ee65a18d7e7df7d5f51d.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/1f60b4b63bd1277ac0d41ebf4535e93f.png"></p>
<h2 id="开启虚拟机-进行安装"><a href="#开启虚拟机-进行安装" class="headerlink" title="开启虚拟机, 进行安装"></a>开启虚拟机, 进行安装</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/59d1166087694f904e3a75f2c318f4bd.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/d5fae39ee477c9363ba3bcfc3d2d503a.png"></p>
<p>正在校验, 可直接选择esc退出, 或者等待一会也是OK的</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/25cbf7231fb0ab26cc2f935fecdbe8d0.png"></p>
<p>直到出现以下界面,开始选择语言:</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/730a3f5d0b1ea5709041acc307dc4180.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/e1a8fffd3633ac22303f02e2f40acbc9.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/cb3631d60a3a86bcf6861f5ca51d3b9d.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/1cb8d198b9060ca4d70fb968de1cf91e.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/b3adb1976dff6f9ef8bd78dcc3808ae8.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/e8e33b99862d81a055b2d10853cc0f9e.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/a65cdc900c833295c596b0bc54ddc0d2.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/69a385044de33649dcf2e7d75ff702cc.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/aa82f21de7811ac1b88d4c1b77bf72a0.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/c789512b25b0e3f847c77ba80a33aab6.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/daa0426e8967b900268a773bd54ae7be.png"></p>
<p>结束后 , 点击重启, 然后进入系统, 到此, 虚拟机搭建工作结束</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/9617bec9e1d248151ed0ba5d310f4613.png"></p>
<h2 id="登录-进入系统"><a href="#登录-进入系统" class="headerlink" title="登录, 进入系统"></a>登录, 进入系统</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/dc8c140862ff40fa5fd424376d743543.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/image-20240428214257693.png" alt="image-20240428214257693"></p>
<h1 id="三、SecureCRT使用"><a href="#三、SecureCRT使用" class="headerlink" title="三、SecureCRT使用"></a>三、SecureCRT使用</h1><h2 id="CRT安装"><a href="#CRT安装" class="headerlink" title="CRT安装"></a>CRT安装</h2><ul>
<li><p>步骤1：安装“scrt_sfx731-x86.exe”</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/1ea6e04cfe010fefd37d7edaf0e9d37e.png"></p>
</li>
<li><p>步骤2：欢迎页面</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/e79f26702d1d4d87b09e9181d499a36d.png"></p>
</li>
<li><p>步骤3：如果是64位操作系统，存在此提示。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/b5c20b58b85117249eb6b84745a15a37.png"></p>
</li>
<li><p>步骤4：同意许可</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/e026d8d81a436101736ac9a41042897d.png"></p>
</li>
<li><p>步骤5：选择配置文件是否共享（默认）</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/a3dacfc787c4e6e84ac08e9292770cd5.png"></p>
</li>
<li><p>步骤6：安装类型，自定义</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/57284bbcb7021de093e29f88f01a313c.png"></p>
</li>
<li><p>步骤7：选择安装路径</p>
</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/51ed63642b766b8fe2cfca0e84ff9d18.png"></p>
<ul>
<li><p>步骤8：快捷方式（默认）</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/9690ba9251d927844bc89e0b4c37bafd.png"></p>
</li>
<li><p>步骤9：开始安装</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/68bfb8bd6901260e82e626ea8f2cbbee.png"></p>
</li>
<li><p>步骤10：完成</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/637272729f3cca5af8f75f7c2c927af5.png"></p>
</li>
</ul>
<h2 id="激活"><a href="#激活" class="headerlink" title="激活"></a>激活</h2><ul>
<li>步骤1：将对应的激活程序拷贝到安装目录下</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/377db3ce091cab904e2a0555ef4bc667.png"></p>
<ul>
<li>步骤2：以“管理员”运行“SecureCRT v7.0注册机.exe”激活程序，打补丁</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/e68a14eff270891be9156a3db9bb6586.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/43570606cd5b6d75138aee4d1a86c71f.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/b543ab4349d3d8d14b738da1442b3cdd.png"></p>
<ul>
<li>步骤3：生成序列号</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/366f291b103bd5514b3e7e057a8f3e39.png"></p>
<ul>
<li>步骤4：运行程序，“SecureCRT 7.3”，并输入激活码</li>
</ul>
<p>1）不输入任何内容，下一步</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/37c061dcc211e2236e91c5c41bce985a.png"></p>
<p>2）点击输入详情选项</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/78e2646c588a835c1bd957f5299326be.png"></p>
<p>3）输入详细内容</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/d941fdc7899bcd3e7bb5e7ab66bc7ad9.png"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/ac7e2b85d4a197b268608981c8b279d9.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/a03d7894dbecbfbfa4f9ea4d20e4b195.png"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/568fabce5d3ebd5da0c7bf0129acdd56.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/9c39228e082af56a01729a47abf8d56c.png"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/ce99693d63323bbadbe6a36d2400454b.png"></p>
<h2 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h2><ul>
<li>步骤1：登录linux成功之后，输入“ifconfig”查询ip地址</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/f0b8bf281d72677dc9ba33c08119c8c8.png"></p>
<ul>
<li><p>步骤2：运行“SecureCRT.exe”进行连接</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/fbb1b0f7a580c45309a3ee03614a5b07.png"></p>
</li>
<li><p>步骤3：保存当次连接</p>
</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/f3092864cf32f794f071762ea6edfff9.png"></p>
<ul>
<li><p>步骤4：输入密码并连接</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/043ca379adb7a5cb848a1f221451e2f2.png"></p>
</li>
</ul>
<h2 id="常见设置"><a href="#常见设置" class="headerlink" title="常见设置"></a>常见设置</h2><ul>
<li><p>设置操作窗口“背景黑色”。运行“SecureCRT 7.3”</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/e3d84dfd72f8dcd37068cd0c1499662e.png"></p>
<p>修改后结果</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/b4dde5f8fee53791157579e45fbfc77e.png"></p>
</li>
<li><p>设置操作窗口“字体”和“字符集”</p>
</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/image-20240428214348102.png" alt="image-20240428214348102"></p>
<h1 id="四、虚拟机卸载"><a href="#四、虚拟机卸载" class="headerlink" title="四、虚拟机卸载"></a>四、虚拟机卸载</h1><h2 id="安装cclear-切换语言"><a href="#安装cclear-切换语言" class="headerlink" title="安装cclear:切换语言"></a>安装cclear:切换语言</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/b1d034dc214ddbbe3dafc75ffa7de2db.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/63887c677db300362ae65e8956766f2c.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/ab7c93c1cfa7e23b5f7cc1e5e5196c55.png"></p>
<h2 id="关闭VMware所有的服务"><a href="#关闭VMware所有的服务" class="headerlink" title="关闭VMware所有的服务"></a>关闭VMware所有的服务</h2><h3 id="打开服务-win-r-输入-services-msc"><a href="#打开服务-win-r-输入-services-msc" class="headerlink" title="打开服务:win+r : 输入 services.msc"></a>打开服务:win+r : 输入 services.msc</h3><h3 id="关闭所有的VMware的服务"><a href="#关闭所有的VMware的服务" class="headerlink" title="关闭所有的VMware的服务"></a>关闭所有的VMware的服务</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/a606e79d76c6db9b5d2d22a6c1ffc5f5.png"></p>
<h2 id="打开卸载程序-卸载VMware"><a href="#打开卸载程序-卸载VMware" class="headerlink" title="打开卸载程序, 卸载VMware"></a>打开卸载程序, 卸载VMware</h2><p>控制面板–&gt; 卸载程序—&gt;删除VMware</p>
<h2 id="使用cclear清除注册表"><a href="#使用cclear清除注册表" class="headerlink" title="使用cclear清除注册表"></a>使用cclear清除注册表</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/52ab043584371472da08d6382287e55f.png"></p>
<h2 id="打开服务-services-msc"><a href="#打开服务-services-msc" class="headerlink" title="打开服务: services.msc"></a>打开服务: services.msc</h2><h3 id="查看VMware服务有没有"><a href="#查看VMware服务有没有" class="headerlink" title="查看VMware服务有没有:"></a>查看VMware服务有没有:</h3><h3 id="如果有"><a href="#如果有" class="headerlink" title="如果有:"></a>如果有:</h3><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/c1648de63f1ea78efcc55b785ba4ed92.png"></p>
<h4 id="查看服务的名称"><a href="#查看服务的名称" class="headerlink" title="查看服务的名称:"></a>查看服务的名称:</h4><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/c6de3f8e98ca78679a29a37a8620b51a.png"></p>
<h4 id="复制服务名称-打开cmd-管理员打开"><a href="#复制服务名称-打开cmd-管理员打开" class="headerlink" title="复制服务名称:打开cmd(管理员打开)"></a>复制服务名称:打开cmd(管理员打开)</h4><p>输入名称: sc delete 服务名称</p>
<p>注意: 需要删除所有的vmware服务</p>
<p>此时: 建议再次运行cclear: 查看有没有新的注册表出现: 如果有, 继续干掉</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/image-20240428214430660.png" alt="image-20240428214430660"></p>
<h1 id="五、大数据环境配置统一"><a href="#五、大数据环境配置统一" class="headerlink" title="五、大数据环境配置统一"></a>五、大数据环境配置统一</h1><h2 id="三台虚拟机创建"><a href="#三台虚拟机创建" class="headerlink" title="三台虚拟机创建"></a>三台虚拟机创建</h2><p><strong>第一种方式：通过iso镜像文件来进行安装(不推荐)</strong></p>
<p><strong>第二种方式：直接复制安装好的虚拟机文件（强烈推荐）</strong></p>
<p>在课程资料里边已经提供了一个安装好的虚拟机node1（注意，为了大家以后环境的统一，尽量使用课程资料中提供的已经安装好的虚拟机!!!!!!），我们需要根据这个虚拟机克隆出另外两台虚拟机出来,注意,步骤如下:</p>
<ol>
<li>使用VMware加载资料中虚拟机node1</li>
</ol>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/aca3afa814d7022bee80064ec6a13945.png"></p>
<p>2）克隆第二台虚拟机，注意克隆虚拟机的时候，虚拟机必须是关闭状态</p>
<p>右键点击node1</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/eba2f1e64e8dedbfa90d81166cecfc04.png"></p>
<p>下一步</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/207fbf8bc2b8f12371f20a61044f3518.png"></p>
<p>下一步</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/b00d3034ef9b67511676207014523190.png"></p>
<p>创建完整克隆，下一步</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/37e338957d502bb4b411efc8d4dc95fa.png"></p>
<p>指定虚拟机名字和存放位置,三台虚拟机的存放路径尽量在一起，不在一起也没关系</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/89afcb476f6ffbfcad4a640be351c9bc.png"></p>
<p>等待克隆完毕</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/7292231b38cde5320ea1ced22573a88d.png"></p>
<p>关闭</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/6c681e3e6d150314ff5ff9a518ef4b11.png"></p>
<ol>
<li>克隆第三台虚拟机，通过node1虚拟机克隆，克隆方式是一样的，注意修改虚拟机的名称和存放位置。</li>
</ol>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/67a23fbb269ebcc5b0066dabb2db017f.png"></p>
<h2 id="设置三台虚拟机的内存和CPU核数设置"><a href="#设置三台虚拟机的内存和CPU核数设置" class="headerlink" title="设置三台虚拟机的内存和CPU核数设置"></a>设置三台虚拟机的内存和CPU核数设置</h2><p>三台虚拟机再加上windows本身, 需要同时运行4台机器, 所以在分配的时候,, 每台虚拟机的内存为: 总内存 ÷ 4，比如电脑总内存为16G,则每台虚拟机内存为4G。</p>
<p>下面是以node1为例对内存进行配置:</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/e9413ea374b62942ccbb9e7b93a2c7e4.png"></p>
<p>CPU核数, 建议每个服务器设置为2核即可, 保证能够更加流畅运行</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/e96d4bc3919f75d4908cd79f27f68625.png"></p>
<h2 id="配置MAC地址"><a href="#配置MAC地址" class="headerlink" title="配置MAC地址"></a>配置MAC地址</h2><p>node2和node3都是从node1克隆过来的，他们的MAC地址都一样，所以需要让node2和node3重新生成MAC地址，生成方式如下:</p>
<p><strong>1、配置node2的MAC地址</strong></p>
<p>1）使用VMware打开node2</p>
<p>2）右键点击node2，选择设置</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/0cf7a8317e70a553db36f01ab9b139aa.png"></p>
<p>3)生成新的MAC地址</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/258c4f62b30361f364d52f80e3253e37.png"></p>
<ol>
<li><strong>配置node3的MAC地址</strong></li>
</ol>
<p><strong>node3的配置方式和node2相同，不再重复</strong></p>
<h2 id="配置IP地址"><a href="#配置IP地址" class="headerlink" title="配置IP地址"></a>配置IP地址</h2><p>三台虚拟机的IP地址配置如下：</p>
<p>node1: 192.168.88.161</p>
<p>node2 192.168.88.162</p>
<p>node3: 192.168.88.163</p>
<p><strong>1：配置node1主机IP</strong></p>
<ol>
<li>修改ip配置文件,设置IP地址</li>
</ol>
<table>
<thead>
<tr>
<th><strong>vim</strong> **&#x2F;<strong>etc</strong>&#x2F;<strong>sysconfig</strong>&#x2F;<strong>network-scripts</strong>&#x2F;**ifcfg-ens33</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/75637fd1a843ba212cf6939fdd56d196.png"></p>
<table>
<thead>
<tr>
<th>TYPE&#x3D;”Ethernet” PROXY_METHOD&#x3D;”none” BROWSER_ONLY&#x3D;”no” BOOTPROTO&#x3D;”static” DEFROUTE&#x3D;”yes” IPV4_FAILURE_FATAL&#x3D;”no” IPV6INIT&#x3D;”yes” IPV6_AUTOCONF&#x3D;”yes” IPV6_DEFROUTE&#x3D;”yes” IPV6_FAILURE_FATAL&#x3D;”no” IPV6_ADDR_GEN_MODE&#x3D;”stable-privacy” NAME&#x3D;”ens33” UUID&#x3D;”dfd8991d-799e-46b2-aaf0-ed2c95098d58” DEVICE&#x3D;”ens33” ONBOOT&#x3D;”yes” IPADDR&#x3D;”192.168.88.161” PREFIX&#x3D;”24” GATEWAY&#x3D;”192.168.88.2” NETMASK&#x3D;”255.255.255.0” DNS1&#x3D;”8.8.8.8” DNS2&#x3D;”114.114.114.114” IPV6_PRIVACY&#x3D;”no”</th>
</tr>
</thead>
</table>
<ol>
<li><strong>重启网络服务</strong></li>
</ol>
<table>
<thead>
<tr>
<th><strong>systemctl restart</strong> network # 重启网络服务</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/6c3674240046800b8fb3fed2b6690739.png"></p>
<ol>
<li><strong>查看ip地址</strong></li>
</ol>
<table>
<thead>
<tr>
<th>ifconfig</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/9059a6fde42fd8f92cbed7a20246215b.png"></p>
<ol>
<li>测试网络连接</li>
</ol>
<table>
<thead>
<tr>
<th><strong>ping</strong> <a target="_blank" rel="noopener" href="http://www.baidu.com/">www.baidu.com</a></th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/a5b327d42bf7e7c26696ebf22a7630eb.png"></p>
<p><strong>2：配置node2主机IP</strong></p>
<ol>
<li>修改ip配置文件,设置IP地址</li>
</ol>
<table>
<thead>
<tr>
<th><strong>vim</strong> **&#x2F;<strong>etc</strong>&#x2F;<strong>sysconfig</strong>&#x2F;<strong>network-scripts</strong>&#x2F;**ifcfg-ens33</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/0e4ea40af8b4099e9c4dedff59777d46.png"></p>
<table>
<thead>
<tr>
<th>TYPE&#x3D;”Ethernet” PROXY_METHOD&#x3D;”none” BROWSER_ONLY&#x3D;”no” BOOTPROTO&#x3D;”static” DEFROUTE&#x3D;”yes” IPV4_FAILURE_FATAL&#x3D;”no” IPV6INIT&#x3D;”yes” IPV6_AUTOCONF&#x3D;”yes” IPV6_DEFROUTE&#x3D;”yes” IPV6_FAILURE_FATAL&#x3D;”no” IPV6_ADDR_GEN_MODE&#x3D;”stable-privacy” NAME&#x3D;”ens33” UUID&#x3D;”dfd8991d-799e-46b2-aaf0-ed2c95098d58” DEVICE&#x3D;”ens33” ONBOOT&#x3D;”yes” IPADDR&#x3D;”192.168.88.162” PREFIX&#x3D;”24” GATEWAY&#x3D;”192.168.88.2” NETMASK&#x3D;”255.255.255.0” DNS1&#x3D;”8.8.8.8” DNS2&#x3D;”114.114.114.114” IPV6_PRIVACY&#x3D;”no”</th>
</tr>
</thead>
</table>
<ol>
<li><strong>重启网络服务</strong></li>
</ol>
<table>
<thead>
<tr>
<th><strong>systemctl restart</strong> network # 重启网络服务</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/6c3674240046800b8fb3fed2b6690739.png"></p>
<ol>
<li><strong>查看ip地址</strong></li>
</ol>
<table>
<thead>
<tr>
<th>ifconfig</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/1d4e246864bcc308b1cc8dbbe643c470.png"></p>
<ol>
<li>测试网络连接</li>
</ol>
<table>
<thead>
<tr>
<th><strong>ping</strong> <a target="_blank" rel="noopener" href="http://www.baidu.com/">www.baidu.com</a></th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/4b683ab9d7d051ccd5e7872c6e552f55.png"></p>
<p><strong>2：配置node3主机IP</strong></p>
<p><strong>node3主机IP的配置方式和node2一样，将其IP地址设置为:192.168.88.163，在这里不再重复。</strong></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/ede0082c504daf4a67fd6fa671537d4d.png"></p>
<table>
<thead>
<tr>
<th>TYPE&#x3D;”Ethernet” PROXY_METHOD&#x3D;”none” BROWSER_ONLY&#x3D;”no” BOOTPROTO&#x3D;”static” DEFROUTE&#x3D;”yes” IPV4_FAILURE_FATAL&#x3D;”no” IPV6INIT&#x3D;”yes” IPV6_AUTOCONF&#x3D;”yes” IPV6_DEFROUTE&#x3D;”yes” IPV6_FAILURE_FATAL&#x3D;”no” IPV6_ADDR_GEN_MODE&#x3D;”stable-privacy” NAME&#x3D;”ens33” UUID&#x3D;”dfd8991d-799e-46b2-aaf0-ed2c95098d58” DEVICE&#x3D;”ens33” ONBOOT&#x3D;”yes” IPADDR&#x3D;”192.168.88.163” PREFIX&#x3D;”24” GATEWAY&#x3D;”192.168.88.2” NETMASK&#x3D;”255.255.255.0” DNS1&#x3D;”8.8.8.8” DNS2&#x3D;”114.114.114.114” IPV6_PRIVACY&#x3D;”no”</th>
</tr>
</thead>
</table>
<h2 id="使用CRT连接三台虚拟机"><a href="#使用CRT连接三台虚拟机" class="headerlink" title="使用CRT连接三台虚拟机"></a>使用CRT连接三台虚拟机</h2><p>1、建立连接</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/bf2514fd1a84baca368349b513f92df2.png"></p>
<p>2、参数配置</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/911ade10af8537b6d873b80f250b7cdd.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/2f9e85612949ad48b8b085b162b7545b.png"></p>
<p>3、设置主题，颜色和仿真</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/6c813e06865bdee70b8b350569ad19a0.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/dc7195a411b0a2e6e2d332bdd16ead4f.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/a4a21f2c3f67744a03db4c3946a0e21c.png"></p>
<p>其他两台基本类似</p>
<h2 id="设置主机名和域名映射"><a href="#设置主机名和域名映射" class="headerlink" title="设置主机名和域名映射"></a>设置主机名和域名映射</h2><ol>
<li><strong>配置每台虚拟机主机名:</strong></li>
</ol>
<p>分别编辑每台虚拟机的hostname文件，直接填写主机名，保存退出即可。</p>
<table>
<thead>
<tr>
<th>vim &#x2F;etc&#x2F;hostname</th>
</tr>
</thead>
</table>
<p>第一台主机主机名为:node1.itcast.cn</p>
<p>第二台主机主机名为: node2.itcast.cn</p>
<p>第三台主机主机名为: node3.itcast.cn</p>
<ol>
<li><strong>配置每台虚拟机域名映射</strong></li>
</ol>
<p>分别编辑每台虚拟机的hosts文件，在原有内容的基础上，填下以下内容:</p>
<p>注意：不要修改文件原来的内容，三台虚拟机的配置内容都一样。</p>
<table>
<thead>
<tr>
<th>vim &#x2F;etc&#x2F;hosts</th>
</tr>
</thead>
</table>
<table>
<thead>
<tr>
<th>192.168.88.161 node1 node1.itcast.cn 192.168.88.162 node2 node2.itcast.cn 192.168.88.163 node3 node3.itcast.cn</th>
</tr>
</thead>
</table>
<p>配置后效果如下:</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/5d0b16ea01f49aff6f253c408b2c750e.png"></p>
<h2 id="关闭三台虚拟机的防火墙和Selinux"><a href="#关闭三台虚拟机的防火墙和Selinux" class="headerlink" title="关闭三台虚拟机的防火墙和Selinux"></a>关闭三台虚拟机的防火墙和Selinux</h2><ol>
<li><strong>关闭每台虚拟机的防火墙</strong></li>
</ol>
<p>在每台虚拟机上分别执行以下指令:</p>
<table>
<thead>
<tr>
<th><strong>systemctl</strong> stop firewalld.service #停止firewall <strong>systemctl</strong> disable firewalld.service #禁止firewall开机启动</th>
</tr>
</thead>
</table>
<p>关闭之后,查看防火墙状态:</p>
<table>
<thead>
<tr>
<th><strong>systemctl</strong> status firewalld.service</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/e0eda1874a8fb0482cb2f57e46191f41.png"></p>
<ol>
<li><strong>关闭每台虚拟机的Selinux</strong></li>
</ol>
<p><strong>1）什么是SELinux ?</strong></p>
<p>1)SELinux是Linux的一种安全子系统</p>
<p>2)Linux中的权限管理是针对于文件的, 而不是针对进程的, 也就是说, 如果root启动了某个进程, 则这个进程可以操作任何一个文件。</p>
<p>3）SELinux在Linux的文件权限之外, 增加了对进程的限制, 进程只能在进程允许的范围内操作资源</p>
<p><strong>2）为什么要关闭SELinux</strong></p>
<p>如果开启了SELinux, 需要做非常复杂的配置, 才能正常使用系统, 在学习阶段, 在非生产环境, 一般不使用SELinux</p>
<p>SELinux的工作模式：</p>
<p><strong>enforcing 强制模式</strong></p>
<p><strong>permissive 宽容模式</strong></p>
<p><strong>disabled 关闭</strong></p>
<ol>
<li><strong>关闭SELinux方式</strong></li>
</ol>
<p>编辑每台虚拟机的Selinux的配置文件</p>
<table>
<thead>
<tr>
<th><strong>vim</strong> **&#x2F;<strong>etc</strong>&#x2F;<strong>selinux</strong>&#x2F;**config</th>
</tr>
</thead>
</table>
<p><strong>Selinux的默认工作模式是强制模式，配置如下：</strong></p>
<p><strong><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/41f241942de45bf2ee9db7d349b3210e.png"></strong></p>
<p><strong>将Selinux工作模式关闭:</strong></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/46e2ed6dae5272bb5406b73cb47eb0e1.png"></p>
<ol>
<li><strong>分别重启三台虚拟机</strong></li>
</ol>
<table>
<thead>
<tr>
<th>reboot</th>
</tr>
</thead>
</table>
<h2 id="三台机器机器免密码登录"><a href="#三台机器机器免密码登录" class="headerlink" title="三台机器机器免密码登录"></a>三台机器机器免密码登录</h2><p><strong>1、为什么要免密登录</strong></p>
<p>Hadoop 节点众多, 所以一般在主节点启动从节点, 这个时候就需要程序自动在主节点登录到从节点中, 如果不能免密就每次都要输入密码, 非常麻烦。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/8c75288d14771b353ccabeac15f76e11.png"></p>
<p><strong>2、免密 SSH 登录的原理</strong></p>
<p>1. 需要先在 B节点 配置 A节点 的公钥</p>
<p>2. A节点 请求 B节点 要求登录</p>
<p>3. B节点 使用 A节点 的公钥, 加密一段随机文本</p>
<p>4. A节点 使用私钥解密, 并发回给 B节点</p>
<p>5. B节点 验证文本是否正确</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/f1c97157066a9504e9faa2741900da48.png"></p>
<p><strong>3、实现步骤</strong></p>
<p><strong>第一步：三台机器生成公钥与私钥</strong></p>
<p>在三台机器执行以下命令，生成公钥与私钥</p>
<table>
<thead>
<tr>
<th>ssh-keygen -t rsa</th>
</tr>
</thead>
</table>
<p>执行该命令之后，按下三个回车即可，然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥），默认保存在&#x2F;root&#x2F;.ssh目录。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/58542094a8684c610f2fe6131e01ade1.png"></p>
<p><strong>第二步：拷贝公钥到同一台机器</strong></p>
<p>三台机器将拷贝公钥到第一台机器</p>
<p>三台机器执行命令：</p>
<table>
<thead>
<tr>
<th>ssh-copy-id node1</th>
</tr>
</thead>
</table>
<p>在执行该命令之后，需要输入yes和node1的密码:</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/e93ef4a4869d189bab16dab085e84041.png"></p>
<p><strong>第三步:复制第一台机器的认证到其他机器</strong></p>
<p>将第一台机器的公钥拷贝到其他机器上</p>
<p>在第一台机器上指行以下命令</p>
<table>
<thead>
<tr>
<th>scp **&#x2F;<strong>root</strong>&#x2F;.<strong>ssh</strong>&#x2F;<strong>authorized_keys node2</strong>:&#x2F;<strong>root</strong>&#x2F;.**ssh scp **&#x2F;<strong>root</strong>&#x2F;.<strong>ssh</strong>&#x2F;<strong>authorized_keys node3</strong>:&#x2F;<strong>root</strong>&#x2F;.**ssh</th>
</tr>
</thead>
</table>
<p>执行命令时，需要输入yes和对方的密码</p>
<p><strong>第三步:测试SSH免密登录</strong></p>
<p>可以在任何一台主机上通过ssh 主机名命令去远程登录到该主机，输入exit退出登录</p>
<p>例如：在node1机器上，免密登录到node2机器上</p>
<table>
<thead>
<tr>
<th>ssh node1 exit</th>
</tr>
</thead>
</table>
<p>执行效果如下：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/65e50e0bfabc5964a1485801def219bb.png"></p>
<h2 id="三台机器时钟同步"><a href="#三台机器时钟同步" class="headerlink" title="三台机器时钟同步"></a>三台机器时钟同步</h2><p><strong>为什么需要时间同步</strong></p>
<p>因为很多分布式系统是有状态的, 比如说存储一个数据, A节点 记录的时间是1, B节点 记录的时间是2, 就会出问题</p>
<p><strong>时钟同步方式</strong></p>
<p>方式一：通过网络进行时钟同步</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/db0a5493142216dcbede587e612232c6.png"></p>
<p>通过网络连接外网进行时钟同步,必须保证虚拟机连上外网</p>
<ol>
<li>启动定时任务</li>
</ol>
<table>
<thead>
<tr>
<th><strong>crontab</strong> -e</th>
</tr>
</thead>
</table>
<p>随后在输入界面键入以下内容，每隔一分钟就去连接阿里云时间同步服务器，进行时钟同步</p>
<table>
<thead>
<tr>
<th>***&#x2F;**1 <strong>*</strong> <strong>*</strong> <strong>*</strong> <strong>*</strong> <strong>&#x2F;<strong>usr</strong>&#x2F;<strong>sbin</strong>&#x2F;<strong>ntpdate -u ntp4.aliyun.com</strong>;</strong></th>
</tr>
</thead>
</table>
<p>方式二：通过某一台机器进行同步</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/8e0bd1f71ecb470761d3f1fee576dee9.png"></p>
<p>以192.168.88.161这台服务器的时间为准进行时钟同步</p>
<h3 id="第一步-在node1虚拟机安装ntp并启动"><a href="#第一步-在node1虚拟机安装ntp并启动" class="headerlink" title="第一步:在node1虚拟机安装ntp并启动"></a>第一步:在node1虚拟机安装ntp并启动</h3><p>安装ntp服务</p>
<table>
<thead>
<tr>
<th><strong>yum</strong> **-**y install ntp</th>
</tr>
</thead>
</table>
<p>启动ntp服务</p>
<table>
<thead>
<tr>
<th><strong>systemctl</strong> start ntpd</th>
</tr>
</thead>
</table>
<p>设置ntpd的服务开机启动</p>
<table>
<thead>
<tr>
<th>#关闭chrony,Chrony是NTP的另一种实现 <strong>systemctl</strong> disable chrony  #设置ntp服务为开机启动 <strong>systemctl</strong> enable ntpd</th>
</tr>
</thead>
</table>
<h3 id="第二步-编辑node1的-etc-ntp-conf文件"><a href="#第二步-编辑node1的-etc-ntp-conf文件" class="headerlink" title="第二步:编辑node1的&#x2F;etc&#x2F;ntp.conf文件"></a>第二步:编辑node1的&#x2F;etc&#x2F;ntp.conf文件</h3><p>编辑node1机器的&#x2F;etc&#x2F;ntp.conf</p>
<table>
<thead>
<tr>
<th><strong>vim</strong> **&#x2F;<strong>etc</strong>&#x2F;**ntp.conf</th>
</tr>
</thead>
</table>
<p>在文件中添加如下内容(授权192.168.88.0-192.168.88.255网段上的所有机器可以从这台机器上查询和同步时间)</p>
<table>
<thead>
<tr>
<th>restrict 192**.<strong>168</strong>.<strong>88</strong>.<strong>0 mask 255</strong>.<strong>255</strong>.<strong>255</strong>.**0 nomodify notrap</th>
</tr>
</thead>
</table>
<p>注释一下四行内容:(集群在局域网中，不使用其他互联网上的时间)</p>
<table>
<thead>
<tr>
<th>#server 0.centos.pool.ntp.org #server 1.centos.pool.ntp.org #server 2.centos.pool.ntp.org #server 3.centos.pool.ntp.org</th>
</tr>
</thead>
</table>
<p>去掉以下内容的注释，如果没有这两行注释，那就自己添加上(当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步)</p>
<table>
<thead>
<tr>
<th>server 127.127.1.0  fudge 127.127.1.0 stratum 10</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/5c06c658e447d1b0fa2426da43528030.png"></p>
<p>配置以下内容，保证BIOS与系统时间同步</p>
<table>
<thead>
<tr>
<th><strong>vim</strong> **&#x2F;<strong>etc</strong>&#x2F;<strong>sysconfig</strong>&#x2F;**ntpd</th>
</tr>
</thead>
</table>
<p>添加一行内容</p>
<table>
<thead>
<tr>
<th>SYNC_HWLOCK&#x3D;yes</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/65ce91760709dbd24e5b873fff022907.png"></p>
<p>重启ntp服务</p>
<table>
<thead>
<tr>
<th>systemctl restart ntpd</th>
</tr>
</thead>
</table>
<h3 id="第三步：另外两台机器与第一台机器时间同步"><a href="#第三步：另外两台机器与第一台机器时间同步" class="headerlink" title="第三步：另外两台机器与第一台机器时间同步"></a>第三步：另外两台机器与第一台机器时间同步</h3><p>另外两台机器与192.168.88.161进行时钟同步，在node2和node3机器上分别进行以下操作</p>
<table>
<thead>
<tr>
<th>crontab -e</th>
</tr>
</thead>
</table>
<p>添加以下内容:(每隔一分钟与node1进行时钟同步)</p>
<table>
<thead>
<tr>
<th>*&#x2F;1 * * * * &#x2F;usr&#x2F;sbin&#x2F;ntpdate 192.168.88.161</th>
</tr>
</thead>
</table>
<h2 id="虚拟机软件安装"><a href="#虚拟机软件安装" class="headerlink" title="虚拟机软件安装"></a>虚拟机软件安装</h2><h3 id="Linux上安装MySQL-node1"><a href="#Linux上安装MySQL-node1" class="headerlink" title="Linux上安装MySQL(node1)"></a>Linux上安装MySQL(node1)</h3><p>注：需要连接互联网，在线 mysql 的安装包，5.6 的版本大约 86M , 仅需在node1安装即可</p>
<h4 id="MySQL在线下载安装"><a href="#MySQL在线下载安装" class="headerlink" title="MySQL在线下载安装"></a>MySQL在线下载安装</h4><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/545eef4249c4d3a7db390f47e11b0ab6.png"></p>
<ol>
<li>查看 CentOS 是否自带的 MySQL，如果已经安装需要卸载。如果没有找到，则表示没有安装。</li>
</ol>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/ba89377c3681be114ee9f1239ce095f4.png"></p>
<p>如果查询到有内容: 使用 rpm -e –nodeps 要卸载名称</p>
<p>例如:</p>
<p>rpm -e –nodeps mysql-libs-5.1.73-8.el6_8.x86_64</p>
<ol start="2">
<li>在线获取 CentOS7 的 mysql 的 rpm 安装文件，直接执行如下命令：</li>
</ol>
<p>wget <a target="_blank" rel="noopener" href="https://repo.mysql.com//mysql80-community-release-el7-1.noarch.rpm">https://repo.mysql.com//mysql80-community-release-el7-1.noarch.rpm</a></p>
<p>这条语句只是下载了一个 rpm 文件，25K 大小</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/2415078118fa4ce97cf1b7896862777b.png"></p>
<ol start="3">
<li>执行安装命令：</li>
</ol>
<table>
<thead>
<tr>
<th>rpm -ivh mysql80-community-release-el7-1.noarch.rpm</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/7713c305f51fa22f58a80e55eac97498.png"></p>
<ol start="4">
<li>得到两个配置文件，在&#x2F;etc&#x2F;yum.repos.d 目录下。</li>
</ol>
<p>4.1) mysql-community.repo 用于指定下载哪个版本的安装包</p>
<p>4.2) mysql-community-source.repo 用于指定下载哪个版本的源码</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/1bf82744d9a43a048e341ce013613c5c.png"></p>
<ol start="5">
<li>修改 MySQL 的下载配置文件</li>
</ol>
<p>进入目录命令：</p>
<table>
<thead>
<tr>
<th>cd &#x2F;etc&#x2F;yum.repos.d</th>
</tr>
</thead>
</table>
<p>编辑配置文件命令：</p>
<table>
<thead>
<tr>
<th>vim mysql-community.repo</th>
</tr>
</thead>
</table>
<p>我们下载 MySQL 5.6，把 5.6 下的 enabled 设置为 1，表示下载。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/fd5b9e7848131fd6cfe6337e25392e65.png"></p>
<p>把 MySQL8 的下载关闭，将 enabled 设置为 0</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/892f507906b19917ae2b21b9a2117ff7.png"></p>
<p>保存修改退出。</p>
<ol start="6">
<li>在当前目录&#x2F;etc&#x2F;yum.repos.d 下执行下面的命令，开始在线下载：客户端，服务器端，开发的工具包。</li>
</ol>
<p>在线下载安装命令介绍：yum（全称为 Yellow dog Updater, Modified）</p>
<p>作用：用于自动从服务器上下载相应 的软件包，自动安装，并且自动下载它的依赖包。</p>
<p>yum（ Yellow dog Updater, Modified）是一个基于 RPM 包管理，能够从指定的服务器自动下载 RPM 包并且 安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。</p>
<ul>
<li><p>语法说明</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/0b3f03faeb3a286bd386d7e8e3241abf.png"></p>
</li>
</ul>
<p>进行在线下载安装 mysql 命令</p>
<p>yum -y install mysql-community-client mysql-community-server mysql-community-devel</p>
<p>这里需要比较长的时间，要从互联网上下载 86M 左右的内容</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/8c66078e0ae68796b98de58f878dc588.png"></p>
<ol start="7">
<li>使用 rpm 命令，可以查询到 mysql 已经安装好的包</li>
</ol>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/93f1277ee8788aa96843325be60f94a4.png"></p>
<h4 id="启动MySQL服务并登陆"><a href="#启动MySQL服务并登陆" class="headerlink" title="启动MySQL服务并登陆"></a>启动MySQL服务并登陆</h4><ol>
<li>启动 mysql 的服务</li>
</ol>
<table>
<thead>
<tr>
<th>systemctl start mysqld</th>
</tr>
</thead>
</table>
<ol start="2">
<li>将 mysql 加到系统服务中并设置开机启动</li>
</ol>
<table>
<thead>
<tr>
<th>systemctl enable mysqld</th>
</tr>
</thead>
</table>
<ol start="3">
<li>登录 mysql，root 用户默认没有密码</li>
</ol>
<table>
<thead>
<tr>
<th>mysql -uroot</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/39715e079639b5c6252e66bad06159e8.png"></p>
<ol start="4">
<li>在 mysql 中修改自己的密码 : 此处设置为 123456</li>
</ol>
<table>
<thead>
<tr>
<th>set password &#x3D; password(‘123456’);</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/6cfb6b9974c6e1cfa8482dd2d4ed2824.png"></p>
<h4 id="设置远程访问权限"><a href="#设置远程访问权限" class="headerlink" title="设置远程访问权限"></a>设置远程访问权限</h4><ol>
<li>开启 mysql 的远程登录权限，默认情况下 mysql 为安全起见，不支持远程登录 mysql，所以需要设置开启，并且刷新权限缓存。远程登录 mysql 的权限登录 mysql 后输入如下命令：</li>
</ol>
<table>
<thead>
<tr>
<th>grant all privileges on *.* to ‘root‘@’%’ identified by ‘123456’;  flush privileges;</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/07424a4deaf692d062a5aad5398abf68.png"></p>
<ol>
<li>确保防火墙已关闭</li>
</ol>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/c58b951e441cc09edb23ed9b965118b0.png"></p>
<h4 id="客户端windows连接linux中mysql"><a href="#客户端windows连接linux中mysql" class="headerlink" title="客户端windows连接linux中mysql"></a>客户端windows连接linux中mysql</h4><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/1367d94bad430690f828bff5c37028fa.png"></p>
<p>到此mysql安装全部结束</p>
<p>注意：如果希望删除卸载 mysql 执行如下命令</p>
<table>
<thead>
<tr>
<th>yum -y remove mysql-community-client mysql-community-server mysql-community-devel</th>
</tr>
</thead>
</table>
<h3 id="JDK安装-所有节点安装"><a href="#JDK安装-所有节点安装" class="headerlink" title="JDK安装(所有节点安装)"></a>JDK安装(所有节点安装)</h3><p>JDK 是个绿色软件，解压并且配置环境变量即可使用, 三个节点都需要安装的</p>
<h4 id="JDK安装步骤"><a href="#JDK安装步骤" class="headerlink" title="JDK安装步骤"></a>JDK安装步骤</h4><ol>
<li>在虚拟机中创建两个目录</li>
</ol>
<table>
<thead>
<tr>
<th>mkdir -p &#x2F;export&#x2F;software 软件包放置的目录 mkdir -p &#x2F;export&#x2F;server 软件安装的目录</th>
</tr>
</thead>
</table>
<ol start="2">
<li><p>进入 &#x2F;export&#x2F;software 目录, 上传jdk的安装包: jdk-8u241-linux-x64.tar.gz</p>
</li>
<li><p>解压压缩包到&#x2F;export&#x2F;server目录下</p>
</li>
</ol>
<table>
<thead>
<tr>
<th>tar -zxvf jdk-8u241-linux-x64.tar.gz -C &#x2F;export&#x2F;server</th>
</tr>
</thead>
</table>
<p>查看解压后的目录,目录中有 jdk1.8.0_144 为 jdk 解压的目录</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/311fc5fca3342f571665041220cea8de.png"></p>
<ol start="4">
<li>配置 jdk 环境变量，打开&#x2F;etc&#x2F;profile 配置文件，将下面配置拷贝进去。export 命令用于将 shell 变量输出为环境变量</li>
</ol>
<table>
<thead>
<tr>
<th>第一步: vi &#x2F;etc&#x2F;profile 第二步: 通过键盘上下键 将光标拉倒最后面 第三步: 然后输入 i, 将一下内容输入即可  #set java environment  JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_65  CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib  PATH&#x3D;$JAVA_HOME&#x2F;bin:$PATH  export JAVA_HOME CLASSPATH PATH  第四步: esc键 然后 :wq 保存退出即可</th>
</tr>
</thead>
</table>
<ol>
<li>重新加载环境变量:</li>
</ol>
<table>
<thead>
<tr>
<th>source &#x2F;etc&#x2F;profile</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/39947a4bd239a829c691b2d4a5d89259.png"></p>
<ol>
<li>配置jdk是否安装成功</li>
</ol>
<table>
<thead>
<tr>
<th>java -version</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/ed3c905e3df93b4a643db8f0f8de3081.png"></p>
<ol start="7">
<li>将jdk分发给node2和node3</li>
</ol>
<table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F; scp -r jdk1.8.0_65&#x2F; node2:$PWD scp -r jdk1.8.0_65&#x2F; node3:$PWD</th>
</tr>
</thead>
</table>
<ol start="8">
<li>在node2和node3配置jdk的环境变量</li>
</ol>
<table>
<thead>
<tr>
<th>scp -r &#x2F;etc&#x2F;profile node2:&#x2F;etc&#x2F; scp -r &#x2F;etc&#x2F;profile node3:&#x2F;etc&#x2F;  分别在node2和node3重新加载环境变量 source &#x2F;etc&#x2F;profile</th>
</tr>
</thead>
</table>
<ol start="9">
<li>测试node2和node3 是否安装成功</li>
</ol>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/e32d9a202c806d8f838b778930a6d204.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/image-20240428214543549.png" alt="image-20240428214543549"></p>
<h1 id="六、Zookeeper集群安装"><a href="#六、Zookeeper集群安装" class="headerlink" title="六、Zookeeper集群安装"></a>六、Zookeeper集群安装</h1><p>Zookeeper集群搭建指的是ZooKeeper分布式模式安装。通常由2n+1台server组成。这是因为为了保证Leader选举（基于Paxos算法的实现）能过得到多数的支持，所以ZooKeeper集群的数量一般为奇数。</p>
<p>Zookeeper运行需要java环境，所以需要提前安装jdk。对于安装leader+follower模式的集群，大致过程如下：</p>
<ul>
<li>配置主机名称到IP地址映射配置</li>
<li>修改ZooKeeper配置文件</li>
<li>远程复制分发安装文件</li>
<li>设置myid</li>
<li>启动ZooKeeper集群</li>
</ul>
<p>如果要想使用Observer模式，可在对应节点的配置文件添加如下配置：</p>
<p>peerType&#x3D;observer</p>
<p>其次，必须在配置文件指定哪些节点被指定为Observer，如：</p>
<p>server.1:node1:2181:3181:observer</p>
<p>这里，我们安装的是leader+follower模式</p>
<table>
<thead>
<tr>
<th>服务器IP</th>
<th>主机名</th>
<th>myid的值</th>
</tr>
</thead>
<tbody><tr>
<td>192.168.88.161</td>
<td>node1</td>
<td>1</td>
</tr>
<tr>
<td>192.168.88.162</td>
<td>node2</td>
<td>2</td>
</tr>
<tr>
<td>192.168.88.163</td>
<td>node3</td>
<td>3</td>
</tr>
</tbody></table>
<h2 id="第一步：下载zookeeeper的压缩包，下载网址如下"><a href="#第一步：下载zookeeeper的压缩包，下载网址如下" class="headerlink" title="第一步：下载zookeeeper的压缩包，下载网址如下"></a>第一步：下载zookeeeper的压缩包，下载网址如下</h2><p><a target="_blank" rel="noopener" href="http://archive.apache.org/dist/zookeeper/">http://archive.apache.org/dist/zookeeper/</a></p>
<p>我们在这个网址下载我们使用的zk版本为3.4.6</p>
<p>下载完成之后，上传到我们的linux的&#x2F;export&#x2F;software路径下准备进行安装</p>
<h2 id="第二步：解压"><a href="#第二步：解压" class="headerlink" title="第二步：解压"></a>第二步：解压</h2><p>在node1主机上，解压zookeeper的压缩包到&#x2F;export&#x2F;server路径下去，然后准备进行安装</p>
<table>
<thead>
<tr>
<th><strong>cd</strong> **&#x2F;<strong>export</strong>&#x2F;**software <strong>tar</strong> **-**zxvf zookeeper.tar.gz -C <strong>&#x2F;export&#x2F;server&#x2F;</strong> cd &#x2F;export&#x2F;server&#x2F; ln -s zookeeper&#x2F; zookeeper</th>
</tr>
</thead>
</table>
<h2 id="第三步：修改配置文件"><a href="#第三步：修改配置文件" class="headerlink" title="第三步：修改配置文件"></a>第三步：修改配置文件</h2><p>在node1主机上，修改配置文件</p>
<table>
<thead>
<tr>
<th><strong>cd</strong> <strong>&#x2F;<strong>export</strong>&#x2F;<strong>server</strong>&#x2F;<strong>zookeeper</strong>&#x2F;<strong>conf</strong>&#x2F;</strong> <strong>cp</strong> zoo_sample.cfg zoo.cfg <strong>mkdir</strong> -p <strong>&#x2F;export&#x2F;server&#x2F;<strong>zookeeper</strong>&#x2F;<strong>zkdatas</strong>&#x2F;</strong> <strong>vim</strong> zoo.cfg</th>
</tr>
</thead>
</table>
<p>修改以下内容</p>
<table>
<thead>
<tr>
<th>#Zookeeper的数据存放目录 dataDir**&#x3D;&#x2F;export&#x2F;server&#x2F;<strong>zookeeper</strong>&#x2F;<strong>zkdatas # 保留多少个快照 autopurge.snapRetainCount</strong>&#x3D;<strong>3 # 日志多少小时清理一次 autopurge.purgeInterval</strong>&#x3D;<strong>1 # 集群中服务器地址 server.1</strong>&#x3D;<strong>node1</strong>:<strong>2888</strong>:<strong>3888 server.2</strong>&#x3D;<strong>node2</strong>:<strong>2888</strong>:<strong>3888 server.3</strong>&#x3D;<strong>node3</strong>:<strong>2888</strong>:**3888</th>
</tr>
</thead>
</table>
<h2 id="第四步：添加myid配置"><a href="#第四步：添加myid配置" class="headerlink" title="第四步：添加myid配置"></a>第四步：添加myid配置</h2><p>在node1主机的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;这个路径下创建一个文件，文件名为myid ,文件内容为1</p>
<table>
<thead>
<tr>
<th><strong>echo</strong> 1 <strong>&gt;</strong> **&#x2F;export&#x2F;server&#x2F;<strong>zookeeper</strong>&#x2F;<strong>zkdatas</strong>&#x2F;**myid</th>
</tr>
</thead>
</table>
<h2 id="第五步：安装包分发并修改myid的值"><a href="#第五步：安装包分发并修改myid的值" class="headerlink" title="第五步：安装包分发并修改myid的值"></a>第五步：安装包分发并修改myid的值</h2><p>在node1主机上，将安装包分发到其他机器</p>
<p>第一台机器上面执行以下两个命令</p>
<table>
<thead>
<tr>
<th>cd <strong>&#x2F;export&#x2F;server&#x2F;</strong> scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.6&#x2F; node2:$PWD  scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.6&#x2F; node3:$PWD</th>
</tr>
</thead>
</table>
<p>第二台机器上建立软连接, 并修改myid的值为2</p>
<table>
<thead>
<tr>
<th><strong>cd &#x2F;export&#x2F;server&#x2F;</strong> <strong>ln -s zookeeper-3.4.6&#x2F; zookeeper</strong>  <strong>echo</strong> 2 <strong>&gt;</strong> **&#x2F;export&#x2F;server&#x2F;<strong>zookeeper</strong>&#x2F;<strong>zkdatas</strong>&#x2F;**myid</th>
</tr>
</thead>
</table>
<p>第三台机器上建立软连接, 并修改myid的值为3</p>
<table>
<thead>
<tr>
<th><strong>cd &#x2F;export&#x2F;server&#x2F;</strong> <strong>ln -s zookeeper-3.4.6&#x2F; zookeeper</strong>  <strong>echo</strong> 3 <strong>&gt;</strong> **&#x2F;export&#x2F;server&#x2F;<strong>zookeeper</strong>&#x2F;<strong>zkdatas</strong>&#x2F;**myid</th>
</tr>
</thead>
</table>
<h2 id="第六步：三台机器启动zookeeper服务"><a href="#第六步：三台机器启动zookeeper服务" class="headerlink" title="第六步：三台机器启动zookeeper服务"></a>第六步：三台机器启动zookeeper服务</h2><p>三台机器分别启动zookeeper服务</p>
<p>这个命令三台机器都要执行</p>
<table>
<thead>
<tr>
<th>**&#x2F;export&#x2F;server&#x2F;<strong>zookeeper</strong>&#x2F;<strong>bin</strong>&#x2F;**zkServer.sh start</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/cca5cbb5f26f645d9bc2936ddfed92fd.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/50f422b74feea602d975e8e603038acb.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/aee6fd5630d78e5c85fc5d230246822c.png"></p>
<p>三台主机分别查看启动状态</p>
<table>
<thead>
<tr>
<th>**&#x2F;export&#x2F;server&#x2F;<strong>zookeeper</strong>&#x2F;<strong>bin</strong>&#x2F;**zkServer.sh status</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/a52770aaaf4e93ee5b7bbb1411360418.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/0001b93befdf151c38f7ac00ffd76024.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/image-20240428214703392.png" alt="image-20240428214703392"></p>
<h1 id="七、hadoop集群搭建"><a href="#七、hadoop集群搭建" class="headerlink" title="七、hadoop集群搭建"></a>七、hadoop集群搭建</h1><h2 id="上传重编译后的hadoop包并解压"><a href="#上传重编译后的hadoop包并解压" class="headerlink" title="上传重编译后的hadoop包并解压"></a>上传重编译后的hadoop包并解压</h2><table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;software&#x2F; rz 上传  解压操作 tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz -C &#x2F;export&#x2F;server&#x2F;  添加软连接: cd &#x2F;export&#x2F;server&#x2F; ln -s hadoop-3.3.0&#x2F; hadoop</th>
</tr>
</thead>
</table>
<h2 id="Hadoop安装包目录结构"><a href="#Hadoop安装包目录结构" class="headerlink" title="Hadoop安装包目录结构"></a>Hadoop安装包目录结构</h2><p>解压hadoop-3.3.0-Centos7-64-with-snappy.tar.gz，目录结构如下：</p>
<p><strong>bin</strong>：Hadoop最基本的管理脚本和使用脚本的目录，这些脚本是sbin目录下管理脚本的基础实现，用户可以直接使用这些脚本管理和使用Hadoop。</p>
<p><strong>etc</strong>：Hadoop配置文件所在的目录，包括core-site,xml、hdfs-site.xml、mapred-site.xml等从Hadoop1.0继承而来的配置文件和yarn-site.xml等Hadoop2.0新增的配置文件。</p>
<p><strong>include</strong>：对外提供的编程库头文件（具体动态库和静态库在lib目录中），这些头文件均是用C++定义的，通常用于C++程序访问HDFS或者编写MapReduce程序。</p>
<p><strong>lib</strong>：该目录包含了Hadoop对外提供的编程动态库和静态库，与include目录中的头文件结合使用。</p>
<p><strong>libexec</strong>：各个服务对用的shell配置文件所在的目录，可用于配置日志输出、启动参数（比如JVM参数）等基本信息。</p>
<p><strong>sbin</strong>：Hadoop管理脚本所在的目录，主要包含HDFS和YARN中各类服务的启动&#x2F;关闭脚本。</p>
<p><strong>share</strong>：Hadoop各个模块编译后的jar包所在的目录，官方自带示例。</p>
<h2 id="Hadoop配置文件修改"><a href="#Hadoop配置文件修改" class="headerlink" title="Hadoop配置文件修改"></a>Hadoop配置文件修改</h2><p>Hadoop安装主要就是配置文件的修改，一般在主节点进行修改，完毕后scp下发给其他各个从节点机器。</p>
<h3 id="hadoop-env-sh"><a href="#hadoop-env-sh" class="headerlink" title="hadoop-env.sh"></a>hadoop-env.sh</h3><p>文件中设置的是Hadoop运行时需要的环境变量。JAVA_HOME是必须设置的，即使我们当前的系统中设置了JAVA_HOME，它也是不认识的，因为Hadoop即使是在本机上执行，它也是把当前的执行环境当成远程服务器。</p>
<table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F; vim hadoop-env.sh  添加: 在54行 export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F;  #文件最后添加: 在第 439行下 export HDFS_NAMENODE_USER&#x3D;root export HDFS_DATANODE_USER&#x3D;root export HDFS_SECONDARYNAMENODE_USER&#x3D;root export YARN_RESOURCEMANAGER_USER&#x3D;root export YARN_NODEMANAGER_USER&#x3D;root</th>
</tr>
</thead>
</table>
<h3 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h3><p>hadoop的核心配置文件，有默认的配置项core-default.xml。</p>
<p>core-default.xml与core-site.xml的功能是一样的，如果在core-site.xml里没有配置的属性，则会自动会获取core-default.xml里的相同属性的值。</p>
<table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F; vim core-site.xml   在文件的configuration的标签中添加以下内容:   &lt;property&gt;  &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;  &lt;value&gt;hdfs:&#x2F;&#x2F;node1.itcast.cn:8020&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;property&gt;  &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;  &lt;value&gt;&#x2F;export&#x2F;data&#x2F;hadoop&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;!– 设置HDFS web UI用户身份 –&gt;  &lt;property&gt;  &lt;name&gt;hadoop.http.staticuser.user&lt;&#x2F;name&gt;  &lt;value&gt;root&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;!– 整合hive –&gt;  &lt;property&gt;  &lt;name&gt;hadoop.proxyuser.root.hosts&lt;&#x2F;name&gt;  &lt;value&gt;*&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;property&gt;  &lt;name&gt;hadoop.proxyuser.root.groups&lt;&#x2F;name&gt;  &lt;value&gt;*&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;</th>
</tr>
</thead>
</table>
<h3 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h3><p>HDFS的核心配置文件，有默认的配置项hdfs-default.xml。</p>
<p>hdfs-default.xml与hdfs-site.xml的功能是一样的，如果在hdfs-site.xml里没有配置的属性，则会自动会获取hdfs-default.xml里的相同属性的值。</p>
<table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F; vim hdfs-site.xml   在文件的configuration的标签中添加以下内容:   &lt;!– 指定secondarynamenode运行位置 –&gt;  &lt;property&gt;  &lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt;  &lt;value&gt;node2.itcast.cn:50090&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;</th>
</tr>
</thead>
</table>
<h3 id="mapred-site-xml"><a href="#mapred-site-xml" class="headerlink" title="mapred-site.xml"></a>mapred-site.xml</h3><p>MapReduce的核心配置文件，有默认的配置项mapred-default.xml。</p>
<p>mapred-default.xml与mapred-site.xml的功能是一样的，如果在mapred-site.xml里没有配置的属性，则会自动会获取mapred-default.xml里的相同属性的值。</p>
<table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F; vim mapred-site.xml   在文件的configuration的标签中添加以下内容:  &lt;property&gt;  &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;  &lt;value&gt;yarn&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;property&gt;  &lt;name&gt;yarn.app.mapreduce.am.env&lt;&#x2F;name&gt;  &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;${HADOOP_HOME}&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;property&gt;  &lt;name&gt;mapreduce.map.env&lt;&#x2F;name&gt;  &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;${HADOOP_HOME}&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;property&gt;  &lt;name&gt;mapreduce.reduce.env&lt;&#x2F;name&gt;  &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;${HADOOP_HOME}&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;</th>
</tr>
</thead>
</table>
<h3 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a>yarn-site.xml</h3><p>YARN的核心配置文件，有默认的配置项yarn-default.xml。</p>
<p>yarn-default.xml与yarn-site.xml的功能是一样的，如果在yarn-site.xml里没有配置的属性，则会自动会获取yarn-default.xml里的相同属性的值。</p>
<table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F; vim yarn-site.xml   在文件的configuration的标签中添加以下内容:   &lt;!– 指定YARN的主角色（ResourceManager）的地址 –&gt;  &lt;property&gt;  &lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt;  &lt;value&gt;node1.itcast.cn&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;!– NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序默认值：”” –&gt;  &lt;property&gt;  &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;  &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;!– 是否将对容器实施物理内存限制 –&gt;  &lt;property&gt;  &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;&#x2F;name&gt;  &lt;value&gt;false&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;!– 是否将对容器实施虚拟内存限制。 –&gt;  &lt;property&gt;  &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;&#x2F;name&gt;  &lt;value&gt;false&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;  &lt;!– 开启日志聚集 –&gt;  &lt;property&gt;  &lt;name&gt;yarn.log-aggregation-enable&lt;&#x2F;name&gt;  &lt;value&gt;true&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;!– 设置yarn历史服务器地址 –&gt;  &lt;property&gt;  &lt;name&gt;yarn.log.server.url&lt;&#x2F;name&gt;  &lt;value&gt;<a target="_blank" rel="noopener" href="http://node1.itcast.cn:19888/jobhistory/logs/">http://node1.itcast.cn:19888/jobhistory/logs\</a>&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;!– 保存的时间7天 –&gt;  &lt;property&gt;  &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;&#x2F;name&gt;  &lt;value&gt;604800&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;</th>
</tr>
</thead>
</table>
<h3 id="workers"><a href="#workers" class="headerlink" title="workers"></a>workers</h3><p>workers文件里面记录的是集群主机名。主要作用是配合一键启动脚本如start-dfs.sh、stop-yarn.sh用来进行集群启动。这时候workers文件里面的主机标记的就是从节点角色所在的机器。</p>
<table>
<thead>
<tr>
<th>vim workers  清空内容后, 添加以下内容: node1.itcast.cn node2.itcast.cn node3.itcast.cn</th>
</tr>
</thead>
</table>
<h2 id="scp同步安装包"><a href="#scp同步安装包" class="headerlink" title="scp同步安装包"></a>scp同步安装包</h2><table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server  scp -r hadoop-3.3.0&#x2F; node2:$PWD  scp -r hadoop-3.3.0&#x2F; node3:$PWD  分发后, 需要在node2和node3上分别创建软连接 cd &#x2F;export&#x2F;server&#x2F; ln -s hadoop-3.3.0&#x2F; hadoop</th>
</tr>
</thead>
</table>
<p>在node1上进行了配置文件的修改，使用scp命令将修改好之后的安装包同步给集群中的其他节点。</p>
<h2 id="Hadoop环境变量"><a href="#Hadoop环境变量" class="headerlink" title="Hadoop环境变量"></a>Hadoop环境变量</h2><p>3台机器都需要配置环境变量文件。</p>
<table>
<thead>
<tr>
<th>vim &#x2F;etc&#x2F;profile  #HADOOP_HOME export HADOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin source &#x2F;etc&#x2F;profile</th>
</tr>
</thead>
</table>
<h2 id="Hadoop集群启动、初体验"><a href="#Hadoop集群启动、初体验" class="headerlink" title="Hadoop集群启动、初体验"></a>Hadoop集群启动、初体验</h2><h3 id="启动方式"><a href="#启动方式" class="headerlink" title="启动方式"></a>启动方式</h3><p>要启动Hadoop集群，需要启动HDFS和YARN两个集群。</p>
<p>注意：<strong>首次启动HDFS时，必须对其进行格式化操作</strong>。本质上是一些清理和准备工作，因为此时的HDFS在物理上还是不存在的。</p>
<p>hadoop namenode -format</p>
<h4 id="单节点逐个启动"><a href="#单节点逐个启动" class="headerlink" title="单节点逐个启动"></a>单节点逐个启动</h4><p>在主节点上使用以下命令启动HDFS NameNode：</p>
<p>$HADOOP_HOME&#x2F;bin&#x2F;hdfs –daemon start namenode</p>
<p>在每个从节点上使用以下命令启动HDFS DataNode：</p>
<p>$HADOOP_HOME&#x2F;bin&#x2F;hdfs –daemon start datanode</p>
<p>在node2上使用以下命令启动HDFS SecondaryNameNode：</p>
<p>$HADOOP_HOME&#x2F;bin&#x2F;hdfs –daemon start secondarynamenode</p>
<p>在主节点上使用以下命令启动YARN ResourceManager：</p>
<p>$HADOOP_HOME&#x2F;bin&#x2F;yarn –daemon start resourcemanager</p>
<p>在每个从节点上使用以下命令启动YARN nodemanager：</p>
<p>$HADOOP_HOME&#x2F;bin&#x2F;yarn –daemon start nodemanager</p>
<p>如果想要停止某个节点上某个角色，只需要把命令中的<strong>start</strong>改为<strong>stop</strong>即可。</p>
<h4 id="脚本一键启动"><a href="#脚本一键启动" class="headerlink" title="脚本一键启动"></a>脚本一键启动</h4><p>如果配置了etc&#x2F;hadoop&#x2F;workers和ssh免密登录，则可以使用程序脚本启动所有Hadoop两个集群的相关进程，在主节点所设定的机器上执行。</p>
<p>hdfs：$HADOOP_PREFIX&#x2F;sbin&#x2F;start-dfs.sh</p>
<p>yarn: $HADOOP_PREFIX&#x2F;sbin&#x2F;start-yarn.sh</p>
<p>停止集群：stop-dfs.sh、stop-yarn.sh</p>
<p>同时还提供了完整的一键化脚本:</p>
<p>start-all.sh 和 stop-all.sh</p>
<h2 id="启动后的效果"><a href="#启动后的效果" class="headerlink" title="启动后的效果"></a>启动后的效果</h2><p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/67f098ffad73e6a7a3738a6a6593faf6.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/7b4befde1f446bceb5eda4d345f315c9.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/29943c8cfa443225fbc3752a22211491.png"></p>
<h2 id="集群web-ui"><a href="#集群web-ui" class="headerlink" title="集群web-ui"></a>集群web-ui</h2><p>一旦Hadoop集群启动并运行，可以通过web-ui进行集群查看，如下所述：</p>
<p>NameNode <a href="http://nn_host:port/">http://nn_host:port/</a> 默认9870.</p>
<p>ResourceManager <a href="http://rm_host:port/">http://rm_host:port/</a> 默认 8088.</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/46c613206a8b0b74ee2f5910d42b7ddd.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/d51e2e72123537c1213b4efed95f24bd.png"></p>
<h2 id="MapReduce-jobHistory"><a href="#MapReduce-jobHistory" class="headerlink" title="MapReduce jobHistory"></a>MapReduce jobHistory</h2><p>JobHistory用来记录已经finished的mapreduce运行日志，日志信息存放于HDFS目录中，默认情况下没有开启此功能，需要在mapred-site.xml中配置并手动启动。</p>
<h3 id="修改mapred-site-xml"><a href="#修改mapred-site-xml" class="headerlink" title="修改mapred-site.xml"></a>修改mapred-site.xml</h3><p>cd &#x2F;export&#x2F;servers&#x2F;hadoop-3.3.0&#x2F;etc&#x2F;hadoop</p>
<p>vim mapred-site.xml</p>
<p>MR JobHistory Server管理的日志的存放位置</p>
<table>
<thead>
<tr>
<th>&lt;property&gt;  &lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt;  &lt;value&gt;node1:10020&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;</th>
</tr>
</thead>
</table>
<p>查看历史服务器已经运行完的Mapreduce作业记录的web地址，需要启动该服务才行</p>
<table>
<thead>
<tr>
<th>&lt;property&gt;  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;&#x2F;name&gt;  &lt;value&gt;node1:19888&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;</th>
</tr>
</thead>
</table>
<h3 id="分发配置到其他机器"><a href="#分发配置到其他机器" class="headerlink" title="分发配置到其他机器"></a>分发配置到其他机器</h3><p>cd &#x2F;export&#x2F;servers&#x2F;hadoop&#x2F;etc&#x2F;hadoop</p>
<p>scp -r mapred-site.xml node2:$PWD</p>
<p>scp –r mapred-site.xml node3:$PWD</p>
<h3 id="启动jobHistoryServer服务进程"><a href="#启动jobHistoryServer服务进程" class="headerlink" title="启动jobHistoryServer服务进程"></a>启动jobHistoryServer服务进程</h3><p>mapred –daemon start historyserver</p>
<p>如果关闭的话 用下述命令</p>
<p>mapred –daemon stop historyserver</p>
<h3 id="页面访问jobhistoryserver"><a href="#页面访问jobhistoryserver" class="headerlink" title="页面访问jobhistoryserver"></a>页面访问jobhistoryserver</h3><p><a target="_blank" rel="noopener" href="http://node1:19888/jobhistory">http://node1:19888/jobhistory</a></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/image-20240428214803835.png" alt="image-20240428214803835"></p>
<h1 id="八、Hive安装操作"><a href="#八、Hive安装操作" class="headerlink" title="八、Hive安装操作"></a>八、Hive安装操作</h1><h2 id="Hive-metastore远程模式安装部署"><a href="#Hive-metastore远程模式安装部署" class="headerlink" title="Hive metastore远程模式安装部署"></a>Hive metastore远程模式安装部署</h2><p>课程中采用<strong>远程模式部署hive的metastore服务</strong>。在<strong>node1机器</strong>上安装。</p>
<p>注意：以下两件事在启动hive之前必须确保正常完成。</p>
<ol>
<li>选择某台机器提前安装mysql,确保具有远程访问的权限。</li>
<li>启动hadoop集群 确保集群正常健康</li>
</ol>
<h3 id="Hadoop中添加用户代理配置"><a href="#Hadoop中添加用户代理配置" class="headerlink" title="Hadoop中添加用户代理配置"></a>Hadoop中添加用户代理配置</h3><table>
<thead>
<tr>
<th>#修改hadoop 配置文件 etc&#x2F;hadoop&#x2F;core-site.xml,加入如下配置项 &lt;property&gt;  &lt;name&gt;hadoop.proxyuser.root.hosts&lt;&#x2F;name&gt;  &lt;value&gt;*&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt;  &lt;name&gt;hadoop.proxyuser.root.groups&lt;&#x2F;name&gt;  &lt;value&gt;*&lt;&#x2F;value&gt; &lt;&#x2F;property&gt;</th>
</tr>
</thead>
</table>
<h3 id="上传安装包-并解压"><a href="#上传安装包-并解压" class="headerlink" title="上传安装包 并解压"></a>上传安装包 并解压</h3><table>
<thead>
<tr>
<th>tar -zxvf apache-hive-3.1.2-bin.tar.gz -C &#x2F;export&#x2F;server&#x2F; cd &#x2F;export&#x2F;server&#x2F; ln -s apache-hive-3.1.2-bin&#x2F; hive  # 解决Hive与Hadoop之间guava版本差异 cd &#x2F;export&#x2F;server&#x2F;hive&#x2F; rm -rf lib&#x2F;guava-19.0.jar cp &#x2F;export&#x2F;server&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib&#x2F;guava-27.0-jre.jar lib&#x2F;</th>
</tr>
</thead>
</table>
<h3 id="修改配置文件hive-env-sh"><a href="#修改配置文件hive-env-sh" class="headerlink" title="修改配置文件hive-env.sh"></a>修改配置文件hive-env.sh</h3><table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F;hive&#x2F;conf  cp hive-env.sh.template hive-env.sh  vim hive-env.sh # 修改一下几处内容: 第48行 export HADOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop export HIVE_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hive&#x2F;conf export HIVE_AUX_JARS_PATH&#x3D;&#x2F;export&#x2F;server&#x2F;hive&#x2F;conf&#x2F;lib</th>
</tr>
</thead>
</table>
<h3 id="添加配置文件hive-site-xml"><a href="#添加配置文件hive-site-xml" class="headerlink" title="添加配置文件hive-site.xml"></a>添加配置文件hive-site.xml</h3><table>
<thead>
<tr>
<th>vim hive-site.xml  添加以下内容:  &lt;configuration&gt;  &lt;!– 存储元数据mysql相关配置 –&gt;  &lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;  &lt;value&gt;jdbc:mysql:&#x2F;&#x2F;node1.itcast.cn:3306&#x2F;hive3?createDatabaseIfNotExist&#x3D;true&amp;useSSL&#x3D;false&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt;  &lt;value&gt;com.mysql.jdbc.Driver&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt;  &lt;value&gt;root&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt;  &lt;value&gt;123456&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;!– H2S运行绑定host –&gt;  &lt;property&gt;  &lt;name&gt;hive.server2.thrift.bind.host&lt;&#x2F;name&gt;  &lt;value&gt;node1.itcast.cn&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;!– 远程模式部署metastore metastore地址 –&gt;  &lt;property&gt;  &lt;name&gt;hive.metastore.uris&lt;&#x2F;name&gt;  &lt;value&gt;thrift:&#x2F;&#x2F;node1.itcast.cn:9083&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;!– 关闭元数据存储授权 –&gt;  &lt;property&gt;  &lt;name&gt;hive.metastore.event.db.notification.api.auth&lt;&#x2F;name&gt;  &lt;value&gt;false&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt; &lt;&#x2F;configuration&gt;</th>
</tr>
</thead>
</table>
<h3 id="上传MySQL驱动"><a href="#上传MySQL驱动" class="headerlink" title="上传MySQL驱动"></a>上传MySQL驱动</h3><table>
<thead>
<tr>
<th>#上传mysql jdbc驱动到hive安装包lib下  mysql-connector-java-5.1.32.jar</th>
</tr>
</thead>
</table>
<h3 id="初始化元数据"><a href="#初始化元数据" class="headerlink" title="初始化元数据"></a>初始化元数据</h3><table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F;hive&#x2F;  bin&#x2F;schematool -initSchema -dbType mysql -verbos #初始化成功会在mysql中创建74张表</th>
</tr>
</thead>
</table>
<h3 id="创建hive存储目录"><a href="#创建hive存储目录" class="headerlink" title="创建hive存储目录"></a>创建hive存储目录</h3><table>
<thead>
<tr>
<th>hadoop fs -mkdir &#x2F;tmp hadoop fs -mkdir -p &#x2F;user&#x2F;hive&#x2F;warehouse hadoop fs -chmod g+w &#x2F;tmp hadoop fs -chmod g+w &#x2F;user&#x2F;hive&#x2F;warehouse</th>
</tr>
</thead>
</table>
<h2 id="metastore-的启动方式"><a href="#metastore-的启动方式" class="headerlink" title="metastore 的启动方式"></a>metastore 的启动方式</h2><table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F;hive #前台启动 关闭ctrl+c .&#x2F;bin&#x2F;hive –service metastore  #前台启动开启debug日志 .&#x2F;bin&#x2F;hive –service metastore –hiveconf hive.root.logger&#x3D;DEBUG,console   #后台启动 进程挂起 关闭使用jps+ kill -9 nohup .&#x2F;bin&#x2F;hive –service metastore &amp; nohup .&#x2F;bin&#x2F;hive –service hiveserver2 &amp;</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/da6524bdc7620522a40ad3bc7af4c307.png"></p>
<h2 id="Hive-Client、Beeline-Client"><a href="#Hive-Client、Beeline-Client" class="headerlink" title="Hive Client、Beeline Client"></a>Hive Client、Beeline Client</h2><h3 id="第一代客户端Hive-Client"><a href="#第一代客户端Hive-Client" class="headerlink" title="第一代客户端Hive Client"></a>第一代客户端Hive Client</h3><p>在hive安装包的bin目录下，有hive提供的第一代客户端 bin&#x2F;hive。使用该客户端可以访问hive的metastore服务。从而达到操作hive的目的。</p>
<p>使用下面的命令启动hive的客户端：</p>
<p>&#x2F;export&#x2F;server&#x2F;hive&#x2F;bin&#x2F;hive</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/8b65a951c657f9d88d0cca960029f887.png"></p>
<p>可以发现官方提示：第一代客户端已经不推荐使用了。</p>
<h3 id="第二代客户端Hive-Beeline-Client"><a href="#第二代客户端Hive-Beeline-Client" class="headerlink" title="第二代客户端Hive Beeline Client"></a>第二代客户端Hive Beeline Client</h3><p>hive经过发展，推出了第二代客户端beeline，但是beeline客户端不是直接访问metastore服务的，而是<strong>需要单独启动hiveserver2服务</strong>。</p>
<p>在hive运行的服务器上，首先启动metastore服务，然后启动hiveserver2服务。</p>
<p>nohup &#x2F;export&#x2F;server&#x2F;hive&#x2F;bin&#x2F;hive –service metastore &amp;</p>
<p>nohup &#x2F;export&#x2F;server&#x2F;hive&#x2F;bin&#x2F;hive –service hiveserver2 &amp;</p>
<p>使用beeline客户端进行连接访问。</p>
<p>&#x2F;export&#x2F;server&#x2F;hive&#x2F;bin&#x2F;beeline</p>
<p>beeline&gt; ! connect jdbc:hive1:&#x2F;&#x2F;node1:10000</p>
<p>Enter username for jdbc:hive1:&#x2F;&#x2F;node1:10000: root</p>
<p>Enter password for jdbc:hive1:&#x2F;&#x2F;node1:10000: *******(任意)<img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/e82161a29ef7c36821318c912f5adbe1.png"></p>
<p>说明: 在其他的节点想要连接hive, 只需要有hive的beeline的客户端即可</p>
<h1 id="九、Spark部署安装"><a href="#九、Spark部署安装" class="headerlink" title="九、Spark部署安装"></a>九、Spark部署安装</h1><h2 id="Spark-Local-模式搭建文档"><a href="#Spark-Local-模式搭建文档" class="headerlink" title="Spark Local 模式搭建文档"></a>Spark Local 模式搭建文档</h2><p>在本地使用单机多线程模拟Spark集群中的各个角色</p>
<h3 id="安装包下载"><a href="#安装包下载" class="headerlink" title="安装包下载"></a>安装包下载</h3><p>目前Spark最新稳定版本：目前Spark最新稳定版本：3.1.x系列</p>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/3.1.2/index.html">https://spark.apache.org/docs/3.1.2/index.html</a></p>
<p>★注意1:</p>
<p>Spark3.0+基于Scala2.12</p>
<p><a target="_blank" rel="noopener" href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html</a></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/0e645c6758c2b31ef4730f38cb26f574.png"></p>
<p>★注意2:</p>
<p>目前企业中使用较多的Spark版本还是Spark2.x,如Spark2.2.0、Spark2.4.5都使用较多，但未来Spark3.X肯定是主流，毕竟官方高版本是对低版本的兼容以及提升</p>
<p><a target="_blank" rel="noopener" href="http://spark.apache.org/releases/spark-release-3-0-0.html">http://spark.apache.org/releases/spark-release-3-0-0.html</a></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/5fcae919b2b1dfd2c00d6388b3e722bf.png"></p>
<h3 id="将安装包上传并解压"><a href="#将安装包上传并解压" class="headerlink" title="将安装包上传并解压"></a>将安装包上传并解压</h3><p>说明: 只需要上传至node1即可, 以下操作都是在node1执行的</p>
<table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;software rz 上传 <img src="C:\Users\admin\Desktop\Diary\media\87a25663acd8df4bf3d3607e51575f55.png"> 解压: tar -zxf spark-3.1.2-bin-hadoop3.2.tgz -C &#x2F;export&#x2F;server&#x2F; <img src="C:\Users\admin\Desktop\Diary\media\f846e0e531e6ad46abf8c0b86bbb8f59.png"> 更名: (两种方式二选一即可, 推荐软连接方案) cd &#x2F;export&#x2F;server <strong>方式一:</strong>  软连接方案: ln -s spark-3.1.2-bin-hadoop3.2 spark 方式二:  直接重命名: mv spark-3.1.2-bin-hadoop3.2 spark <img src="C:\Users\admin\Desktop\Diary\media\484353af88a23372ee69ccb89cb5c62c.png"></th>
</tr>
</thead>
</table>
<p>目录结构说明:</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/d868a52253d365037a977167a3d9da27.png"></p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>Spark的local模式, 开箱即用, 直接启动bin目录下的spark-shell脚本</p>
<table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;bin .&#x2F;spark-shell</th>
</tr>
</thead>
</table>
<p>说明:</p>
<p>sc：SparkContext实例对象：</p>
<p>spark：SparkSession实例对象</p>
<p>4040：Web监控页面端口号</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/3216c081040c1c960bd166b51343ef01.png"></p>
<p>●Spark-shell说明：</p>
<p>1.直接使用.&#x2F;spark-shell</p>
<p>表示使用local 模式启动，在本机启动一个SparkSubmit进程</p>
<p>2.还可指定参数 –master，如：</p>
<p>spark-shell –master local[N] 表示在本地模拟N个线程来运行当前任务</p>
<p>spark-shell –master local[*] 表示使用当前机器上所有可用的资源</p>
<p>3.不携带参数默认就是</p>
<p>spark-shell –master local[*]</p>
<p>4.后续还可以使用–master指定集群地址，表示把任务提交到集群上运行，如</p>
<p>.&#x2F;spark-shell –master spark:&#x2F;&#x2F;node01:7077,node02:7077</p>
<p>5.退出spark-shell</p>
<p><strong>使用 :quit</strong></p>
<h2 id="PySpark环境安装"><a href="#PySpark环境安装" class="headerlink" title="PySpark环境安装"></a>PySpark环境安装</h2><p>同学们可能有疑问, 我们不是学的Spark框架吗? 怎么会安装一个叫做PySpark呢?</p>
<p>这里简单说明一下:</p>
<p><strong>PySpark: 是Python的库, 由Spark官方提供. 专供Python语言使用. 类似Pandas一样,是一个库</strong></p>
<p>Spark: 是一个独立的框架, 包含PySpark的全部功能, 除此之外, Spark框架还包含了对R语言\ Java语言\ Scala语言的支持. 功能更全. 可以认为是通用Spark。</p>
<table>
<thead>
<tr>
<th>功能</th>
<th>PySpark</th>
<th>Spark</th>
</tr>
</thead>
<tbody><tr>
<td>底层语言</td>
<td>Scala(JVM)</td>
<td>Scala(JVM)</td>
</tr>
<tr>
<td>上层语言支持</td>
<td>Python</td>
<td>Python\Java\Scala\R</td>
</tr>
<tr>
<td>集群化\分布式运行</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>定位</td>
<td>Python库 (客户端)</td>
<td>标准框架 (客户端和服务端)</td>
</tr>
<tr>
<td>是否可以Daemon运行</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>使用场景</td>
<td>生产环境集群化运行</td>
<td>生产环境集群化运行</td>
</tr>
</tbody></table>
<p>若安装PySpark需要首先具备Python环境，这里使用Anaconda环境，安装过程如下：</p>
<h3 id="下载Anaconda环境包"><a href="#下载Anaconda环境包" class="headerlink" title="下载Anaconda环境包"></a>下载Anaconda环境包</h3><p>安装版本：<a target="_blank" rel="noopener" href="https://www.anaconda.com/distribution/#download-section">https://www.anaconda.com/distribution/#download-section</a></p>
<p>Python3.8.8版本：<strong>Anaconda3-2021.05-Linux-x86_64.sh</strong></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/aab3c057f75a4fca20a65d5191109589.png"></p>
<h3 id="安装Anaconda环境"><a href="#安装Anaconda环境" class="headerlink" title="安装Anaconda环境"></a>安装Anaconda环境</h3><p><strong>此环境三台节点都是需要安装的, 以下演示在node1安装, 其余两台也是需要安装的</strong></p>
<table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;software rz 上传Anaconda脚本环境 <img src="C:\Users\admin\Desktop\Diary\media\aa62cc077cd953d5dab17452b2af8475.png">  执行脚本: <strong>bash Anaconda3-2021.05-Linux-x86_64.sh</strong> <img src="C:\Users\admin\Desktop\Diary\media\4cc2251d29529b2093e009865d73ee30.png"> 不断输入空格, 直至出现以下解压, 然后输入yes <img src="C:\Users\admin\Desktop\Diary\media\2fe15cd7b4c8cf015f73059f72af79db.png"> 此时, anaconda需要下载相关的依赖包, 时间比较长, 耐心等待即可…. <img src="C:\Users\admin\Desktop\Diary\media\8a70d24293aaa65114cec625c90d6cce.png"> 配置anaconda的环境变量: <strong>vim &#x2F;etc&#x2F;profile</strong> <strong>##增加如下配置</strong> <strong>export ANACONDA_HOME&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin</strong> <strong>export PATH&#x3D;$PATH:$ANACONDA_HOME&#x2F;bin</strong> 重新加载环境变量: <strong>source &#x2F;etc&#x2F;profile</strong>  修改bashrc文件 sudo vim ~&#x2F;.bashrc 添加如下内容: 直接在第二行空行添加即可 export PATH&#x3D;~&#x2F;anaconda3&#x2F;bin:$PATH</th>
</tr>
</thead>
</table>
<p>说明:</p>
<table>
<thead>
<tr>
<th><strong>profile</strong> 其实看名字就能了解大概了, profile 是某个用户唯一的用来设置环境变量的地方, 因为用户可以有多个 shell 比如 bash, sh, zsh 之类的, 但像环境变量这种其实只需要在统一的一个地方初始化就可以了, 而这就是 profile. <strong>bashrc</strong> bashrc 也是看名字就知道, 是专门用来给 bash 做初始化的比如用来初始化 bash 的设置, bash 的代码补全, bash 的别名, bash 的颜色. 以此类推也就还会有 shrc, zshrc 这样的文件存在了, 只是 bash 太常用了而已.</th>
</tr>
</thead>
</table>
<h3 id="启动anaconda并测试"><a href="#启动anaconda并测试" class="headerlink" title="启动anaconda并测试"></a>启动anaconda并测试</h3><p>注意: 请将当前连接node1的节点窗口关闭,然后重新打开,否则无法识别</p>
<table>
<thead>
<tr>
<th>输入 python -V启动: <img src="C:\Users\admin\Desktop\Diary\media\d8af21085ca321cb785239672dc170cc.png"></th>
</tr>
</thead>
</table>
<p>base： 是anaconda的默认的初始环境, 后续我们还可以构建更多的虚拟环境, 用于隔离各个Python环境操作, 如果不想看到base的字样, 也可以选择直接退出即可</p>
<table>
<thead>
<tr>
<th>执行: conda deactivate  <img src="C:\Users\admin\Desktop\Diary\media\1d43de65cc4bc1ce4394f417da19aaf3.png"></th>
</tr>
</thead>
</table>
<p>但是当大家重新访问的时候, 会发现又重新进入了base,如何让其默认不进去呢, 可以选择修改.bashrc这个文件</p>
<table>
<thead>
<tr>
<th>vim ~&#x2F;.bashrc  在文件的末尾添加: conda deactivate  保存退出后, 重新打开会话窗口, 发现就不会在直接进入base了</th>
</tr>
</thead>
</table>
<h3 id="Anaconda相关组件介绍-了解"><a href="#Anaconda相关组件介绍-了解" class="headerlink" title="Anaconda相关组件介绍[了解]"></a>Anaconda相关组件介绍[了解]</h3><p>Anaconda（水蟒）：是一个科学计算软件发行版，集成了大量常用扩展包的环境，包含了 conda、Python 等 180 多个科学计算包及其依赖项，并且支持所有操作系统平台。下载地址：<a target="_blank" rel="noopener" href="https://www.continuum.io/downloads">https://www.continuum.io/downloads</a></p>
<ul>
<li>安装包：pip install xxx,conda install xxx</li>
<li>卸载包：pip uninstall xxx,conda uninstall xxx</li>
<li>升级包：pip install upgrade xxx,conda update xxx</li>
</ul>
<p>**Jupyter Notebook：**启动命令</p>
<table>
<thead>
<tr>
<th>jupyter notebook</th>
</tr>
</thead>
</table>
<p>功能如下：</p>
<ul>
<li>Anaconda自带，无需单独安装</li>
<li>实时查看运行过程</li>
<li>基本的web编辑器（本地）</li>
<li>ipynb 文件分享</li>
<li>可交互式</li>
<li>记录历史运行结果</li>
</ul>
<p>修改jupyter显示的文件路径：</p>
<p>通过jupyter notebook –generate-config命令创建配置文件，之后在进入用户文件夹下面查看.jupyter隐藏文件夹，修改其中文件jupyter_notebook_config.py的202行为计算机本地存在的路径。</p>
<p><strong>IPython：</strong></p>
<p>命令：ipython，其功能如下</p>
<p>1.Anaconda自带，无需单独安装</p>
<p>2.Python的交互式命令行 Shell</p>
<p>3.可交互式</p>
<p>4.记录历史运行结果</p>
<p>5.及时验证想法</p>
<p><strong>Spyder：</strong></p>
<p>命令：spyder，其功能如下</p>
<p>1.Anaconda自带，无需单独安装</p>
<p>2.完全免费，适合熟悉Matlab的用户</p>
<p>3.功能强大，使用简单的图形界面开发环境</p>
<p>下面就Anaconda中的conda命令做详细介绍和配置。</p>
<ol>
<li>conda命令及pip命令</li>
</ol>
<p>conda管理数据科学环境，conda和pip类似均为安装、卸载或管理Python第三方包。</p>
<table>
<thead>
<tr>
<th>conda install 包名 pip install 包名 conda uninstall 包名 pip uninstall 包名 conda install -U 包名 pip install -U 包名</th>
</tr>
</thead>
</table>
<p>（2） Anaconda设置为国内下载镜像</p>
<table>
<thead>
<tr>
<th>conda config –add channels <a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</a> conda config –set show_channel_urls yes</th>
</tr>
</thead>
</table>
<p>（3）conda创建虚拟环境</p>
<table>
<thead>
<tr>
<th>conda env list conda create py_env python&#x3D;3.8.8 #创建python3.8.8环境 activate py_env #激活环境 deactivate py_env #退出环境</th>
</tr>
</thead>
</table>
<h3 id="PySpark安装"><a href="#PySpark安装" class="headerlink" title="PySpark安装"></a>PySpark安装</h3><p>三个节点也是都需要安装pySpark的</p>
<h4 id="方式1：直接安装PySpark"><a href="#方式1：直接安装PySpark" class="headerlink" title="方式1：直接安装PySpark"></a>方式1：直接安装PySpark</h4><p>安装如下：</p>
<table>
<thead>
<tr>
<th>使用PyPI安装PySpark如下：也可以指定版本安装 pip install pyspark 或者指定清华镜像**(对于网络较差的情况)**： <strong>pip install -i <a target="_blank" rel="noopener" href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a> pyspark # 指定清华镜像源</strong> 如果要为特定组件安装额外的依赖项，可以按如下方式安装(此步骤暂不执行，后面Sparksql部分会执行)： pip install pyspark[sql]</th>
</tr>
</thead>
</table>
<p>截图如下：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/fa0667a88a0da253f94e5bf81a508b73.png"></p>
<h4 id="安装-方式2：创建Conda环境安装PySpark"><a href="#安装-方式2：创建Conda环境安装PySpark" class="headerlink" title="[安装]方式2：创建Conda环境安装PySpark"></a>[安装]方式2：创建Conda环境安装PySpark</h4><table>
<thead>
<tr>
<th>#从终端创建新的虚拟环境，如下所示 conda create -n pyspark_env python&#x3D;3.8 <img src="C:\Users\admin\Desktop\Diary\media\a819cfa3efce5dc25d6ae670a0d2a3ec.png"> #创建虚拟环境后，它应该在 Conda 环境列表下可见，可以使用以下命令查看 conda env list <img src="C:\Users\admin\Desktop\Diary\media\f521eb224204217de302777ad992154a.png"> #现在使用以下命令激活新创建的环境： source activate pyspark_env 或者 conda activate pyspark_env <img src="C:\Users\admin\Desktop\Diary\media\b63a2e604a00e514ca415b9ecd407f45.png"> 如果报错: CommandNotFoundError: Your shell has not been properly configured to use ‘conda deactivate’.切换使用 source activate  #您可以在新创建的环境中通过使用PyPI安装PySpark来安装pyspark，例如如下。它将pyspark_env在上面创建的新虚拟环境下安装 PySpark。 pip install pyspark #或者，可以从 Conda 本身安装 PySpark： conda install pyspark <img src="C:\Users\admin\Desktop\Diary\media\0e0ee5dfe4929bcddedce55f88d4530f.png"></th>
</tr>
</thead>
</table>
<h4 id="不推荐-方式3：手动下载安装"><a href="#不推荐-方式3：手动下载安装" class="headerlink" title="[不推荐]方式3：手动下载安装"></a>[不推荐]方式3：手动下载安装</h4><p>将spark对应版本下的python目录下的pyspark复制到anaconda的</p>
<p>Library&#x2F;Python3&#x2F;site-packages&#x2F;目录下即可。</p>
<p>请注意，PySpark 需要JAVA_HOME正确设置的Java 8 或更高版本。如果使用 JDK 11，请设置-Dio.netty.tryReflectionSetAccessible&#x3D;true，Arrow相关功能才可以使用。</p>
<p>扩展：</p>
<table>
<thead>
<tr>
<th>conda虚拟环境 命令 查看所有环境 conda info –envs 新建虚拟环境 conda create -n myenv python&#x3D;3.6  删除虚拟环境 conda remove -n myenv –all  激活虚拟环境 conda activate myenv source activate base  退出虚拟环境 conda deactivate myenv</th>
</tr>
</thead>
</table>
<h3 id="初体验-PySpark-shell方式"><a href="#初体验-PySpark-shell方式" class="headerlink" title="初体验-PySpark shell方式"></a>初体验-PySpark shell方式</h3><p>前面的Spark Shell实际上使用的是Scala交互式Shell，实际上 Spark 也提供了一个用 Python 交互式Shell，即Pyspark。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/82b5b3b6e046585c397a76e21432345d.png"></p>
<table>
<thead>
<tr>
<th>bin&#x2F;pyspark –master local[*]</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/b264ec8067da5af1269abba88a317e4c.png"></p>
<h2 id="Spark-Standalone集群环境"><a href="#Spark-Standalone集群环境" class="headerlink" title="Spark Standalone集群环境"></a>Spark Standalone集群环境</h2><p>Standalone模式是Spark自带的一种集群模式，不同于前面本地模式启动多个进程来模拟集群的环境，Standalone模式是真实地在多个机器之间搭建Spark集群的环境，完全可以利用该模式搭建多机器集群，用于实际的大数据处理。</p>
<table>
<thead>
<tr>
<th>节点</th>
<th>主节点(master)</th>
<th>从节点(worker)</th>
<th>历史服务(history server)</th>
</tr>
</thead>
<tbody><tr>
<td>node1</td>
<td>是</td>
<td>是</td>
<td>是</td>
</tr>
<tr>
<td>node2</td>
<td>否</td>
<td>是</td>
<td>否</td>
</tr>
<tr>
<td>node3</td>
<td>否</td>
<td>是</td>
<td>否</td>
</tr>
</tbody></table>
<h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><p>说明: 直接对local模型下的spark进行更改为standalone模式</p>
<p>【workers】</p>
<table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F; cp workers.template workers vim workers 添加以下内容: node1.itcast.cn node2.itcast.cn node3.itcast.cn</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/364ca8e4af9f597f38fa04566a4b43ce.png"></p>
<p>【spark-env.sh】</p>
<table>
<thead>
<tr>
<th><strong>cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf</strong> <strong>cp spark-env.sh.template spark-env.sh</strong> <strong>vim spark-env.sh</strong> 增加如下内容： <strong>JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F;</strong>  <strong>HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;etc&#x2F;hadoop&#x2F;</strong> <strong>YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;etc&#x2F;hadoop&#x2F;</strong>  <strong>export SPARK_MASTER_HOST&#x3D;node1.itcast.cn</strong> <strong>export SPARK_MASTER_PORT&#x3D;7077</strong>  <strong>SPARK_MASTER_WEBUI_PORT&#x3D;8080</strong> <strong>SPARK_WORKER_CORES&#x3D;2</strong> <strong>SPARK_WORKER_MEMORY&#x3D;2g</strong> <strong>SPARK_WORKER_PORT&#x3D;7078</strong> <strong>SPARK_WORKER_WEBUI_PORT&#x3D;8081</strong>  <strong>SPARK_HISTORY_OPTS&#x3D;”-Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;node1.itcast.cn:8020&#x2F;sparklog&#x2F; -Dspark.history.fs.cleaner.enabled&#x3D;true”</strong></th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/f972b97e09ef7fa0559739e2bb6e79e7.png"></p>
<p>注意:</p>
<p>Jdk,hadoop, yarn的路径, 需要配置为自己的路径(可能与此位置不一致)</p>
<p>History配置中, 需要指定hdfs的地址, 其中端口号为8020或者9820, 大家需要参考hdfs上对应namenode的通信端口号</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/ce12534c0e58a0ec5527fed421f4f279.png"></p>
<p>【配置spark应用日志】</p>
<table>
<thead>
<tr>
<th>第一步: 在HDFS上创建应用运行事件日志目录: <strong>hdfs dfs -mkdir -p &#x2F;sparklog&#x2F;</strong> 第二步: 配置spark-defaults.conf <strong>cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf</strong> <strong>cp spark-defaults.conf.template spark-defaults.conf</strong> <strong>vim spark-defaults.conf</strong> 添加以下内容: <strong>spark.eventLog.enabled true</strong> <strong>spark.eventLog.dir hdfs:&#x2F;&#x2F;node1.itcast.cn:8020&#x2F;sparklog&#x2F;</strong> <strong>spark.eventLog.compress true</strong></th>
</tr>
</thead>
</table>
<p>其中HDFS的地址, 8020 还是9820 需要查看HDFS的界面显示</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/c555da902db248c102c72ed6cdf48f9e.png"></p>
<p>【log4j.properties】</p>
<table>
<thead>
<tr>
<th><strong>cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf</strong> <strong>cp log4j.properties.template log4j.properties</strong> <strong>vim log4j.properties</strong> ## 改变日志级别 <img src="C:\Users\admin\Desktop\Diary\media\17edcdfebbf59185e811766503832b58.png"></th>
</tr>
</thead>
</table>
<h3 id="分发到其他机器"><a href="#分发到其他机器" class="headerlink" title="分发到其他机器"></a>分发到其他机器</h3><p>将配置好的将 Spark 安装包分发给集群中其它机器，命令如下：</p>
<table>
<thead>
<tr>
<th><strong>cd &#x2F;export&#x2F;server&#x2F;</strong> <strong>scp -r spark-3.1.2-bin-hadoop3.2&#x2F; node2:$PWD</strong> <strong>scp -r spark-3.1.2-bin-hadoop3.2&#x2F; node3:$PWD</strong> ##分别在node2, 和node3中创建软连接 <strong>ln -s &#x2F;export&#x2F;server&#x2F;spark-3.1.2-bin-hadoop3.2&#x2F;  &#x2F;export&#x2F;server&#x2F;spark</strong></th>
</tr>
</thead>
</table>
<h3 id="启动spark-Standalone"><a href="#启动spark-Standalone" class="headerlink" title="启动spark Standalone"></a>启动spark Standalone</h3><ul>
<li>启动方式1：集群启动和停止</li>
</ul>
<p>在主节点上启动spark集群</p>
<table>
<thead>
<tr>
<th><strong>cd &#x2F;export&#x2F;server&#x2F;spark</strong> <strong>sbin&#x2F;start-all.sh</strong> <img src="C:\Users\admin\Desktop\Diary\media\7e4653ea6d67762465506bd6180dbf8b.png"> <img src="C:\Users\admin\Desktop\Diary\media\cf9e7c20dbf665e88a3718b48677e570.png"> <img src="C:\Users\admin\Desktop\Diary\media\aea3f953b9614faeb73afdce827435e8.png"> <strong>sbin&#x2F;start-history-server.sh</strong> <img src="C:\Users\admin\Desktop\Diary\media\ea7436f262b83df84287152c34d9c29f.png"></th>
</tr>
</thead>
</table>
<p>在主节点上停止spark集群</p>
<table>
<thead>
<tr>
<th><strong>&#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;stop-all.sh</strong></th>
</tr>
</thead>
</table>
<ul>
<li>启动方式2：单独启动和停止</li>
</ul>
<p>在 master 安装节点上启动和停止 master：</p>
<table>
<thead>
<tr>
<th><strong>start-master.sh</strong> <strong>stop-master.sh</strong></th>
</tr>
</thead>
</table>
<p>在 Master 所在节点上启动和停止worker(work指的是slaves 配置文件中的主机名)</p>
<table>
<thead>
<tr>
<th><strong>start-slaves.sh</strong> <strong>stop-slaves.sh</strong></th>
</tr>
</thead>
</table>
<ul>
<li>WEB UI页面</li>
</ul>
<table>
<thead>
<tr>
<th><a target="_blank" rel="noopener" href="http://node1:8080/"><strong>http://node1:8080/</strong></a></th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/2b5fa4ff96882e79c58bb7f3da8d765c.png"></p>
<p>可以看出，配置了3个Worker进程实例，每个Worker实例为1核1GB内存，总共是3核 3GB 内存。目前显示的Worker资源都是空闲的，当向Spark集群提交应用之后，Spark就会分配相应的资源给程序使用，可以在该页面看到资源的使用情况。</p>
<ul>
<li>历史服务器HistoryServer：</li>
</ul>
<table>
<thead>
<tr>
<th><strong>&#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-history-server.sh</strong></th>
</tr>
</thead>
</table>
<p>WEB UI页面地址：<a target="_blank" rel="noopener" href="http://bigdata-cdh01.itcast.cn:18080/">http://node1:18080</a></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/35b4d1baaacba439eabc4aa3ee0a77e1.png"></p>
<h3 id="连接集群"><a href="#连接集群" class="headerlink" title="连接集群"></a>连接集群</h3><p>【spark-shell 连接】</p>
<table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F;spark bin&#x2F;spark-shell –master spark:&#x2F;&#x2F;node1:7077</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/aa2c3286e86476e4d651fc369a8ad0a5.png"></p>
<p>【pyspark 连接】</p>
<table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F;spark .&#x2F;bin&#x2F;pyspark –master spark:&#x2F;&#x2F;node1:7077</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/bc2bc8ac260f9b4e884c7f2f69ab8f51.png"></p>
<h2 id="Spark-Standalone-HA-模式安装"><a href="#Spark-Standalone-HA-模式安装" class="headerlink" title="Spark Standalone HA 模式安装"></a>Spark Standalone HA 模式安装</h2><p>多master模式 其中一个为active节点, 另一个为standby状态</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/b00d8dc6a475a74e8d648abdfb173f33.png" alt="http:&#x2F;&#x2F;images0.cnblogs.com&#x2F;i&#x2F;469775&#x2F;201405&#x2F;092146069481533.png"></p>
<p>注意: HA模式需要依赖于zookeeper进行协助管理</p>
<h3 id="修改配置"><a href="#修改配置" class="headerlink" title="修改配置"></a>修改配置</h3><table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf vim spark-env.sh 注释或删除MASTER_HOST内容: # SPARK_MASTER_HOST&#x3D;node1.itcast.cn 增加以下配置: SPARK_DAEMON_JAVA_OPTS&#x3D;”-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER -Dspark.deploy.zookeeper.url&#x3D; node1.itcast. cn:2181,node2.itcast.cn:2181,node3.itcast.cn:2181 -Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark-ha”</th>
</tr>
</thead>
</table>
<p>说明函数说明:</p>
<p><strong>spark.deploy.recoveryMode：恢复模式</strong></p>
<p><strong>spark.deploy.zookeeper.url：ZooKeeper的Server地址</strong></p>
<p><strong>spark.deploy.zookeeper.dir：保存集群元数据信息的文件、目录。包括Worker、Driver、Application信息。</strong></p>
<h3 id="配置分发"><a href="#配置分发" class="headerlink" title="配置分发"></a>配置分发</h3><table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf scp -r spark-env.sh node2:$PWD scp -r spark-env.sh node3:$PWD</th>
</tr>
</thead>
</table>
<h3 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h3><table>
<thead>
<tr>
<th>首先启动zookeeper服务: 三个节点都需要启动 zkServer.sh status –查看状态 zkServer.sh stop –停止命令 zkServer.sh start –启动命令  接着在node1启动spark集群 cd &#x2F;export&#x2F;server&#x2F;spark .&#x2F;sbin&#x2F;start-all.sh  最后在node2上单独启动一个master cd &#x2F;export&#x2F;server&#x2F;spark .&#x2F;sbin&#x2F;start-master.sh</th>
</tr>
</thead>
</table>
<p>查看WebUI</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="http://node1:8080/">http://node1:8080/</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://node2:8080/">http://node2:8080/</a></p>
<p>默认情况下，先启动Master就为Active Master，如下截图所示：</p>
</li>
</ul>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/c94d50b028db6d7a84826e8fd8bf45b6.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/2b7f5c472909d0fed7a3d989b37e3501.png"></p>
<h2 id="Spark-on-YARN-环境搭建"><a href="#Spark-on-YARN-环境搭建" class="headerlink" title="Spark on YARN 环境搭建"></a>Spark on YARN 环境搭建</h2><h3 id="修改spark-env-sh"><a href="#修改spark-env-sh" class="headerlink" title="修改spark-env.sh"></a>修改spark-env.sh</h3><table>
<thead>
<tr>
<th>cd <strong>&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf</strong> <strong>vim &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F;spark-env.sh</strong>  添加以下内容: <strong>HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop</strong> <strong>YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop</strong>  <strong>同步到其他两台</strong> <strong>cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf</strong> <strong>scp -r spark-env.sh node2:$PWD</strong> <strong>scp -r spark-env.sh node3:$PWD</strong></th>
</tr>
</thead>
</table>
<h3 id="修改hadoop的yarn-site-xml"><a href="#修改hadoop的yarn-site-xml" class="headerlink" title="修改hadoop的yarn-site.xml"></a>修改hadoop的yarn-site.xml</h3><p>node1修改</p>
<table>
<thead>
<tr>
<th>cd <strong>&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F;</strong> <strong>vim &#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F;yarn-site.xml</strong>  <strong>添加以下内容:</strong> &lt;configuration&gt;  &lt;!– 配置yarn主节点的位置 –&gt;  &lt;property&gt;  &lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt;  &lt;value&gt;node1.itcast.cn&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;  &lt;property&gt;  &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;  &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;  &lt;!– 设置yarn集群的内存分配方案 –&gt;  &lt;property&gt;  &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;&#x2F;name&gt;  &lt;value&gt;20480&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;  &lt;property&gt;  &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;&#x2F;name&gt;  &lt;value&gt;2048&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;  &lt;property&gt;  &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;&#x2F;name&gt;  &lt;value&gt;2.1&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;  &lt;!– 开启日志聚合功能 –&gt;  &lt;property&gt;  &lt;name&gt;yarn.log-aggregation-enable&lt;&#x2F;name&gt;  &lt;value&gt;true&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;  &lt;!– 设置聚合日志在hdfs上的保存时间 –&gt;  &lt;property&gt;  &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;&#x2F;name&gt;  &lt;value&gt;604800&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;  &lt;!– 设置yarn历史服务器地址 –&gt;  &lt;property&gt;  &lt;name&gt;yarn.log.server.url&lt;&#x2F;name&gt;  &lt;value&gt;<a target="_blank" rel="noopener" href="http://node1.itcast.cn:19888/jobhistory/logs/">http://node1.itcast.cn:19888/jobhistory/logs\</a>&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;  &lt;!– 关闭yarn内存检查 –&gt;  &lt;property&gt;  &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;&#x2F;name&gt;  &lt;value&gt;false&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;  &lt;property&gt;  &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;&#x2F;name&gt;  &lt;value&gt;false&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt; &lt;&#x2F;configuration&gt;</th>
</tr>
</thead>
</table>
<p>将其同步到其他两台</p>
<table>
<thead>
<tr>
<th><strong>cd &#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop</strong> <strong>scp -r yarn-site.xml node2:$PWD</strong> <strong>scp -r yarn-site.xml node3:$PWD</strong></th>
</tr>
</thead>
</table>
<h3 id="Spark设置历史服务地址"><a href="#Spark设置历史服务地址" class="headerlink" title="Spark设置历史服务地址"></a>Spark设置历史服务地址</h3><table>
<thead>
<tr>
<th><strong>cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf</strong> <strong>vim spark-defaults.conf</strong>  <strong>添加以下内容:</strong> <strong>spark.eventLog.enabled true</strong> <strong>spark.eventLog.dir hdfs:&#x2F;&#x2F;node1:9820&#x2F;sparklog&#x2F;</strong> <strong>spark.eventLog.compress true</strong> <strong>spark.yarn.historyServer.address node1.itcast.cn:18080</strong></th>
</tr>
</thead>
</table>
<p>设置日志级别:</p>
<table>
<thead>
<tr>
<th><strong>cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf</strong> <strong>vim log4j.properties</strong>  <strong>修改以下内容:</strong> <img src="C:\Users\admin\Desktop\Diary\media\1f0510356b798e923b4e912dd33fc9fc.png"></th>
</tr>
</thead>
</table>
<p>同步到其他节点</p>
<table>
<thead>
<tr>
<th><strong>cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf</strong> <strong>scp -r spark-defaults.conf log4j.properties node2:$PWD</strong> <strong>scp -r spark-defaults.conf log4j.properties node3:$PWD</strong></th>
</tr>
</thead>
</table>
<h3 id="配置依赖spark-jar包"><a href="#配置依赖spark-jar包" class="headerlink" title="配置依赖spark jar包"></a>配置依赖spark jar包</h3><p>**当Spark Application应用提交运行在YARN上时，默认情况下，每次提交应用都需要将依赖Spark相关jar包上传到YARN 集群中，为了节省提交时间和存储空间，将Spark相关jar包上传到HDFS目录中，**设置属性告知Spark Application应用。</p>
<table>
<thead>
<tr>
<th><strong>hadoop fs -mkdir -p &#x2F;spark&#x2F;jars&#x2F;</strong> <strong>hadoop fs -put &#x2F;export&#x2F;server&#x2F;spark&#x2F;jars&#x2F;* &#x2F;spark&#x2F;jars&#x2F;</strong></th>
</tr>
</thead>
</table>
<p>修改spark-defaults.conf</p>
<table>
<thead>
<tr>
<th><strong>cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf</strong> <strong>vim spark-defaults.conf</strong> <strong>添加以下内容:</strong> <strong>spark.yarn.jars hdfs:&#x2F;&#x2F;node1.itcast.cn:8020&#x2F;spark&#x2F;jars&#x2F;*</strong></th>
</tr>
</thead>
</table>
<p>同步到其他节点</p>
<table>
<thead>
<tr>
<th><strong>cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf</strong> <strong>scp -r spark-defaults.conf root@node2:$PWD</strong> <strong>scp -r spark-defaults.conf root@node3:$PWD</strong></th>
</tr>
</thead>
</table>
<h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><p>Spark Application运行在YARN上时，上述配置完成</p>
<p>启动服务：HDFS、YARN、MRHistoryServer和Spark HistoryServer，命令如下：</p>
<table>
<thead>
<tr>
<th>## 启动HDFS和YARN服务，在node1执行命令 <strong>start-dfs.sh</strong> <strong>start-yarn.sh</strong> <strong>或</strong> <strong>start-all.sh</strong> <strong>注意：在onyarn模式下不需要启动start-all.sh（jps查看一下看不到worker和master）</strong> ## 启动MRHistoryServer服务，在node1执行命令 <strong>mapred –daemon start</strong>  ## 启动Spark HistoryServer服务，，在node1执行命令 <strong>&#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-history-server.sh</strong></th>
</tr>
</thead>
</table>
<ul>
<li>Spark HistoryServer服务WEB UI页面地址：</li>
</ul>
<table>
<thead>
<tr>
<th><a target="_blank" rel="noopener" href="http://node1:18080/">http://node1:18080/</a></th>
</tr>
</thead>
</table>
<h3 id="提交测试"><a href="#提交测试" class="headerlink" title="提交测试"></a>提交测试</h3><p>先将圆周率PI程序提交运行在YARN上，命令如下：</p>
<table>
<thead>
<tr>
<th>&#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit \ –master yarn \ –conf “spark.pyspark.driver.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3” \ –conf “spark.pyspark.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3” \ &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py \ 10</th>
</tr>
</thead>
</table>
<p>运行完成在YARN 监控页面截图如下：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/00370cbbd23f1f2e2653618b5fabd06c.png"></p>
<p>设置资源信息，提交运行pi程序至YARN上，命令如下：</p>
<table>
<thead>
<tr>
<th><strong>&#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit \</strong> <strong>–master yarn \</strong> <strong>–driver-memory 512m \</strong> <strong>–executor-memory 512m \</strong> <strong>–executor-cores 1 \</strong> <strong>–num-executors 2 \</strong> <strong>–queue default \</strong> <strong>–conf “spark.pyspark.driver.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3” \</strong> <strong>–conf “spark.pyspark.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3” \</strong> <strong>&#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py \</strong> <strong>10</strong></th>
</tr>
</thead>
</table>
<p>当pi应用运行YARN上完成以后，从8080 WEB 页面点击应用历史服务连接，查看应用运行状态信息。</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/d6f61e81d5bc796eddb106e775ae2632.png"></p>
<h2 id="第七章-SparkSQL与Hive整合"><a href="#第七章-SparkSQL与Hive整合" class="headerlink" title="第七章 SparkSQL与Hive整合"></a>第七章 SparkSQL与Hive整合</h2><h3 id="SparkSQL整合Hive步骤"><a href="#SparkSQL整合Hive步骤" class="headerlink" title="SparkSQL整合Hive步骤"></a>SparkSQL整合Hive步骤</h3><h4 id="第一步：将hive-site-xml拷贝到spark安装路径conf目录"><a href="#第一步：将hive-site-xml拷贝到spark安装路径conf目录" class="headerlink" title="第一步：将hive-site.xml拷贝到spark安装路径conf目录"></a>第一步：将hive-site.xml拷贝到spark安装路径conf目录</h4><p>node1执行以下命令来拷贝hive-site.xml到所有的spark安装服务器上面去</p>
<table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F;hive&#x2F;conf cp hive-site.xml &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F; scp hive-site.xml <a href="mailto:&#x72;&#x6f;&#x6f;&#x74;&#64;&#x6e;&#x6f;&#100;&#x65;&#x32;&#46;&#105;&#116;&#99;&#97;&#115;&#x74;&#x2e;&#99;&#110;&#58;&#x2f;&#101;&#120;&#x70;&#111;&#x72;&#116;&#x2f;&#115;&#x65;&#114;&#118;&#101;&#x72;&#x2f;&#x73;&#x70;&#97;&#x72;&#x6b;&#x2f;&#99;&#111;&#x6e;&#x66;&#47;">root@node2:&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F;</a> scp hive-site.xml <a href="mailto:&#x72;&#111;&#x6f;&#x74;&#64;&#x6e;&#x6f;&#x64;&#101;&#x33;&#x2e;&#105;&#116;&#x63;&#x61;&#x73;&#116;&#x2e;&#x63;&#110;&#58;&#x2f;&#x65;&#120;&#x70;&#111;&#114;&#116;&#x2f;&#115;&#x65;&#114;&#118;&#x65;&#x72;&#x2f;&#115;&#x70;&#x61;&#x72;&#107;&#x2f;&#x63;&#111;&#x6e;&#x66;&#47;">root@node3:&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F;</a></th>
</tr>
</thead>
</table>
<h4 id="第二步：将mysql的连接驱动包拷贝到spark的jars目录下"><a href="#第二步：将mysql的连接驱动包拷贝到spark的jars目录下" class="headerlink" title="第二步：将mysql的连接驱动包拷贝到spark的jars目录下"></a>第二步：将mysql的连接驱动包拷贝到spark的jars目录下</h4><p>node1执行以下命令将连接驱动包拷贝到spark的jars目录下，三台机器都要进行拷贝</p>
<table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F;hive&#x2F;lib cp mysql-connector-java-5.1.32.jar &#x2F;export&#x2F;server&#x2F;spark&#x2F;jars&#x2F; scp mysql-connector-java-5.1.32.jar root@node2:&#x2F;export&#x2F;server&#x2F;spark&#x2F;jars&#x2F; scp mysql-connector-java-5.1.32.jar root@node3:&#x2F;export&#x2F;server&#x2F;spark&#x2F;jars&#x2F;</th>
</tr>
</thead>
</table>
<h4 id="第三步：Hive开启MetaStore服务"><a href="#第三步：Hive开启MetaStore服务" class="headerlink" title="第三步：Hive开启MetaStore服务"></a>第三步：Hive开启MetaStore服务</h4><p>(1)修改 hive&#x2F;conf&#x2F;hive-site.xml新增如下配置</p>
<table>
<thead>
<tr>
<th><em>远程模式部署metastore 服务地址</em> &lt;?xml version&#x3D;”1.0”?&gt; &lt;?xml-stylesheet type&#x3D;”text&#x2F;xsl” href&#x3D;”configuration.xsl”?&gt; &lt;configuration&gt;  &lt;property&gt;  &lt;name&gt;hive.metastore.uris&lt;&#x2F;name&gt;  &lt;value&gt;thrift:&#x2F;&#x2F;node1.itcast.cn:9083&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt; &lt;&#x2F;configuration&gt;</th>
</tr>
</thead>
</table>
<p>2: 后台启动 Hive MetaStore服务</p>
<p>前台启动：</p>
<table>
<thead>
<tr>
<th>bin&#x2F;hive –service metastore</th>
</tr>
</thead>
</table>
<p>后台启动：</p>
<table>
<thead>
<tr>
<th><strong>nohup &#x2F;export&#x2F;server&#x2F;hive&#x2F;bin&#x2F;hive –service metastore 2&gt;&amp;1 &gt;&gt; &#x2F;var&#x2F;log.log &amp;</strong></th>
</tr>
</thead>
</table>
<p>完整的hive-site.xml文件</p>
<table>
<thead>
<tr>
<th>&lt;configuration&gt;  &lt;!– 存储元数据mysql相关配置 –&gt;  &lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;  &lt;value&gt; jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;hive?createDatabaseIfNotExist&#x3D;true&amp;useSSL&#x3D;false&amp;useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt;  &lt;value&gt;com.mysql.jdbc.Driver&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt;  &lt;value&gt;root&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt;  &lt;value&gt;123456&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;  &lt;!– H2S运行绑定host –&gt;  &lt;property&gt;  &lt;name&gt;hive.server2.thrift.bind.host&lt;&#x2F;name&gt;  &lt;value&gt;node1&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;!– 远程模式部署metastore 服务地址 –&gt;  &lt;property&gt;  &lt;name&gt;hive.metastore.uris&lt;&#x2F;name&gt;  &lt;value&gt;thrift:&#x2F;&#x2F;node1:9083&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;!– 关闭元数据存储授权 –&gt;  &lt;property&gt;  &lt;name&gt;hive.metastore.event.db.notification.api.auth&lt;&#x2F;name&gt;  &lt;value&gt;false&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;   &lt;!– 关闭元数据存储版本的验证 –&gt;  &lt;property&gt;  &lt;name&gt;hive.metastore.schema.verification&lt;&#x2F;name&gt;  &lt;value&gt;false&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt; &lt;&#x2F;configuration&gt;</th>
</tr>
</thead>
</table>
<h4 id="第四步：测试Sparksql整合Hive是否成功"><a href="#第四步：测试Sparksql整合Hive是否成功" class="headerlink" title="第四步：测试Sparksql整合Hive是否成功"></a>第四步：测试Sparksql整合Hive是否成功</h4><ul>
<li>[方式1]Spark-sql方式测试</li>
</ul>
<p>先启动hadoop集群，在启动spark集群，确保启动成功之后node1执行命令，指明master地址、每一个executor的内存大小、一共所需要的核数、mysql数据库连接驱动：</p>
<table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F;spark bin&#x2F;spark-sql –master local[2] –executor-memory 512m –total-executor-cores 1 或 bin&#x2F;spark-sql –master spark:&#x2F;&#x2F;node1.itcast.cn:7077 –executor-memory 512m –total-executor-cores 1</th>
</tr>
</thead>
</table>
<p>执行成功后的界面：进入到spark-sql 客户端命令行界面</p>
<p>查看当前有哪些数据库， 并创建数据库</p>
<table>
<thead>
<tr>
<th>show databases; create database sparkhive;</th>
</tr>
</thead>
</table>
<p>看到数据的结果，说明sparksql整合hive成功！</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/86fbd39fac2fb94acf4ac64ee148435d.png"></p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/5aee880483374804295c1fe9751366f2.png"></p>
<p>注意：日志太多，我们可以修改spark的日志输出级别(conf&#x2F;log4j.properties)</p>
<p>注意：</p>
<p>在spark2.0版本后由于出现了sparkSession，在初始化sqlContext的时候，会设置默认的spark.sql.warehouse.dir&#x3D;spark-warehouse,</p>
<p>此时将hive与sparksql整合完成之后，在通过spark-sql脚本启动的时候，还是会在那里启动spark-sql脚本，就会在当前目录下创建一个spark.sql.warehouse.dir为spark-warehouse的目录，存放由spark-sql创建数据库和创建表的数据信息，与之前hive的数据息不是放在同一个路径下（可以互相访问）。但是此时spark-sql中表的数据在本地，不利于操作，也不安全。</p>
<p>所有在启动的时候需要加上这样一个参数：</p>
<p>--conf spark.sql.warehouse.dir&#x3D;hdfs:&#x2F;&#x2F;node1:9820&#x2F;user&#x2F;hive&#x2F;warehouse</p>
<p>保证spark-sql启动时不在产生新的存放数据的目录，sparksql与hive最终使用的是hive同一存放数据的目录。如果使用的是spark2.0之前的版本，由于没有sparkSession，不会出现spark.sql.warehouse.dir配置项，不会出现上述问题。</p>
<p>**Spark2之后最后的执行脚本，**node1执行以下命令重新进去spark-sql</p>
<table>
<thead>
<tr>
<th>cd &#x2F;export&#x2F;server&#x2F;spark bin&#x2F;spark-sql \  –master spark:&#x2F;&#x2F;node1:7077 \  –executor-memory 512m –total-executor-cores 1 \  –conf spark.sql.warehouse.dir&#x3D;hdfs:&#x2F;&#x2F;node1:9820&#x2F;user&#x2F;hive&#x2F;warehouse</th>
</tr>
</thead>
</table>
<ul>
<li>[方式2]PySpark-Shell方式启动：</li>
</ul>
<table>
<thead>
<tr>
<th>bin&#x2F;spark-shell –master local[3] spark.sql(“show databases”).show</th>
</tr>
</thead>
</table>
<p>如下图：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/d74e82ddc0a6925a825bd2caa87adbe4.png"></p>
<ul>
<li>[方式3]PySpark-Shell方式启动：</li>
</ul>
<table>
<thead>
<tr>
<th>bin&#x2F;pyspark –master local[2] spark.sql(“show databases”).show</th>
</tr>
</thead>
</table>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/48333fd08651d371435d23af5b51a475.png"></p>
<h3 id="PyCharm整合Hive"><a href="#PyCharm整合Hive" class="headerlink" title="PyCharm整合Hive"></a>PyCharm整合Hive</h3><h4 id="操作准备"><a href="#操作准备" class="headerlink" title="操作准备"></a>操作准备</h4><p>●原理</p>
<p>Hive表的<strong>元数据库</strong>中，描述了有哪些database、table、以及表有多少列，每一列是什么类型，以及表的数据保存在hdfs的什么位置</p>
<p>执行HQL时，先到MySQL元数据库中查找描述信息，然后解析HQL并根据描述信息生成MR任务，简单来说Hive就是将SQL根据MySQL中元数据信息转成MapReduce执行，但是速度慢</p>
<p>使用SparkSQL整合Hive其实就是<strong>让SparkSQL去加载Hive 的元数据库，然后通过SparkSQL执行引擎去操作Hive表</strong></p>
<p>所以首先需要开启Hive的元数据库服务，让SparkSQL能够加载元数据</p>
<p>●API</p>
<p>在Spark2.0之后，SparkSession对HiveContext和SqlContext在进行了统一</p>
<p>可以通过操作SparkSession来操作HiveContext和SqlContext。</p>
<h4 id="SparkSQL整合Hive-MetaStore"><a href="#SparkSQL整合Hive-MetaStore" class="headerlink" title="SparkSQL整合Hive MetaStore"></a>SparkSQL整合Hive MetaStore</h4><p>默认Spark 有一个内置的 MateStore，使用 Derby 嵌入式数据库保存数据【上面案例】，但是这种方式不适合生产环境，因为这种模式同一时间只能有一个 SparkSession 使用，所以生产环境更推荐<strong>使用 Hive 的 MetaStore</strong></p>
<p>SparkSQL 整合 Hive 的 MetaStore 主要思路就是要通过配置能够访问它，并且能够使用 HDFS保存WareHouse，所以可以直接拷贝Hadoop和Hive的配置文件到Spark的配置目录。</p>
<h4 id="使用SparkSQL操作集群Hive表"><a href="#使用SparkSQL操作集群Hive表" class="headerlink" title="使用SparkSQL操作集群Hive表"></a>使用SparkSQL操作集群Hive表</h4><p>如下为HiveContext的源码解析：</p>
<p><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/img/b642f5674d407cf3e982f7d772857070.png"></p>
<p>在PyCharm中开发应用，集成Hive读取表的数据进行分析，构建SparkSession时需要设置HiveMetaStore服务器地址及集成Hive选项：</p>
<p>范例演示代码如下：</p>
<table>
<thead>
<tr>
<th><strong># -*- coding: utf-8 -*- # Program function： from</strong> pyspark.sql <strong>import</strong> SparkSession <strong>import</strong> os  os.environ[<strong>‘SPARK_HOME’</strong>] &#x3D; <strong>‘&#x2F;export&#x2F;servers&#x2F;spark’ <strong>PYSPARK_PYTHON &#x3D; <strong>“&#x2F;root&#x2F;anaconda3&#x2F;envs&#x2F;pyspark_env&#x2F;bin&#x2F;python” # 当存在多个版本时，不指定很可能会导致出错 <strong>os.environ[</strong>“PYSPARK_PYTHON”</strong>] &#x3D; PYSPARK_PYTHON os.environ[</strong>“PYSPARK_DRIVER_PYTHON”</strong>] &#x3D; PYSPARK_PYTHON <strong>if</strong> _<em>name</em>_ &#x3D;&#x3D; <strong>‘<strong>main</strong>‘</strong>:  <strong># _SPARK_HOST &#x3D; “spark:&#x2F;&#x2F;node1:7077” ** _SPARK_HOST &#x3D; <strong>“local[3]” ** _APP_NAME &#x3D; <strong>“test” ** spark &#x3D; SparkSession.builder \  .master(_SPARK_HOST) \  .appName(_APP_NAME) \  .enableHiveSupport() \  .getOrCreate()  spark.sparkContext.setLogLevel(</strong>“WARN”</strong>)  <strong>#PROJECT_ROOT &#x3D; os.path.dirname(os.path.realpath(<strong>file</strong>)) # 获取项目根目录  # print(PROJECT_ROOT)#&#x2F;export&#x2F;pyfolder1&#x2F;pyspark-chapter03_3.8&#x2F;main  #path &#x3D; os.path.join(PROJECT_ROOT, “data\\edge\\0_fuse.txt”) # 文件路径  # 查看有哪些表 ** spark.sql(</strong>“show databases”</strong>).show()  spark.sql(<strong>“use sparkhive”</strong>).show()  spark.sql(<strong>“show tables”</strong>).show()  <strong># 创建表 ** spark.sql(  <strong>“create table if not exists person (id int, name string, age int) row format delimited fields terminated by ‘,’”</strong>)  <strong># 加载数据, 数据为当前目录下的person.txt(和src平级) ** spark.sql(</strong>“LOAD DATA LOCAL INPATH ‘&#x2F;export&#x2F;pyfolder1&#x2F;pyspark-chapter03_3.8&#x2F;data&#x2F;student.csv’ INTO TABLE person”</strong>)  <strong># 查询数据 ** spark.sql(</strong>“select** <em>*</em> <strong>from person “</strong>).show()  print(<strong>“&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;”</strong>)  <strong>import</strong> pyspark.sql.functions <strong>as</strong> fn  spark.read \  .table(<strong>“person “</strong>) \  .groupBy(<strong>“name”</strong>) \  .agg(fn.round(fn.avg(<strong>“age”</strong>), 2).alias(<strong>“avg_age”</strong>)) \  .show(10, truncate&#x3D;<strong>False</strong>)  spark.stop()</th>
</tr>
</thead>
</table>
<p>运行程序结果如下：</p>
<p><img src="C:\Users\admin\Desktop\Diary\media\2a1fa5b5b69e5140cdaa23e783fa731f.png"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://liamjohnson-w.github.io">Luck威</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://liamjohnson-w.github.io/2024/04/28/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">https://liamjohnson-w.github.io/2024/04/28/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">环境搭建</a></div><div class="post_share"><div class="social-share" data-image="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/tx.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> Donate</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/05/01/2024.05.01/" title="Document"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Document</div></div></a></div><div class="next-post pull-right"><a href="/2024/04/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA(CDH%E7%89%88)/" title="大数据集群环境搭建(CDH版)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">大数据集群环境搭建(CDH版)</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/07/26/2024.07.26/" title="数据分析环境搭建"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-26</div><div class="title">数据分析环境搭建</div></div></a></div><div><a href="/2024/04/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA(CDH%E7%89%88)/" title="大数据集群环境搭建(CDH版)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-27</div><div class="title">大数据集群环境搭建(CDH版)</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/tx.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Luck威</div><div class="author-info__description">机器都在学习,你有什么理由不学习?</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">230</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">58</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/weiswift/"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/weiswift" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:1265019024@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">网站由Github服务器托管,感谢支持！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81VMware%E7%9A%84%E5%AE%89%E8%A3%85"><span class="toc-number">1.</span> <span class="toc-text">一、VMware的安装</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AE%89%E8%A3%85%E6%93%8D%E4%BD%9C"><span class="toc-number">2.</span> <span class="toc-text">二、虚拟机安装操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA"><span class="toc-number">2.1.</span> <span class="toc-text">创建虚拟机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0linux%E7%9A%84iso%E9%95%9C%E5%83%8F%E6%96%87%E4%BB%B6"><span class="toc-number">2.2.</span> <span class="toc-text">添加linux的iso镜像文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%80%E5%90%AF%E8%99%9A%E6%8B%9F%E6%9C%BA-%E8%BF%9B%E8%A1%8C%E5%AE%89%E8%A3%85"><span class="toc-number">2.3.</span> <span class="toc-text">开启虚拟机, 进行安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%99%BB%E5%BD%95-%E8%BF%9B%E5%85%A5%E7%B3%BB%E7%BB%9F"><span class="toc-number">2.4.</span> <span class="toc-text">登录, 进入系统</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81SecureCRT%E4%BD%BF%E7%94%A8"><span class="toc-number">3.</span> <span class="toc-text">三、SecureCRT使用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CRT%E5%AE%89%E8%A3%85"><span class="toc-number">3.1.</span> <span class="toc-text">CRT安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB"><span class="toc-number">3.2.</span> <span class="toc-text">激活</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%9E%E6%8E%A5"><span class="toc-number">3.3.</span> <span class="toc-text">连接</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E8%AE%BE%E7%BD%AE"><span class="toc-number">3.4.</span> <span class="toc-text">常见设置</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%8D%B8%E8%BD%BD"><span class="toc-number">4.</span> <span class="toc-text">四、虚拟机卸载</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85cclear-%E5%88%87%E6%8D%A2%E8%AF%AD%E8%A8%80"><span class="toc-number">4.1.</span> <span class="toc-text">安装cclear:切换语言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E9%97%ADVMware%E6%89%80%E6%9C%89%E7%9A%84%E6%9C%8D%E5%8A%A1"><span class="toc-number">4.2.</span> <span class="toc-text">关闭VMware所有的服务</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%93%E5%BC%80%E6%9C%8D%E5%8A%A1-win-r-%E8%BE%93%E5%85%A5-services-msc"><span class="toc-number">4.2.1.</span> <span class="toc-text">打开服务:win+r : 输入 services.msc</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E9%97%AD%E6%89%80%E6%9C%89%E7%9A%84VMware%E7%9A%84%E6%9C%8D%E5%8A%A1"><span class="toc-number">4.2.2.</span> <span class="toc-text">关闭所有的VMware的服务</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%93%E5%BC%80%E5%8D%B8%E8%BD%BD%E7%A8%8B%E5%BA%8F-%E5%8D%B8%E8%BD%BDVMware"><span class="toc-number">4.3.</span> <span class="toc-text">打开卸载程序, 卸载VMware</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8cclear%E6%B8%85%E9%99%A4%E6%B3%A8%E5%86%8C%E8%A1%A8"><span class="toc-number">4.4.</span> <span class="toc-text">使用cclear清除注册表</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%93%E5%BC%80%E6%9C%8D%E5%8A%A1-services-msc"><span class="toc-number">4.5.</span> <span class="toc-text">打开服务: services.msc</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8BVMware%E6%9C%8D%E5%8A%A1%E6%9C%89%E6%B2%A1%E6%9C%89"><span class="toc-number">4.5.1.</span> <span class="toc-text">查看VMware服务有没有:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E6%9E%9C%E6%9C%89"><span class="toc-number">4.5.2.</span> <span class="toc-text">如果有:</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E6%9C%8D%E5%8A%A1%E7%9A%84%E5%90%8D%E7%A7%B0"><span class="toc-number">4.5.2.1.</span> <span class="toc-text">查看服务的名称:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%8D%E5%88%B6%E6%9C%8D%E5%8A%A1%E5%90%8D%E7%A7%B0-%E6%89%93%E5%BC%80cmd-%E7%AE%A1%E7%90%86%E5%91%98%E6%89%93%E5%BC%80"><span class="toc-number">4.5.2.2.</span> <span class="toc-text">复制服务名称:打开cmd(管理员打开)</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E7%BB%9F%E4%B8%80"><span class="toc-number">5.</span> <span class="toc-text">五、大数据环境配置统一</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%88%9B%E5%BB%BA"><span class="toc-number">5.1.</span> <span class="toc-text">三台虚拟机创建</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AE%E4%B8%89%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%86%85%E5%AD%98%E5%92%8CCPU%E6%A0%B8%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-number">5.2.</span> <span class="toc-text">设置三台虚拟机的内存和CPU核数设置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%85%8D%E7%BD%AEMAC%E5%9C%B0%E5%9D%80"><span class="toc-number">5.3.</span> <span class="toc-text">配置MAC地址</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%85%8D%E7%BD%AEIP%E5%9C%B0%E5%9D%80"><span class="toc-number">5.4.</span> <span class="toc-text">配置IP地址</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8CRT%E8%BF%9E%E6%8E%A5%E4%B8%89%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA"><span class="toc-number">5.5.</span> <span class="toc-text">使用CRT连接三台虚拟机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AE%E4%B8%BB%E6%9C%BA%E5%90%8D%E5%92%8C%E5%9F%9F%E5%90%8D%E6%98%A0%E5%B0%84"><span class="toc-number">5.6.</span> <span class="toc-text">设置主机名和域名映射</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E9%97%AD%E4%B8%89%E5%8F%B0%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E9%98%B2%E7%81%AB%E5%A2%99%E5%92%8CSelinux"><span class="toc-number">5.7.</span> <span class="toc-text">关闭三台虚拟机的防火墙和Selinux</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E5%8F%B0%E6%9C%BA%E5%99%A8%E6%9C%BA%E5%99%A8%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95"><span class="toc-number">5.8.</span> <span class="toc-text">三台机器机器免密码登录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E5%8F%B0%E6%9C%BA%E5%99%A8%E6%97%B6%E9%92%9F%E5%90%8C%E6%AD%A5"><span class="toc-number">5.9.</span> <span class="toc-text">三台机器时钟同步</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5-%E5%9C%A8node1%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AE%89%E8%A3%85ntp%E5%B9%B6%E5%90%AF%E5%8A%A8"><span class="toc-number">5.9.1.</span> <span class="toc-text">第一步:在node1虚拟机安装ntp并启动</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5-%E7%BC%96%E8%BE%91node1%E7%9A%84-etc-ntp-conf%E6%96%87%E4%BB%B6"><span class="toc-number">5.9.2.</span> <span class="toc-text">第二步:编辑node1的&#x2F;etc&#x2F;ntp.conf文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A%E5%8F%A6%E5%A4%96%E4%B8%A4%E5%8F%B0%E6%9C%BA%E5%99%A8%E4%B8%8E%E7%AC%AC%E4%B8%80%E5%8F%B0%E6%9C%BA%E5%99%A8%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5"><span class="toc-number">5.9.3.</span> <span class="toc-text">第三步：另外两台机器与第一台机器时间同步</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85"><span class="toc-number">5.10.</span> <span class="toc-text">虚拟机软件安装</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Linux%E4%B8%8A%E5%AE%89%E8%A3%85MySQL-node1"><span class="toc-number">5.10.1.</span> <span class="toc-text">Linux上安装MySQL(node1)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MySQL%E5%9C%A8%E7%BA%BF%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85"><span class="toc-number">5.10.1.1.</span> <span class="toc-text">MySQL在线下载安装</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8MySQL%E6%9C%8D%E5%8A%A1%E5%B9%B6%E7%99%BB%E9%99%86"><span class="toc-number">5.10.1.2.</span> <span class="toc-text">启动MySQL服务并登陆</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AE%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE%E6%9D%83%E9%99%90"><span class="toc-number">5.10.1.3.</span> <span class="toc-text">设置远程访问权限</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%A2%E6%88%B7%E7%AB%AFwindows%E8%BF%9E%E6%8E%A5linux%E4%B8%ADmysql"><span class="toc-number">5.10.1.4.</span> <span class="toc-text">客户端windows连接linux中mysql</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#JDK%E5%AE%89%E8%A3%85-%E6%89%80%E6%9C%89%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85"><span class="toc-number">5.10.2.</span> <span class="toc-text">JDK安装(所有节点安装)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#JDK%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4"><span class="toc-number">5.10.2.1.</span> <span class="toc-text">JDK安装步骤</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD%E3%80%81Zookeeper%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85"><span class="toc-number">6.</span> <span class="toc-text">六、Zookeeper集群安装</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E4%B8%8B%E8%BD%BDzookeeeper%E7%9A%84%E5%8E%8B%E7%BC%A9%E5%8C%85%EF%BC%8C%E4%B8%8B%E8%BD%BD%E7%BD%91%E5%9D%80%E5%A6%82%E4%B8%8B"><span class="toc-number">6.1.</span> <span class="toc-text">第一步：下载zookeeeper的压缩包，下载网址如下</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A%E8%A7%A3%E5%8E%8B"><span class="toc-number">6.2.</span> <span class="toc-text">第二步：解压</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">6.3.</span> <span class="toc-text">第三步：修改配置文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E6%AD%A5%EF%BC%9A%E6%B7%BB%E5%8A%A0myid%E9%85%8D%E7%BD%AE"><span class="toc-number">6.4.</span> <span class="toc-text">第四步：添加myid配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E6%AD%A5%EF%BC%9A%E5%AE%89%E8%A3%85%E5%8C%85%E5%88%86%E5%8F%91%E5%B9%B6%E4%BF%AE%E6%94%B9myid%E7%9A%84%E5%80%BC"><span class="toc-number">6.5.</span> <span class="toc-text">第五步：安装包分发并修改myid的值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%85%AD%E6%AD%A5%EF%BC%9A%E4%B8%89%E5%8F%B0%E6%9C%BA%E5%99%A8%E5%90%AF%E5%8A%A8zookeeper%E6%9C%8D%E5%8A%A1"><span class="toc-number">6.6.</span> <span class="toc-text">第六步：三台机器启动zookeeper服务</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%83%E3%80%81hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA"><span class="toc-number">7.</span> <span class="toc-text">七、hadoop集群搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8A%E4%BC%A0%E9%87%8D%E7%BC%96%E8%AF%91%E5%90%8E%E7%9A%84hadoop%E5%8C%85%E5%B9%B6%E8%A7%A3%E5%8E%8B"><span class="toc-number">7.1.</span> <span class="toc-text">上传重编译后的hadoop包并解压</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop%E5%AE%89%E8%A3%85%E5%8C%85%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84"><span class="toc-number">7.2.</span> <span class="toc-text">Hadoop安装包目录结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E4%BF%AE%E6%94%B9"><span class="toc-number">7.3.</span> <span class="toc-text">Hadoop配置文件修改</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#hadoop-env-sh"><span class="toc-number">7.3.1.</span> <span class="toc-text">hadoop-env.sh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#core-site-xml"><span class="toc-number">7.3.2.</span> <span class="toc-text">core-site.xml</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs-site-xml"><span class="toc-number">7.3.3.</span> <span class="toc-text">hdfs-site.xml</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mapred-site-xml"><span class="toc-number">7.3.4.</span> <span class="toc-text">mapred-site.xml</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#yarn-site-xml"><span class="toc-number">7.3.5.</span> <span class="toc-text">yarn-site.xml</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#workers"><span class="toc-number">7.3.6.</span> <span class="toc-text">workers</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#scp%E5%90%8C%E6%AD%A5%E5%AE%89%E8%A3%85%E5%8C%85"><span class="toc-number">7.4.</span> <span class="toc-text">scp同步安装包</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-number">7.5.</span> <span class="toc-text">Hadoop环境变量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop%E9%9B%86%E7%BE%A4%E5%90%AF%E5%8A%A8%E3%80%81%E5%88%9D%E4%BD%93%E9%AA%8C"><span class="toc-number">7.6.</span> <span class="toc-text">Hadoop集群启动、初体验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8%E6%96%B9%E5%BC%8F"><span class="toc-number">7.6.1.</span> <span class="toc-text">启动方式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%95%E8%8A%82%E7%82%B9%E9%80%90%E4%B8%AA%E5%90%AF%E5%8A%A8"><span class="toc-number">7.6.1.1.</span> <span class="toc-text">单节点逐个启动</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%84%9A%E6%9C%AC%E4%B8%80%E9%94%AE%E5%90%AF%E5%8A%A8"><span class="toc-number">7.6.1.2.</span> <span class="toc-text">脚本一键启动</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8%E5%90%8E%E7%9A%84%E6%95%88%E6%9E%9C"><span class="toc-number">7.7.</span> <span class="toc-text">启动后的效果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E7%BE%A4web-ui"><span class="toc-number">7.8.</span> <span class="toc-text">集群web-ui</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MapReduce-jobHistory"><span class="toc-number">7.9.</span> <span class="toc-text">MapReduce jobHistory</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9mapred-site-xml"><span class="toc-number">7.9.1.</span> <span class="toc-text">修改mapred-site.xml</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%8F%91%E9%85%8D%E7%BD%AE%E5%88%B0%E5%85%B6%E4%BB%96%E6%9C%BA%E5%99%A8"><span class="toc-number">7.9.2.</span> <span class="toc-text">分发配置到其他机器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8jobHistoryServer%E6%9C%8D%E5%8A%A1%E8%BF%9B%E7%A8%8B"><span class="toc-number">7.9.3.</span> <span class="toc-text">启动jobHistoryServer服务进程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%B5%E9%9D%A2%E8%AE%BF%E9%97%AEjobhistoryserver"><span class="toc-number">7.9.4.</span> <span class="toc-text">页面访问jobhistoryserver</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AB%E3%80%81Hive%E5%AE%89%E8%A3%85%E6%93%8D%E4%BD%9C"><span class="toc-number">8.</span> <span class="toc-text">八、Hive安装操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive-metastore%E8%BF%9C%E7%A8%8B%E6%A8%A1%E5%BC%8F%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2"><span class="toc-number">8.1.</span> <span class="toc-text">Hive metastore远程模式安装部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop%E4%B8%AD%E6%B7%BB%E5%8A%A0%E7%94%A8%E6%88%B7%E4%BB%A3%E7%90%86%E9%85%8D%E7%BD%AE"><span class="toc-number">8.1.1.</span> <span class="toc-text">Hadoop中添加用户代理配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8A%E4%BC%A0%E5%AE%89%E8%A3%85%E5%8C%85-%E5%B9%B6%E8%A7%A3%E5%8E%8B"><span class="toc-number">8.1.2.</span> <span class="toc-text">上传安装包 并解压</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6hive-env-sh"><span class="toc-number">8.1.3.</span> <span class="toc-text">修改配置文件hive-env.sh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6hive-site-xml"><span class="toc-number">8.1.4.</span> <span class="toc-text">添加配置文件hive-site.xml</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8A%E4%BC%A0MySQL%E9%A9%B1%E5%8A%A8"><span class="toc-number">8.1.5.</span> <span class="toc-text">上传MySQL驱动</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-number">8.1.6.</span> <span class="toc-text">初始化元数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAhive%E5%AD%98%E5%82%A8%E7%9B%AE%E5%BD%95"><span class="toc-number">8.1.7.</span> <span class="toc-text">创建hive存储目录</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#metastore-%E7%9A%84%E5%90%AF%E5%8A%A8%E6%96%B9%E5%BC%8F"><span class="toc-number">8.2.</span> <span class="toc-text">metastore 的启动方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive-Client%E3%80%81Beeline-Client"><span class="toc-number">8.3.</span> <span class="toc-text">Hive Client、Beeline Client</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E4%BB%A3%E5%AE%A2%E6%88%B7%E7%AB%AFHive-Client"><span class="toc-number">8.3.1.</span> <span class="toc-text">第一代客户端Hive Client</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E4%BB%A3%E5%AE%A2%E6%88%B7%E7%AB%AFHive-Beeline-Client"><span class="toc-number">8.3.2.</span> <span class="toc-text">第二代客户端Hive Beeline Client</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B9%9D%E3%80%81Spark%E9%83%A8%E7%BD%B2%E5%AE%89%E8%A3%85"><span class="toc-number">9.</span> <span class="toc-text">九、Spark部署安装</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Local-%E6%A8%A1%E5%BC%8F%E6%90%AD%E5%BB%BA%E6%96%87%E6%A1%A3"><span class="toc-number">9.1.</span> <span class="toc-text">Spark Local 模式搭建文档</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E5%8C%85%E4%B8%8B%E8%BD%BD"><span class="toc-number">9.1.1.</span> <span class="toc-text">安装包下载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%86%E5%AE%89%E8%A3%85%E5%8C%85%E4%B8%8A%E4%BC%A0%E5%B9%B6%E8%A7%A3%E5%8E%8B"><span class="toc-number">9.1.2.</span> <span class="toc-text">将安装包上传并解压</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95"><span class="toc-number">9.1.3.</span> <span class="toc-text">测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PySpark%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85"><span class="toc-number">9.2.</span> <span class="toc-text">PySpark环境安装</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BDAnaconda%E7%8E%AF%E5%A2%83%E5%8C%85"><span class="toc-number">9.2.1.</span> <span class="toc-text">下载Anaconda环境包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Anaconda%E7%8E%AF%E5%A2%83"><span class="toc-number">9.2.2.</span> <span class="toc-text">安装Anaconda环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8anaconda%E5%B9%B6%E6%B5%8B%E8%AF%95"><span class="toc-number">9.2.3.</span> <span class="toc-text">启动anaconda并测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Anaconda%E7%9B%B8%E5%85%B3%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D-%E4%BA%86%E8%A7%A3"><span class="toc-number">9.2.4.</span> <span class="toc-text">Anaconda相关组件介绍[了解]</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PySpark%E5%AE%89%E8%A3%85"><span class="toc-number">9.2.5.</span> <span class="toc-text">PySpark安装</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E5%BC%8F1%EF%BC%9A%E7%9B%B4%E6%8E%A5%E5%AE%89%E8%A3%85PySpark"><span class="toc-number">9.2.5.1.</span> <span class="toc-text">方式1：直接安装PySpark</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85-%E6%96%B9%E5%BC%8F2%EF%BC%9A%E5%88%9B%E5%BB%BAConda%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85PySpark"><span class="toc-number">9.2.5.2.</span> <span class="toc-text">[安装]方式2：创建Conda环境安装PySpark</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8D%E6%8E%A8%E8%8D%90-%E6%96%B9%E5%BC%8F3%EF%BC%9A%E6%89%8B%E5%8A%A8%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85"><span class="toc-number">9.2.5.3.</span> <span class="toc-text">[不推荐]方式3：手动下载安装</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E4%BD%93%E9%AA%8C-PySpark-shell%E6%96%B9%E5%BC%8F"><span class="toc-number">9.2.6.</span> <span class="toc-text">初体验-PySpark shell方式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Standalone%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83"><span class="toc-number">9.3.</span> <span class="toc-text">Spark Standalone集群环境</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">9.3.1.</span> <span class="toc-text">修改配置文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%8F%91%E5%88%B0%E5%85%B6%E4%BB%96%E6%9C%BA%E5%99%A8"><span class="toc-number">9.3.2.</span> <span class="toc-text">分发到其他机器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8spark-Standalone"><span class="toc-number">9.3.3.</span> <span class="toc-text">启动spark Standalone</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9E%E6%8E%A5%E9%9B%86%E7%BE%A4"><span class="toc-number">9.3.4.</span> <span class="toc-text">连接集群</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Standalone-HA-%E6%A8%A1%E5%BC%8F%E5%AE%89%E8%A3%85"><span class="toc-number">9.4.</span> <span class="toc-text">Spark Standalone HA 模式安装</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE"><span class="toc-number">9.4.1.</span> <span class="toc-text">修改配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E5%88%86%E5%8F%91"><span class="toc-number">9.4.2.</span> <span class="toc-text">配置分发</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4"><span class="toc-number">9.4.3.</span> <span class="toc-text">启动集群</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-on-YARN-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="toc-number">9.5.</span> <span class="toc-text">Spark on YARN 环境搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9spark-env-sh"><span class="toc-number">9.5.1.</span> <span class="toc-text">修改spark-env.sh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9hadoop%E7%9A%84yarn-site-xml"><span class="toc-number">9.5.2.</span> <span class="toc-text">修改hadoop的yarn-site.xml</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark%E8%AE%BE%E7%BD%AE%E5%8E%86%E5%8F%B2%E6%9C%8D%E5%8A%A1%E5%9C%B0%E5%9D%80"><span class="toc-number">9.5.3.</span> <span class="toc-text">Spark设置历史服务地址</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E4%BE%9D%E8%B5%96spark-jar%E5%8C%85"><span class="toc-number">9.5.4.</span> <span class="toc-text">配置依赖spark jar包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8%E6%9C%8D%E5%8A%A1"><span class="toc-number">9.5.5.</span> <span class="toc-text">启动服务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E4%BA%A4%E6%B5%8B%E8%AF%95"><span class="toc-number">9.5.6.</span> <span class="toc-text">提交测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%83%E7%AB%A0-SparkSQL%E4%B8%8EHive%E6%95%B4%E5%90%88"><span class="toc-number">9.6.</span> <span class="toc-text">第七章 SparkSQL与Hive整合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkSQL%E6%95%B4%E5%90%88Hive%E6%AD%A5%E9%AA%A4"><span class="toc-number">9.6.1.</span> <span class="toc-text">SparkSQL整合Hive步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E5%B0%86hive-site-xml%E6%8B%B7%E8%B4%9D%E5%88%B0spark%E5%AE%89%E8%A3%85%E8%B7%AF%E5%BE%84conf%E7%9B%AE%E5%BD%95"><span class="toc-number">9.6.1.1.</span> <span class="toc-text">第一步：将hive-site.xml拷贝到spark安装路径conf目录</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A%E5%B0%86mysql%E7%9A%84%E8%BF%9E%E6%8E%A5%E9%A9%B1%E5%8A%A8%E5%8C%85%E6%8B%B7%E8%B4%9D%E5%88%B0spark%E7%9A%84jars%E7%9B%AE%E5%BD%95%E4%B8%8B"><span class="toc-number">9.6.1.2.</span> <span class="toc-text">第二步：将mysql的连接驱动包拷贝到spark的jars目录下</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9AHive%E5%BC%80%E5%90%AFMetaStore%E6%9C%8D%E5%8A%A1"><span class="toc-number">9.6.1.3.</span> <span class="toc-text">第三步：Hive开启MetaStore服务</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E6%AD%A5%EF%BC%9A%E6%B5%8B%E8%AF%95Sparksql%E6%95%B4%E5%90%88Hive%E6%98%AF%E5%90%A6%E6%88%90%E5%8A%9F"><span class="toc-number">9.6.1.4.</span> <span class="toc-text">第四步：测试Sparksql整合Hive是否成功</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PyCharm%E6%95%B4%E5%90%88Hive"><span class="toc-number">9.6.2.</span> <span class="toc-text">PyCharm整合Hive</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%93%8D%E4%BD%9C%E5%87%86%E5%A4%87"><span class="toc-number">9.6.2.1.</span> <span class="toc-text">操作准备</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SparkSQL%E6%95%B4%E5%90%88Hive-MetaStore"><span class="toc-number">9.6.2.2.</span> <span class="toc-text">SparkSQL整合Hive MetaStore</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8SparkSQL%E6%93%8D%E4%BD%9C%E9%9B%86%E7%BE%A4Hive%E8%A1%A8"><span class="toc-number">9.6.2.3.</span> <span class="toc-text">使用SparkSQL操作集群Hive表</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/07/18/Project/" title="AI PROJECT"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AI PROJECT"/></a><div class="content"><a class="title" href="/2025/07/18/Project/" title="AI PROJECT">AI PROJECT</a><time datetime="2025-07-17T16:00:00.000Z" title="Created 2025-07-18 00:00:00">2025-07-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/" title="机器学习公式推导"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/pusheen2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习公式推导"/></a><div class="content"><a class="title" href="/2025/07/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/" title="机器学习公式推导">机器学习公式推导</a><time datetime="2025-07-15T16:00:00.000Z" title="Created 2025-07-16 00:00:00">2025-07-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/29/InterviewQuestions/" title="面试题"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/c1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="面试题"/></a><div class="content"><a class="title" href="/2025/06/29/InterviewQuestions/" title="面试题">面试题</a><time datetime="2025-06-28T16:00:00.000Z" title="Created 2025-06-29 00:00:00">2025-06-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/22/DK67/" title="DK67双模切换"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/%E7%94%9F%E6%88%90%E7%8C%AB%E5%92%AA%E5%9B%BE%E7%89%87.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="DK67双模切换"/></a><div class="content"><a class="title" href="/2025/06/22/DK67/" title="DK67双模切换">DK67双模切换</a><time datetime="2025-06-21T16:00:00.000Z" title="Created 2025-06-22 00:00:00">2025-06-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/22/NLP_Base/" title="NLP自然语言处理"><img src="https://wei-blog.oss-cn-beijing.aliyuncs.com/24-07/%E7%94%9F%E6%88%90%E7%8C%AB%E5%92%AA%E5%9B%BE%E7%89%87%20(2).png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="NLP自然语言处理"/></a><div class="content"><a class="title" href="/2025/06/22/NLP_Base/" title="NLP自然语言处理">NLP自然语言处理</a><time datetime="2025-06-21T16:00:00.000Z" title="Created 2025-06-22 00:00:00">2025-06-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Luck威</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Welcome to 小威の <a target="_blank" rel="noopener" href="https://www.cnblogs.com/liam-sliversucks/">Blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? '' : ''

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>